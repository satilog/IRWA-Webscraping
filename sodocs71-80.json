[
    {
        "Q_Title": "Plotting with seaborn using the matplotlib object-oriented interface",
        "A_Content": "  It depends a bit on which seaborn function you are using.  The plotting functions in seaborn are broadly divided into two classes    \"Axes-level\" functions, including regplot, boxplot, kdeplot, and many others \"Figure-level\" functions, including lmplot, factorplot, jointplot and one or two others   The first group is identified by taking an explicit ax argument and returning an Axes object. As this suggests, you can use them in an \"object oriented\" style by passing your Axes to them:  f, (ax1, ax2) = plt.subplots(2) sns.regplot(x, y, ax=ax1) sns.kdeplot(x, ax=ax2)   Axes-level functions will only draw onto an Axes and won't otherwise mess with the figure, so they can coexist perfectly happily in an object-oriented matplotlib script.  The second group of functions (Figure-level) are distinguished by the fact that the resulting plot can potentially include several Axes which are always organized in a \"meaningful\" way. That means that the functions need to have total control over the figure, so it isn't possible to plot, say, an lmplot onto one that already exists. Calling the function always initializes a figure and sets it up for the specific plot it's drawing.  However, once you've called lmplot, it will return an object of the type FacetGrid. This object has some methods for operating on the resulting plot that know a bit about the structure of the plot. It also exposes the underlying figure and array of axes at the FacetGrid.fig and FacetGrid.axes arguments. The jointplot function is very similar, but it uses a JointGrid object. So you can still use these functions in an object-oriented context, but all of your customization has to come after you've called the function.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "oop",
            "matplotlib",
            "seaborn"
        ],
        "URL": "https://stackoverflow.com/questions/23969619/plotting-with-seaborn-using-the-matplotlib-object-oriented-interface",
        "A_Votes": "158",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I strongly prefer using matplotlib in OOP style:  f, axarr = plt.subplots(2, sharex=True) axarr[0].plot(...) axarr[1].plot(...)   This makes it easier to keep track of multiple figures and subplots.  Question: How to use seaborn this way? Or, how to change this example to OOP style? How to tell seaborn plotting functions like lmplot which Figure or Axes it plots to?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Check if file is symlink in python",
        "A_Content": "  To determine if a directory entry is a symlink use this:     os.path.islink(path)      Return True if path refers to a directory entry that is a symbolic   link. Always False if symbolic links are not supported.   For instance, given:  drwxr-xr-x   2 root root  4096 2011-11-10 08:14 bin/ drwxrwxrwx   1 root root    57 2011-07-10 05:11 initrd.img -> boot/initrd.img-2..  >>> import os.path >>> os.path.islink('initrd.img') True >>> os.path.islink('bin') False      ",
        "Language": "Python",
        "Tags": [
            "python",
            "operating-system"
        ],
        "URL": "https://stackoverflow.com/questions/11068419/check-if-file-is-symlink-in-python",
        "A_Votes": "106",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In python, is there a function to check if a given file/directory is a symlink ? For example, for the below files, my wrapper function should return True.  # ls -l total 0 lrwxrwxrwx 1 root root 8 2012-06-16 18:58 dir -> ../temp/ lrwxrwxrwx 1 root root 6 2012-06-16 18:55 link -> ../log      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Check if file is symlink in python",
        "A_Content": "  For python 3.4 and up, you can use the Path class  from pathlib import Path   # rpd is a symbolic link >>> Path('rdp').is_symlink() True >>> Path('README').is_symlink() False   You have to be careful when using the is_symlink() method.  It will return True even the target of the link is non-existent as long as the the named object is a symlink.  For example (Linux/Unix):  ln -s ../nonexistentfile flnk   Then, in your current directory fire up python  >>> from pathlib import Path >>> Path('flnk').is_symlink() True >>> Path('flnk').exists() False   The programmer has to decide what he/she realy wants.  Python 3 seems to have renamed a lots of classes. It might be worthwhile to read the manual page for the Path class: https://docs.python.org/3/library/pathlib.html     ",
        "Language": "Python",
        "Tags": [
            "python",
            "operating-system"
        ],
        "URL": "https://stackoverflow.com/questions/11068419/check-if-file-is-symlink-in-python",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, is there a function to check if a given file/directory is a symlink ? For example, for the below files, my wrapper function should return True.  # ls -l total 0 lrwxrwxrwx 1 root root 8 2012-06-16 18:58 dir -> ../temp/ lrwxrwxrwx 1 root root 6 2012-06-16 18:55 link -> ../log      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Check if file is symlink in python",
        "A_Content": "  Without the intention to bloat this topic, but I was redirected to this page as I was looking for symlink's to find them and convert them to real files and found this script within the python tools library.  #Source https://github.com/python/cpython/blob/master/Tools/scripts/mkreal.py   import sys import os from stat import *  BUFSIZE = 32*1024  def mkrealfile(name):     st = os.stat(name) # Get the mode     mode = S_IMODE(st[ST_MODE])     linkto = os.readlink(name) # Make sure again it's a symlink     f_in = open(name, 'r') # This ensures it's a file     os.unlink(name)     f_out = open(name, 'w')     while 1:         buf = f_in.read(BUFSIZE)         if not buf: break         f_out.write(buf)     del f_out # Flush data to disk before changing mode     os.chmod(name, mode)      mkrealfile(\"/Users/test/mysymlink\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "operating-system"
        ],
        "URL": "https://stackoverflow.com/questions/11068419/check-if-file-is-symlink-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, is there a function to check if a given file/directory is a symlink ? For example, for the below files, my wrapper function should return True.  # ls -l total 0 lrwxrwxrwx 1 root root 8 2012-06-16 18:58 dir -> ../temp/ lrwxrwxrwx 1 root root 6 2012-06-16 18:55 link -> ../log      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Converting a float to a string without rounding it",
        "A_Content": "  Some form of rounding is often unavoidable when dealing with floating point numbers.  This is because numbers that you can express exactly in base 10 cannot always be expressed exactly in base 2 (which your computer uses).  For example:  >>> .1 0.10000000000000001   In this case, you're seeing .1 converted to a string using repr:  >>> repr(.1) '0.10000000000000001'   I believe python chops off the last few digits when you use str() in order to work around this problem, but it's a partial workaround that doesn't substitute for understanding what's going on.  >>> str(.1) '0.1'   I'm not sure exactly what problems \"rounding\" is causing you.  Perhaps you would do better with string formatting as a way to more precisely control your output?  e.g.  >>> '%.5f' % .1 '0.10000' >>> '%.5f' % .12345678 '0.12346'   Documentation here.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "floating-point",
            "rounding"
        ],
        "URL": "https://stackoverflow.com/questions/1317558/converting-a-float-to-a-string-without-rounding-it",
        "A_Votes": "102",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm making a program that, for reasons not needed to be explained, requires a float to be converted into a string to be counted with len(). However, str(float(x)) results in x being rounded when converted to a string, which throws the entire thing off. Does anyone know of a fix for it? Here's the code being used if you want to know:  len(str(float(x)/3))      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Converting a float to a string without rounding it",
        "A_Content": "  len(repr(float(x)/3))   However I must say that this isn't as reliable as you think.  Floats are entered/displayed as decimal numbers, but your computer (in fact, your standard C library) stores them as binary. You get some side effects from this transition:  >>> print len(repr(0.1)) 19 >>> print repr(0.1) 0.10000000000000001   The explanation on why this happens is in this chapter of the python tutorial.  A solution would be to use a type that specifically tracks decimal numbers, like python's decimal.Decimal:  >>> print len(str(decimal.Decimal('0.1'))) 3      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "floating-point",
            "rounding"
        ],
        "URL": "https://stackoverflow.com/questions/1317558/converting-a-float-to-a-string-without-rounding-it",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm making a program that, for reasons not needed to be explained, requires a float to be converted into a string to be counted with len(). However, str(float(x)) results in x being rounded when converted to a string, which throws the entire thing off. Does anyone know of a fix for it? Here's the code being used if you want to know:  len(str(float(x)/3))      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Converting a float to a string without rounding it",
        "A_Content": "  Other answers already pointed out that the representation of floating numbers is a thorny issue, to say the least.  Since you don't give enough context in your question, I cannot know if the decimal module can be useful for your needs:  http://docs.python.org/library/decimal.html  Among other things you can explicitly specify the precision that you wish to obtain (from the docs):  >>> getcontext().prec = 6 >>> Decimal('3.0') Decimal('3.0') >>> Decimal('3.1415926535') Decimal('3.1415926535') >>> Decimal('3.1415926535') + Decimal('2.7182818285') Decimal('5.85987') >>> getcontext().rounding = ROUND_UP >>> Decimal('3.1415926535') + Decimal('2.7182818285') Decimal('5.85988')   A simple example from my prompt (python 2.6):  >>> import decimal >>> a = decimal.Decimal('10.000000001') >>> a Decimal('10.000000001') >>> print a 10.000000001 >>> b = decimal.Decimal('10.00000000000000000000000000900000002') >>> print b 10.00000000000000000000000000900000002 >>> print str(b) 10.00000000000000000000000000900000002 >>> len(str(b/decimal.Decimal('3.0'))) 29   Maybe this can help? decimal is in python stdlib since 2.4, with additions in python 2.6.  Hope this helps, Francesco     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "floating-point",
            "rounding"
        ],
        "URL": "https://stackoverflow.com/questions/1317558/converting-a-float-to-a-string-without-rounding-it",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm making a program that, for reasons not needed to be explained, requires a float to be converted into a string to be counted with len(). However, str(float(x)) results in x being rounded when converted to a string, which throws the entire thing off. Does anyone know of a fix for it? Here's the code being used if you want to know:  len(str(float(x)/3))      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Converting a float to a string without rounding it",
        "A_Content": "  I know this is too late but for those who are coming here for the first time, I'd like to post a solution. I have a float value index and a string imgfile and I had the same problem as you. This is how I fixed the issue  index = 1.0 imgfile = 'data/2.jpg' out = '%.1f,%s' % (index,imgfile) print out   The output is  1.0,data/2.jpg   You may modify this formatting example as per your convenience.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "floating-point",
            "rounding"
        ],
        "URL": "https://stackoverflow.com/questions/1317558/converting-a-float-to-a-string-without-rounding-it",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm making a program that, for reasons not needed to be explained, requires a float to be converted into a string to be counted with len(). However, str(float(x)) results in x being rounded when converted to a string, which throws the entire thing off. Does anyone know of a fix for it? Here's the code being used if you want to know:  len(str(float(x)/3))      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Python integer ranges",
        "A_Content": "  Python has arbitrary precision integers so there is no true fixed maximum.  You're only limited by available memory.  In Python 2, there are two types, int and long.  ints use a C type, while longs are arbitrary precision.  You can use sys.maxint to find the maximum int.  But ints are automatically promoted to long, so you usually don't need to worry about it:  sys.maxint + 1   works fine and returns a long.  sys.maxint does not even exist in Python 3, since int and long were unified into a single arbitrary precision int type.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer"
        ],
        "URL": "https://stackoverflow.com/questions/4581842/python-integer-ranges",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In Python, is there a way to get the largest integer one can use? Is there some pre-defined constant like INT_MAX?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Python integer ranges",
        "A_Content": "  import re import sys import platform import struct  formats = {           \"int\":{\"p_type\":\"integer\", \"format\":\"i\"},           \"float\":{\"p_type\":\"float\", \"format\":\"f\"},           \"double\":{\"p_type\":\"float\", \"format\":\"d\"}           }      sys_bit = int(re.findall(r\"\\d+\", platform.architecture()[0])[0])/8 for k,v in formats.iteritems():     print \"max {0} {1}\".format(k, 2**struct.Struct(v[\"format\"]).size*sys_bit-1)-1)     print \"min {0} {1}\".format(k, -2**struct.Struct(v[\"format\"]).size*sys_bit-1))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer"
        ],
        "URL": "https://stackoverflow.com/questions/4581842/python-integer-ranges",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, is there a way to get the largest integer one can use? Is there some pre-defined constant like INT_MAX?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Pycharm import RuntimeWarning after updating to 2016.2",
        "A_Content": "  This is a known issue introduced with the 2016.2 release. Progress can be followed on the JetBrains website here. According to this page it is due to be fixed in the 2017.1 release. You can follow the utrunner.py workaround that others have mentioned in the meantime - a copy of this file is attached to the linked ticket.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "import",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/38569992/pycharm-import-runtimewarning-after-updating-to-2016-2",
        "A_Votes": "65",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    After updating to new version 2016.2, I am getting  RuntimeWarning: Parent module 'tests' not found while handling absolute import   import unittest RuntimeWarning: Parent module 'tests' not found while handling absolute import   import datetime as dt   'tests' is a package inside my main app package, and I receive these warnings when I try to execute unit tests inside this folder. This issue only came up after updating to 2016.2. Besides the warnings, the remaining code works fine.  Edit: This is a known issue - https://youtrack.jetbrains.com/issue/PY-20171. They are suggesting to replace utrunner.py in PyCharm installation folder.     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Pycharm import RuntimeWarning after updating to 2016.2",
        "A_Content": "  The latest recommendation (Dec. 19, 2016) is to put this line at the top of your unit test script:  from __future__ import absolute_import      ",
        "Language": "Python",
        "Tags": [
            "python",
            "import",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/38569992/pycharm-import-runtimewarning-after-updating-to-2016-2",
        "A_Votes": "36",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After updating to new version 2016.2, I am getting  RuntimeWarning: Parent module 'tests' not found while handling absolute import   import unittest RuntimeWarning: Parent module 'tests' not found while handling absolute import   import datetime as dt   'tests' is a package inside my main app package, and I receive these warnings when I try to execute unit tests inside this folder. This issue only came up after updating to 2016.2. Besides the warnings, the remaining code works fine.  Edit: This is a known issue - https://youtrack.jetbrains.com/issue/PY-20171. They are suggesting to replace utrunner.py in PyCharm installation folder.     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Pycharm import RuntimeWarning after updating to 2016.2",
        "A_Content": "  On OS X I've fixed this by replacing   Applications/PyCharm.app/Contents/helpers/pycharm/utrunner.py    with an older version that can be found at      http://code.metager.de/source/xref/jetbrains/intellij/community/python/helpers/pycharm/utrunner.py     ",
        "Language": "Python",
        "Tags": [
            "python",
            "import",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/38569992/pycharm-import-runtimewarning-after-updating-to-2016-2",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After updating to new version 2016.2, I am getting  RuntimeWarning: Parent module 'tests' not found while handling absolute import   import unittest RuntimeWarning: Parent module 'tests' not found while handling absolute import   import datetime as dt   'tests' is a package inside my main app package, and I receive these warnings when I try to execute unit tests inside this folder. This issue only came up after updating to 2016.2. Besides the warnings, the remaining code works fine.  Edit: This is a known issue - https://youtrack.jetbrains.com/issue/PY-20171. They are suggesting to replace utrunner.py in PyCharm installation folder.     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Pycharm import RuntimeWarning after updating to 2016.2",
        "A_Content": "  On Ubuntu 16.04 Bobby's solution also works:  Just replace your local utrunner.py file at   /usr/local/pycharm-edu-3.0/helpers/pycharm/utrunner.py   by the one from the Jetbrains website: http://code.metager.de/source/xref/jetbrains/intellij/community/python/helpers/pycharm/utrunner.py      ",
        "Language": "Python",
        "Tags": [
            "python",
            "import",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/38569992/pycharm-import-runtimewarning-after-updating-to-2016-2",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After updating to new version 2016.2, I am getting  RuntimeWarning: Parent module 'tests' not found while handling absolute import   import unittest RuntimeWarning: Parent module 'tests' not found while handling absolute import   import datetime as dt   'tests' is a package inside my main app package, and I receive these warnings when I try to execute unit tests inside this folder. This issue only came up after updating to 2016.2. Besides the warnings, the remaining code works fine.  Edit: This is a known issue - https://youtrack.jetbrains.com/issue/PY-20171. They are suggesting to replace utrunner.py in PyCharm installation folder.     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Pycharm import RuntimeWarning after updating to 2016.2",
        "A_Content": "  On Windows 10 Bobby's solution also works:  Just replace your local utrunner.py file at  C:\\Program Files (x86)\\JetBrains\\PyCharm 2016.3\\helpers\\pycharm   with the one from the Jetbrains website: http://code.metager.de/source/xref/jetbrains/intellij/community/python/helpers/pycharm/utrunner.py     ",
        "Language": "Python",
        "Tags": [
            "python",
            "import",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/38569992/pycharm-import-runtimewarning-after-updating-to-2016-2",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After updating to new version 2016.2, I am getting  RuntimeWarning: Parent module 'tests' not found while handling absolute import   import unittest RuntimeWarning: Parent module 'tests' not found while handling absolute import   import datetime as dt   'tests' is a package inside my main app package, and I receive these warnings when I try to execute unit tests inside this folder. This issue only came up after updating to 2016.2. Besides the warnings, the remaining code works fine.  Edit: This is a known issue - https://youtrack.jetbrains.com/issue/PY-20171. They are suggesting to replace utrunner.py in PyCharm installation folder.     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "What is the cause of the Bad Request Error when submitting form in Flask application?",
        "A_Content": "  The solution was simple and uncovered in the comments. As addressed in this question, Form sending error, Flask, and pointed out by Sean Vieira,      ...the issue is that Flask raises an HTTP error when it fails to find a   key in the args and form dictionaries. What Flask assumes by default   is that if you are asking for a particular key and it's not there then   something got left out of the request and the entire request is   invalid.   In other words, if only one form element that you request in Python cannot be found in HTML, then the POST request is not valid and the error appears, in my case without any irregularities in the traceback. For me, it was a lack of consistency with spelling: in the HTML, I labeled various form inputs  <input name=\"question1_field\" placeholder=\"question one\">   while in Python, when there was a POST called, I grab a nonexistent form with  request.form['question1']   whereas, to be consistent with my HTML form names, it needed to be   request.form['question1_field']   I hope this helps.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "forms",
            "post",
            "flask",
            "bad-request"
        ],
        "URL": "https://stackoverflow.com/questions/14105452/what-is-the-cause-of-the-bad-request-error-when-submitting-form-in-flask-applica",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    After reading many similar sounding problems and the relevant Flask docs, I cannot seem to figure out what is generating the following error upon submitting a form:     400 Bad Request      The browser (or proxy) sent a request that this server could not understand.   While the form always displays properly, the bad request happens when I submit an HTML form that ties to either of these functions:   @app.route('/app/business', methods=['GET', 'POST']) def apply_business():     if request.method == 'POST':             new_account = Business(name=request.form['name_field'], email=request.form['email_field'], account_type=\"business\",              q1=request.form['q1_field'], q2=request.form['q2_field'], q3=request.form['q3_field'], q4=request.form['q4_field'],              q5=request.form['q5_field'], q6=request.form['q6_field'], q7=request.form['q7_field'],             account_status=\"pending\", time=datetime.datetime.utcnow())         db.session.add(new_account)         db.session.commit()         session['name'] = request.form['name_field']             return redirect(url_for('success'))     return render_template('application.html', accounttype=\"business\")            @app.route('/app/student', methods=['GET', 'POST']) def apply_student():         if request.method == 'POST':             new_account = Student(name=request.form['name_field'], email=request.form['email_field'], account_type=\"student\",              q1=request.form['q1_field'], q2=request.form['q2_field'], q3=request.form['q3_field'], q4=request.form['q4_field'],              q5=request.form['q5_field'], q6=request.form['q6_field'], q7=request.form['q7_field'], q8=request.form['q8_field'],              q9=request.form['q9_field'], q10=request.form['q10_field'],             account_status=\"pending\", time=datetime.datetime.utcnow())         db.session.add(new_account)         db.session.commit()         session['name'] = request.form['name_field']             return redirect(url_for('success'))      return render_template('application.html', accounttype=\"student\")   The relevant part of HTML is  <html> <head>     <title>apply</title> </head> <body>     {% if accounttype==\"business\" %}     <form action=\"{{ url_for('apply_business') }}\" method=post class=\"application_form\">     {% elif accounttype==\"student\" %}     <form action=\"{{ url_for('apply_student') }}\" method=post class=\"application_form\">          {% endif %}      <p>Full Name:</p>     <input name=\"name_field\" placeholder=\"First and Last\">     <p>Email Address:</p>     <input name=\"email_field\" placeholder=\"your@email.com\">     ...   The problem for most people was not calling GET or POST, but I am doing just that in both functions, and I double checked to make sure I imported everything necessary, such as from flask import request. I also queried the database and confirmed that the additions from the form weren't added.  In the Flask app, I was requesting form fields that were labeled slightly different in the HTML form. Keeping the names consistent is a must. More can be read at this question Form sending error, Flask     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Why does sys.exit() not exit when called inside a thread in Python?",
        "A_Content": "  sys.exit() raises the SystemExit exception, as does thread.exit(). So, when sys.exit() raises that exception inside that thread, it has the same effect as calling thread.exit(), which is why only the thread exits.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/905189/why-does-sys-exit-not-exit-when-called-inside-a-thread-in-python",
        "A_Votes": "50",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    This could be a stupid question, but I'm testing out some of my assumptions about Python and I'm confused as to why the following code snippet would not exit when called in the thread, but would exit when called in the main thread.  import sys, time from threading import Thread  def testexit():     time.sleep(5)     sys.exit()     print \"post thread exit\"  t = Thread(target = testexit) t.start() t.join() print \"pre main exit, post thread exit\" sys.exit() print \"post main exit\"   The docs for sys.exit() state that the call should exit from Python. I can see from the output of this program that \"post thread exit\" is never printed, but the main thread just keeps on going even after the thread calls exit.   Is a separate instance of the interpreter being created for each thread, and the call to exit() is just exiting that separate instance? If so, how does the threading implementation manage access to shared resources? What if I did want to exit the program from the thread (not that I actually want to, but just so I understand)?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Why does sys.exit() not exit when called inside a thread in Python?",
        "A_Content": "     What if I did want to exit the program from the thread (not that I   actually want to, but just so I understand)?   At least on Linux you could do something like:  os.kill(os.getpid(), signal.SIGINT)   This send a SIGINT to the main thread where it can be handled as a KeyboardInterrupt.  BTW: On Windows you can only send a SIGTERM signal, which cannot be caught from Python. (there I would prefer calling os._exit as suggested by Helmut)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/905189/why-does-sys-exit-not-exit-when-called-inside-a-thread-in-python",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This could be a stupid question, but I'm testing out some of my assumptions about Python and I'm confused as to why the following code snippet would not exit when called in the thread, but would exit when called in the main thread.  import sys, time from threading import Thread  def testexit():     time.sleep(5)     sys.exit()     print \"post thread exit\"  t = Thread(target = testexit) t.start() t.join() print \"pre main exit, post thread exit\" sys.exit() print \"post main exit\"   The docs for sys.exit() state that the call should exit from Python. I can see from the output of this program that \"post thread exit\" is never printed, but the main thread just keeps on going even after the thread calls exit.   Is a separate instance of the interpreter being created for each thread, and the call to exit() is just exiting that separate instance? If so, how does the threading implementation manage access to shared resources? What if I did want to exit the program from the thread (not that I actually want to, but just so I understand)?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Why does sys.exit() not exit when called inside a thread in Python?",
        "A_Content": "     What if I did want to exit the program   from the thread (not that I actually   want to, but just so I understand)?   My preferred method is Erlang-ish message passing.  Slightly simlified, I do it like this:  import sys, time import threading import Queue # thread-safe  class CleanExit:   pass  ipq = Queue.Queue()  def testexit(ipq):   time.sleep(5)   ipq.put(CleanExit)   return  threading.Thread(target=testexit, args=(ipq,)).start() while True:   print \"Working...\"   time.sleep(1)   try:     if ipq.get_nowait() == CleanExit:       sys.exit()   except Queue.Empty:     pass      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/905189/why-does-sys-exit-not-exit-when-called-inside-a-thread-in-python",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This could be a stupid question, but I'm testing out some of my assumptions about Python and I'm confused as to why the following code snippet would not exit when called in the thread, but would exit when called in the main thread.  import sys, time from threading import Thread  def testexit():     time.sleep(5)     sys.exit()     print \"post thread exit\"  t = Thread(target = testexit) t.start() t.join() print \"pre main exit, post thread exit\" sys.exit() print \"post main exit\"   The docs for sys.exit() state that the call should exit from Python. I can see from the output of this program that \"post thread exit\" is never printed, but the main thread just keeps on going even after the thread calls exit.   Is a separate instance of the interpreter being created for each thread, and the call to exit() is just exiting that separate instance? If so, how does the threading implementation manage access to shared resources? What if I did want to exit the program from the thread (not that I actually want to, but just so I understand)?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Why does sys.exit() not exit when called inside a thread in Python?",
        "A_Content": "     What if I did want to exit the program   from the thread?   Apart from the method Deestan described you can call os._exit (notice the underscore). Before using it make sure that you understand that it does no cleanups (like calling __del__ or similar).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/905189/why-does-sys-exit-not-exit-when-called-inside-a-thread-in-python",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This could be a stupid question, but I'm testing out some of my assumptions about Python and I'm confused as to why the following code snippet would not exit when called in the thread, but would exit when called in the main thread.  import sys, time from threading import Thread  def testexit():     time.sleep(5)     sys.exit()     print \"post thread exit\"  t = Thread(target = testexit) t.start() t.join() print \"pre main exit, post thread exit\" sys.exit() print \"post main exit\"   The docs for sys.exit() state that the call should exit from Python. I can see from the output of this program that \"post thread exit\" is never printed, but the main thread just keeps on going even after the thread calls exit.   Is a separate instance of the interpreter being created for each thread, and the call to exit() is just exiting that separate instance? If so, how does the threading implementation manage access to shared resources? What if I did want to exit the program from the thread (not that I actually want to, but just so I understand)?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Why does sys.exit() not exit when called inside a thread in Python?",
        "A_Content": "  Is the fact that \"pre main exit, post thread exit\" is printed what's bothering you?  Unlike some other languages (like Java) where the analog to sys.exit (System.exit, in Java's case) causes the VM/process/interpreter to immediately stop, Python's sys.exit just throws an exception: a SystemExit exception in particular.  Here are the docs for sys.exit (just print sys.exit.__doc__):     Exit the interpreter by raising SystemExit(status).   If the status is omitted or None, it defaults to zero (i.e., success).   If the status is numeric, it will be used as the system exit status.   If it is another kind of object, it will be printed and the system   exit status will be one (i.e., failure).   This has a few consequences:   in a thread it just kills the current thread, not the entire process (assuming it gets all the way to the top of the stack...)  object destructors (__del__) are potentially invoked as the stack frames that reference those objects are unwound finally blocks are executed as the stack unwinds you can catch a SystemExit exception   The last is possibly the most surprising, and is yet another reason why you should almost never have an unqualified except statement in your Python code.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/905189/why-does-sys-exit-not-exit-when-called-inside-a-thread-in-python",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This could be a stupid question, but I'm testing out some of my assumptions about Python and I'm confused as to why the following code snippet would not exit when called in the thread, but would exit when called in the main thread.  import sys, time from threading import Thread  def testexit():     time.sleep(5)     sys.exit()     print \"post thread exit\"  t = Thread(target = testexit) t.start() t.join() print \"pre main exit, post thread exit\" sys.exit() print \"post main exit\"   The docs for sys.exit() state that the call should exit from Python. I can see from the output of this program that \"post thread exit\" is never printed, but the main thread just keeps on going even after the thread calls exit.   Is a separate instance of the interpreter being created for each thread, and the call to exit() is just exiting that separate instance? If so, how does the threading implementation manage access to shared resources? What if I did want to exit the program from the thread (not that I actually want to, but just so I understand)?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "How to customize a requirements.txt for multiple environments?",
        "A_Content": "  You can cascade your requirements files and use the \"-r\" flag to tell pip to include the contents of one file inside another. You can break out your requirements into a modular folder hierarchy like this:  `-- django_project_root |-- requirements |   |-- common.txt |   |-- dev.txt |   `-- prod.txt `-- requirements.txt   The files' contents would look like this:  common.txt:  # Contains requirements common to all environments req1==1.0 req2==1.0 req3==1.0 ...   dev.txt:  # Specifies only dev-specific requirements # But imports the common ones too -r common.txt dev_req==1.0 ...   prod.txt:  # Same for prod... -r common.txt prod_req==1.0 ...   Outside of Heroku, you can now setup environments like this:  pip install -r requirements/dev.txt   or  pip install -r requirements/prod.txt   Since Heroku looks specifically for \"requirements.txt\" at the project root, it should just mirror prod, like this:  requirements.txt:  # Mirrors prod -r requirements/prod.txt      ",
        "Language": "Python",
        "Tags": [
            "python",
            "deployment",
            "heroku",
            "requirements.txt"
        ],
        "URL": "https://stackoverflow.com/questions/17803829/how-to-customize-a-requirements-txt-for-multiple-environments",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have two branches, Development and Production. Each has dependencies, some of which are different. Development points to dependencies that are themselves in development. Likewise for Production. I need to deploy to Heroku which expects each branch's dependencies in a single file called 'requirements.txt'.  What is the best way to organize?  What I've thought of:   Maintain separate requirements files, one in each branch (must survive frequent merges!) Tell Heroku which requirements file I want to use (environment variable?) Write deploy scripts (create temp branch, modify requirements file, commit, deploy, delete temp branch)      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "How to customize a requirements.txt for multiple environments?",
        "A_Content": "  A viable option today which didn't exist when the original question and answer was posted is to use pipenv instead of pip to manage dependencies.  With pipenv, manually managing two separate requirement files like with pip is no longer necessary, and instead pipenv manages the development and production packages itself via interactions on the command line.  To install a package for use in both production and development:  pipenv install <package>   To install a package for the development environment only:  pipenv install <package> --dev   Via those commands, pipenv stores and manages the environment configuration in two files (Pipfile and Pipfile.lock). Heroku's current Python buildpack natively supports pipenv and will configure itself from Pipfile.lock if it exists instead of requirements.txt.  See the pipenv link for full documentation of the tool.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "deployment",
            "heroku",
            "requirements.txt"
        ],
        "URL": "https://stackoverflow.com/questions/17803829/how-to-customize-a-requirements-txt-for-multiple-environments",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have two branches, Development and Production. Each has dependencies, some of which are different. Development points to dependencies that are themselves in development. Likewise for Production. I need to deploy to Heroku which expects each branch's dependencies in a single file called 'requirements.txt'.  What is the best way to organize?  What I've thought of:   Maintain separate requirements files, one in each branch (must survive frequent merges!) Tell Heroku which requirements file I want to use (environment variable?) Write deploy scripts (create temp branch, modify requirements file, commit, deploy, delete temp branch)      ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "How can I write data in YAML format in a file?",
        "A_Content": "  import yaml  data = dict(     A = 'a',     B = dict(         C = 'c',         D = 'd',         E = 'e',     ) )  with open('data.yml', 'w') as outfile:     yaml.dump(data, outfile, default_flow_style=False)   The default_flow_style=False parameter is necessary to produce the format you want (flow style), otherwise for nested collections it produces block style:  A: a B: {C: c, D: d, E: e}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "formatting",
            "yaml",
            "pyyaml"
        ],
        "URL": "https://stackoverflow.com/questions/12470665/how-can-i-write-data-in-yaml-format-in-a-file",
        "A_Votes": "108",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I need to write the below data to yaml file using Python:  {A:a, B:{C:c, D:d, E:e}}    i.e., dictionary in a dictionary. How can I achieve this?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "How can I write data in YAML format in a file?",
        "A_Content": "  Link to the PyYAML documentation showing the difference for the default_flow_style parameter.  To write it to a file in block mode (often more readable):  d = {'A':'a', 'B':{'C':'c', 'D':'d', 'E':'e'}} with open('result.yml', 'w') as yaml_file:     yaml.dump(d, yaml_file, default_flow_style=False)   produces:  A: a B:   C: c   D: d   E: e      ",
        "Language": "Python",
        "Tags": [
            "python",
            "formatting",
            "yaml",
            "pyyaml"
        ],
        "URL": "https://stackoverflow.com/questions/12470665/how-can-i-write-data-in-yaml-format-in-a-file",
        "A_Votes": "52",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to write the below data to yaml file using Python:  {A:a, B:{C:c, D:d, E:e}}    i.e., dictionary in a dictionary. How can I achieve this?     ",
        "Q_Votes": "66"
    },
    {
        "Q_Title": "Assign pandas dataframe column dtypes",
        "A_Content": "  Since 0.17, you have to use the explicit conversions:  pd.to_datetime, pd.to_timedelta and pd.to_numeric   (As mentioned below, no more \"magic\", convert_objects has been deprecated in 0.17)  df = pd.DataFrame({'x': {0: 'a', 1: 'b'}, 'y': {0: '1', 1: '2'}, 'z': {0: '2018-05-01', 1: '2018-05-02'}})  df.dtypes  x    object y    object z    object dtype: object  df     x  y           z 0  a  1  2018-05-01 1  b  2  2018-05-02   You can apply these to each column you want to convert:  df[\"y\"] = pd.to_numeric(df[\"y\"]) df[\"z\"] = pd.to_datetime(df[\"z\"])     df     x  y          z 0  a  1 2018-05-01 1  b  2 2018-05-02  df.dtypes  x            object y             int64 z    datetime64[ns] dtype: object   and confirm the dtype is updated.    OLD/DEPRECATED ANSWER for pandas 0.12 - 0.16: You can use convert_objects to infer better dtypes:  In [21]: df Out[21]:     x  y 0  a  1 1  b  2  In [22]: df.dtypes Out[22]:  x    object y    object dtype: object  In [23]: df.convert_objects(convert_numeric=True) Out[23]:     x  y 0  a  1 1  b  2  In [24]: df.convert_objects(convert_numeric=True).dtypes Out[24]:  x    object y     int64 dtype: object   Magic! (Sad to see it deprecated.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)  import pandas as pd print pd.DataFrame([['a','1'],['b','2']],                    dtype={'x':'object','y':'int'},                    columns=['x','y'])   I get  ValueError: entry not a 2- or 3- tuple   The only way I can set them is by looping through each column variable and recasting with astype.   dtypes = {'x':'object','y':'int'} mydata = pd.DataFrame([['a','1'],['b','2']],                       columns=['x','y']) for c in mydata.columns:     mydata[c] = mydata[c].astype(dtypes[c]) print mydata['y'].dtype   #=> int64   Is there a better way?     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "Assign pandas dataframe column dtypes",
        "A_Content": "  For those coming from Google (etc.) such as myself:  convert_objects has been deprecated since 0.17 - if you use it, you get a warning like this one:   FutureWarning: convert_objects is deprecated.  Use the data-type specific converters  pd.to_datetime, pd.to_timedelta and pd.to_numeric.   You should do something like the following:   df =df.astype(np.float)  df[\"A\"] =pd.to_numeric(df[\"A\"])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes",
        "A_Votes": "56",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)  import pandas as pd print pd.DataFrame([['a','1'],['b','2']],                    dtype={'x':'object','y':'int'},                    columns=['x','y'])   I get  ValueError: entry not a 2- or 3- tuple   The only way I can set them is by looping through each column variable and recasting with astype.   dtypes = {'x':'object','y':'int'} mydata = pd.DataFrame([['a','1'],['b','2']],                       columns=['x','y']) for c in mydata.columns:     mydata[c] = mydata[c].astype(dtypes[c]) print mydata['y'].dtype   #=> int64   Is there a better way?     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "Assign pandas dataframe column dtypes",
        "A_Content": "  you can set the types explicitly with pandas DataFrame.astype(dtype, copy=True, raise_on_error=True, **kwargs) and pass in a dictionary with the dtypes you want to dtype  here's an example:  import pandas as pd wheel_number = 5 car_name = 'jeep' minutes_spent = 4.5  # set the columns data_columns = ['wheel_number', 'car_name', 'minutes_spent']  # create an empty dataframe data_df = pd.DataFrame(columns = data_columns) df_temp = pd.DataFrame([[wheel_number, car_name, minutes_spent]],columns = data_columns) data_df = data_df.append(df_temp, ignore_index=True)   In [11]: data_df.dtypes Out[11]: wheel_number     float64 car_name          object minutes_spent    float64 dtype: object  data_df = data_df.astype(dtype= {\"wheel_number\":\"int64\",         \"car_name\":\"object\",\"minutes_spent\":\"float64\"})   now you can see that it's changed  In [18]: data_df.dtypes Out[18]: wheel_number       int64 car_name          object minutes_spent    float64      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)  import pandas as pd print pd.DataFrame([['a','1'],['b','2']],                    dtype={'x':'object','y':'int'},                    columns=['x','y'])   I get  ValueError: entry not a 2- or 3- tuple   The only way I can set them is by looping through each column variable and recasting with astype.   dtypes = {'x':'object','y':'int'} mydata = pd.DataFrame([['a','1'],['b','2']],                       columns=['x','y']) for c in mydata.columns:     mydata[c] = mydata[c].astype(dtypes[c]) print mydata['y'].dtype   #=> int64   Is there a better way?     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "Assign pandas dataframe column dtypes",
        "A_Content": "  Another way to set the column types is to first construct a numpy record array with your desired types, fill it out and then pass it to a DataFrame constructor.  import pandas as pd import numpy as np      x = np.empty((10,), dtype=[('x', np.uint8), ('y', np.float64)]) df = pd.DataFrame(x)  df.dtypes ->  x      uint8 y    float64      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)  import pandas as pd print pd.DataFrame([['a','1'],['b','2']],                    dtype={'x':'object','y':'int'},                    columns=['x','y'])   I get  ValueError: entry not a 2- or 3- tuple   The only way I can set them is by looping through each column variable and recasting with astype.   dtypes = {'x':'object','y':'int'} mydata = pd.DataFrame([['a','1'],['b','2']],                       columns=['x','y']) for c in mydata.columns:     mydata[c] = mydata[c].astype(dtypes[c]) print mydata['y'].dtype   #=> int64   Is there a better way?     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "Assign pandas dataframe column dtypes",
        "A_Content": "  facing similar problem to you. In my case I have 1000's of files from cisco logs that I need to parse manually.  In order to be flexible with fields and types I have successfully tested using StringIO + read_cvs which indeed does accept a dict for the dtype specification.  I usually get each of the files  ( 5k-20k lines) into a buffer and create the dtype dictionaries dynamically.  Eventually I concatenate ( with categorical... thanks to 0.19)  these dataframes into a large data frame that I dump into hdf5.   Something along these lines  import pandas as pd import io   output = io.StringIO() output.write('A,1,20,31\\n') output.write('B,2,21,32\\n') output.write('C,3,22,33\\n') output.write('D,4,23,34\\n')  output.seek(0)   df=pd.read_csv(output, header=None,         names=[\"A\",\"B\",\"C\",\"D\"],         dtype={\"A\":\"category\",\"B\":\"float32\",\"C\":\"int32\",\"D\":\"float64\"},         sep=\",\"        )  df.info()  <class 'pandas.core.frame.DataFrame'> RangeIndex: 5 entries, 0 to 4 Data columns (total 4 columns): A    5 non-null category B    5 non-null float32 C    5 non-null int32 D    5 non-null float64 dtypes: category(1), float32(1), float64(1), int32(1) memory usage: 205.0 bytes None   Not very pythonic.... but does the job  Hope it helps.  JC     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)  import pandas as pd print pd.DataFrame([['a','1'],['b','2']],                    dtype={'x':'object','y':'int'},                    columns=['x','y'])   I get  ValueError: entry not a 2- or 3- tuple   The only way I can set them is by looping through each column variable and recasting with astype.   dtypes = {'x':'object','y':'int'} mydata = pd.DataFrame([['a','1'],['b','2']],                       columns=['x','y']) for c in mydata.columns:     mydata[c] = mydata[c].astype(dtypes[c]) print mydata['y'].dtype   #=> int64   Is there a better way?     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "Assign pandas dataframe column dtypes",
        "A_Content": "  You're better off using typed np.arrays, and then pass the data and column names as a dictionary.  import numpy as np import pandas as pd # Feature: np arrays are 1: efficient, 2: can be pre-sized x = np.array(['a', 'b'], dtype=object) y = np.array([ 1 ,  2 ], dtype=np.int32) df = pd.DataFrame({    'x' : x,    # Feature: column name is near data array    'y' : y,    }  )      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)  import pandas as pd print pd.DataFrame([['a','1'],['b','2']],                    dtype={'x':'object','y':'int'},                    columns=['x','y'])   I get  ValueError: entry not a 2- or 3- tuple   The only way I can set them is by looping through each column variable and recasting with astype.   dtypes = {'x':'object','y':'int'} mydata = pd.DataFrame([['a','1'],['b','2']],                       columns=['x','y']) for c in mydata.columns:     mydata[c] = mydata[c].astype(dtypes[c]) print mydata['y'].dtype   #=> int64   Is there a better way?     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "KeyError in module 'threading' after a successful py.test run",
        "A_Content": "  I observed a similar issue and decided to see what's going on exactly - let me describe my findings. I hope someone will find it useful.  Short story  It is indeed related to monkey-patching the threading module. In fact, I can easily trigger the exception by importing the threading module before monkey-patching threads. The following 2 lines are enough:  import threading import gevent.monkey; gevent.monkey.patch_thread()   When executed it spits the message about ignored KeyError:  (env)czajnik@autosan:~$ python test.py  Exception KeyError: KeyError(139924387112272,) in <module 'threading' from '/usr/lib/python2.7/threading.pyc'> ignored   If you swap the import lines, the problem is gone.   Long story  I could stop my debugging here, but I decided it's worth to understand the exact cause of the problem.  First step was to find the code that prints the message about ignored exception. It was a little hard for me to find it (grepping for Exception.*ignored yielded nothing), but grepping around CPython source code I've eventually found a function called void PyErr_WriteUnraisable(PyObject *obj) in Python/error.c, with a very interesting comment:  /* Call when an exception has occurred but there is no way for Python    to handle it.  Examples: exception in __del__ or during GC. */   I decided to check who's calling it, with a little help from gdb, just to get the following C-level stack trace:  #0  0x0000000000542c40 in PyErr_WriteUnraisable () #1  0x00000000004af2d3 in Py_Finalize () #2  0x00000000004aa72e in Py_Main () #3  0x00007ffff68e576d in __libc_start_main (main=0x41b980 <main>, argc=2,     ubp_av=0x7fffffffe5f8, init=<optimized out>, fini=<optimized out>,      rtld_fini=<optimized out>, stack_end=0x7fffffffe5e8) at libc-start.c:226 #4  0x000000000041b9b1 in _start ()   Now we can clearly see that the exception is thrown while Py_Finalize executes - this call is responsible for shutting down the Python interpreter, freeing allocated memory, etc. It's called just before exitting.  Next step was to look at Py_Finalize() code (it's in Python/pythonrun.c). The very first call it makes is wait_for_thread_shutdown() - worth looking at, as we know the problem is related to threading. This function in turn calls _shutdown callable in the threading module. Good, we can go back to python code now.   Looking at threading.py I've found the following interesting parts:  class _MainThread(Thread):      def _exitfunc(self):         self._Thread__stop()         t = _pickSomeNonDaemonThread()         if t:             if __debug__:                 self._note(\"%s: waiting for other threads\", self)         while t:             t.join()             t = _pickSomeNonDaemonThread()         if __debug__:             self._note(\"%s: exiting\", self)         self._Thread__delete()  # Create the main thread object, # and make it available for the interpreter # (Py_Main) as threading._shutdown.  _shutdown = _MainThread()._exitfunc   Clearly, the responsibility of threading._shutdown() call is to join all non-daemon threads and delete main thread (whatever that means exactly). I decided to patch threading.py a bit - wrap the whole _exitfunc() body with try/except and print the stack trace with traceback module. This gave the following trace:  Traceback (most recent call last):   File \"/usr/lib/python2.7/threading.py\", line 785, in _exitfunc     self._Thread__delete()   File \"/usr/lib/python2.7/threading.py\", line 639, in __delete     del _active[_get_ident()] KeyError: 26805584   Now we know the exact place where the exception is thrown - inside Thread.__delete() method.   The rest of the story is obvious after reading threading.py for a while. The _active dictionary maps thread IDs (as returned by _get_ident()) to Thread instances, for all threads created. When threading module is loaded, an instance of _MainThread class is always created and added to _active (even if no other threads are explicitly created).   The problem is that one of the methods patched by gevent's monkey-patching is _get_ident() - original one maps to thread.get_ident(), monkey-patching replaces it with green_thread.get_ident(). Obviously both calls return different IDs for main thread.  Now, if threading module is loaded before monkey-patching, _get_ident() call returns one value when _MainThread instance is created and added to _active, and another value at the time _exitfunc() is called - hence KeyError in del _active[_get_ident()].  On the contrary, if monkey-patching is done before threading is loaded, all is fine - at the time _MainThread instance is being added to _active, _get_ident() is already patched, and the same thread ID is returned at cleanup time. That's it!  To make sure I import modules in the right order, I added the following snippet to my code, just before monkey-patching call:  import sys if 'threading' in sys.modules:         raise Exception('threading module loaded before patching!') import gevent.monkey; gevent.monkey.patch_thread()   I hope you find my debugging story useful :)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "gevent",
            "py.test"
        ],
        "URL": "https://stackoverflow.com/questions/8774958/keyerror-in-module-threading-after-a-successful-py-test-run",
        "A_Votes": "210",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm running a set of tests with py.test. They pass. Yippie! But I'm getting this message:  Exception KeyError: KeyError(4427427920,) in <module 'threading' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc'> ignored   How should I go about tracking down the source of that? (I'm not using threading directly, but am using gevent.)     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "KeyError in module 'threading' after a successful py.test run",
        "A_Content": "  You could use this:  import sys if 'threading' in sys.modules:     del sys.modules['threading'] import gevent import gevent.socket import gevent.monkey gevent.monkey.patch_all()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "gevent",
            "py.test"
        ],
        "URL": "https://stackoverflow.com/questions/8774958/keyerror-in-module-threading-after-a-successful-py-test-run",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm running a set of tests with py.test. They pass. Yippie! But I'm getting this message:  Exception KeyError: KeyError(4427427920,) in <module 'threading' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc'> ignored   How should I go about tracking down the source of that? (I'm not using threading directly, but am using gevent.)     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "KeyError in module 'threading' after a successful py.test run",
        "A_Content": "  I had a similar problem with a gevent prototype script.  The Greenlet callback was executing fine and I was synchronizing back to the main thread via g.join(). For my problem, I had to call gevent.shutdown() to shutdown (what I assume is) the Hub. After I manually shutdown the event loop, the program terminates properly without that error.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "gevent",
            "py.test"
        ],
        "URL": "https://stackoverflow.com/questions/8774958/keyerror-in-module-threading-after-a-successful-py-test-run",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm running a set of tests with py.test. They pass. Yippie! But I'm getting this message:  Exception KeyError: KeyError(4427427920,) in <module 'threading' from '/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.pyc'> ignored   How should I go about tracking down the source of that? (I'm not using threading directly, but am using gevent.)     ",
        "Q_Votes": "67"
    },
    {
        "Q_Title": "Overloaded functions in python?",
        "A_Content": "  EDIT For the new single dispatch generic functions in Python 3.4, see http://www.python.org/dev/peps/pep-0443/  You generally don't need to overload functions in Python. Python is dynamically typed, and supports optional arguments to functions.  def myfunction(first, second, third = None):     if third is None:         #just use first and second     else:         #use all three  myfunction(1, 2) # third will be None, so enter the 'if' clause myfunction(3, 4, 5) # third isn't None, it's 5, so enter the 'else' clause      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "arguments",
            "overloading"
        ],
        "URL": "https://stackoverflow.com/questions/7113032/overloaded-functions-in-python",
        "A_Votes": "88",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is it possible to have overloaded functions in Python? In C# I would do something like  void myfunction (int first, string second) { //some code } void myfunction (int first, string second , float third) { //some different code }   and then when I call the function it would differentiate between the two based on the number of arguments. Is it possible to do something similar in Python?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Overloaded functions in python?",
        "A_Content": "  in normal python you can't do what you want.  there are two close approximations:  def myfunction(first, second, *args):     # args is a tuple of extra arguments  def myfunction(first, second, third=None):     # third is optional   however, if you really want to do this, you can certainly make it work (at the risk of offending the traditionalists ;o).  in short, you would write a wrapper(*args) function that checks the number of arguments and delegates as appropriate.  this kind of \"hack\" is usually done via decorators.  in this case you could achieve something like:  from typing import overload  @overload def myfunction(first):     ....  @myfunction.overload def myfunction(first, second):     ....  @myfunction.overload def myfunction(first, second, third):     ....   and you'd implement this by making the overload(first_fn) function (or constructor) return a callable object where the __call__(*args) method does the delegation explained above and the overload(another_fn) method adds extra functions that can be delegated to.  you can see an example of something similar here http://acooke.org/pytyp/pytyp.spec.dispatch.html but that is overloading methods by type.  it's a very similar approach...  UPDATE: and something similar (using argument types) is being added to python 3 - http://www.python.org/dev/peps/pep-0443/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "arguments",
            "overloading"
        ],
        "URL": "https://stackoverflow.com/questions/7113032/overloaded-functions-in-python",
        "A_Votes": "38",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to have overloaded functions in Python? In C# I would do something like  void myfunction (int first, string second) { //some code } void myfunction (int first, string second , float third) { //some different code }   and then when I call the function it would differentiate between the two based on the number of arguments. Is it possible to do something similar in Python?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Overloaded functions in python?",
        "A_Content": "  Yes, it's possible. I wrote code below in Python 3.2.1:  def overload(*functions):     return lambda *args, **kwargs: functions[len(args)](*args, **kwargs)   Usage:  myfunction=overload(no_arg_func, one_arg_func, two_arg_func)   Note that the lambda returned by the overload functions choose function to call depending on number of unnamed arguments.  The solution isn't perfect, but at the moment I can't write anything better.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "arguments",
            "overloading"
        ],
        "URL": "https://stackoverflow.com/questions/7113032/overloaded-functions-in-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to have overloaded functions in Python? In C# I would do something like  void myfunction (int first, string second) { //some code } void myfunction (int first, string second , float third) { //some different code }   and then when I call the function it would differentiate between the two based on the number of arguments. Is it possible to do something similar in Python?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Overloaded functions in python?",
        "A_Content": "  Not possible directly. You can use explicit type checks on the arguments given though, although this is generally frowned upon.  Python is dynamic. If you are unsure what an object can do, just try: and call a method on it, then except: errors.  If you don't need to overload based on types but just on number of arguments, use keyword arguments.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "arguments",
            "overloading"
        ],
        "URL": "https://stackoverflow.com/questions/7113032/overloaded-functions-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to have overloaded functions in Python? In C# I would do something like  void myfunction (int first, string second) { //some code } void myfunction (int first, string second , float third) { //some different code }   and then when I call the function it would differentiate between the two based on the number of arguments. Is it possible to do something similar in Python?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Overloaded functions in python?",
        "A_Content": "  overloading methods is tricky in python. However, there could be usage of passing the dict, list or primitive variables.  I have tried something for my use cases, this could help here to understand people to overload the methods.  Let's take the example use in one of the stackoverflow thread:  a class overload method with call the methods from different class.  def add_bullet(sprite=None, start=None, headto=None, spead=None, acceleration=None):  pass the arguments from remote class:  add_bullet(sprite = 'test', start=Yes,headto={'lat':10.6666,'long':10.6666},accelaration=10.6}  OR  add_bullet(sprite = 'test', start=Yes,headto={'lat':10.6666,'long':10.6666},speed=['10','20,'30']}  So, handling is being achieved for list, Dictionary or primitive variables from method overloading.  try it out for your codes     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "arguments",
            "overloading"
        ],
        "URL": "https://stackoverflow.com/questions/7113032/overloaded-functions-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to have overloaded functions in Python? In C# I would do something like  void myfunction (int first, string second) { //some code } void myfunction (int first, string second , float third) { //some different code }   and then when I call the function it would differentiate between the two based on the number of arguments. Is it possible to do something similar in Python?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How to log source file name and line number in Python",
        "A_Content": "  Sure, check formatters in logging docs. Specifically the lineno and pathname variables.     %(pathname)s  Full pathname of the source file where the logging call was issued(if available).      %(filename)s  Filename portion of pathname.      %(module)s    Module (name portion of filename).      %(funcName)s  Name of function containing the logging call.      %(lineno)d    Source line number where the logging call was issued (if available).   Looks something like this:  formatter = logging.Formatter('[%(asctime)s] p%(process)s {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s','%m-%d %H:%M:%S')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging"
        ],
        "URL": "https://stackoverflow.com/questions/533048/how-to-log-source-file-name-and-line-number-in-python",
        "A_Votes": "132",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is it possible to decorate/extend the python standard logging system, so that when a logging method is invoked it also logs the file and the line number where it was invoked or maybe the method that invoked it?     ",
        "Q_Votes": "68"
    },
    {
        "Q_Title": "How to log source file name and line number in Python",
        "A_Content": "  On top of Seb's very useful answer, here is a handy code snippet that demonstrates the logger usage with a reasonable format:  #!/usr/bin/env python import logging  logging.basicConfig(format='%(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s',     datefmt='%d-%m-%Y:%H:%M:%S',     level=logging.DEBUG)  logger = logging.getLogger('stackoverflow_rocks') logger.debug(\"This is a debug log\") logger.info(\"This is an info log\") logger.critical(\"This is critical\") logger.error(\"An error occurred\")   Generates this output:  06-06-2017:17:07:02,158 DEBUG    [log.py:11] This is a debug log 06-06-2017:17:07:02,158 INFO     [log.py:12] This is an info log 06-06-2017:17:07:02,158 CRITICAL [log.py:13] This is critical 06-06-2017:17:07:02,158 ERROR    [log.py:14] An error occurred      ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging"
        ],
        "URL": "https://stackoverflow.com/questions/533048/how-to-log-source-file-name-and-line-number-in-python",
        "A_Votes": "24",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to decorate/extend the python standard logging system, so that when a logging method is invoked it also logs the file and the line number where it was invoked or maybe the method that invoked it?     ",
        "Q_Votes": "68"
    },
    {
        "Q_Title": "Python, default keyword arguments after variable length positional arguments",
        "A_Content": "  It does work, but only in Python 3. See PEP 3102. From glancing over the \"what's new\" documents, it seems that there is no 2.x backport, so you're out of luck. You'll have to accept any keyword arguments (**kwargs) and manually parse it. You can use d.get(k, default) to either get d[k] or default if that's not there. To remove an argument from kwargs, e.g. before calling a super class' method, use d.pop.    Note that in def get(self, *args, raw=False, vars=None):, the raw=False and vars=None have nothing to do with keyword arguments. Those are default argument values. Arguments with a default value may be passed positionally, and arguments without a default value may be passed by keyword:  def f(a=1): pass f(2)  # works, passing a positionally def f(a): pass f(a=2)  # works, passing a by keyword   Similarly, keyword-only arguments are not required to have a default value. Coming after the *args argument is what marks them as keyword-only, not the presence of a default value:  def f(*args, a): pass # a is a mandatory, keyword-only argument      ",
        "Language": "Python",
        "Tags": [
            "python",
            "variadic-functions",
            "python-2.x",
            "named-parameters",
            "default-parameters"
        ],
        "URL": "https://stackoverflow.com/questions/5940180/python-default-keyword-arguments-after-variable-length-positional-arguments",
        "A_Votes": "63",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I thought I could use named parameters after variable-length positional parameters in a function call in Python 2, but I get a SyntaxError when importing a python class. I'm writing with the following \"get\" method, for example:  class Foo(object):     def __init__(self):         print \"You have created a Foo.\"      def get(self, *args, raw=False, vars=None):         print len(args)         print raw         print vars   The error looks like:  def get(self, *args, raw=False, vars=None):                      ^ SyntaxError: invalid syntax   I'd like to be able to call the method several ways:  f = Foo() f.get(arg1, arg2) f.get(arg1, raw=True) f.get(arg1, arg2, raw=True, vars=something)   etc.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python, default keyword arguments after variable length positional arguments",
        "A_Content": "  Python's syntax doesn't allow variable args in function and keyword arguments with default value at the same time. If you must have keyword arguments along with arbitrary number of positional arguments, you need to allow arbitrary number of keyword arguments as well.  This is a common pattern to provide default values for keyword arguments, as well as allowing any number of positional arguments:  def foo(*args, **kwargs):    raw = kwargs.pop('raw', False)    vars = kwargs.pop('vars', None)   If you don't use the extra keyword arguments at all, you have nothing to worry about. This makes the function a bit less self-documenting, which you can make up with a properly written docstring.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variadic-functions",
            "python-2.x",
            "named-parameters",
            "default-parameters"
        ],
        "URL": "https://stackoverflow.com/questions/5940180/python-default-keyword-arguments-after-variable-length-positional-arguments",
        "A_Votes": "42",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I thought I could use named parameters after variable-length positional parameters in a function call in Python 2, but I get a SyntaxError when importing a python class. I'm writing with the following \"get\" method, for example:  class Foo(object):     def __init__(self):         print \"You have created a Foo.\"      def get(self, *args, raw=False, vars=None):         print len(args)         print raw         print vars   The error looks like:  def get(self, *args, raw=False, vars=None):                      ^ SyntaxError: invalid syntax   I'd like to be able to call the method several ways:  f = Foo() f.get(arg1, arg2) f.get(arg1, raw=True) f.get(arg1, arg2, raw=True, vars=something)   etc.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Using Python String Formatting with Lists",
        "A_Content": "  print s % tuple(x)   instead of   print s % (x)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "formatting",
            "string-formatting"
        ],
        "URL": "https://stackoverflow.com/questions/7568627/using-python-string-formatting-with-lists",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I construct a string s in Python 2.6.5 which will have a varying number of %s tokens, which match the number of entries in list x. I need to write out a formatted string. The following doesn't work, but indicates what I'm trying to do. In this example, there are three %s tokens and the list has three entries.  s = '%s BLAH %s FOO %s BAR' x = ['1', '2', '3'] print s % (x)   I'd like the output string to be:  1 BLAH 2 FOO 3 BAR     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Using Python String Formatting with Lists",
        "A_Content": "  You should take a look to the format method of python. You could then define your formatting string like this :  >>> s = '{0} BLAH {1} BLAH BLAH {2} BLAH BLAH BLAH' >>> x = ['1', '2', '3'] >>> print s.format(*x) '1 BLAH 2 BLAH BLAH 3 BLAH BLAH BLAH'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "formatting",
            "string-formatting"
        ],
        "URL": "https://stackoverflow.com/questions/7568627/using-python-string-formatting-with-lists",
        "A_Votes": "115",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I construct a string s in Python 2.6.5 which will have a varying number of %s tokens, which match the number of entries in list x. I need to write out a formatted string. The following doesn't work, but indicates what I'm trying to do. In this example, there are three %s tokens and the list has three entries.  s = '%s BLAH %s FOO %s BAR' x = ['1', '2', '3'] print s % (x)   I'd like the output string to be:  1 BLAH 2 FOO 3 BAR     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Using Python String Formatting with Lists",
        "A_Content": "  Following this resource page, if the length of x is varying, we can use:  ', '.join(['%.2f']*len(x))   to create a place holder for each element from the list x. Here is the example:  x = [1/3.0, 1/6.0, 0.678] s = (\"elements in the list are [\"+', '.join(['%.2f']*len(x))+\"]\") % tuple(x) print s >>> elements in the list are [0.33, 0.17, 0.68]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "formatting",
            "string-formatting"
        ],
        "URL": "https://stackoverflow.com/questions/7568627/using-python-string-formatting-with-lists",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I construct a string s in Python 2.6.5 which will have a varying number of %s tokens, which match the number of entries in list x. I need to write out a formatted string. The following doesn't work, but indicates what I'm trying to do. In this example, there are three %s tokens and the list has three entries.  s = '%s BLAH %s FOO %s BAR' x = ['1', '2', '3'] print s % (x)   I'd like the output string to be:  1 BLAH 2 FOO 3 BAR     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Using Python String Formatting with Lists",
        "A_Content": "  Since I just learned about this cool thing(indexing into lists from within a format string) I'm adding to this old question.   s = '{x[0]} BLAH {x[1]} FOO {x[2]} BAR' x = ['1', '2', '3'] print s.format (x=x)   However, I still haven't figured out how to do slicing(inside of the format string '\"{x[2:4]}\".format...,) and would love to figure it out if anyone has an idea, however I suspect that you simply cannot do that.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "formatting",
            "string-formatting"
        ],
        "URL": "https://stackoverflow.com/questions/7568627/using-python-string-formatting-with-lists",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I construct a string s in Python 2.6.5 which will have a varying number of %s tokens, which match the number of entries in list x. I need to write out a formatted string. The following doesn't work, but indicates what I'm trying to do. In this example, there are three %s tokens and the list has three entries.  s = '%s BLAH %s FOO %s BAR' x = ['1', '2', '3'] print s % (x)   I'd like the output string to be:  1 BLAH 2 FOO 3 BAR     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Using Python String Formatting with Lists",
        "A_Content": "  This was a fun question! Another way to handle this with the .format method  for variable length lists is to use a function that takes full advantage of list unpacking. In the following example I don't use any fancy formatting, but that can easily be changed to suit your needs.  list_1 = [1,2,3,4,5,6] list_2 = [1,2,3,4,5,6,7,8]  # Create a function to easily repeat on many lists: def ListToFormattedString(alist):     # Each item is right-adjusted, width=3     formatted_list = ['{:>3}' for item in alist]      s = ','.join(formatted_list)     return s.format(*alist)  # Example output: >>>ListToFormattedString(list_1) '  1,  2,  3,  4,  5,  6' >>>ListToFormattedString(list_2) '  1,  2,  3,  4,  5,  6,  7,  8'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "formatting",
            "string-formatting"
        ],
        "URL": "https://stackoverflow.com/questions/7568627/using-python-string-formatting-with-lists",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I construct a string s in Python 2.6.5 which will have a varying number of %s tokens, which match the number of entries in list x. I need to write out a formatted string. The following doesn't work, but indicates what I'm trying to do. In this example, there are three %s tokens and the list has three entries.  s = '%s BLAH %s FOO %s BAR' x = ['1', '2', '3'] print s % (x)   I'd like the output string to be:  1 BLAH 2 FOO 3 BAR     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Using Python String Formatting with Lists",
        "A_Content": "  Here's a little improvised answer to using format with print() on a list.   How about this: (py3)  sample_list = ['cat', 'dog', 'bunny', 'pig'] print(\"Your list of animals are: {}, {}, {} and {}\".format(*sample_list))   Read the docs here on using format.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "formatting",
            "string-formatting"
        ],
        "URL": "https://stackoverflow.com/questions/7568627/using-python-string-formatting-with-lists",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I construct a string s in Python 2.6.5 which will have a varying number of %s tokens, which match the number of entries in list x. I need to write out a formatted string. The following doesn't work, but indicates what I'm trying to do. In this example, there are three %s tokens and the list has three entries.  s = '%s BLAH %s FOO %s BAR' x = ['1', '2', '3'] print s % (x)   I'd like the output string to be:  1 BLAH 2 FOO 3 BAR     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  I would suggest the following:   (Most likely) You haven't installed one of the dependencies of your tag library. Check the imports inside the current_tags.py module. Make sure the application that includes the tag library is registered in settings.py under INSTALLED_APPS. Make sure that you can successfully import the tag library.  python manage.py shell >>> from app.templatetags import current_tags   This boils down what the following link recommends, which is that the error itself tends to mislead you about where it's looking for a template from. It silently ignores errors on import, which means current_tags.py itself might have a syntax error or another reason why it raises ImportError.   If everything else fails, check this link: http://www.b-list.org/weblog/2007/dec/04/magic-tags/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "78",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  I had this problem and fixed it by adding a blank __init__.py file in my appname/templatetags/ directory.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "74",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  Possibilities are many:   You haven't reset your dev server. You have dependency loop in templatetag file. You misspelled something (directory, folder, template name in 'load', etc.). You forgot about adding the app to INSTALLED_APPS.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "62",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  Restart the server has solved the issue for me. They must have mentioned it in the documentation.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  I was getting the same error but for a different reason so I'll tell you (in case someone else comes the same problem).  I had everything right but I had my custom tag inside a folder named template_tags and after a long search I found out that it had to be templatetags, and now it works. So check the folder name is exactly templatetags.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  suppose you have the following structure:  -- Application_Name  -------templatetags  --------------init.py  --------------templates_extras.py  -------init.py  -------settings.py  -- manage.py  You have to make sure of the following:   your application itself inside which your \"templatetags\" is resident is actually installed in INSTALLED_APPS in settings.py (e.g. \"Application_Name\") your tag module itself that exists inside \"templatetags\" is already installed in INSTALLED_APP in settings.py (e.g. \"ApplicationName.templatetags.tempaltes_extras\") keep sure you have \"init.py\" under templatetags directory you have to restart the server  In some cases you have to remove all generated *.pyc if it did not work then retry again       ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  \"custom tags\" is not a valid tag library error, more often is occurred because the custom tags are not loaded into the app.  place an empty init.py inside the same folder where your \"custom template tag\" is placed and run the below code on the terminal to load the custom template tags  touch __init__.py      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  Please ensure your templatetags folder is initialized with python, if you are in doubt, just take bakup of all files,  Remove all files, Inside templatetags folder create init.py file only, then restart your server,  Now your folder is under Python, then do your stuff.  This works for me...     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  Make sure the load statement is correct. It should be the name of the file, not the name of the app. For instance, if you have this app:  appname  __init__.py  templatetags   __init__.py   foobarfilename.py   Then you should put this in your template:  {% load foobarfilename %}   Of course, you should check the other answers too.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  After you have created the template tag and it should be within the 'templatetags' package within an app installed in the settings.INSTALLED_APPS, make sure you restart your dev-server.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  For me, it was the mistake of putting quotes around the library name in load tag.  WRONG: {% load 'library_name' %}  CORRECT: {% load library_name %}  Refer to other answers too. I solved a couple of those problems too before landing here.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  For others facing this . Suppose your App name is MyApp and your tag folder name is templatetags then in settings.py you should have :  INSTALLED_APPS = [ 'MyApp', 'MyApp.templatetags' ]   Both your django app and your tag folder which is under your app package are needed there.  -> MyApp     ---> models.py     ---> views.py     ---> templatetags       -----> __init__.py       -----> app_filters.py   And in your template file :  {% load app_filters %}   Also app_filters.py be like :  # coding=utf-8 from django import template  register = template.Library()   @register.filter(name='get_item') def get_item(dictionary, key):     return dictionary.get(key)   check all above steps and you may find the issue.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  Maybe someone will find this useful: somehow I had named the directory 'templatetags ' instead of 'templatetags', that is -- with a space at the end. Took hours to finally realize.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  All of the advice listed here didn't help me. So in my specific case the problem was that the templatetag had to be loaded from a third-party app, and I manually copied source folder with that app into src folder in my virtualenv. Then I ran python setup.py install inside that folder. After that django could not load this module.   Then I removed the source and installed folder of this app and installed it using pip install -r requirements.txt after adding a relevant line into requirements.txt file. It was downloaded into the src folder, installed and everything began working properly. Hope this helps someone.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django: 'current_tags' is not a valid tag library",
        "A_Content": "  In my case I have created library instance using tag variable instead of register variable   tag = template.Library()   But it should be  register = template.Library()      To be a valid tag library, the module must contain a module-level   variable named register that is a template.Library instance, in which   all the tags and filters are registered      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "portability"
        ],
        "URL": "https://stackoverflow.com/questions/5493776/django-current-tags-is-not-a-valid-tag-library",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a small Django project I received from a friend. The code works perfectly on his system. However, on my system I get the following error message when running the server:     TemplateSyntaxError at /      'current_tags' is not a valid tag library: Template library current_tags not found, tried django.templatetags.current_tags   The problem is with a line in an html file:  {% load current_tags %}   This exact same code works on his system with no errors. What could that be?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove frame from matplotlib (pyplot.figure vs matplotlib.figure ) (frameon=False Problematic in matplotlib)",
        "A_Content": "  First off, if you're using savefig, be aware that it will override the figure's background color when saving unless you specify otherwise (e.g. fig.savefig('blah.png', transparent=True)).  However, to remove the axes' and figure's background on-screen, you'll need to set both ax.patch and fig.patch to be invisible.    E.g.  import matplotlib.pyplot as plt  fig, ax = plt.subplots() ax.plot(range(10))  for item in [fig, ax]:     item.patch.set_visible(False)  with open('test.png', 'w') as outfile:     fig.canvas.print_png(outfile)     (Of course, you can't tell the difference on SO's white background, but everything is transparent...)  If you don't want to show anything other than the line, turn the axis off as well using ax.axis('off'):  import matplotlib.pyplot as plt  fig, ax = plt.subplots() ax.plot(range(10))  fig.patch.set_visible(False) ax.axis('off')  with open('test.png', 'w') as outfile:     fig.canvas.print_png(outfile)     In that case, though, you may want to make the axes take up the full figure.  If you manually specify the location of the axes, you can tell it to take up the full figure (alternately, you can use subplots_adjust, but this is simpler for the case of a single axes).   import matplotlib.pyplot as plt  fig = plt.figure(frameon=False) ax = fig.add_axes([0, 0, 1, 1]) ax.axis('off')  ax.plot(range(10))  with open('test.png', 'w') as outfile:     fig.canvas.print_png(outfile)        ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame",
        "A_Votes": "100",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    To remove frame in figure, I write   frameon=False   works perfect with pyplot.figure, but with matplotlib.Figure it only removes the gray background, the frame stays . Also, I only want the lines to show, and all the rest of figure be transparent.  with pyplot I can do what I want, I want to do it with matplotlib for some long reason I 'd rather not mention to extend my question.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove frame from matplotlib (pyplot.figure vs matplotlib.figure ) (frameon=False Problematic in matplotlib)",
        "A_Content": "  ax.axis('off'), will as Joe Kington pointed out, remove everything except the plotted line.  For those wanting to only remove the frame (border), and keep labels, tickers etc, one can do that by accessing the spines object on the axis. Given an axis object ax, the following should remove borders on all four sides:  ax.spines['top'].set_visible(False) ax.spines['right'].set_visible(False) ax.spines['bottom'].set_visible(False) ax.spines['left'].set_visible(False)   And, in case of removing x and y ticks from the plot:    ax.get_xaxis().set_ticks([])  ax.get_yaxis().set_ticks([])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame",
        "A_Votes": "122",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    To remove frame in figure, I write   frameon=False   works perfect with pyplot.figure, but with matplotlib.Figure it only removes the gray background, the frame stays . Also, I only want the lines to show, and all the rest of figure be transparent.  with pyplot I can do what I want, I want to do it with matplotlib for some long reason I 'd rather not mention to extend my question.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove frame from matplotlib (pyplot.figure vs matplotlib.figure ) (frameon=False Problematic in matplotlib)",
        "A_Content": "  Building up on @peeol's excellent answer, you can also remove the frame by doing  for spine in plt.gca().spines.values():     spine.set_visible(False)   To give an example (the entire code sample can be found at the end of this post), let's say you have a bar plot like this,     you can remove the frame with the commands above and then either keep the x- and ytick labels (plot not shown) or remove them as well doing  plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')   In this case, one can then label the bars directly; the final plot could look like this (code can be found below):    Here is the entire code that is necessary to generate the plots:  import matplotlib.pyplot as plt import numpy as np  plt.figure()  xvals = list('ABCDE') yvals = np.array(range(1, 6))  position = np.arange(len(xvals))  mybars = plt.bar(position, yvals, align='center', linewidth=0) plt.xticks(position, xvals)  plt.title('My great data') # plt.show()  # get rid of the frame for spine in plt.gca().spines.values():     spine.set_visible(False)  # plt.show() # remove all the ticks and directly label each bar with respective value plt.tick_params(top='off', bottom='off', left='off', right='off', labelleft='off', labelbottom='on')  # plt.show()  # direct label each bar with Y axis values for bari in mybars:     height = bari.get_height()     plt.gca().text(bari.get_x() + bari.get_width()/2, bari.get_height()-0.2, str(int(height)),                  ha='center', color='white', fontsize=15) plt.show()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame",
        "A_Votes": "34",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    To remove frame in figure, I write   frameon=False   works perfect with pyplot.figure, but with matplotlib.Figure it only removes the gray background, the frame stays . Also, I only want the lines to show, and all the rest of figure be transparent.  with pyplot I can do what I want, I want to do it with matplotlib for some long reason I 'd rather not mention to extend my question.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove frame from matplotlib (pyplot.figure vs matplotlib.figure ) (frameon=False Problematic in matplotlib)",
        "A_Content": "  I use to do so:  from pylab import * axes(frameon = 0) ... show()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    To remove frame in figure, I write   frameon=False   works perfect with pyplot.figure, but with matplotlib.Figure it only removes the gray background, the frame stays . Also, I only want the lines to show, and all the rest of figure be transparent.  with pyplot I can do what I want, I want to do it with matplotlib for some long reason I 'd rather not mention to extend my question.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove frame from matplotlib (pyplot.figure vs matplotlib.figure ) (frameon=False Problematic in matplotlib)",
        "A_Content": "  for spine in plt.gca().spines.values():     spine.set_visible(False) #Indentation updated..      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/14908576/how-to-remove-frame-from-matplotlib-pyplot-figure-vs-matplotlib-figure-frame",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    To remove frame in figure, I write   frameon=False   works perfect with pyplot.figure, but with matplotlib.Figure it only removes the gray background, the frame stays . Also, I only want the lines to show, and all the rest of figure be transparent.  with pyplot I can do what I want, I want to do it with matplotlib for some long reason I 'd rather not mention to extend my question.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Does Python have a cleaner way to express if x contains a|b|c|d? [duplicate]",
        "A_Content": "  The Pythonic approach would be to use any():  if any(s in x for s in (a,b,c,d,e,f,g)):   From the linked documentation:      any(iterable)      Return True if any element of the iterable is true. If the iterable is empty, return False. Equivalent to:  def any(iterable):     for element in iterable:         if element:             return True     return False    Also, notice that I've used a tuple instead of a list here. If your a-g values are pre-defined, then a tuple would indeed be preferred. See: Are tuples more efficient than lists in Python?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "if-statement",
            "python-3.x",
            "substring"
        ],
        "URL": "https://stackoverflow.com/questions/19714041/does-python-have-a-cleaner-way-to-express-if-x-contains-abcd",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              Check if a Python list item contains a string inside another string                                        13 answers                                          The Pythonic way to check if a string x is a substring of y is:  if x in y:   Finding if x is equivalent to a, b, c, d, e, f or g is also Pythonic:  if x in [a,b,c,d,e,f,g]:   But checking if some string x contains either a, b, c, d, e, f or g seems clunky:  if a in x or b in x or c in x or d in x or e in x or f in x or g in x   Is there a more Pythonic method of checking if a string x contains an element of a list?  I know it is trivial to write this myself using a loop or using a regex:  re.search('(dog|cat|bird|mouse|elephant|pig|cow)', x)   but I was wondering if there was a cleaner way that does not involve regex.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Does Python have a cleaner way to express if x contains a|b|c|d? [duplicate]",
        "A_Content": "  if any(q in x for q in [a,b,c,d,e,f,g]):   I think that's about as short & Pythonic as you can get it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "if-statement",
            "python-3.x",
            "substring"
        ],
        "URL": "https://stackoverflow.com/questions/19714041/does-python-have-a-cleaner-way-to-express-if-x-contains-abcd",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Check if a Python list item contains a string inside another string                                        13 answers                                          The Pythonic way to check if a string x is a substring of y is:  if x in y:   Finding if x is equivalent to a, b, c, d, e, f or g is also Pythonic:  if x in [a,b,c,d,e,f,g]:   But checking if some string x contains either a, b, c, d, e, f or g seems clunky:  if a in x or b in x or c in x or d in x or e in x or f in x or g in x   Is there a more Pythonic method of checking if a string x contains an element of a list?  I know it is trivial to write this myself using a loop or using a regex:  re.search('(dog|cat|bird|mouse|elephant|pig|cow)', x)   but I was wondering if there was a cleaner way that does not involve regex.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Does Python have a cleaner way to express if x contains a|b|c|d? [duplicate]",
        "A_Content": "  A bit late to the party, but  not frozenset(x).isdisjoint(frozenset(y))   would work, and may be faster (algorithmically, but maybe not for smaller test cases).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "if-statement",
            "python-3.x",
            "substring"
        ],
        "URL": "https://stackoverflow.com/questions/19714041/does-python-have-a-cleaner-way-to-express-if-x-contains-abcd",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Check if a Python list item contains a string inside another string                                        13 answers                                          The Pythonic way to check if a string x is a substring of y is:  if x in y:   Finding if x is equivalent to a, b, c, d, e, f or g is also Pythonic:  if x in [a,b,c,d,e,f,g]:   But checking if some string x contains either a, b, c, d, e, f or g seems clunky:  if a in x or b in x or c in x or d in x or e in x or f in x or g in x   Is there a more Pythonic method of checking if a string x contains an element of a list?  I know it is trivial to write this myself using a loop or using a regex:  re.search('(dog|cat|bird|mouse|elephant|pig|cow)', x)   but I was wondering if there was a cleaner way that does not involve regex.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Does Python have a cleaner way to express if x contains a|b|c|d? [duplicate]",
        "A_Content": "  without using any but simply max  def is_in(symbol, lst):     return max([symbol in x for x in lst])   print is_in('a',['ae','br','tl']) print is_in('c',['ae','br','tl'])   gives  >>>  True False      ",
        "Language": "Python",
        "Tags": [
            "python",
            "if-statement",
            "python-3.x",
            "substring"
        ],
        "URL": "https://stackoverflow.com/questions/19714041/does-python-have-a-cleaner-way-to-express-if-x-contains-abcd",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Check if a Python list item contains a string inside another string                                        13 answers                                          The Pythonic way to check if a string x is a substring of y is:  if x in y:   Finding if x is equivalent to a, b, c, d, e, f or g is also Pythonic:  if x in [a,b,c,d,e,f,g]:   But checking if some string x contains either a, b, c, d, e, f or g seems clunky:  if a in x or b in x or c in x or d in x or e in x or f in x or g in x   Is there a more Pythonic method of checking if a string x contains an element of a list?  I know it is trivial to write this myself using a loop or using a regex:  re.search('(dog|cat|bird|mouse|elephant|pig|cow)', x)   but I was wondering if there was a cleaner way that does not involve regex.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - convert string to list [duplicate]",
        "A_Content": "  states.split() will return  ['Alaska',  'Alabama',  'Arkansas',  'American',  'Samoa',  'Arizona',  'California',  'Colorado']   If you need one random from them, then you have to use the random module:  import random  states = \"... ...\"  random_state = random.choice(states.split())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/8266529/python-convert-string-to-list",
        "A_Votes": "75",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   String to list in Python       i have a string like this :     states = \"Alaska Alabama Arkansas American Samoa Arizona California Colorado\"   and I want to split it into a list like this     states = {Alaska, Alabama, Arkansas, American, Samoa, ....}   I am new in python.  Help me please. :-))  edit: I need to make random choice from states and make it like variable .     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - convert string to list [duplicate]",
        "A_Content": "  try   states.split()   it returns the list  ['Alaska',  'Alabama',  'Arkansas',  'American',  'Samoa',  'Arizona',  'California',  'Colorado']   and this returns the random element of the list  import random random.choice(states.split())   split statement parses the string and returns the list, by default it's divided into the list by spaces, if you specify the string it's divided by this string, so for example  states.split('Ari')   returns   ['Alaska Alabama Arkansas American Samoa ', 'zona California Colorado']   Btw, list is in python interpretated with [] brackets instead of {} brackets, {} brackets are used for dictionaries, you can read more on this here  I see you are probably new to python, so I'd give you some advice how to use python's great documentation  Almost everything you need can be found here You can use also python included documentation, open python console and write help() If you don't know what to do with some object, I'd install ipython, write statement and press Tab, great tool which helps you with interacting with the language  I just wrote this here to show that python is great tool also because it's great documentation and it's really powerful to know this     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/8266529/python-convert-string-to-list",
        "A_Votes": "49",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   String to list in Python       i have a string like this :     states = \"Alaska Alabama Arkansas American Samoa Arizona California Colorado\"   and I want to split it into a list like this     states = {Alaska, Alabama, Arkansas, American, Samoa, ....}   I am new in python.  Help me please. :-))  edit: I need to make random choice from states and make it like variable .     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - convert string to list [duplicate]",
        "A_Content": "  states = \"Alaska Alabama Arkansas American Samoa Arizona California Colorado\" list = states.split (' ')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/8266529/python-convert-string-to-list",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   String to list in Python       i have a string like this :     states = \"Alaska Alabama Arkansas American Samoa Arizona California Colorado\"   and I want to split it into a list like this     states = {Alaska, Alabama, Arkansas, American, Samoa, ....}   I am new in python.  Help me please. :-))  edit: I need to make random choice from states and make it like variable .     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - convert string to list [duplicate]",
        "A_Content": "  states_list = states.split(' ')   In regards to your edit:  from random import choice random_state = choice(states_list)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/8266529/python-convert-string-to-list",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   String to list in Python       i have a string like this :     states = \"Alaska Alabama Arkansas American Samoa Arizona California Colorado\"   and I want to split it into a list like this     states = {Alaska, Alabama, Arkansas, American, Samoa, ....}   I am new in python.  Help me please. :-))  edit: I need to make random choice from states and make it like variable .     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - convert string to list [duplicate]",
        "A_Content": "  Use string's split() method.  states.split()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/8266529/python-convert-string-to-list",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   String to list in Python       i have a string like this :     states = \"Alaska Alabama Arkansas American Samoa Arizona California Colorado\"   and I want to split it into a list like this     states = {Alaska, Alabama, Arkansas, American, Samoa, ....}   I am new in python.  Help me please. :-))  edit: I need to make random choice from states and make it like variable .     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Check if value already exists within list of dictionaries?",
        "A_Content": "  Here's one way to do it:  if not any(d['main_color'] == 'red' for d in a):     # does not exist   The part in parentheses is a generator expression that returns True for each dictionary that has the key-value pair you are looking for, otherwise False.    If the key could also be missing the above code can give you a KeyError. You can fix this by using get and providing a default value.  if not any(d.get('main_color', None) == 'red' for d in a):     # does not exist      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "dictionary"
        ],
        "URL": "https://stackoverflow.com/questions/3897499/check-if-value-already-exists-within-list-of-dictionaries",
        "A_Votes": "154",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've got a Python list of dictionaries, as follows:  a = [     {'main_color': 'red', 'second_color':'blue'},     {'main_color': 'yellow', 'second_color':'green'},     {'main_color': 'yellow', 'second_color':'blue'}, ]   I'd like to check whether a dictionary with a particular key/value already exists in the list, as follows:  // is a dict with 'main_color'='red' in the list already? // if not: add item      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Check if value already exists within list of dictionaries?",
        "A_Content": "  Maybe this helps:  a = [{ 'main_color': 'red', 'second_color':'blue'},      { 'main_color': 'yellow', 'second_color':'green'},      { 'main_color': 'yellow', 'second_color':'blue'}]  def in_dictlist((key, value), my_dictlist):     for this in my_dictlist:         if this[key] == value:             return this     return {}  print in_dictlist(('main_color','red'), a) print in_dictlist(('main_color','pink'), a)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "dictionary"
        ],
        "URL": "https://stackoverflow.com/questions/3897499/check-if-value-already-exists-within-list-of-dictionaries",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a Python list of dictionaries, as follows:  a = [     {'main_color': 'red', 'second_color':'blue'},     {'main_color': 'yellow', 'second_color':'green'},     {'main_color': 'yellow', 'second_color':'blue'}, ]   I'd like to check whether a dictionary with a particular key/value already exists in the list, as follows:  // is a dict with 'main_color'='red' in the list already? // if not: add item      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Check if value already exists within list of dictionaries?",
        "A_Content": "  Perhaps a function along these lines is what you're after:   def add_unique_to_dict_list(dict_list, key, value):   for d in dict_list:      if key in d:         return d[key]    dict_list.append({ key: value })   return value      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "dictionary"
        ],
        "URL": "https://stackoverflow.com/questions/3897499/check-if-value-already-exists-within-list-of-dictionaries",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a Python list of dictionaries, as follows:  a = [     {'main_color': 'red', 'second_color':'blue'},     {'main_color': 'yellow', 'second_color':'green'},     {'main_color': 'yellow', 'second_color':'blue'}, ]   I'd like to check whether a dictionary with a particular key/value already exists in the list, as follows:  // is a dict with 'main_color'='red' in the list already? // if not: add item      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Chained method calls indentation style in Python [duplicate]",
        "A_Content": "  This is a case where a line continuation character is preferred to open parentheses.  ShortName.objects.distinct() \\          .filter().values()      # looks better   The need for this style becomes more obvious as method names get longer and as methods start taking arguments:  return some_collection.get_objects(locator=l5) \\                       .get_distinct(case_insensitive=True) \\                       .filter(predicate=query(q5)) \\                       .values()   PEP 8 is intend to be interpreted with a measure of common-sense and an eye for both the practical and the beautiful.  Happily violate any PEP 8 guideline that results in ugly or hard to read code.  That being said, if you frequently find yourself at odds with PEP 8, it may be a sign that there are readability issues that transcend your choice of whitespace :-)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "coding-style",
            "pep8"
        ],
        "URL": "https://stackoverflow.com/questions/8683178/chained-method-calls-indentation-style-in-python",
        "A_Votes": "119",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              How to break a line of chained methods in Python?                                        8 answers                                          From reading PEP-8, I get it that you should put the closing parenthesis on the same line as the last argument in function calls:  ShortName.objects.distinct().filter(     product__photo__stockitem__isnull=False)   Probably, long expressions are best to avoid at all. But if it's undesirable, how would you go about multiple chained method calls? Should the closing paren be on a new line?  ShortName.objects.distinct().filter(     product__photo__stockitem__isnull=False ).values_list('value', flat=True)   What about no-arguments methods? How to write them on multiple lines without referencing the intermediate return values?  ShortName.objects.distinct(     ).filter().values() # looks ugly     Update: There's a duplicate question of How to break a line of chained methods in Python?. The accepted answer suggests a familiar from jQuery style of starting each new line with a dot. The author doesn't provide any reasons or authoritative references, so I'd like to get a confirmation on such style or an alternative.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Chained method calls indentation style in Python [duplicate]",
        "A_Content": "  I think the best is to use () to force line joining, and to do this:  (ShortName.objects.distinct() # Look ma!  .filter(product__photo__stickitem__isnull=False) # Comments are allowed  .values_list('value', flat=True))   It's not ideal, but I like that it stands out visually and makes it somewhat obvious what the chain of calls is. It allows end-of-line comments, which \\ newline does not.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "coding-style",
            "pep8"
        ],
        "URL": "https://stackoverflow.com/questions/8683178/chained-method-calls-indentation-style-in-python",
        "A_Votes": "42",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              How to break a line of chained methods in Python?                                        8 answers                                          From reading PEP-8, I get it that you should put the closing parenthesis on the same line as the last argument in function calls:  ShortName.objects.distinct().filter(     product__photo__stockitem__isnull=False)   Probably, long expressions are best to avoid at all. But if it's undesirable, how would you go about multiple chained method calls? Should the closing paren be on a new line?  ShortName.objects.distinct().filter(     product__photo__stockitem__isnull=False ).values_list('value', flat=True)   What about no-arguments methods? How to write them on multiple lines without referencing the intermediate return values?  ShortName.objects.distinct(     ).filter().values() # looks ugly     Update: There's a duplicate question of How to break a line of chained methods in Python?. The accepted answer suggests a familiar from jQuery style of starting each new line with a dot. The author doesn't provide any reasons or authoritative references, so I'd like to get a confirmation on such style or an alternative.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get the first element of each tuple in a list in Python",
        "A_Content": "  Use a list comprehension:  res_list = [x[0] for x in rows]   Below is a demonstration:  >>> rows = [(1, 2), (3, 4), (5, 6)] >>> [x[0] for x in rows] [1, 3, 5] >>>     Alternately, you could use unpacking instead of x[0]:  res_list = [x for x,_ in rows]   Below is a demonstration:  >>> lst = [(1, 2), (3, 4), (5, 6)] >>> [x for x,_ in lst] [1, 3, 5] >>>   Both methods practically do the same thing, so you can choose whichever you like.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/22412258/get-the-first-element-of-each-tuple-in-a-list-in-python",
        "A_Votes": "121",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    An SQL query gives me a list of tuples, like this:  [(elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), ...]   I'd like to have all the first elements of each tuple. Right now I use this:  rows = cur.fetchall() res_list = [] for row in rows:     res_list += [row[0]]   But I think there might be a better syntax to do it. Do you know a better way?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get the first element of each tuple in a list in Python",
        "A_Content": "  If you don't want to use list comprehension by some reasons, you can use map and operator.itemgetter:  >>> from operator import itemgetter >>> rows = [(1, 2), (3, 4), (5, 6)] >>> map(itemgetter(1), rows) [2, 4, 6] >>>      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/22412258/get-the-first-element-of-each-tuple-in-a-list-in-python",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    An SQL query gives me a list of tuples, like this:  [(elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), ...]   I'd like to have all the first elements of each tuple. Right now I use this:  rows = cur.fetchall() res_list = [] for row in rows:     res_list += [row[0]]   But I think there might be a better syntax to do it. Do you know a better way?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get the first element of each tuple in a list in Python",
        "A_Content": "  The functional way of achieving this is to unzip the list using:  sample = [(2, 9), (2, 9), (8, 9), (10, 9), (23, 26), (1, 9), (43, 44)] first,snd = zip(*sample) print first,snd (2, 2, 8, 10, 23, 1, 43) (9, 9, 9, 9, 26, 9, 44)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/22412258/get-the-first-element-of-each-tuple-in-a-list-in-python",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    An SQL query gives me a list of tuples, like this:  [(elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), ...]   I'd like to have all the first elements of each tuple. Right now I use this:  rows = cur.fetchall() res_list = [] for row in rows:     res_list += [row[0]]   But I think there might be a better syntax to do it. Do you know a better way?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get the first element of each tuple in a list in Python",
        "A_Content": "  You can use list comprehension:  res_list = [i[0] for i in rows]   This should make the trick     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/22412258/get-the-first-element-of-each-tuple-in-a-list-in-python",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    An SQL query gives me a list of tuples, like this:  [(elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), ...]   I'd like to have all the first elements of each tuple. Right now I use this:  rows = cur.fetchall() res_list = [] for row in rows:     res_list += [row[0]]   But I think there might be a better syntax to do it. Do you know a better way?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get the first element of each tuple in a list in Python",
        "A_Content": "  res_list = [x[0] for x in rows]   c.f. http://docs.python.org/3/tutorial/datastructures.html#list-comprehensions  For a discussion on why to prefer comprehensions over higher-order functions such as map, go to http://www.artima.com/weblogs/viewpost.jsp?thread=98196.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/22412258/get-the-first-element-of-each-tuple-in-a-list-in-python",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    An SQL query gives me a list of tuples, like this:  [(elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), (elt1, elt2), ...]   I'd like to have all the first elements of each tuple. Right now I use this:  rows = cur.fetchall() res_list = [] for row in rows:     res_list += [row[0]]   But I think there might be a better syntax to do it. Do you know a better way?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  Update July 2016  The easiest way to use batch normalization in TensorFlow is through the higher-level interfaces provided in either contrib/layers, tflearn, or slim.  Previous answer if you want to DIY: The documentation string for this has improved since the release - see the docs comment in the master branch instead of the one you found.  It clarifies, in particular, that it's the output from tf.nn.moments.  You can see a very simple example of its use in the batch_norm test code.  For a more real-world use example, I've included below the helper class and use notes that I scribbled up for my own use (no warranty provided!):  \"\"\"A helper class for managing batch normalization state.                     This class is designed to simplify adding batch normalization                (http://arxiv.org/pdf/1502.03167v3.pdf) to your model by                     managing the state variables associated with it.                              Important use note:  The function get_assigner() returns                     an op that must be executed to save the updated state.                       A suggested way to do this is to make execution of the                       model optimizer force it, e.g., by:                                             update_assignments = tf.group(bn1.get_assigner(),                                                          bn2.get_assigner())                            with tf.control_dependencies([optimizer]):                                     optimizer = tf.group(update_assignments)                                  \"\"\"  import tensorflow as tf   class ConvolutionalBatchNormalizer(object):   \"\"\"Helper class that groups the normalization logic and variables.            Use:                                                                             ewma = tf.train.ExponentialMovingAverage(decay=0.99)                         bn = ConvolutionalBatchNormalizer(depth, 0.001, ewma, True)                  update_assignments = bn.get_assigner()                                       x = bn.normalize(y, train=training?)                                         (the output x will be batch-normalized).                                 \"\"\"    def __init__(self, depth, epsilon, ewma_trainer, scale_after_norm):     self.mean = tf.Variable(tf.constant(0.0, shape=[depth]),                             trainable=False)     self.variance = tf.Variable(tf.constant(1.0, shape=[depth]),                                 trainable=False)     self.beta = tf.Variable(tf.constant(0.0, shape=[depth]))     self.gamma = tf.Variable(tf.constant(1.0, shape=[depth]))     self.ewma_trainer = ewma_trainer     self.epsilon = epsilon     self.scale_after_norm = scale_after_norm    def get_assigner(self):     \"\"\"Returns an EWMA apply op that must be invoked after optimization.\"\"\"     return self.ewma_trainer.apply([self.mean, self.variance])    def normalize(self, x, train=True):     \"\"\"Returns a batch-normalized version of x.\"\"\"     if train:       mean, variance = tf.nn.moments(x, [0, 1, 2])       assign_mean = self.mean.assign(mean)       assign_variance = self.variance.assign(variance)       with tf.control_dependencies([assign_mean, assign_variance]):         return tf.nn.batch_norm_with_global_normalization(             x, mean, variance, self.beta, self.gamma,             self.epsilon, self.scale_after_norm)     else:       mean = self.ewma_trainer.average(self.mean)       variance = self.ewma_trainer.average(self.variance)       local_beta = tf.identity(self.beta)       local_gamma = tf.identity(self.gamma)       return tf.nn.batch_norm_with_global_normalization(           x, mean, variance, local_beta, local_gamma,           self.epsilon, self.scale_after_norm)   Note that I called it a ConvolutionalBatchNormalizer because it pins the use of tf.nn.moments to sum across axes 0, 1, and 2, whereas for non-convolutional use you might only want axis 0.  Feedback appreciated if you use it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "53",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  As of TensorFlow 1.0 (February 2017) there's also the high-level tf.layers.batch_normalization API included in TensorFlow itself.  It's super simple to use:  # Set this to True for training and False for testing training = tf.placeholder(tf.bool)  x = tf.layers.dense(input_x, units=100) x = tf.layers.batch_normalization(x, training=training) x = tf.nn.relu(x)   ...except that it adds extra ops to the graph (for updating its mean and variance variables) in such a way that they won't be dependencies of your training op. You can either just run the ops separately:  extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) sess.run([train_op, extra_update_ops], ...)   or add the update ops as dependencies of your training op manually, then just run your training op as normal:  extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) with tf.control_dependencies(extra_update_ops):     train_op = optimizer.minimize(loss) ... sess.run([train_op], ...)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "39",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  The following works fine for me, it does not require invoking EMA-apply outside.  import numpy as np import tensorflow as tf from tensorflow.python import control_flow_ops  def batch_norm(x, n_out, phase_train, scope='bn'):     \"\"\"     Batch normalization on convolutional maps.     Args:         x:           Tensor, 4D BHWD input maps         n_out:       integer, depth of input maps         phase_train: boolean tf.Varialbe, true indicates training phase         scope:       string, variable scope     Return:         normed:      batch-normalized maps     \"\"\"     with tf.variable_scope(scope):         beta = tf.Variable(tf.constant(0.0, shape=[n_out]),                                      name='beta', trainable=True)         gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),                                       name='gamma', trainable=True)         batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')         ema = tf.train.ExponentialMovingAverage(decay=0.5)          def mean_var_with_update():             ema_apply_op = ema.apply([batch_mean, batch_var])             with tf.control_dependencies([ema_apply_op]):                 return tf.identity(batch_mean), tf.identity(batch_var)          mean, var = tf.cond(phase_train,                             mean_var_with_update,                             lambda: (ema.average(batch_mean), ema.average(batch_var)))         normed = tf.nn.batch_normalization(x, mean, var, beta, gamma, 1e-3)     return normed   Example:  import math  n_in, n_out = 3, 16 ksize = 3 stride = 1 phase_train = tf.placeholder(tf.bool, name='phase_train') input_image = tf.placeholder(tf.float32, name='input_image') kernel = tf.Variable(tf.truncated_normal([ksize, ksize, n_in, n_out],                                    stddev=math.sqrt(2.0/(ksize*ksize*n_out))),                                    name='kernel') conv = tf.nn.conv2d(input_image, kernel, [1,stride,stride,1], padding='SAME') conv_bn = batch_norm(conv, n_out, phase_train) relu = tf.nn.relu(conv_bn)  with tf.Session() as session:     session.run(tf.initialize_all_variables())     for i in range(20):         test_image = np.random.rand(4,32,32,3)         sess_outputs = session.run([relu],           {input_image.name: test_image, phase_train.name: True})      ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "32",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  There is also an \"official\" batch normalization layer coded by the developers. They don't have very good docs on how to use it but here is how to use it (according to me):  from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm  def batch_norm_layer(x,train_phase,scope_bn):     bn_train = batch_norm(x, decay=0.999, center=True, scale=True,     updates_collections=None,     is_training=True,     reuse=None, # is this right?     trainable=True,     scope=scope_bn)     bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,     updates_collections=None,     is_training=False,     reuse=True, # is this right?     trainable=True,     scope=scope_bn)     z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)     return z   to actually use it you need to create a placeholder for train_phase that indicates if you are in training or inference phase (as in train_phase = tf.placeholder(tf.bool, name='phase_train')). Its value can be filled during inference or training with a tf.session as in:  test_error = sess.run(fetches=cross_entropy, feed_dict={x: batch_xtest, y_:batch_ytest, train_phase: False})   or during training:  sess.run(fetches=train_step, feed_dict={x: batch_xs, y_:batch_ys, train_phase: True})   I'm pretty sure this is correct according to the discussion in github.    Seems there is another useful link:  http://r2rt.com/implementing-batch-normalization-in-tensorflow.html     ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  You can simply use the build-in batch_norm layer:  batch_norm = tf.cond(is_train,      lambda: tf.contrib.layers.batch_norm(prev, activation_fn=tf.nn.relu, is_training=True, reuse=None),     lambda: tf.contrib.layers.batch_norm(prev, activation_fn =tf.nn.relu, is_training=False, reuse=True))   where prev is the output of your previous layer (can be both fully-connected or a convolutional layer) and is_train is a boolean placeholder. Just use batch_norm as the input to the next layer, then.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  Since someone recently edited this, I'd like to clarify that this is no longer an issue.  This answer does not seem correct  When phase_train is set to false, it still updates the ema mean and variance. This can be verified with the following code snippet.  x = tf.placeholder(tf.float32, [None, 20, 20, 10], name='input') phase_train = tf.placeholder(tf.bool, name='phase_train')  # generate random noise to pass into batch norm x_gen = tf.random_normal([50,20,20,10]) pt_false = tf.Variable(tf.constant(True))  #generate a constant variable to pass into batch norm y = x_gen.eval()  [bn, bn_vars] = batch_norm(x, 10, phase_train)  tf.initialize_all_variables().run() train_step = lambda: bn.eval({x:x_gen.eval(), phase_train:True}) test_step = lambda: bn.eval({x:y, phase_train:False}) test_step_c = lambda: bn.eval({x:y, phase_train:True})  # Verify that this is different as expected, two different x's have different norms print(train_step()[0][0][0]) print(train_step()[0][0][0])  # Verify that this is same as expected, same x's (y) have same norm print(train_step_c()[0][0][0]) print(train_step_c()[0][0][0])  # THIS IS DIFFERENT but should be they same, should only be reading from the ema. print(test_step()[0][0][0]) print(test_step()[0][0][0])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  Using TensorFlow built-in batch_norm layer, below is the code to load data, build a network with one hidden ReLU layer and L2 normalization and introduce batch normalization for both hidden and out layer. This runs fine and trains fine. Just FYI this example is mostly built upon the data and code from Udacity DeepLearning course. P.S. Yes, parts of it were discussed one way or another in answers earlier but I decided to gather in one code snippet everything so that you have example of whole network training process with Batch Normalization and its evaluation  # These are all the modules we'll be using later. Make sure you can import them # before proceeding further. from __future__ import print_function import numpy as np import tensorflow as tf from six.moves import cPickle as pickle  pickle_file = '/home/maxkhk/Documents/Udacity/DeepLearningCourse/SourceCode/tensorflow/examples/udacity/notMNIST.pickle'  with open(pickle_file, 'rb') as f:   save = pickle.load(f)   train_dataset = save['train_dataset']   train_labels = save['train_labels']   valid_dataset = save['valid_dataset']   valid_labels = save['valid_labels']   test_dataset = save['test_dataset']   test_labels = save['test_labels']   del save  # hint to help gc free up memory   print('Training set', train_dataset.shape, train_labels.shape)   print('Validation set', valid_dataset.shape, valid_labels.shape)   print('Test set', test_dataset.shape, test_labels.shape)  image_size = 28 num_labels = 10  def reformat(dataset, labels):   dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)   # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]   labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)   return dataset, labels train_dataset, train_labels = reformat(train_dataset, train_labels) valid_dataset, valid_labels = reformat(valid_dataset, valid_labels) test_dataset, test_labels = reformat(test_dataset, test_labels) print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape)   def accuracy(predictions, labels):   return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))           / predictions.shape[0])   #for NeuralNetwork model code is below #We will use SGD for training to save our time. Code is from Assignment 2 #beta is the new parameter - controls level of regularization. #Feel free to play with it - the best one I found is 0.001 #notice, we introduce L2 for both biases and weights of all layers  batch_size = 128 beta = 0.001  #building tensorflow graph graph = tf.Graph() with graph.as_default():       # Input data. For the training data, we use a placeholder that will be fed   # at run time with a training minibatch.   tf_train_dataset = tf.placeholder(tf.float32,                                     shape=(batch_size, image_size * image_size))   tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))   tf_valid_dataset = tf.constant(valid_dataset)   tf_test_dataset = tf.constant(test_dataset)    #introduce batchnorm   tf_train_dataset_bn = tf.contrib.layers.batch_norm(tf_train_dataset)     #now let's build our new hidden layer   #that's how many hidden neurons we want   num_hidden_neurons = 1024   #its weights   hidden_weights = tf.Variable(     tf.truncated_normal([image_size * image_size, num_hidden_neurons]))   hidden_biases = tf.Variable(tf.zeros([num_hidden_neurons]))    #now the layer itself. It multiplies data by weights, adds biases   #and takes ReLU over result   hidden_layer = tf.nn.relu(tf.matmul(tf_train_dataset_bn, hidden_weights) + hidden_biases)    #adding the batch normalization layerhi()   hidden_layer_bn = tf.contrib.layers.batch_norm(hidden_layer)    #time to go for output linear layer   #out weights connect hidden neurons to output labels   #biases are added to output labels     out_weights = tf.Variable(     tf.truncated_normal([num_hidden_neurons, num_labels]))      out_biases = tf.Variable(tf.zeros([num_labels]))      #compute output     out_layer = tf.matmul(hidden_layer_bn,out_weights) + out_biases   #our real output is a softmax of prior result   #and we also compute its cross-entropy to get our loss   #Notice - we introduce our L2 here   loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(     out_layer, tf_train_labels) +     beta*tf.nn.l2_loss(hidden_weights) +     beta*tf.nn.l2_loss(hidden_biases) +     beta*tf.nn.l2_loss(out_weights) +     beta*tf.nn.l2_loss(out_biases)))    #now we just minimize this loss to actually train the network   optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)    #nice, now let's calculate the predictions on each dataset for evaluating the   #performance so far   # Predictions for the training, validation, and test data.   train_prediction = tf.nn.softmax(out_layer)   valid_relu = tf.nn.relu(  tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)   valid_prediction = tf.nn.softmax( tf.matmul(valid_relu, out_weights) + out_biases)     test_relu = tf.nn.relu( tf.matmul( tf_test_dataset, hidden_weights) + hidden_biases)   test_prediction = tf.nn.softmax(tf.matmul(test_relu, out_weights) + out_biases)    #now is the actual training on the ANN we built #we will run it for some number of steps and evaluate the progress after  #every 500 steps  #number of steps we will train our ANN num_steps = 3001  #actual training with tf.Session(graph=graph) as session:   tf.initialize_all_variables().run()   print(\"Initialized\")   for step in range(num_steps):     # Pick an offset within the training data, which has been randomized.     # Note: we could use better randomization across epochs.     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)     # Generate a minibatch.     batch_data = train_dataset[offset:(offset + batch_size), :]     batch_labels = train_labels[offset:(offset + batch_size), :]     # Prepare a dictionary telling the session where to feed the minibatch.     # The key of the dictionary is the placeholder node of the graph to be fed,     # and the value is the numpy array to feed to it.     feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}     _, l, predictions = session.run(       [optimizer, loss, train_prediction], feed_dict=feed_dict)     if (step % 500 == 0):       print(\"Minibatch loss at step %d: %f\" % (step, l))       print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))       print(\"Validation accuracy: %.1f%%\" % accuracy(         valid_prediction.eval(), valid_labels))       print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How could I use batch normalization in TensorFlow?",
        "A_Content": "  So a simple example of the use of this batchnorm class:  from bn_class import *  with tf.name_scope('Batch_norm_conv1') as scope:     ewma = tf.train.ExponentialMovingAverage(decay=0.99)                       bn_conv1 = ConvolutionalBatchNormalizer(num_filt_1, 0.001, ewma, True)                update_assignments = bn_conv1.get_assigner()      a_conv1 = bn_conv1.normalize(a_conv1, train=bn_train)      h_conv1 = tf.nn.relu(a_conv1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to use batch normalization in TensorFlow. I found the related C++ source code in core/ops/nn_ops.cc. However, I did not find it documented on tensorflow.org.  BN has different semantics in MLP and CNN, so I am not sure what exactly this BN does.  I did not find a method called MovingMoments either.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python extending with - using super() Python 3 vs Python 2",
        "A_Content": "   super() (without arguments) was introduced in Python 3 (along with __class__):  super() -> same as super(__class__, self)   so that would be the Python 2 equivalent for new-style classes:  super(CurrentClass, self)  for old-style classes you can always use:   class Classname(OldStyleParent):     def __init__(self, *args, **kwargs):         OldStyleParent.__init__(self, *args, **kwargs)       ",
        "Language": "Python",
        "Tags": [
            "python",
            "inheritance",
            "configparser"
        ],
        "URL": "https://stackoverflow.com/questions/10482953/python-extending-with-using-super-python-3-vs-python-2",
        "A_Votes": "120",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Originally I wanted to ask this question, but then I found it was already thought of before...   Googling around I found this example of extending configparser. The following works with Python 3:  $ python3 Python 3.2.3rc2 (default, Mar 21 2012, 06:59:51)  [GCC 4.6.3] on linux2 >>> from configparser import  SafeConfigParser >>> class AmritaConfigParser(SafeConfigParser): ...     def __init_(self): ...         super().__init__() ...  >>> cfg = AmritaConfigParser()   But not with Python 2:  >>> class AmritaConfigParser(SafeConfigParser): ...       def __init__(self): ...           super(SafeConfigParser).init() ...  >>> cfg = AmritaConfigParser() Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"<stdin>\", line 3, in __init__ TypeError: must be type, not classob   Then I read a little bit on Python New Class vs. Old Class styles (e.g. here. And now I am wondering, I can do:  class MyConfigParser(ConfigParser.ConfigParser):       def Write(self, fp):           \"\"\"override the module's original write funcition\"\"\"           ....       def MyWrite(self, fp):           \"\"\"Define new function and inherit all others\"\"\"   But, shouldn't I call init? Is this in Python 2 the equivalent:   class AmritaConfigParser(ConfigParser.SafeConfigParser):     #def __init__(self):     #    super().__init__() # Python3 syntax, or rather, new style class syntax ...     #     # is this the equivalent of the above ?      def __init__(self):         ConfigParser.SafeConfigParser.__init__(self)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python extending with - using super() Python 3 vs Python 2",
        "A_Content": "  In a single inheritance case (when you subclass one class only), your new class inherits methods of the base class. This includes __init__. So if you don't define it in your class, you will get the one from the base.  Things start being complicated if you introduce multiple inheritance (subclassing more than one class at a time). This is because if more than one base class has __init__, your class will inherit the first one only.  In such cases, you should really use super if you can, I'll explain why. But not always you can. The problem is that all your base classes must also use it (and their base classes as well -- the whole tree).  If that is the case, then this will also work correctly (in Python 3 but you could rework it into Python 2 -- it also has super):  class A:     def __init__(self):         print('A')         super().__init__()  class B:     def __init__(self):         print('B')         super().__init__()  class C(A, B):     pass  C() #prints: #A #B   Notice how both base classes use super even though they don't have their own base classes.  What super does is: it calls the method from the next class in MRO (method resolution order). The MRO for C is: (C, A, B, object). You can print C.__mro__ to see it.  So, C inherits __init__ from A and super in A.__init__ calls B.__init__ (B follows A in MRO).  So by doing nothing in C, you end up calling both, which is what you want.  Now if you were not using super, you would end up inheriting A.__init__ (as before) but this time there's nothing that would call B.__init__ for you.  class A:     def __init__(self):         print('A')  class B:     def __init__(self):         print('B')  class C(A, B):     pass  C() #prints: #A   To fix that you have to define C.__init__:  class C(A, B):     def __init__(self):         A.__init__(self)         B.__init__(self)   The problem with that is that in more complicated MI trees, __init__ methods of some classes may end up being called more than once whereas super/MRO guarantee that they're called just once.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "inheritance",
            "configparser"
        ],
        "URL": "https://stackoverflow.com/questions/10482953/python-extending-with-using-super-python-3-vs-python-2",
        "A_Votes": "34",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Originally I wanted to ask this question, but then I found it was already thought of before...   Googling around I found this example of extending configparser. The following works with Python 3:  $ python3 Python 3.2.3rc2 (default, Mar 21 2012, 06:59:51)  [GCC 4.6.3] on linux2 >>> from configparser import  SafeConfigParser >>> class AmritaConfigParser(SafeConfigParser): ...     def __init_(self): ...         super().__init__() ...  >>> cfg = AmritaConfigParser()   But not with Python 2:  >>> class AmritaConfigParser(SafeConfigParser): ...       def __init__(self): ...           super(SafeConfigParser).init() ...  >>> cfg = AmritaConfigParser() Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"<stdin>\", line 3, in __init__ TypeError: must be type, not classob   Then I read a little bit on Python New Class vs. Old Class styles (e.g. here. And now I am wondering, I can do:  class MyConfigParser(ConfigParser.ConfigParser):       def Write(self, fp):           \"\"\"override the module's original write funcition\"\"\"           ....       def MyWrite(self, fp):           \"\"\"Define new function and inherit all others\"\"\"   But, shouldn't I call init? Is this in Python 2 the equivalent:   class AmritaConfigParser(ConfigParser.SafeConfigParser):     #def __init__(self):     #    super().__init__() # Python3 syntax, or rather, new style class syntax ...     #     # is this the equivalent of the above ?      def __init__(self):         ConfigParser.SafeConfigParser.__init__(self)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python extending with - using super() Python 3 vs Python 2",
        "A_Content": "  In short, they are equivalent.  Let's have a history view:  (1) at first, the function looks like this.      class MySubClass(MySuperClass):         def __init__(self):             MySuperClass.__init__(self)   (2) to make code more abstract (and more portable). A common method to get Super-Class is invented like:      super(<class>, <instance>)   And init function can be:      class MySubClassBetter(MySuperClass):         def __init__(self):             super(MySubClassBetter, self).__init__()   However  requiring an explicit passing of both the class and instance break the DRY (Don't Repeat Yourself) rule a bit.   (3) in V3. It is more smart,       super()   is enough in most case. You can refer to http://www.python.org/dev/peps/pep-3135/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "inheritance",
            "configparser"
        ],
        "URL": "https://stackoverflow.com/questions/10482953/python-extending-with-using-super-python-3-vs-python-2",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Originally I wanted to ask this question, but then I found it was already thought of before...   Googling around I found this example of extending configparser. The following works with Python 3:  $ python3 Python 3.2.3rc2 (default, Mar 21 2012, 06:59:51)  [GCC 4.6.3] on linux2 >>> from configparser import  SafeConfigParser >>> class AmritaConfigParser(SafeConfigParser): ...     def __init_(self): ...         super().__init__() ...  >>> cfg = AmritaConfigParser()   But not with Python 2:  >>> class AmritaConfigParser(SafeConfigParser): ...       def __init__(self): ...           super(SafeConfigParser).init() ...  >>> cfg = AmritaConfigParser() Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"<stdin>\", line 3, in __init__ TypeError: must be type, not classob   Then I read a little bit on Python New Class vs. Old Class styles (e.g. here. And now I am wondering, I can do:  class MyConfigParser(ConfigParser.ConfigParser):       def Write(self, fp):           \"\"\"override the module's original write funcition\"\"\"           ....       def MyWrite(self, fp):           \"\"\"Define new function and inherit all others\"\"\"   But, shouldn't I call init? Is this in Python 2 the equivalent:   class AmritaConfigParser(ConfigParser.SafeConfigParser):     #def __init__(self):     #    super().__init__() # Python3 syntax, or rather, new style class syntax ...     #     # is this the equivalent of the above ?      def __init__(self):         ConfigParser.SafeConfigParser.__init__(self)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python extending with - using super() Python 3 vs Python 2",
        "A_Content": "  Just to have a simple and complete example for Python 3, which most people seem to be using now.  class MySuper(object):     def __init__(self,a):         self.a = a  class MySub(MySuper):     def __init__(self,a,b):         self.b = b         super().__init__(a)  my_sub = MySub(42,'chickenman') print(my_sub.a) print(my_sub.b)   gives  42 chickenman      ",
        "Language": "Python",
        "Tags": [
            "python",
            "inheritance",
            "configparser"
        ],
        "URL": "https://stackoverflow.com/questions/10482953/python-extending-with-using-super-python-3-vs-python-2",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Originally I wanted to ask this question, but then I found it was already thought of before...   Googling around I found this example of extending configparser. The following works with Python 3:  $ python3 Python 3.2.3rc2 (default, Mar 21 2012, 06:59:51)  [GCC 4.6.3] on linux2 >>> from configparser import  SafeConfigParser >>> class AmritaConfigParser(SafeConfigParser): ...     def __init_(self): ...         super().__init__() ...  >>> cfg = AmritaConfigParser()   But not with Python 2:  >>> class AmritaConfigParser(SafeConfigParser): ...       def __init__(self): ...           super(SafeConfigParser).init() ...  >>> cfg = AmritaConfigParser() Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"<stdin>\", line 3, in __init__ TypeError: must be type, not classob   Then I read a little bit on Python New Class vs. Old Class styles (e.g. here. And now I am wondering, I can do:  class MyConfigParser(ConfigParser.ConfigParser):       def Write(self, fp):           \"\"\"override the module's original write funcition\"\"\"           ....       def MyWrite(self, fp):           \"\"\"Define new function and inherit all others\"\"\"   But, shouldn't I call init? Is this in Python 2 the equivalent:   class AmritaConfigParser(ConfigParser.SafeConfigParser):     #def __init__(self):     #    super().__init__() # Python3 syntax, or rather, new style class syntax ...     #     # is this the equivalent of the above ?      def __init__(self):         ConfigParser.SafeConfigParser.__init__(self)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python extending with - using super() Python 3 vs Python 2",
        "A_Content": "  Another python3 implementation that involves the use of Abstract classes with super().  You should remember that        super().init(name, 10)   has the same effect as       Person.init(self, name, 10)   Remember there's a hidden 'self' in super(), So the same object passes on to the superclass init method and the attributes are added to the object that called it.  Hence super()gets translated to  Person and then if you include the hidden self, you get the above code frag.    from abc import ABCMeta, abstractmethod class Person(metaclass=ABCMeta):     name = \"\"     age = 0      def __init__(self, personName, personAge):         self.name = personName         self.age = personAge      @abstractmethod     def showName(self):         pass      @abstractmethod     def showAge(self):         pass   class Man(Person):      def __init__(self, name, height):         self.height = height         # Person.__init__(self, name, 10)         super().__init__(name, 10)  # same as Person.__init__(self, name, 10)         # basically used to call the superclass init . This is used incase you want to call subclass init         # and then also call superclass's init.         # Since there's a hidden self in the super's parameters, when it's is called,         # the superclasses attributes are a part of the same object that was sent out in the super() method      def showIdentity(self):         return self.name, self.age, self.height      def showName(self):         pass      def showAge(self):         pass   a = Man(\"piyush\", \"179\") print(a.showIdentity())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "inheritance",
            "configparser"
        ],
        "URL": "https://stackoverflow.com/questions/10482953/python-extending-with-using-super-python-3-vs-python-2",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Originally I wanted to ask this question, but then I found it was already thought of before...   Googling around I found this example of extending configparser. The following works with Python 3:  $ python3 Python 3.2.3rc2 (default, Mar 21 2012, 06:59:51)  [GCC 4.6.3] on linux2 >>> from configparser import  SafeConfigParser >>> class AmritaConfigParser(SafeConfigParser): ...     def __init_(self): ...         super().__init__() ...  >>> cfg = AmritaConfigParser()   But not with Python 2:  >>> class AmritaConfigParser(SafeConfigParser): ...       def __init__(self): ...           super(SafeConfigParser).init() ...  >>> cfg = AmritaConfigParser() Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"<stdin>\", line 3, in __init__ TypeError: must be type, not classob   Then I read a little bit on Python New Class vs. Old Class styles (e.g. here. And now I am wondering, I can do:  class MyConfigParser(ConfigParser.ConfigParser):       def Write(self, fp):           \"\"\"override the module's original write funcition\"\"\"           ....       def MyWrite(self, fp):           \"\"\"Define new function and inherit all others\"\"\"   But, shouldn't I call init? Is this in Python 2 the equivalent:   class AmritaConfigParser(ConfigParser.SafeConfigParser):     #def __init__(self):     #    super().__init__() # Python3 syntax, or rather, new style class syntax ...     #     # is this the equivalent of the above ?      def __init__(self):         ConfigParser.SafeConfigParser.__init__(self)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Add column to dataframe with default value",
        "A_Content": "  df['Name']='abc' will add the new column and set all rows to that value:  In [79]:  df Out[79]:          Date, Open, High,  Low,  Close 0  01-01-2015,  565,  600,  400,    450 In [80]:  df['Name'] = 'abc' df Out[80]:          Date, Open, High,  Low,  Close Name 0  01-01-2015,  565,  600,  400,    450  abc      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/29517072/add-column-to-dataframe-with-default-value",
        "A_Votes": "113",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.  Existing df:  Date, Open, High, Low, Close 01-01-2015, 565, 600, 400, 450   New df:  Name, Date, Open, High, Low, Close abc, 01-01-2015, 565, 600, 400, 450   I know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.  Im not entirely sure how to do that.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Add column to dataframe with default value",
        "A_Content": "  You can use insert to specify where you want to new column to be.  In this case, I use 0 to place the new column at the left.  df.insert(0, 'Name', 'abc')    Name        Date  Open  High  Low  Close 0  abc  01-01-2015   565   600  400    450      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/29517072/add-column-to-dataframe-with-default-value",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.  Existing df:  Date, Open, High, Low, Close 01-01-2015, 565, 600, 400, 450   New df:  Name, Date, Open, High, Low, Close abc, 01-01-2015, 565, 600, 400, 450   I know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.  Im not entirely sure how to do that.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Add column to dataframe with default value",
        "A_Content": "  Single liner works  df['Name'] = 'abc'   Creates a Name column and sets all rows to abc value     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/29517072/add-column-to-dataframe-with-default-value",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.  Existing df:  Date, Open, High, Low, Close 01-01-2015, 565, 600, 400, 450   New df:  Name, Date, Open, High, Low, Close abc, 01-01-2015, 565, 600, 400, 450   I know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.  Im not entirely sure how to do that.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Add column to dataframe with default value",
        "A_Content": "  Summing up what the others have suggested, and adding a third way  You can:    assign(**kwargs):  df.assign(Name='abc')  access the new column series (it will be created) and set it:  df['Name'] = 'abc'  insert(loc, column, value, allow_duplicates=False)  df.insert(0, 'Name', 'abc')   where the argument loc ( 0 <= loc <= len(columns) ) allows you to insert the column where you want.   'loc' gives you the index that your column will be at after the insertion. For example, the code above inserts the column Name as the 0-th column, i.e. it will be inserted before the first column, becoming the new first column. (Indexing starts from 0).   All these methods allow you to add a new column from a Series as well (just substitute the 'abc' default argument above with the series).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/29517072/add-column-to-dataframe-with-default-value",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.  Existing df:  Date, Open, High, Low, Close 01-01-2015, 565, 600, 400, 450   New df:  Name, Date, Open, High, Low, Close abc, 01-01-2015, 565, 600, 400, 450   I know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.  Im not entirely sure how to do that.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I gzip compress a string in Python?",
        "A_Content": "  Pick a suitable module from http://docs.python.org/library/archiving.html -- either gzip or zlib, depending on your exact needs.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "compression",
            "gzip"
        ],
        "URL": "https://stackoverflow.com/questions/8506897/how-do-i-gzip-compress-a-string-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How do I gzip compress a string in Python?  gzip.GzipFile exists, but that's for file objects - what about with plain strings?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I gzip compress a string in Python?",
        "A_Content": "  If you want to produce a complete gzip-compatible binary string, with the header etc, you could use gzip.GzipFile together with StringIO:  import StringIO import gzip out = StringIO.StringIO() with gzip.GzipFile(fileobj=out, mode=\"w\") as f:   f.write(\"This is mike number one, isn't this a lot of fun?\") out.getvalue()  # returns '\\x1f\\x8b\\x08\\x00\\xbd\\xbe\\xe8N\\x02\\xff\\x0b\\xc9\\xc8,V\\x00\\xa2\\xdc\\xcc\\xecT\\x85\\xbc\\xd2\\xdc\\xa4\\xd4\"\\x85\\xfc\\xbcT\\x1d\\xa0X\\x9ez\\x89B\\tH:Q!\\'\\xbfD!?M!\\xad4\\xcf\\x1e\\x00w\\xd4\\xea\\xf41\\x00\\x00\\x00'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "compression",
            "gzip"
        ],
        "URL": "https://stackoverflow.com/questions/8506897/how-do-i-gzip-compress-a-string-in-python",
        "A_Votes": "132",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I gzip compress a string in Python?  gzip.GzipFile exists, but that's for file objects - what about with plain strings?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I gzip compress a string in Python?",
        "A_Content": "  The easiest way is the zlib encoding:  compressed_value = s.encode(\"zlib\")   Then you decompress it with:  plain_string_again = compressed_value.decode(\"zlib\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "compression",
            "gzip"
        ],
        "URL": "https://stackoverflow.com/questions/8506897/how-do-i-gzip-compress-a-string-in-python",
        "A_Votes": "54",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I gzip compress a string in Python?  gzip.GzipFile exists, but that's for file objects - what about with plain strings?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I gzip compress a string in Python?",
        "A_Content": "  For those who want to compress a Pandas dataframe in JSON format:  Tested with Python 3.6 and Pandas 0.23  import sys import zlib, lzma, bz2 import math  def convert_size(size_bytes):     if size_bytes == 0:         return \"0B\"     size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")     i = int(math.floor(math.log(size_bytes, 1024)))     p = math.pow(1024, i)     s = round(size_bytes / p, 2)     return \"%s %s\" % (s, size_name[i])  dataframe = pd.read_csv('...') # your CSV file dataframe_json = dataframe.to_json(orient='split') data = dataframe_json.encode() compressed_data = bz2.compress(data) decompressed_data = bz2.decompress(compressed_data).decode() dataframe_aux = pd.read_json(decompressed_data, orient='split')  #Original data size:  10982455 10.47 MB #Encoded data size:  10982439 10.47 MB #Compressed data size:  1276457 1.22 MB (lzma, slow), 2087131 1.99 MB (zlib, fast), 1410908 1.35 MB (bz2, fast) #Decompressed data size:  10982455 10.47 MB print('Original data size: ', sys.getsizeof(dataframe_json), convert_size(sys.getsizeof(dataframe_json))) print('Encoded data size: ', sys.getsizeof(data), convert_size(sys.getsizeof(data))) print('Compressed data size: ', sys.getsizeof(compressed_data), convert_size(sys.getsizeof(compressed_data))) print('Decompressed data size: ', sys.getsizeof(decompressed_data), convert_size(sys.getsizeof(decompressed_data)))  print(dataframe.head()) print(dataframe_aux.head())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "compression",
            "gzip"
        ],
        "URL": "https://stackoverflow.com/questions/8506897/how-do-i-gzip-compress-a-string-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I gzip compress a string in Python?  gzip.GzipFile exists, but that's for file objects - what about with plain strings?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Fastest way to convert a dict's keys & values from `unicode` to `str`?",
        "A_Content": "  DATA = { u'spam': u'eggs', u'foo': frozenset([u'Gah!']), u'bar': { u'baz': 97 },          u'list': [u'list', (True, u'Maybe'), set([u'and', u'a', u'set', 1])]}  def convert(data):     if isinstance(data, basestring):         return str(data)     elif isinstance(data, collections.Mapping):         return dict(map(convert, data.iteritems()))     elif isinstance(data, collections.Iterable):         return type(data)(map(convert, data))     else:         return data  print DATA print convert(DATA) # Prints: # {u'list': [u'list', (True, u'Maybe'), set([u'and', u'a', u'set', 1])], u'foo': frozenset([u'Gah!']), u'bar': {u'baz': 97}, u'spam': u'eggs'} # {'bar': {'baz': 97}, 'foo': frozenset(['Gah!']), 'list': ['list', (True, 'Maybe'), set(['and', 'a', 'set', 1])], 'spam': 'eggs'}   Assumptions:   You've imported the collections module and can make use of the abstract base classes it provides You're happy to convert using the default encoding (use data.encode('utf-8') rather than str(data) if you need an explicit encoding).   If you need to support other container types, hopefully it's obvious how to follow the pattern and add cases for them.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "casting",
            "types"
        ],
        "URL": "https://stackoverflow.com/questions/1254454/fastest-way-to-convert-a-dicts-keys-values-from-unicode-to-str",
        "A_Votes": "141",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm receiving a dict from one \"layer\" of code upon which some calculations/modifications are performed before passing it onto another \"layer\". The original dict's keys & \"string\" values are unicode, but the layer they're being passed onto only accepts str.  This is going to be called often, so I'd like to know what would be the fastest way to convert something like:  { u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } }   ...to:  { 'spam': 'eggs', 'foo': True, 'bar': { 'baz': 97 } }   ...bearing in mind the non-\"string\" values need to stay as their original type.  Any thoughts?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Fastest way to convert a dict's keys & values from `unicode` to `str`?",
        "A_Content": "  I know I'm late on this one:  def convert_keys_to_string(dictionary):     \"\"\"Recursively converts dictionary keys to strings.\"\"\"     if not isinstance(dictionary, dict):         return dictionary     return dict((str(k), convert_keys_to_string(v))          for k, v in dictionary.items())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "casting",
            "types"
        ],
        "URL": "https://stackoverflow.com/questions/1254454/fastest-way-to-convert-a-dicts-keys-values-from-unicode-to-str",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm receiving a dict from one \"layer\" of code upon which some calculations/modifications are performed before passing it onto another \"layer\". The original dict's keys & \"string\" values are unicode, but the layer they're being passed onto only accepts str.  This is going to be called often, so I'd like to know what would be the fastest way to convert something like:  { u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } }   ...to:  { 'spam': 'eggs', 'foo': True, 'bar': { 'baz': 97 } }   ...bearing in mind the non-\"string\" values need to stay as their original type.  Any thoughts?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Fastest way to convert a dict's keys & values from `unicode` to `str`?",
        "A_Content": "  If you wanted to do this inline and didn't need recursive descent, this might work:  DATA = { u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } } print DATA # \"{ u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } }\"  STRING_DATA = dict([(str(k), v) for k, v in data.items()]) print STRING_DATA # \"{ 'spam': 'eggs', 'foo': True, 'bar': { u'baz': 97 } }\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "casting",
            "types"
        ],
        "URL": "https://stackoverflow.com/questions/1254454/fastest-way-to-convert-a-dicts-keys-values-from-unicode-to-str",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm receiving a dict from one \"layer\" of code upon which some calculations/modifications are performed before passing it onto another \"layer\". The original dict's keys & \"string\" values are unicode, but the layer they're being passed onto only accepts str.  This is going to be called often, so I'd like to know what would be the fastest way to convert something like:  { u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } }   ...to:  { 'spam': 'eggs', 'foo': True, 'bar': { 'baz': 97 } }   ...bearing in mind the non-\"string\" values need to stay as their original type.  Any thoughts?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Fastest way to convert a dict's keys & values from `unicode` to `str`?",
        "A_Content": "  def to_str(key, value):     if isinstance(key, unicode):         key = str(key)     if isinstance(value, unicode):         value = str(value)     return key, value   pass key and value to it, and add recursion to your code to account for inner dictionary.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "casting",
            "types"
        ],
        "URL": "https://stackoverflow.com/questions/1254454/fastest-way-to-convert-a-dicts-keys-values-from-unicode-to-str",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm receiving a dict from one \"layer\" of code upon which some calculations/modifications are performed before passing it onto another \"layer\". The original dict's keys & \"string\" values are unicode, but the layer they're being passed onto only accepts str.  This is going to be called often, so I'd like to know what would be the fastest way to convert something like:  { u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } }   ...to:  { 'spam': 'eggs', 'foo': True, 'bar': { 'baz': 97 } }   ...bearing in mind the non-\"string\" values need to stay as their original type.  Any thoughts?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Fastest way to convert a dict's keys & values from `unicode` to `str`?",
        "A_Content": "  for a non-nested dict (since the title does not mention that case, it might be interesting for other people)  {str(k): str(v) for k, v in my_dict.items()}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "casting",
            "types"
        ],
        "URL": "https://stackoverflow.com/questions/1254454/fastest-way-to-convert-a-dicts-keys-values-from-unicode-to-str",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm receiving a dict from one \"layer\" of code upon which some calculations/modifications are performed before passing it onto another \"layer\". The original dict's keys & \"string\" values are unicode, but the layer they're being passed onto only accepts str.  This is going to be called often, so I'd like to know what would be the fastest way to convert something like:  { u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } }   ...to:  { 'spam': 'eggs', 'foo': True, 'bar': { 'baz': 97 } }   ...bearing in mind the non-\"string\" values need to stay as their original type.  Any thoughts?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Fastest way to convert a dict's keys & values from `unicode` to `str`?",
        "A_Content": "  To make it all inline (non-recursive):  {str(k):(str(v) if isinstance(v, unicode) else v) for k,v in my_dict.items()}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "casting",
            "types"
        ],
        "URL": "https://stackoverflow.com/questions/1254454/fastest-way-to-convert-a-dicts-keys-values-from-unicode-to-str",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm receiving a dict from one \"layer\" of code upon which some calculations/modifications are performed before passing it onto another \"layer\". The original dict's keys & \"string\" values are unicode, but the layer they're being passed onto only accepts str.  This is going to be called often, so I'd like to know what would be the fastest way to convert something like:  { u'spam': u'eggs', u'foo': True, u'bar': { u'baz': 97 } }   ...to:  { 'spam': 'eggs', 'foo': True, 'bar': { 'baz': 97 } }   ...bearing in mind the non-\"string\" values need to stay as their original type.  Any thoughts?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  Every Task object has a .request property, which contains it AsyncRequest object. Accordingly, the following line gives the state of a Task task:  task.AsyncResult(task.request.id).state      ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "51",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  Return the task_id (which is given from .delay()) and ask the celery instance afterwards about the state:  x = method.delay(1,2) print x.task_id   When asking, get a new AsyncResult using this task_id:  from celery.result import AsyncResult res = AsyncResult(\"your-task-id\") res.ready()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "74",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  Creating an AsyncResult object from the task id is the way recommended in the FAQ to obtain the task status when the only thing you have is the task id.  However, as of Celery 3.x, there are significant caveats that could bite people if they do not pay attention to them. It really depends on the specific use-case scenario.  By default, Celery does not record a \"running\" state.  In order for Celery to record that a task is running, you must set task_track_started to True. Here is a simple task that tests this:  @app.task(bind=True) def test(self):     print self.AsyncResult(self.request.id).state   When task_track_started is False, which is the default, the state show is PENDING even though the task has started. If you set task_track_started to True, then the state will be STARTED.  The state PENDING means \"I don't know.\"  An AsyncResult with the state PENDING does not mean anything more than that Celery does not know the status of the task. This could be because of any number of reasons.  For one thing, AsyncResult can be constructed with invalid task ids. Such \"tasks\" will be deemed pending by Celery:  >>> task.AsyncResult(\"invalid\").status 'PENDING'   Ok, so nobody is going to feed obviously invalid ids to AsyncResult. Fair enough, but it also has for effect that AsyncResult will also consider a task that has successfully run but that Celery has forgotten as being PENDING. Again, in some use-case scenarios this can be a problem. Part of the issue hinges on how Celery is configured to keep the results of tasks, because it depends on the availability of the \"tombstones\" in the results backend. (\"Tombstones\" is the term use in the Celery documentation for the data chunks that record how the task ended.) Using AsyncResult won't work at all if task_ignore_result is True. A more vexing problem is that Celery expires the tombstones by default. The result_expires setting by default is set to 24 hours. So if you launch a task, and record the id in long-term storage, and more 24 hours later, you create an AsyncResult with it, the status will be PENDING.   All \"real tasks\" start in the PENDING state. So getting PENDING on a task could mean that the task was requested but never progressed further than this (for whatever reason). Or it could mean the task ran but Celery forgot its state.  Ouch! AsyncResult won't work for me. What else can I do?  I prefer to keep track of goals than keep track of the tasks themselves. I do keep some task information but it is really secondary to keeping track of the goals. The goals are stored in storage independent from Celery. When a request needs to perform a computation depends on some goal having been achieved, it checks whether the goal has already been achieved, if yes, then it uses this cached goal, otherwise it starts the task that will effect the goal, and sends to the client that made the HTTP request a response that indicates it should wait for a result.    The variable names and hyperlinks above are for Celery 4.x. In 3.x the corresponding variables and hyperlinks are: CELERY_TRACK_STARTED, CELERY_IGNORE_RESULT, CELERY_TASK_RESULT_EXPIRES.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "38",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  You can also create custom states and update it's value duting task execution. This example is from docs:  @app.task(bind=True) def upload_files(self, filenames):     for i, file in enumerate(filenames):         if not self.request.called_directly:             self.update_state(state='PROGRESS',                 meta={'current': i, 'total': len(filenames)})   http://celery.readthedocs.org/en/latest/userguide/tasks.html#custom-states     ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  Old question but I recently ran into this problem.   If you're trying to get the task_id you can do it like this:   import celery from celery_app import add from celery import uuid  task_id = uuid() result = add.apply_async((2, 2), task_id=task_id)   Now you know exactly what the task_id is and can now use it to get the AsyncResult:  # grab the AsyncResult  result = celery.result.AsyncResult(task_id)  # print the task id print result.task_id 09dad9cf-c9fa-4aee-933f-ff54dae39bdf  # print the AsyncResult's status print result.status SUCCESS  # print the result returned  print result.result 4      ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  Apart from above Programmatic approach Using Flower Task status can be easily seen.  Real-time monitoring using Celery Events. Flower is a web based tool for monitoring and administrating Celery clusters.   Task progress and history Ability to show task details (arguments, start time, runtime, and more)  Graphs and statistics   Official Document:  Flower - Celery monitoring tool  Installation:  $ pip install flower   Usage:  http://localhost:5555      ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  Try:  task.AsyncResult(task.request.id).state   this will provide the Celery Task status. If Celery Task is already is under FAILURE state it will throw an Exception:   raised unexpected: KeyError('exc_type',)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  for simple tasks, we can use http://flower.readthedocs.io/en/latest/screenshots.html and http://policystat.github.io/jobtastic/ to do the monitoring.   and for complicated tasks, say a task which deals with a lot other modules. We recommend manually record the progress and message on the specific task unit.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to check task status in Celery?",
        "A_Content": "  I found helpful information in the  Celery Project Workers Guide inspecting-workers  For my case, I am checking to see if Celery is running.   inspect_workers = task.app.control.inspect() if inspect_workers.registered() is None:     state = 'FAILURE' else:     state = str(task.state)    You can play with inspect to get your needs.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "web-services",
            "celery",
            "django-celery"
        ],
        "URL": "https://stackoverflow.com/questions/9034091/how-to-check-task-status-in-celery",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How does one check whether a task is running in celery (specifically, I'm using celery-django)?  I've read the documentation, and I've googled, but I can't see a call like:  my_example_task.state() == RUNNING   My use-case is that I have an external (java) service for transcoding. When I send a document to be transcoded, I want to check if the task that runs that service is running, and if not, to (re)start it.  I'm using the current stable versions - 2.4, I believe.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  Assuming you are only looking for simple obfuscation that will obscure things from the very casual observer, and you aren't looking to use third party libraries. I'd recommend something like the Vigenere cipher. It is one of the strongest of the simple ancient ciphers.  Vigenre cipher  It's quick and easy to implement. Something like:  import base64  def encode(key, string):     encoded_chars = []     for i in xrange(len(string)):         key_c = key[i % len(key)]         encoded_c = chr(ord(string[i]) + ord(key_c) % 256)         encoded_chars.append(encoded_c)     encoded_string = \"\".join(encoded_chars)     return base64.urlsafe_b64encode(encoded_string)   Decode is pretty much the same, except you subtract the key.  It is much harder to break if the strings you are encoding are short, and/or if it is hard to guess the length of the passphrase used.   If you are looking for something cryptographic, PyCrypto is probably your best bet, though previous answers overlook some details: ECB mode in PyCrypto requires your message to be a multiple of 16 characters in length. So, you must pad. Also, if you want to use them as URL parameters, use base64.urlsafe_b64_encode(), rather than the standard one. This replaces a few of the characters in the base64 alphabet with URL-safe characters (as it's name suggests).  However, you should be ABSOLUTELY certain that this very thin layer of obfuscation suffices for your needs before using this. The Wikipedia article I linked to provides detailed instructions for breaking the cipher, so anyone with a moderate amount of determination could easily break it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "45",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  As you explicitly state that you want obscurity not security, we'll avoid reprimanding you for the weakness of what you suggest :)  So, using PyCrypto:  from Crypto.Cipher import AES import base64  msg_text = 'test some plain text here'.rjust(32) secret_key = '1234567890123456' # create new & store somewhere safe  cipher = AES.new(secret_key,AES.MODE_ECB) # never use ECB in strong systems obviously encoded = base64.b64encode(cipher.encrypt(msg_text)) # ... decoded = cipher.decrypt(base64.b64decode(encoded)) print decoded.strip()   If someone gets a hold of your database and your code base, they will be able to decode the encrypted data. Keep your secret_key safe!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "52",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  The \"encoded_c\" mentioned in the @smehmood's Vigenere cipher answer should be \"key_c\".  Here are working encode/decode functions.  import base64 def encode(key, clear):     enc = []     for i in range(len(clear)):         key_c = key[i % len(key)]         enc_c = chr((ord(clear[i]) + ord(key_c)) % 256)         enc.append(enc_c)     return base64.urlsafe_b64encode(\"\".join(enc))  def decode(key, enc):     dec = []     enc = base64.urlsafe_b64decode(enc)     for i in range(len(enc)):         key_c = key[i % len(key)]         dec_c = chr((256 + ord(enc[i]) - ord(key_c)) % 256)         dec.append(dec_c)     return \"\".join(dec)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "40",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  Here's a Python 3 version of the functions from @qneill 's answer:  import base64 def encode(key, clear):     enc = []     for i in range(len(clear)):         key_c = key[i % len(key)]         enc_c = chr((ord(clear[i]) + ord(key_c)) % 256)         enc.append(enc_c)     return base64.urlsafe_b64encode(\"\".join(enc).encode()).decode()  def decode(key, enc):     dec = []     enc = base64.urlsafe_b64decode(enc).decode()     for i in range(len(enc)):         key_c = key[i % len(key)]         dec_c = chr((256 + ord(enc[i]) - ord(key_c)) % 256)         dec.append(dec_c)     return \"\".join(dec)   The extra encode/decodes are needed because Python 3 has split strings/byte arrays into two different concepts, and updated their APIs to reflect that..     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "33",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  As has been mentioned the PyCrypto library contains a suite of ciphers. The XOR cipher can be used to do the dirty work if you don't want to do it yourself:  from Crypto.Cipher import XOR import base64  def encrypt(key, plaintext):   cipher = XOR.new(key)   return base64.b64encode(cipher.encrypt(plaintext))  def decrypt(key, ciphertext):   cipher = XOR.new(key)   return cipher.decrypt(base64.b64decode(ciphertext))   Even though it only provides minimal security I'd still recommend using a random looking key without any space characters (as XOR'ing an ASCII [a-zA-Z] character with a space just flips the case).  The cipher works as follows without having to pad the plaintext:  >>> encrypt('notsosecretkey', 'Attack at dawn!') 'LxsAEgwYRQIGRRAKEhdP'  >>> decrypt('notsosecretkey', encrypt('notsosecretkey', 'Attack at dawn!')) 'Attack at dawn!'   Credit to https://stackoverflow.com/a/2490376/241294 for the base64 encode/decode functions (I'm a python newbie).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "23",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  Here's an implementation of URL Safe encryption and Decryption using AES(PyCrypto) and base64.       import base64     from Crypto import Random     from Crypto.Cipher import AES      AKEY = 'mysixteenbytekey' # AES key must be either 16, 24, or 32 bytes long      iv = Random.new().read(AES.block_size)       def encode(message):         obj = AES.new(AKEY, AES.MODE_CFB, iv)         return base64.urlsafe_b64encode(obj.encrypt(message))       def decode(cipher):         obj2 = AES.new(AKEY, AES.MODE_CFB, iv)         return obj2.decrypt(base64.urlsafe_b64decode(cipher))   If you face some issue like this https://bugs.python.org/issue4329 ( TypeError: character mapping must return integer, None or unicode )         use str(cipher) while decoding as follows  return obj2.decrypt(base64.urlsafe_b64decode(str(cipher)))      In [13]: encode(\"Hello World\")     Out[13]: b'67jjg-8_RyaJ-28='      In [14]: %timeit encode(\"Hello World\")     100000 loops, best of 3: 13.9 s per loop      In [15]: decode(b'67jjg-8_RyaJ-28=')     Out[15]: b'Hello World'      In [16]: %timeit decode(b'67jjg-8_RyaJ-28=')     100000 loops, best of 3: 15.2 s per loop      ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  Working encode/decode functions in python3 (adapted very little from qneill's answer):  def encode(key, clear):     enc = []     for i in range(len(clear)):         key_c = key[i % len(key)]         enc_c = (ord(clear[i]) + ord(key_c)) % 256         enc.append(enc_c)     return base64.urlsafe_b64encode(bytes(enc))  def decode(key, enc):     dec = []     enc = base64.urlsafe_b64decode(enc)     for i in range(len(enc)):         key_c = key[i % len(key)]         dec_c = chr((256 + enc[i] - ord(key_c)) % 256)         dec.append(dec_c)     return \"\".join(dec)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  Thanks for some great answers. Nothing original to add, but here are some progressive rewrites of qneill's answer using some useful Python facilities. I hope you agree they simplify and clarify the code.  import base64   def qneill_encode(key, clear):     enc = []     for i in range(len(clear)):         key_c = key[i % len(key)]         enc_c = chr((ord(clear[i]) + ord(key_c)) % 256)         enc.append(enc_c)     return base64.urlsafe_b64encode(\"\".join(enc))   def qneill_decode(key, enc):     dec = []     enc = base64.urlsafe_b64decode(enc)     for i in range(len(enc)):         key_c = key[i % len(key)]         dec_c = chr((256 + ord(enc[i]) - ord(key_c)) % 256)         dec.append(dec_c)     return \"\".join(dec)      enumerate()-- pair the items in a list with their index      iterate over the characters in a string   def encode_enumerate(key, clear):     enc = []     for i, ch in enumerate(clear):         key_c = key[i % len(key)]         enc_c = chr((ord(ch) + ord(key_c)) % 256)         enc.append(enc_c)     return base64.urlsafe_b64encode(\"\".join(enc))   def decode_enumerate(key, enc):     dec = []     enc = base64.urlsafe_b64decode(enc)     for i, ch in enumerate(enc):         key_c = key[i % len(key)]         dec_c = chr((256 + ord(ch) - ord(key_c)) % 256)         dec.append(dec_c)     return \"\".join(dec)      build lists using a list comprehension   def encode_comprehension(key, clear):     enc = [chr((ord(clear_char) + ord(key[i % len(key)])) % 256)                 for i, clear_char in enumerate(clear)]     return base64.urlsafe_b64encode(\"\".join(enc))   def decode_comprehension(key, enc):     enc = base64.urlsafe_b64decode(enc)     dec = [chr((256 + ord(ch) - ord(key[i % len(key)])) % 256)            for i, ch in enumerate(enc)]     return \"\".join(dec)      Often in Python there's no need for list indexes at all. Eliminate loop index variables entirely using zip and cycle:   from itertools import cycle   def encode_zip_cycle(key, clear):     enc = [chr((ord(clear_char) + ord(key_char)) % 256)                 for clear_char, key_char in zip(clear, cycle(key))]     return base64.urlsafe_b64encode(\"\".join(enc))   def decode_zip_cycle(key, enc):     enc = base64.urlsafe_b64decode(enc)     dec = [chr((256 + ord(enc_char) - ord(key_char)) % 256)                 for enc_char, key_char in zip(enc, cycle(key))]     return \"\".join(dec)      and some tests...   msg = 'The quick brown fox jumps over the lazy dog.' key = 'jMG6JV3QdtRh3EhCHWUi' print('cleartext: {0}'.format(msg)) print('ciphertext: {0}'.format(encode_zip_cycle(key, msg)))  encoders = [qneill_encode, encode_enumerate, encode_comprehension, encode_zip_cycle] decoders = [qneill_decode, decode_enumerate, decode_comprehension, decode_zip_cycle]  # round-trip check for each pair of implementations matched_pairs = zip(encoders, decoders) assert all([decode(key, encode(key, msg)) == msg for encode, decode in matched_pairs]) print('Round-trips for encoder-decoder pairs: all tests passed')  # round-trip applying each kind of decode to each kind of encode to prove equivalent from itertools import product all_combinations = product(encoders, decoders) assert all(decode(key, encode(key, msg)) == msg for encode, decode in all_combinations) print('Each encoder and decoder can be swapped with any other: all tests passed')  >>> python crypt.py cleartext: The quick brown fox jumps over the lazy dog. ciphertext: vrWsVrvLnLTPlLTaorzWY67GzYnUwrSmvXaix8nmctybqoivqdHOic68rmQ= Round-trips for encoder-decoder pairs: all tests passed Each encoder and decoder can be swapped with any other: all tests passed      ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  This works but password length should be exactly 8. This is simple and requires pyDes.  from pyDes import *  def encode(data,password):     k = des(password, CBC, \"\\0\\0\\0\\0\\0\\0\\0\\0\", pad=None, padmode=PAD_PKCS5)     d = k.encrypt(data)     return d  def decode(data,password):     k = des(password, CBC, \"\\0\\0\\0\\0\\0\\0\\0\\0\", pad=None, padmode=PAD_PKCS5)     d = k.decrypt(data)     return d  x = encode('John Doe', 'mypass12') y = decode(x,'mypass12')  print x print y   OUTPUT:  .\\\\S+`; John Doe      ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  Simple way is using the library, and PyCrypto is the good one.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  External libraries provide secret-key encryption algorithms.  For example, the Cypher module in PyCrypto offers a selection of many encryption algorithms:   Crypto.Cipher.AES Crypto.Cipher.ARC2 Crypto.Cipher.ARC4 Crypto.Cipher.Blowfish Crypto.Cipher.CAST Crypto.Cipher.DES Crypto.Cipher.DES3 Crypto.Cipher.IDEA Crypto.Cipher.RC5 Crypto.Cipher.XOR   MeTooCrypto is a Python wrapper for OpenSSL, and provides (among other functions) a full-strength general purpose cryptography library. Included are symmetric ciphers (like AES).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  if you want secure encryption:  for python 2, you should use keyczar http://www.keyczar.org/  for python 3, until keyczar is available, i have written simple-crypt http://pypi.python.org/pypi/simple-crypt  both these will use key strengthening which makes them more secure than most other answers here.  and since they're so easy to use you might want to use them even when security is not critical...     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  If you want to be safe, you can use Fernet, which is cryptographically sound. You can use a static \"salt\" if you don't want to store it separately - you will only lose dictionary and rainbow attack prevention. I chose it because I can pick long or short passwords, which is not so easy with AES.  from cryptography.fernet import Fernet from cryptography.hazmat.backends import default_backend from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC import base64  #set password password = \"mysecretpassword\" #set message message = \"secretmessage\"  kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32, salt=\"staticsalt\", iterations=100000, backend=default_backend()) key = base64.urlsafe_b64encode(kdf.derive(password)) f = Fernet(key)  #encrypt encrypted = f.encrypt(message) print encrypted  #decrypt decrypted = f.decrypt(encrypted) print decrypted   If that's too complicated, someone suggested simplecrypt   from simplecrypt import encrypt, decrypt ciphertext = encrypt('password', plaintext) plaintext = decrypt('password', ciphertext)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  You can use AES to encrypt your string with a password. Though, you'll want to chose a strong enough password so people can't easily guess what it is (sorry I can't help it. I'm a wannabe security weenie).  AES is strong with a good key size, but it's also easy to use with PyCrypto.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Simple way to encode a string according to a password?",
        "A_Content": "  An other implementation of @qneill code which include CRC checksum of the original message, it throw an exception if the check fail:  import hashlib import struct import zlib  def vigenere_encode(text, key):     text = '{}{}'.format(text, struct.pack('i', zlib.crc32(text)))      enc = []     for i in range(len(text)):         key_c = key[i % len(key)]         enc_c = chr((ord(text[i]) + ord(key_c)) % 256)         enc.append(enc_c)      return base64.urlsafe_b64encode(\"\".join(enc))   def vigenere_decode(encoded_text, key):     dec = []     encoded_text = base64.urlsafe_b64decode(encoded_text)     for i in range(len(encoded_text)):         key_c = key[i % len(key)]         dec_c = chr((256 + ord(encoded_text[i]) - ord(key_c)) % 256)         dec.append(dec_c)      dec = \"\".join(dec)     checksum = dec[-4:]     dec = dec[:-4]      assert zlib.crc32(dec) == struct.unpack('i', checksum)[0], 'Decode Checksum Error'      return dec      ",
        "Language": "Python",
        "Tags": [
            "python",
            "encryption",
            "passwords"
        ],
        "URL": "https://stackoverflow.com/questions/2490334/simple-way-to-encode-a-string-according-to-a-password",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Does Python have a built-in, simple way of encoding/decoding strings using a password?  Something like this:  >>> encode('John Doe', password = 'mypass') 'sjkl28cn2sx0' >>> decode('sjkl28cn2sx0', password = 'mypass') 'John Doe'   So the string \"John Doe\" gets encrypted as 'sjkl28cn2sx0'. To get the original string, I would \"unlock\" that string with the key 'mypass', which is a password in my source code. I'd like this to be the way I can encrypt/decrypt a Word document with a password.  I would like to use these encrypted strings as URL parameters. My goal is obfuscation, not strong security; nothing mission critical is being encoded. I realize I could use a database table to store keys and values, but am trying to be minimalist.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  hasattr internally and rapidly performs the same task as the try/except block: it's a very specific, optimized, one-task tool and thus should be preferred, when applicable, to the very general-purpose alternative.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "73",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  Any benches that illustrate difference in performance?  timeit it's your friend  $ python -mtimeit -s 'class C(object): a = 4 c = C()' 'hasattr(c, \"nonexistent\")' 1000000 loops, best of 3: 1.87 usec per loop $ python -mtimeit -s 'class C(object): a = 4 c = C()' 'hasattr(c, \"a\")' 1000000 loops, best of 3: 0.446 usec per loop $ python -mtimeit -s 'class C(object): a = 4 c = C()' 'try:  c.a except:  pass' 1000000 loops, best of 3: 0.247 usec per loop $ python -mtimeit -s 'class C(object): a = 4 c = C()' 'try:  c.nonexistent except:  pass' 100000 loops, best of 3: 3.13 usec per loop $         |positive|negative hasattr|  0.446 |  1.87  try    |  0.247 |  3.13      ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "78",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  I almost always use hasattr: it's the correct choice for most cases.  The problematic case is when a class overrides __getattr__: hasattr will catch all exceptions instead of catching just AttributeError like you expect. In other words, the code below will print b: False even though it would be more appropriate to see a ValueError exception:  class X(object):     def __getattr__(self, attr):         if attr == 'a':             return 123         if attr == 'b':             raise ValueError('important error from your database')         raise AttributeError  x = X() print 'a:', hasattr(x, 'a') print 'b:', hasattr(x, 'b') print 'c:', hasattr(x, 'c')   The important error has thus disappeared. This has been fixed in Python 3.2 (issue9666) where hasattr now only catches AttributeError.  An easy workaround is to write a utility function like this:  _notset = object()  def safehasattr(thing, attr):     return getattr(thing, attr, _notset) is not _notset   This let's getattr deal with the situation and it can then raise the appropriate exception.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  There is a third, and often better, alternative:  attr = getattr(obj, 'attribute', None) if attr is not None:      print attr   Advantages:   getattr does not have the bad exception-swallowing behavior pointed out by Martin Geiser - in old Pythons, hasattr will even swallow a KeyboardInterrupt. The normal reason you're checking if the object has an attribute is so that you can use the attribute, and this naturally leads in to it. The attribute is read off atomically, and is safe from other threads changing the object.  (Though, if this is a major concern you might want to consider locking the object before accessing it.) It's shorter than try/finally and often shorter than hasattr. A broad except AttributeError block can catch other AttributeErrors than the one you're expecting, which can lead to confusing behaviour. Accessing an attribute is slower than accessing a local variable (especially if it's not a plain instance attribute).  (Though, to be honest, micro-optimization in Python is often a fool's errand.)   One thing to be careful of is if you care about the case where obj.attribute is set to None, you'll need to use a different sentinel value.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  I would say it depends on whether your function may accept objects without the attribute by design, e.g. if you have two callers to the function, one providing an object with the attribute and the other providing an object without it.   If the only case where you'll get an object without the attribute is due to some error, I would recommend using the exceptions mechanism even though it may be slower, because I believe it is a cleaner design.   Bottom line: I think it's a design and readability issue rather than an efficiency issue.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  If it's just one attribute you're testing, I'd say use hasattr.  However, if you're doing several accesses to attributes which may or may not exist then using a try block may save you some typing.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  If not having the attribute is not an error condition, the exception handling variant has a problem: it would catch also AttributeErrors that might come internally when accessing obj.attribute (for instance because attribute is a property so that accessing it calls some code).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  I'd suggest option 2. Option 1 has a race condition if some other thread is adding or removing the attribute.  Also python has an Idiom, that EAFP ('easier to ask forgiveness than permission') is better than LBYL ('look before you leap').     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  From a practical point of view, in most languages using a conditional will always be consderably faster than handling an exception.  If you're wanting to handle the case of an attribute not existing somewhere outside of the current function, the exception is the better way to go. An indicator that you may want to be using an exception instead of a conditional is that the conditional merely sets a flag and aborts the current operation, and something elsewhere checks this flag and takes action based on that.  That said, as Rax Olgud points out, communication with others is one important attribute of code, and what you want to say by saying \"this is an exceptional situation\" rather than \"this is is something I expect to happen\" may be more important.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  The first.  Shorter is better. Exceptions should be exceptional.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  At least when it is up to just what's going on in the program, leaving out the human part of readability, etc. (which is actually most of the time more imortant than performance (at least in this case - with that performance span), as Roee Adler and others pointed out).  Nevertheless looking at it from that perspective, it then becomes a matter of choosing between  try: getattr(obj, attr) except: ...   and  try: obj.attr except: ...   since hasattr just uses the first case to determine the result. Food for thought ;-)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "hasattr() vs try-except block to deal with non-existent attributes",
        "A_Content": "  This subject was covered in the EuroPython 2016 talk Writing faster Python by Sebastian Witowski. Here's a reproduction of his slide with the performance summary. He also uses the terminology look before you leap in this discussion, worth mentioning here to tag that keyword.     If the attribute is actually missing then begging for forgiveness will   be slower than asking for permissions. So as a rule of thumb you can   use the ask for permission way if know that it is very likely that the   attribute will be missing or other problems that you can predict.   Otherwise if you expect code will result in most of the times readable   code   3 PERMISSIONS OR FORGIVENESS?  # CASE 1 -- Attribute Exists class Foo(object):     hello = 'world' foo = Foo()  if hasatter(foo, 'hello'):     foo.hello ## 149ns ##  try:     foo.hello except AttributeError:     pass ## 43.1 ns ## ## 3.5 times faster   # CASE 2 -- Attribute Absent class Bar(object):     pass bar = Bar()  if hasattr(bar, 'hello'):     bar.hello ## 428 ns ##  try:     bar.hello except AttributeError :     pass ## 536 ns ## ## 25% slower      ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling",
            "attributes",
            "hasattr"
        ],
        "URL": "https://stackoverflow.com/questions/903130/hasattr-vs-try-except-block-to-deal-with-non-existent-attributes",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    if hasattr(obj, 'attribute'):     # do somthing   vs  try:     # access obj.attribute except AttributeError, e:     # deal with AttributeError   Which should be preferred and why?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "  There are a few problems with this. If you use split and join, some white space characters will be ignored. The built-in capitalize and title methods do not ignore white space.   >>> 'There     is a way'.title() 'There     Is A Way'   If a sentence starts with an article, you do not want the first word of a title in lowercase.  Keeping these in mind:  import re  def title_except(s, exceptions):     word_list = re.split(' ', s)       # re.split behaves as expected     final = [word_list[0].capitalize()]     for word in word_list[1:]:         final.append(word if word in exceptions else word.capitalize())     return \" \".join(final)  articles = ['a', 'an', 'of', 'the', 'is'] print title_except('there is a    way', articles) # There is a    Way print title_except('a whim   of an elephant', articles) # A Whim   of an Elephant      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "130",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "  Use the titlecase.py module! Works only for English.  >>> from titlecase import titlecase >>> titlecase('i am a foobar bazbar') 'I Am a Foobar Bazbar'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "44",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "  There are these methods:  >>> mytext = u'i am a foobar bazbar' >>> print mytext.capitalize() I am a foobar bazbar >>> print mytext.title() I Am A Foobar Bazbar   There's no lowercase article option. You'd have to code that yourself, probably by using a list of articles you want to lower.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "  Stuart Colville has made a Python port of a Perl script written by John Gruber to convert strings into title case but avoids capitalizing small words based on rules from the New York Times Manual of style, as well as catering for several special cases.  Some of the cleverness of these scripts:   they capitalizes small words like if, in, of, on, etc., but will un-capitalize them if theyre erroneously capitalized in the input. the scripts assume that words with capitalized letters other than the first character are already correctly capitalized. This means they will leave a word like iTunes alone, rather than mangling it into ITunes or, worse, Itunes. they skip over any words with line dots; example.com and del.icio.us will remain lowercase. they have hard-coded hacks specifically to deal with odd cases, like AT&T and Q&A, both of which contain small words (at and a) which normally should be lowercase. The first and last word of the title are always capitalized, so input such as Nothing to be afraid of will be turned into Nothing to Be Afraid Of. A small word after a colon will be capitalized.   You can download it here.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "  capitalize (word)   This should do. I get it differently.  >>> mytext = u'i am a foobar bazbar' >>> mytext.capitalize() u'I am a foobar bazbar' >>>   Ok as said in reply above, you have to make a custom capitalize:  mytext = u'i am a foobar bazbar'  def xcaptilize(word):     skipList = ['a', 'an', 'the', 'am']     if word not in skipList:         return word.capitalize()     return word  k = mytext.split(\" \")  l = map(xcaptilize, k) print \" \".join(l)      This outputs  I am a Foobar Bazbar      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "  Python 2.7's title method has a flaw in it.  value.title()   will return Carpenter'S Assistant when value is Carpenter's Assistant  The best solution is probably the one from @BioGeek using titlecase from Stuart Colville.  Which is the same solution proposed by @Etienne.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "   not_these = ['a','the', 'of'] thestring = 'the secret of a disappointed programmer' print ' '.join(word                if word in not_these                else word.title()                for word in thestring.capitalize().split(' ')) \"\"\"Output: The Secret of a Disappointed Programmer \"\"\"   The title starts with capitalized word and that does not match the article.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Titlecasing a string with exceptions",
        "A_Content": "  One-liner using list comprehension and the ternary operator  reslt = \" \".join([word.title() if word not in \"the a on in of an\" else word for word in \"Wow, a python one liner for titles\".split(\" \")]) print(reslt)   Breakdown:  for word in \"Wow, a python one liner for titles\".split(\" \") Splits the string into an list and initiates a for loop (in the list comprehenstion)  word.title() if word not in \"the a on in of an\" else word uses native method title() to title case the string if it's not an article  \" \".join joins the list elements with a seperator of (space)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "title-case"
        ],
        "URL": "https://stackoverflow.com/questions/3728655/titlecasing-a-string-with-exceptions",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a standard way in Python to titlecase a string (i.e. words start with uppercase characters, all remaining cased characters have lowercase) but leaving articles like and, in, and of lowercased?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  It looks like you don't have the python mysql package installed, try:  pip install mysql-python   or if not using a virtual environment (on *nix hosts):  sudo pip install mysql-python      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  you have to install python-mysqldb - Python interface to MySQL  Try  sudo apt-get install python-mysqldb     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "31",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  If you get errors trying to install mysqlclient with pip, you may lack the mysql dev library. Install it by running:  apt-get install libmysqlclient-dev   and try again to install mysqlclient:  pip install mysqlclient      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  When I set up Django development environment for PyCharm in Mac OS X Mountain Lion with python, mysql, sequel pro application I got error same as owner of this thread. However, my answer for them who is running python-mysqldb under Mac OS Mountain Lion x86_x64 (MySql and Python also should be same architecture) and already tried everything like pip and etc. In order fix this problem do following steps:   Download MySql for Python from here Untar downloaded file. In terminal window do following: tar xvfz downloade.tar. cd /to untared directory Run sudo python setup.py install If you get error something like this: \"Environment Error: /usr/local/bin/mysql_config not found\" then try to add path ass follows: \"export PATH=$PATH:/usr/local/mysql/bin\". But id did not helped to me and I found another solution. In the end of command execution error output which looks like this:  File \"/path_to_file/MySQL-python-1.2.4b4/setup_posix.py\", line 25, in mysql_config raise EnvironmentError(\"%s not found\" % (mysql_config.path,)) Open setup_posix.py with vim and go to line 25 (In your case it can be different unless if it is same version).  Line 25 should look like this after your editing unless your mysql have symbolic link like follows '/usr/local/mysql/bin/':   f = popen(\"%s --%s\" % ('/usr/local/mysql/bin/mysql_config', what)) After this I got another error as following:  django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: dlopen(/Library/Python/2.7/site-packages/MySQL_python-1.2.4b4-py2.7-macosx-10.8-intel.egg/_mysql.so, 2): Library not loaded: libmysqlclient.18.dylib Referenced from: /Library/Python/2.7/site-packages/MySQL_python-1.2.4b4-py2.7-macosx-10.8-intel.egg/_mysql.so Reason: image not found Finally I did following in console:  sudo ln -s /usr/local/mysql/lib/libmysqlclient.18.dylib /usr/lib/libmysqlclient.18.dylib   Currently everything works fine. So I hope it will be helpful for somebody who uses Mac. :)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  My answer is similar to @Ron-E, but I got a few more errors/corrections so I'm putting my steps below for Mac OSX on Mavericks and Python 2.7.6.   Install Python mysql package (if you get a success message, then ignore the below steps)  pip install mysql-python  When I did the above, I got the error \"EnvironmentError: mysql_config not found\"          To fix this, I did the below in terminal:  export PATH=$PATH:/usr/local/mysql/bin  When I reran step 1, I get a new error \"error: command 'cc' failed with exit status 1\"      To fix this, I did the below in terminal:   export CFLAGS=-Qunused-arguments  export CPPFLAGS=-Qunused-arguments  I reran step 1 and got the success message 'Successfully installed mysql-python'!      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  You are missing the python mysqldb library. Use this command (for Debian/Ubuntu) to install it: sudo apt-get install python-mysqldb     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  Download and install Mysql-python from here for windows environment.  http://www.lfd.uci.edu/~gohlke/pythonlibs/#mysql-python.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  On Ubuntu it is advised to use the distributions repository.  sudo apt-get install python-mysqldb      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  This happened with me as well and I believe this has become a common error not only for Django developers but also for Flask as well, so the way I solved this issue was using brew.   brew install mysql sudo pip install mysql-python   This way every single issue was solved and both frameworks work absolutely fine.  P.S.: For those who use macports (such as myself), this can be an issue as brew works in a different level, my advice is to use brew instead of macports  I hope I could be helpful.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  With the same error message as Will, it worked for me to install mysql first as the missing file will be added during the installation. So after  brew install mysql  pip install mysql-python   ran without errors.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  It is because it did not find sql connector. try:  pip install mysqlclient      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  For Ubuntu 16.04 and 18.04 or python 3 versions  sudo apt-get install python3-mysqldb      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  Just to add to other answers, if you're using Django, it is advisable that you install mysql-python BEFORE installing Django.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  I wasted a lot of time on this. Turns out that the default database library is not supported for Python 3. You have to use a different one.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  if the error looks like this   django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/_mysql.so, 2): Library not loaded:  /usr/local/opt/mysql/lib/libmysqlclient.20.dylib Referenced from: /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/_mysql.so   then try :  pip install python-mysqldb      ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  Faced similar issue. I tried installing mysql-python using pip, but it failed due to gcc dependency errors.  The solution that worked for me  conda install mysql-python   Please note that I already had anaconda installed, which didn't had gcc dependency.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  Maybe you can try the following mode of operation: sudo python manage.py runserver 0.0.0.0:8000     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb",
        "A_Content": "  Seems like you don't have permission to the Python folder. Try sudo chown -R $USER /Library/Python/2.7     ",
        "Language": "Python",
        "Tags": [
            "python",
            "mysql",
            "database",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/15312732/django-core-exceptions-improperlyconfigured-error-loading-mysqldb-module-no-mo",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The problem Im facing while trying to connect to database for mysql. I have also given the database settings that i have used.   Traceback (most recent call last):  File \"manage.py\", line 10, in <module>  execute_from_command_line(sys.argv)  File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 453, in execute_from_command_line utility.execute() File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 392, in execute self.fetch_command(subcommand).run_from_argv(self.argv) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 272, in fetch_command klass = load_command_class(app_name, subcommand) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/__init__.py\", line 77, in load_command_class module = import_module('%s.management.commands.%s' % (app_name, name)) File \"/home/arundhati/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/commands/syncdb.py\", line 8, in <module> from django.core.management.sql import custom_sql_for_model, emit_post_sync_signal File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/core/management/sql.py\", line 9, in <module> from django.db import models File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 40, in <module> backend = load_backend(connection.settings_dict['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/__init__.py\", line 34, in __getattr__ return getattr(connections[DEFAULT_DB_ALIAS], item) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 93, in __getitem__ backend = load_backend(db['ENGINE']) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/utils.py\", line 27, in load_backend return import_module('.base', backend_name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/utils/importlib.py\", line 35, in import_module __import__(name) File \"/home/ar/Desktop/test/testprac/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py\", line 17, in <module> raise ImproperlyConfigured(\"Error loading MySQLdb module: %s\" % e) django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named MySQLdb   Databse Settings::  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.         'NAME': 'ar_test_db',                      # Or path to database file if using          # The following settings are not used with sqlite3:         'USER': '',         'PASSWORD': '',         'HOST': '',                      # Empty for localhost through domain sockets or   '127.0.0.1' for localhost through TCP.         'PORT': '',                      # Set to empty string for default.     } }   Thanks a lot for the help !!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: How to Redirect Output with Subprocess?",
        "A_Content": "  UPDATE: os.system is discouraged, albeit still available in Python 3.    Use os.system:  os.system(my_cmd)   If you really want to use subprocess, here's the solution (mostly lifted from the documentation for subprocess):  p = subprocess.Popen(my_cmd, shell=True) os.waitpid(p.pid, 0)   OTOH, you can avoid system calls entirely:  import shutil  with open('myfile', 'w') as outfile:     for infile in ('file1', 'file2', 'file3'):         shutil.copyfileobj(open(infile), outfile)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "subprocess"
        ],
        "URL": "https://stackoverflow.com/questions/4965159/python-how-to-redirect-output-with-subprocess",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    What I do in the command line:  cat file1 file2 file3 > myfile   What I want to do with python:  import subprocess, shlex my_cmd = 'cat file1 file2 file3 > myfile' args = shlex.split(my_cmd) subprocess.call(args) # spits the output in the window i call my python program      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: How to Redirect Output with Subprocess?",
        "A_Content": "  To answer your original question, to redirect output, just pass an open file handle for the stdout argument to subprocess.call:  # Use a list of args instead of a string input_files = ['file1', 'file2', 'file3'] my_cmd = ['cat'] + input_files with open('myfile', \"w\") as outfile:     subprocess.call(my_cmd, stdout=outfile)   But as others have pointed out, the use of an external command like cat for this purpose is completely extraneous.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "subprocess"
        ],
        "URL": "https://stackoverflow.com/questions/4965159/python-how-to-redirect-output-with-subprocess",
        "A_Votes": "188",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What I do in the command line:  cat file1 file2 file3 > myfile   What I want to do with python:  import subprocess, shlex my_cmd = 'cat file1 file2 file3 > myfile' args = shlex.split(my_cmd) subprocess.call(args) # spits the output in the window i call my python program      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: How to Redirect Output with Subprocess?",
        "A_Content": "     @PoltoS I want to join some files and then process the resulting file. I thought using cat was the easiest alternative. Is there a better/pythonic way to do it?   Of course:   with open('myfile', 'w') as outfile:     for infilename in ['file1', 'file2', 'file3']:         with open(infilename) as infile:             outfile.write(infile.read())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "subprocess"
        ],
        "URL": "https://stackoverflow.com/questions/4965159/python-how-to-redirect-output-with-subprocess",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What I do in the command line:  cat file1 file2 file3 > myfile   What I want to do with python:  import subprocess, shlex my_cmd = 'cat file1 file2 file3 > myfile' args = shlex.split(my_cmd) subprocess.call(args) # spits the output in the window i call my python program      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: How to Redirect Output with Subprocess?",
        "A_Content": "  One interesting case would be to update a file by appending similar file to it. Then one would not have to create a new file in the process. It is particularly useful in the case where a large file need to be appended. Here is one possibility using teminal command line directly from python.  import subprocess32 as sub  with open(\"A.csv\",\"a\") as f:     f.flush()     sub.Popen([\"cat\",\"temp.csv\"],stdout=f)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "subprocess"
        ],
        "URL": "https://stackoverflow.com/questions/4965159/python-how-to-redirect-output-with-subprocess",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What I do in the command line:  cat file1 file2 file3 > myfile   What I want to do with python:  import subprocess, shlex my_cmd = 'cat file1 file2 file3 > myfile' args = shlex.split(my_cmd) subprocess.call(args) # spits the output in the window i call my python program      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: How to Redirect Output with Subprocess?",
        "A_Content": "  size = 'ffprobe -v error -show_entries format=size -of default=noprint_wrappers=1:nokey=1 dump.mp4 > file' proc = subprocess.Popen(shlex.split(size), shell=True) time.sleep(1) proc.terminate() #proc.kill() modify it by a suggestion size = \"\" with open('file', 'r') as infile:     for line in infile.readlines():         size += line.strip()  print(size) os.remove('file')   When you use subprocess , the process must be killed.This is an example.If you don't kill the process , file will be empty and you can read nothing.It can run on Windows.I can`t make sure that it can run on Unix.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "subprocess"
        ],
        "URL": "https://stackoverflow.com/questions/4965159/python-how-to-redirect-output-with-subprocess",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What I do in the command line:  cat file1 file2 file3 > myfile   What I want to do with python:  import subprocess, shlex my_cmd = 'cat file1 file2 file3 > myfile' args = shlex.split(my_cmd) subprocess.call(args) # spits the output in the window i call my python program      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Consistently create same random numpy array",
        "A_Content": "  Simply seed the random number generator with a fixed value, e.g.  numpy.random.seed(42)   This way, you'll always get the same random number sequence.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/5836335/consistently-create-same-random-numpy-array",
        "A_Votes": "73",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am waiting for another developer to finish a piece of code that will return an np array of shape (100,2000) with values of either -1,0, or 1.  In the meantime, I want to randomly create an array of the same characteristics so I can get a head start on my development and testing. The thing is that I want this randomly created array to be the same each time, so that I'm not testing against an array that keeps changing its value each time I re-run my process.  I can create my array like this, but is there a way to create it so that it's the same each time. I can pickle the object and unpickle it, but wondering if there's another way.  r = np.random.randint(3, size=(100, 2000)) - 1      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Consistently create same random numpy array",
        "A_Content": "  Create your own instance of numpy.random.RandomState() with your chosen seed. Do not use numpy.random.seed() except to work around inflexible libraries that do not let you pass around your own RandomState instance.  [~] |1> from numpy.random import RandomState  [~] |2> prng = RandomState(1234567890)  [~] |3> prng.randint(-1, 2, size=10) array([ 1,  1, -1,  0,  0, -1,  1,  0, -1, -1])  [~] |4> prng2 = RandomState(1234567890)  [~] |5> prng2.randint(-1, 2, size=10) array([ 1,  1, -1,  0,  0, -1,  1,  0, -1, -1])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/5836335/consistently-create-same-random-numpy-array",
        "A_Votes": "152",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am waiting for another developer to finish a piece of code that will return an np array of shape (100,2000) with values of either -1,0, or 1.  In the meantime, I want to randomly create an array of the same characteristics so I can get a head start on my development and testing. The thing is that I want this randomly created array to be the same each time, so that I'm not testing against an array that keeps changing its value each time I re-run my process.  I can create my array like this, but is there a way to create it so that it's the same each time. I can pickle the object and unpickle it, but wondering if there's another way.  r = np.random.randint(3, size=(100, 2000)) - 1      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Consistently create same random numpy array",
        "A_Content": "  If you are using other functions relying on a random state, you can't just set and overall seed, but should instead create a function to generate your random list of number and set the seed as a parameter of the function. This will not disturb any other random generators in the code:   # Random states def get_states(random_state, low, high, size):     rs = np.random.RandomState(random_state)     states = rs.randint(low=low, high=high, size=size)     return states  # Call function states = get_states(random_state=42, low=2, high=28347, size=25)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/5836335/consistently-create-same-random-numpy-array",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am waiting for another developer to finish a piece of code that will return an np array of shape (100,2000) with values of either -1,0, or 1.  In the meantime, I want to randomly create an array of the same characteristics so I can get a head start on my development and testing. The thing is that I want this randomly created array to be the same each time, so that I'm not testing against an array that keeps changing its value each time I re-run my process.  I can create my array like this, but is there a way to create it so that it's the same each time. I can pickle the object and unpickle it, but wondering if there's another way.  r = np.random.randint(3, size=(100, 2000)) - 1      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  The answer is to use Welford's algorithm, which is very clearly defined after the \"naive methods\" in:   Wikipedia: Algorithms for calculating variance   It's more numerically stable than either the two-pass or online simple sum of squares collectors suggested in other responses.  The stability only really matters when you have lots of values that are close to each other as they lead to what is known as \"catastrophic cancellation\" in the floating point literature.  You might also want to brush up on the difference between dividing by the number of samples (N) and N-1 in the variance calculation (squared deviation).  Dividing by N-1 leads to an unbiased estimate of variance from the sample, whereas dividing by N on average underestimates variance (because it doesn't take into account the variance between the sample mean and the true mean).  I wrote two blog entries on the topic which go into more details, including how to delete previous values online:   Computing Sample Mean and Variance Online in One Pass Deleting Values in Welfords Algorithm for Online Mean and Variance   You can also take a look at my Java implement; the javadoc, source, and unit tests are all online:   Javadoc: stats.OnlineNormalEstimator Source: stats.OnlineNormalEstimator.java JUnit Source: test.unit.stats.OnlineNormalEstimatorTest.java LingPipe Home Page      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "98",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  The basic answer is to accumulate the sum of both x (call it 'sum_x1') and x2 (call it 'sum_x2') as you go.  The value of the standard deviation is then:  stdev = sqrt((sum_x2 / n) - (mean * mean))    where  mean = sum_x / n   This is the sample standard deviation; you get the population standard deviation using 'n' instead of 'n - 1' as the divisor.  You may need to worry about the numerical stability of taking the difference between two large numbers if you are dealing with large samples.  Go to the external references in other answers (Wikipedia, etc) for more information.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "68",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  Perhaps not what you were asking, but ... If you use a numpy array, it will do the work for you, efficiently:  from numpy import array  nums = array(((0.01, 0.01, 0.02, 0.04, 0.03),               (0.00, 0.02, 0.02, 0.03, 0.02),               (0.01, 0.02, 0.02, 0.03, 0.02),               (0.01, 0.00, 0.01, 0.05, 0.03)))  print nums.std(axis=1) # [ 0.0116619   0.00979796  0.00632456  0.01788854]  print nums.mean(axis=1) # [ 0.022  0.018  0.02   0.02 ]   By the way, there's some interesting discussion in this blog post and comments on one-pass methods for computing means and variances:   http://lingpipe-blog.com/2009/03/19/computing-sample-mean-variance-online-one-pass/      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  Here is a literal pure Python translation of the Welford's algorithm implementation from http://www.johndcook.com/standard_deviation.html:  https://github.com/liyanage/python-modules/blob/master/running_stats.py  class RunningStats:      def __init__(self):         self.n = 0         self.old_m = 0         self.new_m = 0         self.old_s = 0         self.new_s = 0      def clear(self):         self.n = 0      def push(self, x):         self.n += 1          if self.n == 1:             self.old_m = self.new_m = x             self.old_s = 0         else:             self.new_m = self.old_m + (x - self.old_m) / self.n             self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)              self.old_m = self.new_m             self.old_s = self.new_s      def mean(self):         return self.new_m if self.n else 0.0      def variance(self):         return self.new_s / (self.n - 1) if self.n > 1 else 0.0      def standard_deviation(self):         return math.sqrt(self.variance())   Usage:  rs = RunningStats() rs.push(17.0); rs.push(19.0); rs.push(24.0);  mean = rs.mean(); variance = rs.variance(); stdev = rs.standard_deviation();      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  The Python runstats Module is for just this sort of thing. Install runstats from PyPI:  pip install runstats   Runstats summaries can produce the mean, variance, standard deviation, skewness, and kurtosis  in a single pass of data. We can use this to create your \"running\" version.  from runstats import Statistics  stats = [Statistics() for num in range(len(data[0]))]  for row in data:      for index, val in enumerate(row):         stats[index].push(val)      for index, stat in enumerate(stats):         print 'Index', index, 'mean:', stat.mean()         print 'Index', index, 'standard deviation:', stat.stddev()   Statistics summaries are based on the Knuth and Welford method for computing standard deviation in one pass as described in the Art of Computer Programming, Vol 2, p. 232, 3rd edition. The benefit of this is numerically stable and accurate results.  Disclaimer: I am the author the Python runstats module.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  Have a look at PDL (pronounced \"piddle!\").   This is the Perl Data Language which is designed for high precision mathematics and scientific computing.  Here is an example using your figures....  use strict; use warnings; use PDL;  my $figs = pdl [     [0.01, 0.01, 0.02, 0.04, 0.03],     [0.00, 0.02, 0.02, 0.03, 0.02],     [0.01, 0.02, 0.02, 0.03, 0.02],     [0.01, 0.00, 0.01, 0.05, 0.03], ];  my ( $mean, $prms, $median, $min, $max, $adev, $rms ) = statsover( $figs );  say \"Mean scores:     \", $mean; say \"Std dev? (adev): \", $adev; say \"Std dev? (prms): \", $prms; say \"Std dev? (rms):  \", $rms;    Which produces:  Mean scores:     [0.022 0.018 0.02 0.02] Std dev? (adev): [0.0104 0.0072 0.004 0.016] Std dev? (prms): [0.013038405 0.010954451 0.0070710678 0.02] Std dev? (rms):  [0.011661904 0.009797959 0.0063245553 0.017888544]    Have a look at PDL::Primitive for more information on the statsover function.  This seems to suggest that ADEV is the \"standard deviation\".    However it maybe PRMS (which Sinan's Statistics::Descriptive example show) or RMS (which ars's NumPy example shows).  I guess one of these three must be right ;-)  For more PDL information have a look at:   pdl.perl.org  (official PDL page). PDL quick reference guide on PerlMonks Dr. Dobb's article on PDL PDL Wiki Wikipedia entry for PDL Sourceforge project page for PDL      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  Statistics::Descriptive is a very decent Perl module for these types of calculations:  #!/usr/bin/perl  use strict; use warnings;  use Statistics::Descriptive qw( :all );  my $data = [     [ 0.01, 0.01, 0.02, 0.04, 0.03 ],     [ 0.00, 0.02, 0.02, 0.03, 0.02 ],     [ 0.01, 0.02, 0.02, 0.03, 0.02 ],     [ 0.01, 0.00, 0.01, 0.05, 0.03 ], ];  my $stat = Statistics::Descriptive::Full->new; # You also have the option of using sparse data structures  for my $ref ( @$data ) {     $stat->add_data( @$ref );     printf \"Running mean: %f\\n\", $stat->mean;     printf \"Running stdev: %f\\n\", $stat->standard_deviation; } __END__   Output:  C:\\Temp> g Running mean: 0.022000 Running stdev: 0.013038 Running mean: 0.020000 Running stdev: 0.011547 Running mean: 0.020000 Running stdev: 0.010000 Running mean: 0.020000 Running stdev: 0.012566      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  How big is your array? Unless it is zillions of elements long, don't worry about looping through it twice. The code is simple and easily tested.  My preference would be to use the numpy array maths extension to convert your array of arrays into a numpy 2D array and get the standard deviation directly:  >>> x = [ [ 1, 2, 4, 3, 4, 5 ], [ 3, 4, 5, 6, 7, 8 ] ] * 10 >>> import numpy >>> a = numpy.array(x) >>> a.std(axis=0)  array([ 1. ,  1. ,  0.5,  1.5,  1.5,  1.5]) >>> a.mean(axis=0) array([ 2. ,  3. ,  4.5,  4.5,  5.5,  6.5])   If that's not an option and you need a pure Python solution, keep reading...  If your array is   x = [        [ 1, 2, 4, 3, 4, 5 ],       [ 3, 4, 5, 6, 7, 8 ],       .... ]   Then the standard deviation is:  d = len(x[0]) n = len(x) sum_x = [ sum(v[i] for v in x) for i in range(d) ] sum_x2 = [ sum(v[i]**2 for v in x) for i in range(d) ] std_dev = [ sqrt((sx2 - sx**2)/N)  for sx, sx2 in zip(sum_x, sum_x2) ]   If you are determined to loop through your array only once, the running sums can be combined.  sum_x  = [ 0 ] * d sum_x2 = [ 0 ] * d for v in x:    for i, t in enumerate(v):    sum_x[i] += t    sum_x2[i] += t**2   This isn't nearly as elegant as the list comprehension solution above.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  I think this issue will help you. Standard deviation     ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  You could look at the Wikipedia article on Standard Deviation, in particular the section about Rapid calculation methods.  There's also an article I found that uses Python, you should be able to use the code in it without much change: Subliminal Messages - Running Standard Deviations.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  n=int(raw_input(\"Enter no. of terms:\"))  L=[]  for i in range (1,n+1):      x=float(raw_input(\"Enter term:\"))      L.append(x)  sum=0  for i in range(n):      sum=sum+L[i]  avg=sum/n  sumdev=0  for j in range(n):      sumdev=sumdev+(L[j]-avg)**2  dev=(sumdev/n)**0.5  print \"Standard deviation is\", dev      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  As the following answer describes: Does pandas/scipy/numpy provide a cumulative standard deviation function? The Python Pandas module contains a method to calculate the running or cumulative standard deviation. For that you'll have to convert your data into a pandas dataframe (or a series if it is 1D), but there are functions for that.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  Here's a \"one-liner\", spread over multiple lines, in functional programming style:  def variance(data, opt=0):     return (lambda (m2, i, _): m2 / (opt + i - 1))(         reduce(             lambda (m2, i, avg), x:             (                 m2 + (x - avg) ** 2 * i / (i + 1),                 i + 1,                 avg + (x - avg) / (i + 1)             ),             data,             (0, 0, 0)))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to efficiently calculate a running standard deviation?",
        "A_Content": "  I like to express the update this way:  def running_update(x, N, mu, var):     '''         @arg x: the current data sample         @arg N : the number of previous samples         @arg mu: the mean of the previous samples         @arg var : the variance over the previous samples         @retval (N+1, mu', var') -- updated mean, variance and count     '''     N = N + 1     rho = 1.0/N     d = x - mu     mu += rho*d     var += rho*((1-rho)*d**2 - var)     return (N, mu, var)   so that a one-pass function would look like this:  def one_pass(data):     N = 0     mu = 0.0     var = 0.0     for x in data:         N = N + 1         rho = 1.0/N         d = x - mu         mu += rho*d         var += rho*((1-rho)*d**2 - var)         # could yield here if you want partial results    return (N, mu, var)   note that this is calculating the sample variance (1/N), not the unbiased estimate of the population variance (which uses a 1/(N-1) normalzation factor).  Unlike the other answers, the variable, var, that is tracking the running variance does not grow in proportion to the number of samples. At all times it is just the variance of the set of samples seen so far (there is no final \"dividing by n\" in getting the variance).  In a class it would look like this:  class RunningMeanVar(object):     def __init__(self):         self.N = 0         self.mu = 0.0         self.var = 0.0     def push(self, x):         self.N = self.N + 1         rho = 1.0/N         d = x-self.mu         self.mu += rho*d         self.var += + rho*((1-rho)*d**2-self.var)     # reset, accessors etc. can be setup as you see fit   This also works for weighted samples:  def running_update(w, x, N, mu, var):     '''         @arg w: the weight of the current sample         @arg x: the current data sample         @arg mu: the mean of the previous N sample         @arg var : the variance over the previous N samples         @arg N : the number of previous samples         @retval (N+w, mu', var') -- updated mean, variance and count     '''     N = N + w     rho = w/N     d = x - mu     mu += rho*d     var += rho*((1-rho)*d**2 - var)     return (N, mu, var)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "perl",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/1174984/how-to-efficiently-calculate-a-running-standard-deviation",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of lists of numbers, e.g.:  [0] (0.01, 0.01, 0.02, 0.04, 0.03) [1] (0.00, 0.02, 0.02, 0.03, 0.02) [2] (0.01, 0.02, 0.02, 0.03, 0.02)      ... [n] (0.01, 0.00, 0.01, 0.05, 0.03)   What I would like to do is efficiently calculate the mean and standard deviation at each index of a list, across all array elements.  To do the mean, I have been looping through the array and summing the value at a given index of a list. At the end, I divide each value in my \"averages list\" by n.  To do the standard deviation, I loop through again, now that I have the mean calculated.  I would like to avoid going through the array twice, once for the mean and then once for the SD (after I have a mean).   Is there an efficient method for calculating both values, only going through the array once? Any code in an interpreted language (e.g. Perl or Python) or pseudocode is fine.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "defaultdict of defaultdict, nested",
        "A_Content": "  For an arbitrary number of levels:  def rec_dd():     return defaultdict(rec_dd)  >>> x = rec_dd() >>> x['a']['b']['c']['d'] defaultdict(<function rec_dd at 0x7f0dcef81500>, {}) >>> print json.dumps(x) {\"a\": {\"b\": {\"c\": {\"d\": {}}}}}   Of course you could also do this with a lambda, but I find lambdas to be less readable.  In any case it would look like this:  rec_dd = lambda: defaultdict(rec_dd)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "recursion",
            "defaultdict"
        ],
        "URL": "https://stackoverflow.com/questions/19189274/defaultdict-of-defaultdict-nested",
        "A_Votes": "108",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is there a way to make a defaultdict also be the default for the defaultdict?  IOW, if I do:  x = defaultdict(...stuff...) x[0][1][0] {}   That's what I want. I'll probably just end up using the bunch pattern, but when I realized I didn't know how to do this, it got me interested.  So, I can do:  x = defaultdict(defaultdict)   But that's only one level:  x[0] {} x[0][0] KeyError: 0   There are recipes that can do this. But can it be done simply just using the normal defaultdict arguments?  Note that someone marked this as a duplicate of Python: defaultdict of defaultdict?, but this isn't the same question... that question was how to do a two-level defaultdict; this one is how to do an infinite-level recursive defaultdict.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "defaultdict of defaultdict, nested",
        "A_Content": "  The other answers here tell you how to create a defaultdict which contains \"infinitely many\" defaultdict, but they fail to address what I think may have been your initial need which was to simply have a two-depth defaultdict.  You may have been looking for:   defaultdict(lambda: defaultdict(dict))   The reasons why you might prefer this construct are:   It is more explicit than the recursive solution, and therefore likely more understandable to the reader. This enables the \"leaf\" of the defaultdict to be something other than a dictionary, e.g.,: defaultdict(lambda: defaultdict(list)) or defaultdict(lambda: defaultdict(set))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "recursion",
            "defaultdict"
        ],
        "URL": "https://stackoverflow.com/questions/19189274/defaultdict-of-defaultdict-nested",
        "A_Votes": "100",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to make a defaultdict also be the default for the defaultdict?  IOW, if I do:  x = defaultdict(...stuff...) x[0][1][0] {}   That's what I want. I'll probably just end up using the bunch pattern, but when I realized I didn't know how to do this, it got me interested.  So, I can do:  x = defaultdict(defaultdict)   But that's only one level:  x[0] {} x[0][0] KeyError: 0   There are recipes that can do this. But can it be done simply just using the normal defaultdict arguments?  Note that someone marked this as a duplicate of Python: defaultdict of defaultdict?, but this isn't the same question... that question was how to do a two-level defaultdict; this one is how to do an infinite-level recursive defaultdict.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "defaultdict of defaultdict, nested",
        "A_Content": "  There is a nifty trick for doing that:  tree = lambda: defaultdict(tree)   Then you can create your x with x = tree().     ",
        "Language": "Python",
        "Tags": [
            "python",
            "recursion",
            "defaultdict"
        ],
        "URL": "https://stackoverflow.com/questions/19189274/defaultdict-of-defaultdict-nested",
        "A_Votes": "30",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to make a defaultdict also be the default for the defaultdict?  IOW, if I do:  x = defaultdict(...stuff...) x[0][1][0] {}   That's what I want. I'll probably just end up using the bunch pattern, but when I realized I didn't know how to do this, it got me interested.  So, I can do:  x = defaultdict(defaultdict)   But that's only one level:  x[0] {} x[0][0] KeyError: 0   There are recipes that can do this. But can it be done simply just using the normal defaultdict arguments?  Note that someone marked this as a duplicate of Python: defaultdict of defaultdict?, but this isn't the same question... that question was how to do a two-level defaultdict; this one is how to do an infinite-level recursive defaultdict.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "defaultdict of defaultdict, nested",
        "A_Content": "  Similar to BrenBarn's solution, but doesn't contain the name of the variable tree twice, so it works even after changes to the variable dictionary:  tree = (lambda f: f(f))(lambda a: (lambda: defaultdict(a(a))))   Then you can create each new x with x = tree().    For the def version, we can use function closure scope to protect the data structure from the flaw where existing instances stop working if the tree name is rebound.  It looks like this:  from collections import defaultdict  def tree():     def the_tree():         return defaultdict(the_tree)     return the_tree()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "recursion",
            "defaultdict"
        ],
        "URL": "https://stackoverflow.com/questions/19189274/defaultdict-of-defaultdict-nested",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to make a defaultdict also be the default for the defaultdict?  IOW, if I do:  x = defaultdict(...stuff...) x[0][1][0] {}   That's what I want. I'll probably just end up using the bunch pattern, but when I realized I didn't know how to do this, it got me interested.  So, I can do:  x = defaultdict(defaultdict)   But that's only one level:  x[0] {} x[0][0] KeyError: 0   There are recipes that can do this. But can it be done simply just using the normal defaultdict arguments?  Note that someone marked this as a duplicate of Python: defaultdict of defaultdict?, but this isn't the same question... that question was how to do a two-level defaultdict; this one is how to do an infinite-level recursive defaultdict.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  This is pretty clear, actually.  Many folks learn it quickly.  You can use a comment to help them.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  sorted((minval, value, maxval))[1]   for example:  >>> minval=3 >>> maxval=7 >>> for value in range(10): ...   print sorted((minval, value, maxval))[1] ...  3 3 3 3 4 5 6 7 7 7      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "65",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  See numpy.clip:  index = numpy.clip(index, 0, len(my_list) - 1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "35",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  many interesting answers here, all about the same, except... which one's faster?  import numpy np_clip = numpy.clip mm_clip = lambda x, l, u: max(l, min(u, x)) s_clip = lambda x, l, u: sorted((x, l, u))[1] py_clip = lambda x, l, u: l if x < l else u if x > u else x     >>> import random >>> rrange = random.randrange >>> %timeit mm_clip(rrange(100), 10, 90) 1000000 loops, best of 3: 1.02 s per loop >>> %timeit s_clip(rrange(100), 10, 90) 1000000 loops, best of 3: 1.21 s per loop >>> %timeit np_clip(rrange(100), 10, 90) 100000 loops, best of 3: 6.12 s per loop >>> %timeit py_clip(rrange(100), 10, 90) 1000000 loops, best of 3: 783 ns per loop   paxdiablo has it!, use plain ol' python.  The numpy version is, perhaps not surprisingly, the slowest of the lot.  Probably because it's looking for arrays, where the other versions just order their arguments.       ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "31",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  Chaining max() and min() together is the normal idiom I've seen. If you find it hard to read, write a helper function to encapsulate the operation:  def clamp(minimum, x, maximum):     return max(minimum, min(x, maximum))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  Whatever happened to my beloved readable Python language? :-)  Seriously, just make it a function:  def addInRange (val, add, minval, maxval):     newval = val + add     if newval < minval: return minval     if newval > maxval: return maxval     return newval   then just call it with something like:  val = addInRange (val, 7, 0, 42)   Or a simpler, more flexible, solution where you do the calculation yourself:  def restrict (val, minval, maxval):     if val < minval: return minval     if val > maxval: return maxval     return val  x = restrict (x+10, 0, 42)   If you wanted to, you could even make the min/max a list so it looks more \"mathematically pure\":  x = restrict (val+7, [0, 42])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  If your code seems too unwieldy, a function might help:  def clamp(minvalue, value, maxvalue):     return max(minvalue, min(value, maxvalue))  new_index = clamp(0, new_index, len(mylist)-1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  This one seems more pythonic to me:  >>> def clip(val, min_, max_): ...     return min_ if val < min_ else max_ if val > max_ else val   A few tests:  >>> clip(5, 2, 7) 5 >>> clip(1, 2, 7) 2 >>> clip(8, 2, 7) 7      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to clamp an integer to some range?",
        "A_Content": "  Avoid writing functions for such small tasks, unless you apply them often, as it will clutter up your code.  for individual values:  min(clamp_max, max(clamp_min, value))   for lists of values:  map(lambda x: min(clamp_max, max(clamp_min, x)), values)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "clamp"
        ],
        "URL": "https://stackoverflow.com/questions/4092528/how-to-clamp-an-integer-to-some-range",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code:  new_index = index + offset if new_index < 0:     new_index = 0 if new_index >= len(mylist):     new_index = len(mylist) - 1 return mylist[new_index]   Basically, I calculate a new index and use that to find some element from a list. In order to make sure the index is inside the bounds of the list, I needed to write those 2 if statements spread into 4 lines. That's quite verbose, a bit ugly... Dare I say, it's quite un-pythonic.  Is there any other simpler and more compact solution? (and more pythonic)  Yes, i know I can use if else in one line, but it is not readable:  new_index = 0 if new_index < 0 else len(mylist) - 1 if new_index >= len(mylist) else new_index   I also know I can chain max() and min() together. It's more compact, but I feel it's kinda obscure, more difficult to find bugs if I type it wrong. In other words, I don't find it very straightforward.  new_index = max(0, min(new_index, len(mylist)-1))      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  Use the walklevel function.  import os  def walklevel(some_dir, level=1):     some_dir = some_dir.rstrip(os.path.sep)     assert os.path.isdir(some_dir)     num_sep = some_dir.count(os.path.sep)     for root, dirs, files in os.walk(some_dir):         yield root, dirs, files         num_sep_this = root.count(os.path.sep)         if num_sep + level <= num_sep_this:             del dirs[:]   It works just like os.walk, but you can pass it a level parameter that indicates how deep the recursion will go.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "76",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  Don't use os.walk.  Example:  import os  root = \"C:\\\\\" for item in os.listdir(root):     if os.path.isfile(os.path.join(root, item)):         print item      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "171",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  I think the solution is actually very simple.  use   break   to only do first iteration of the for loop, there must be a more elegant way.  for root, dirs, files in os.walk(dir_name):     for f in files:         ...         ...     break ...   The first time you call os.walk, it returns tulips for the current directory, then on next loop the contents of the next directory.    Take original script and just add a break.  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")         break     return outputList      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "29",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  The suggestion to use listdir is a good one.  The direct answer to your question is root, dirs, files = os.walk(dir_name).next()     ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  If you have more complex requirements than just the top directory (eg ignore VCS dirs etc), you can also modify the list of directories to prevent os.walk recursing through them.  ie:  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         dirs[:] = [d for d in dirs if is_good(d)]         for f in files:             do_stuff()   Note - be careful to mutate the list, rather than just rebind it.  Obviously os.walk doesn't know about the external rebinding.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  You could use os.listdir() which returns a list of names (for both files and directories) in a given directory. If you need to distinguish between files and directories, call os.stat() on each name.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  The same idea with listdir, but shorter:  [f for f in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, f))]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  In Python 3, I was able to do this:  import os dir = \"/path/to/files/\"  #List all files immediately under this folder: print ( next( os.walk(dir) )[2] )  #List all folders immediately under this folder: print ( next( os.walk(dir) )[1] )      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  for path, dirs, files in os.walk('.'):     print path, dirs, files     del dirs[:] # go only one level deep      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  Felt like throwing my 2 pence in.  baselevel = len(rootdir.split(\"\\\\\")) for subdirs, dirs, files in os.walk(rootdir):     curlevel = len(subdirs.split(\"\\\\\"))     if curlevel <= baselevel + 1:         [do stuff]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  You could also do the following:  for path, subdirs, files in os.walk(dir_name):     for name in files:         if path == \".\": #this will filter the files in the current directory              #code here      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  This is how I solved it  if recursive:     items = os.walk(target_directory) else:     items = [next(os.walk(target_directory))]  ...      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  There is a catch when using listdir.  The os.path.isdir(identifier) must be an absolute path.  To pick subdirectories you do:  for dirname in os.listdir(rootdir):   if os.path.isdir(os.path.join(rootdir, dirname)):      print(\"I got a subdirectory: %s\" % dirname)   The alternative is to change to the directory to do the testing without the os.path.join().     ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  You can use this snippet  for root, dirs, files in os.walk(directory):     if level > 0:         # do some stuff     else:         break     level-=1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "os.walk without digging into directories below",
        "A_Content": "  create a list of excludes, use fnmatch to skip the directory structure and do the process  excludes= ['a\\*\\b', 'c\\d\\e'] for root, directories, files in os.walk('Start_Folder'):     if not any(fnmatch.fnmatch(nf_root, pattern) for pattern in excludes):         for root, directories, files in os.walk(nf_root):             ....             do the process             ....   same as for 'includes':  if **any**(fnmatch.fnmatch(nf_root, pattern) for pattern in **includes**):      ",
        "Language": "Python",
        "Tags": [
            "python",
            "file",
            "os.walk"
        ],
        "URL": "https://stackoverflow.com/questions/229186/os-walk-without-digging-into-directories-below",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I limit os.walk to only return files in the directory I provide it?  def _dir_list(self, dir_name, whitelist):     outputList = []     for root, dirs, files in os.walk(dir_name):         for f in files:             if os.path.splitext(f)[1] in whitelist:                 outputList.append(os.path.join(root, f))             else:                 self._email_to_(\"ignore\")     return outputList      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  I think you're reading those stats incorrectly. They show that Python is up to about 400 times slower than C++ and with the exception of a single case, Python is more of a memory hog. When it comes to source size though, Python wins flat out.  My experiences with Python show the same definite trend that Python is on the order of between 10 and 100 times slower than C++ when doing any serious number crunching.  There are many reasons for this, the major ones being: a) Python is interpreted, while C++ is compiled; b) Python has no primitives, everything including the builtin types (int, float, etc.) are objects; c) a Python list can hold objects of different type, so each entry has to store additional data about its type. These all severely hinder both runtime and memory consumption.  This is no reason to ignore Python though. A lot of software doesn't require much time or memory even with the 100 time slowness factor. Development cost is where Python wins with the simple and concise style. This improvement on development cost often outweighs the cost of additional cpu and memory resources. When it doesn't, however, then C++ wins.     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "191",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  All the slowest (>100x) usages of Python on the shootout are scientific operations that require high GFlop/s count. You should NOT use python for those anyways. The correct way to use python is to import a module that does those calculations, and then go have a relaxing afternoon with your family. That is the pythonic way :)     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "109",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  My experience is the same as the benchmarks.  Python can be slow and uses more memory.  I write much, much less code and it works the first time with much less debugging.  Since it manages memory for me, I don't have to do any memory management, saving hours of chasing down core leaks.  What's your question?     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  Source size is not really a sensible thing to measure. For example, the following shell script:  cat foobar   is much shorter than either its Python or C++ equivalents.     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  Also: Psyco vs. C++.  It's still a bad comparison, since noone would do the numbercrunchy stuff benchmarks tend to focus on in pure Python anyway. A better one would be comparing the performance of realistic applications, or C++ versus NumPy, to get an idea whether your program will be noticeably slower.     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  The problem here is that you have two different languages that solve two different problems... its like comparing C++ with assembler.  Python is for rapid application development and for when performance is a minimal concern.  C++ is not for rapid application development and inherits a legacy of speed from C - for low level programming.     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  I think those stats show that Python is much slower and uses more memory for those benchmarks - are you sure you're reading them the right way up?  In my experience, which is mostly with writing network- and file-system-bound programs in Python, Python isn't significantly slower in any way that matters.  For that kind of work, its benefits outweigh its costs.     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is Python faster and lighter than C++? [closed]",
        "A_Content": "  It's the same problem with managed and easy to use programming language as always - they are slow (and sometimes memory-eating).  These are languages to do control rather than processing. If I would have to write application to transform images and had to use Python too all the processing could be written in C++ and connected to Python via bindings while interface and process control would be definetely Python.     ",
        "Language": "Python",
        "Tags": [
            "c++",
            "python",
            "performance",
            "memory",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/801657/is-python-faster-and-lighter-than-c",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've always thought that Python's advantages are code readibility and development speed, but time and memory usage were not as good as those of C++.  These stats struck me really hard.  What does your experience tell you about Python vs C++ time and memory usage?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "TensorFlow saving into/loading a graph from a file",
        "A_Content": "  There are many ways to approach the problem of saving a model in TensorFlow, which can make it a bit confusing. Taking each of your sub-questions in turn:   The checkpoint files (produced e.g. by calling saver.save() on a tf.train.Saver object) contain only the weights, and any other variables defined in the same program. To use them in another program, you must re-create the associated graph structure (e.g. by running code to build it again, or calling tf.import_graph_def()), which tells TensorFlow what to do with those weights. Note that calling saver.save() also produces a file containing a MetaGraphDef, which contains a graph and details of how to associate the weights from a checkpoint with that graph. See the tutorial for more details. tf.train.write_graph() only writes the graph structure; not the weights. Bazel is unrelated to reading or writing TensorFlow graphs. (Perhaps I misunderstand your question: feel free to clarify it in a comment.) A frozen graph can be loaded using tf.import_graph_def(). In this case, the weights are (typically) embedded in the graph, so you don't need to load a separate checkpoint. The main change would be to update the names of the tensor(s) that are fed into the model, and the names of the tensor(s) that are fetched from the model. In the TensorFlow Android demo, this would correspond to the inputName and outputName strings that are passed to TensorFlowClassifier.initializeTensorFlow(). The GraphDef is the program structure, which typically does not change through the training process. The checkpoint is a snapshot of the state of a training process, which typically changes at every step of the training process. As a result, TensorFlow uses different storage formats for these types of data, and the low-level API provides different ways to save and load them. Higher-level libraries, such as the MetaGraphDef libraries, Keras, and skflow build on these mechanisms to provide more convenient ways to save and restore an entire model.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "tensorflow",
            "protocol-buffers"
        ],
        "URL": "https://stackoverflow.com/questions/38947658/tensorflow-saving-into-loading-a-graph-from-a-file",
        "A_Votes": "62",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    From what I've gathered so far, there are several different ways of dumping a TensorFlow graph into a file and then loading it into another program, but I haven't been able to find clear examples/information on how they work. What I already know is this:   Save the model's variables into a checkpoint file (.ckpt) using a tf.train.Saver() and restore them later (source) Save a model into a .pb file and load it back in using tf.train.write_graph() and tf.import_graph_def() (source) Load in a model from a .pb file, retrain it, and dump it into a new .pb file using Bazel (source) Freeze the graph to save the graph and weights together (source) Use as_graph_def() to save the model, and for weights/variables, map them into constants (source)   However, I haven't been able to clear up several questions regarding these different methods:   Regarding checkpoint files, do they only save the trained weights of a model? Could checkpoint files be loaded into a new program, and be used to run the model, or do they simply serve as ways to save the weights in a model at a certain time/stage? Regarding tf.train.write_graph(), are the weights/variables saved as well? Regarding Bazel, can it only save into/load from .pb files for retraining? Is there a simple Bazel command just to dump a graph into a .pb? Regarding freezing, can a frozen graph be loaded in using tf.import_graph_def()? The Android demo for TensorFlow loads in Google's Inception model from a .pb file. If I wanted to substitute my own .pb file, how would I go about doing that? Would I need to change any native code/methods? In general, what exactly is the difference between all these methods? Or more broadly, what is the difference between as_graph_def()/.ckpt/.pb?   In short, what I'm looking for is a method to save both a graph (as in, the various operations and such) and its weights/variables into a file, which can then be used to load the graph and weights into another program, for use (not necessarily continuing/retraining).  Documentation about this topic isn't very straightforward, so any answers/information would be greatly appreciated.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Process to convert simple Python script into Windows executable",
        "A_Content": "  PyInstaller will create a single-file executable if you use the --onefile option (though what it actually does is extracts then runs itself).  There's a simple PyInstaller tutorial here.  If you have any questions about using it, please post them...     ",
        "Language": "Python",
        "Tags": [
            "python",
            "packaging",
            "compilation",
            "py2exe",
            "software-distribution"
        ],
        "URL": "https://stackoverflow.com/questions/2136837/process-to-convert-simple-python-script-into-windows-executable",
        "A_Votes": "28",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I wrote a script that will help a Windows user in her daily life. I want to simply send her the .exe and not ask her to install python, dlls or have to deal with any additional files.  I've read plenty of the stackoverflow entries regarding compiling Python scripts into executable files. I am a bit confused as there are many options but some seem dated (no updates since 2008) and none were simple enough for me not to be asking this right now after a few hours spent on this.   I'm hoping there's a better, up-to-date way to do this.  I looked into:   pylunch py2exe cx_Freeze py2app (only for Mac) pyinstaller bbfreeze   but either I couldn't get them to work or couldn't understand how to get the result I need. The closest I got was with py2exe but it still gave me the MSVCR71.dll  I would appreciate a step-by-step answer as I was also unable to follow some of the tweaking answers here that require some prior understanding of how to use py2exe or some of the other tools.  I'm using Python 2.5 as one of the modules is only available for that version.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Process to convert simple Python script into Windows executable",
        "A_Content": "  Using py2exe, include this in your setup.py:    from distutils.core import setup import py2exe, sys, os  sys.argv.append('py2exe')  setup(     options = {'py2exe': {'bundle_files': 1}},     windows = [{'script': \"YourScript.py\"}],     zipfile = None, )   then you can run it through command prompt / Idle, both works for me. Hope it helps     ",
        "Language": "Python",
        "Tags": [
            "python",
            "packaging",
            "compilation",
            "py2exe",
            "software-distribution"
        ],
        "URL": "https://stackoverflow.com/questions/2136837/process-to-convert-simple-python-script-into-windows-executable",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I wrote a script that will help a Windows user in her daily life. I want to simply send her the .exe and not ask her to install python, dlls or have to deal with any additional files.  I've read plenty of the stackoverflow entries regarding compiling Python scripts into executable files. I am a bit confused as there are many options but some seem dated (no updates since 2008) and none were simple enough for me not to be asking this right now after a few hours spent on this.   I'm hoping there's a better, up-to-date way to do this.  I looked into:   pylunch py2exe cx_Freeze py2app (only for Mac) pyinstaller bbfreeze   but either I couldn't get them to work or couldn't understand how to get the result I need. The closest I got was with py2exe but it still gave me the MSVCR71.dll  I would appreciate a step-by-step answer as I was also unable to follow some of the tweaking answers here that require some prior understanding of how to use py2exe or some of the other tools.  I'm using Python 2.5 as one of the modules is only available for that version.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Process to convert simple Python script into Windows executable",
        "A_Content": "  i would recommend going to http://sourceforge.net/projects/py2exe/files/latest/download?source=files to download py2exe. Then make a python file named setup.py.  Inside it, type  from distutils.core import setup import py2exe setup(console=['nameoffile.py'])   Save in your user folder Also save the file you want converted in that same folder  Run window's command prompt type in setup.py install py2exe  It should print many lines of code...  Next, open the dist folder.  Run the exe file.  If there are needed files for the program to work, move them to the folder  Copy/Send the dist folder to person.  Optional: Change the name of the dist folder  Hope it works!:)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "packaging",
            "compilation",
            "py2exe",
            "software-distribution"
        ],
        "URL": "https://stackoverflow.com/questions/2136837/process-to-convert-simple-python-script-into-windows-executable",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I wrote a script that will help a Windows user in her daily life. I want to simply send her the .exe and not ask her to install python, dlls or have to deal with any additional files.  I've read plenty of the stackoverflow entries regarding compiling Python scripts into executable files. I am a bit confused as there are many options but some seem dated (no updates since 2008) and none were simple enough for me not to be asking this right now after a few hours spent on this.   I'm hoping there's a better, up-to-date way to do this.  I looked into:   pylunch py2exe cx_Freeze py2app (only for Mac) pyinstaller bbfreeze   but either I couldn't get them to work or couldn't understand how to get the result I need. The closest I got was with py2exe but it still gave me the MSVCR71.dll  I would appreciate a step-by-step answer as I was also unable to follow some of the tweaking answers here that require some prior understanding of how to use py2exe or some of the other tools.  I'm using Python 2.5 as one of the modules is only available for that version.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Process to convert simple Python script into Windows executable",
        "A_Content": "  I would join @Nicholas in recommending PyInstaller (with the --onefile flag), but be warned: do not use the \"latest release\", PyInstaller 1.3 -- it's years old.  Use the \"pre-release\" 1.4, download it here -- or even better the code from the svn repo -- install SVN and run svn co http://svn.pyinstaller.org/trunk pyinstaller.  As @Nicholas implies, dynamic libraries cannot be run from the same file as the rest of the executable -- but fortunately they can be packed together with all the rest in a \"self-unpacking\" executable that will unpack itself into some temporary directory as needed; PyInstaller does a good job at this (and at many other things -- py2exe is more popular, but pyinstaller in my opinion is preferable in all other respects).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "packaging",
            "compilation",
            "py2exe",
            "software-distribution"
        ],
        "URL": "https://stackoverflow.com/questions/2136837/process-to-convert-simple-python-script-into-windows-executable",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I wrote a script that will help a Windows user in her daily life. I want to simply send her the .exe and not ask her to install python, dlls or have to deal with any additional files.  I've read plenty of the stackoverflow entries regarding compiling Python scripts into executable files. I am a bit confused as there are many options but some seem dated (no updates since 2008) and none were simple enough for me not to be asking this right now after a few hours spent on this.   I'm hoping there's a better, up-to-date way to do this.  I looked into:   pylunch py2exe cx_Freeze py2app (only for Mac) pyinstaller bbfreeze   but either I couldn't get them to work or couldn't understand how to get the result I need. The closest I got was with py2exe but it still gave me the MSVCR71.dll  I would appreciate a step-by-step answer as I was also unable to follow some of the tweaking answers here that require some prior understanding of how to use py2exe or some of the other tools.  I'm using Python 2.5 as one of the modules is only available for that version.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Process to convert simple Python script into Windows executable",
        "A_Content": "  1) Get py2exe from here, according to your Python version.  2) Make a file called \"setup.py\" in the same folder as the script you want to convert, having the following code:   from distutils.core import setup import py2exe setup(console=['myscript.py']) #change 'myscript' to your script    3) Go to command prompt, navigate to that folder, and type:   python setup.py py2exe    4) It will generate a \"dist\" folder in the same folder as the script. This folder contains the .exe file.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "packaging",
            "compilation",
            "py2exe",
            "software-distribution"
        ],
        "URL": "https://stackoverflow.com/questions/2136837/process-to-convert-simple-python-script-into-windows-executable",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I wrote a script that will help a Windows user in her daily life. I want to simply send her the .exe and not ask her to install python, dlls or have to deal with any additional files.  I've read plenty of the stackoverflow entries regarding compiling Python scripts into executable files. I am a bit confused as there are many options but some seem dated (no updates since 2008) and none were simple enough for me not to be asking this right now after a few hours spent on this.   I'm hoping there's a better, up-to-date way to do this.  I looked into:   pylunch py2exe cx_Freeze py2app (only for Mac) pyinstaller bbfreeze   but either I couldn't get them to work or couldn't understand how to get the result I need. The closest I got was with py2exe but it still gave me the MSVCR71.dll  I would appreciate a step-by-step answer as I was also unable to follow some of the tweaking answers here that require some prior understanding of how to use py2exe or some of the other tools.  I'm using Python 2.5 as one of the modules is only available for that version.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Process to convert simple Python script into Windows executable",
        "A_Content": "  you may want to see if your app can run under IronPython. If so, you can compile it to an exe  http://www.codeplex.com/IronPython     ",
        "Language": "Python",
        "Tags": [
            "python",
            "packaging",
            "compilation",
            "py2exe",
            "software-distribution"
        ],
        "URL": "https://stackoverflow.com/questions/2136837/process-to-convert-simple-python-script-into-windows-executable",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I wrote a script that will help a Windows user in her daily life. I want to simply send her the .exe and not ask her to install python, dlls or have to deal with any additional files.  I've read plenty of the stackoverflow entries regarding compiling Python scripts into executable files. I am a bit confused as there are many options but some seem dated (no updates since 2008) and none were simple enough for me not to be asking this right now after a few hours spent on this.   I'm hoping there's a better, up-to-date way to do this.  I looked into:   pylunch py2exe cx_Freeze py2app (only for Mac) pyinstaller bbfreeze   but either I couldn't get them to work or couldn't understand how to get the result I need. The closest I got was with py2exe but it still gave me the MSVCR71.dll  I would appreciate a step-by-step answer as I was also unable to follow some of the tweaking answers here that require some prior understanding of how to use py2exe or some of the other tools.  I'm using Python 2.5 as one of the modules is only available for that version.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Process to convert simple Python script into Windows executable",
        "A_Content": "  You could create an installer for you EXE file by: 1. Press WinKey + R 2. Type \"iexpress\" (without quotes) into the run window 3. Complete the wizard for creating the installation program. 4. Distribute the completed EXE.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "packaging",
            "compilation",
            "py2exe",
            "software-distribution"
        ],
        "URL": "https://stackoverflow.com/questions/2136837/process-to-convert-simple-python-script-into-windows-executable",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I wrote a script that will help a Windows user in her daily life. I want to simply send her the .exe and not ask her to install python, dlls or have to deal with any additional files.  I've read plenty of the stackoverflow entries regarding compiling Python scripts into executable files. I am a bit confused as there are many options but some seem dated (no updates since 2008) and none were simple enough for me not to be asking this right now after a few hours spent on this.   I'm hoping there's a better, up-to-date way to do this.  I looked into:   pylunch py2exe cx_Freeze py2app (only for Mac) pyinstaller bbfreeze   but either I couldn't get them to work or couldn't understand how to get the result I need. The closest I got was with py2exe but it still gave me the MSVCR71.dll  I would appreciate a step-by-step answer as I was also unable to follow some of the tweaking answers here that require some prior understanding of how to use py2exe or some of the other tools.  I'm using Python 2.5 as one of the modules is only available for that version.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  The lack of NaN rep in integer columns is a pandas \"gotcha\".  The usual workaround is to simply use floats.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  If you can modify your stored data, use a sentinel value for missing id. A common use case, inferred by the column name,  being that id is an integer, strictly greater than zero, you could use 0 as a sentinel value so that you can write  if row['id']:    regular_process(row) else:    special_process(row)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  My use case is munging data prior to loading in a DB table:  df[col] = df[col].fillna(-1) df[col] = df[col].astype(int) df[col] = df[col].astype(str) df[col] = df[col].replace('-1', np.nan)   Remove NaNs, convert to int, convert to str and then reinsert NANs.  It's not pretty but it gets the job done!      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  Assuming your DateColumn formatted 3312018.0 should be converted to 03/31/2018 as a string.  And, some records are missing or 0.  df['DateColumn'] = df['DateColumn'].astype(int) df['DateColumn'] = df['DateColumn'].astype(str) df['DateColumn'] = df['DateColumn'].apply(lambda x: x.zfill(8)) df.loc[df['DateColumn'] == '00000000','DateColumn'] = '01011980' df['DateColumn'] = pd.to_datetime(df['DateColumn'], format=\"%m%d%Y\") df['DateColumn'] = df['DateColumn'].apply(lambda x: x.strftime('%m/%d/%Y'))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  I ran into this issue working with pyspark. As this is a python frontend for code running on a jvm, it requires type safety and using float instead of int is not an option. I worked around the issue by wrapping the pandas pd.read_csv in a function that will fill user-defined columns with user-defined fill values before casting them to the required type. Here is what I ended up using:  def custom_read_csv(file_path, custom_dtype = None, fill_values = None, **kwargs):     if custom_dtype is None:         return pd.read_csv(file_path, **kwargs)     else:         assert 'dtype' not in kwargs.keys()         df = pd.read_csv(file_path, dtype = {}, **kwargs)         for col, typ in custom_dtype.items():             if fill_values is None or col not in fill_values.keys():                 fill_val = -1             else:                 fill_val = fill_values[col]             df[col] = df[col].fillna(fill_val).astype(typ)     return df      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  Convert to float (ignoring errors) and then convert the result to an int.  df['id'] = df['id'].astype(float, errors='ignore').astype(int)    Alternatively:  df['id'] = df['id'].replace(np.nan,0)    And then use the regular expression:  df['id'] = df['id'].astype(int)   In the case of numbers originally formatted as strings (like '35' instead of 35) the following helps:  df['id'] = df['id'].apply(lambda x: int(x))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  First remove the rows which contain NaN. Then do Integer conversion on remaining rows. At Last insert the removed rows again. Hope it will work     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Convert Pandas column containing NaNs to dtype `int`",
        "A_Content": "  You could use .dropna() if it is OK to drop the rows with the NaN values.  df = df.dropna(subset=['id'])   Alternatively, use .fillna() and .astype() to replace the NaN with values and convert them to int.  I ran into this problem when processing a CSV file with large integers, while some of them were missing (NaN). Using float as the type was not an option, because I might loose the precision.  My solution was to use str as the intermediate type.  Then you can convert the string to int as you please later in the code. I replaced NaN with 0, but you could choose any value.  df = pd.read_csv(filename, dtype={'id':str}) df[\"id\"] = df[\"id\"].fillna(\"0\").astype(int)   For the illustration, here is an example how floats may loose the precision:  s = \"12345678901234567890\" f = float(s) i = int(f) i2 = int(s) print (f, i, i2)   And the output is:  1.2345678901234567e+19 12345678901234567168 12345678901234567890      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "na"
        ],
        "URL": "https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.  When I try to cast the id column to integer while reading the .csv, I get:  df= pd.read_csv(\"data.csv\", dtype={'id': int})  error: Integer column has NA values   Alternatively, I tried to convert the column type after reading as below, but this time I get:  df= pd.read_csv(\"data.csv\")  df[['id']] = df[['id']].astype(int) error: Cannot convert NA to integer   How can I tackle this?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "decorators in the python standard lib (@deprecated specifically)",
        "A_Content": "  Here's some snippet, modified from those cited by Leandro:  import warnings import functools  def deprecated(func):     \"\"\"This is a decorator which can be used to mark functions     as deprecated. It will result in a warning being emitted     when the function is used.\"\"\"     @functools.wraps(func)     def new_func(*args, **kwargs):         warnings.simplefilter('always', DeprecationWarning)  # turn off filter         warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),                       category=DeprecationWarning,                       stacklevel=2)         warnings.simplefilter('default', DeprecationWarning)  # reset filter         return func(*args, **kwargs)     return new_func  # Examples  @deprecated def some_old_function(x, y):     return x + y  class SomeClass:     @deprecated     def some_old_method(self, x, y):         return x + y   Because in some interpreters the first solution exposed (without filter handling) may result in a warning suppression.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "decorator"
        ],
        "URL": "https://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to mark routines as deprecated, but apparently there's no standard library decorator for deprecation. I am aware of recipes for it and the warnings module, but my question is: why is there no standard library decorator for this (common) task ?   Additional question: are there standard decorators in the standard library at all ?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "decorators in the python standard lib (@deprecated specifically)",
        "A_Content": "  Here is another solution:  This decorator (a decorator factory in fact) allow you to give a reason message. It is also more useful to help the developer to diagnose the problem by giving the source filename and line number.  EDIT: This code use Zero's recommendation: it replace warnings.warn_explicit line by warnings.warn(msg, category=DeprecationWarning, stacklevel=2),  which prints the function call site rather than the function definition site. It makes debugging easier.  EDIT2: This version allow the developper to specify an optional \"reason\" message.  import functools import inspect import warnings  string_types = (type(b''), type(u''))   def deprecated(reason):     \"\"\"     This is a decorator which can be used to mark functions     as deprecated. It will result in a warning being emitted     when the function is used.     \"\"\"      if isinstance(reason, string_types):          # The @deprecated is used with a 'reason'.         #         # .. code-block:: python         #         #    @deprecated(\"please, use another function\")         #    def old_function(x, y):         #      pass          def decorator(func1):              if inspect.isclass(func1):                 fmt1 = \"Call to deprecated class {name} ({reason}).\"             else:                 fmt1 = \"Call to deprecated function {name} ({reason}).\"              @functools.wraps(func1)             def new_func1(*args, **kwargs):                 warnings.simplefilter('always', DeprecationWarning)                 warnings.warn(                     fmt1.format(name=func1.__name__, reason=reason),                     category=DeprecationWarning,                     stacklevel=2                 )                 warnings.simplefilter('default', DeprecationWarning)                 return func1(*args, **kwargs)              return new_func1          return decorator      elif inspect.isclass(reason) or inspect.isfunction(reason):          # The @deprecated is used without any 'reason'.         #         # .. code-block:: python         #         #    @deprecated         #    def old_function(x, y):         #      pass          func2 = reason          if inspect.isclass(func2):             fmt2 = \"Call to deprecated class {name}.\"         else:             fmt2 = \"Call to deprecated function {name}.\"          @functools.wraps(func2)         def new_func2(*args, **kwargs):             warnings.simplefilter('always', DeprecationWarning)             warnings.warn(                 fmt2.format(name=func2.__name__),                 category=DeprecationWarning,                 stacklevel=2             )             warnings.simplefilter('default', DeprecationWarning)             return func2(*args, **kwargs)          return new_func2      else:         raise TypeError(repr(type(reason)))   You can use this decorator for functions, methods and classes.  Here is a simple example:  @deprecated(\"use another function\") def some_old_function(x, y):     return x + y   class SomeClass(object):     @deprecated(\"use another method\")     def some_old_method(self, x, y):         return x + y   @deprecated(\"use another class\") class SomeOldClass(object):     pass   some_old_function(5, 3) SomeClass().some_old_method(8, 9) SomeOldClass()   You'll get:  deprecated_example.py:59: DeprecationWarning: Call to deprecated function or method some_old_function (use another function).   some_old_function(5, 3) deprecated_example.py:60: DeprecationWarning: Call to deprecated function or method some_old_method (use another method).   SomeClass().some_old_method(8, 9) deprecated_example.py:61: DeprecationWarning: Call to deprecated class SomeOldClass (use another class).   SomeOldClass()   EDIT3: This decorator is now part of the Deprecated library:   Python package index (PyPi) GitHub website Read The Docs EBook on Lulu.com   New stable release v1.2.3 \uD83C\uDF89     ",
        "Language": "Python",
        "Tags": [
            "python",
            "decorator"
        ],
        "URL": "https://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to mark routines as deprecated, but apparently there's no standard library decorator for deprecation. I am aware of recipes for it and the warnings module, but my question is: why is there no standard library decorator for this (common) task ?   Additional question: are there standard decorators in the standard library at all ?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "decorators in the python standard lib (@deprecated specifically)",
        "A_Content": "  You can create a utils file  import warnings  def deprecated(message):   def deprecated_decorator(func):       def deprecated_func(*args, **kwargs):           warnings.warn(\"{} is a deprecated function. {}\".format(func.__name__, message),                         category=DeprecationWarning,                         stacklevel=2)           warnings.simplefilter('default', DeprecationWarning)           return func(*args, **kwargs)       return deprecated_func   return deprecated_decorator   And then import the deprecation decorator as follows:  from .utils import deprecated  @deprecated(\"Use method yyy instead\") def some_method()\"  pass      ",
        "Language": "Python",
        "Tags": [
            "python",
            "decorator"
        ],
        "URL": "https://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to mark routines as deprecated, but apparently there's no standard library decorator for deprecation. I am aware of recipes for it and the warnings module, but my question is: why is there no standard library decorator for this (common) task ?   Additional question: are there standard decorators in the standard library at all ?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "decorators in the python standard lib (@deprecated specifically)",
        "A_Content": "  I guess the reason is that Python code can't be processed statically (as it done for C++ compilers), you can't get warning about using some things before actually using it. I don't think that it's a good idea to spam user of your script with a bunch of messages \"Warning: this developer of this script is using deprecated API\".  Update: but you can create decorator which will transform original function into another. New function will mark/check switch telling that this function was called already and will show message only on turning switch into on state. And/or at exit it may print list of all deprecated functions used in program.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "decorator"
        ],
        "URL": "https://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to mark routines as deprecated, but apparently there's no standard library decorator for deprecation. I am aware of recipes for it and the warnings module, but my question is: why is there no standard library decorator for this (common) task ?   Additional question: are there standard decorators in the standard library at all ?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "decorators in the python standard lib (@deprecated specifically)",
        "A_Content": "  As muon suggested, you can install the deprecation package for this.     The deprecation library provides a deprecated decorator and a fail_if_not_removed decorator for your tests.   Installation  pip install deprecation   Example Usage  import deprecation  @deprecation.deprecated(deprecated_in=\"1.0\", removed_in=\"2.0\",                         current_version=__version__,                         details=\"Use the bar function instead\") def foo():     \"\"\"Do some stuff\"\"\"     return 1   See http://deprecation.readthedocs.io/ for the full documentation.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "decorator"
        ],
        "URL": "https://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to mark routines as deprecated, but apparently there's no standard library decorator for deprecation. I am aware of recipes for it and the warnings module, but my question is: why is there no standard library decorator for this (common) task ?   Additional question: are there standard decorators in the standard library at all ?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "decorators in the python standard lib (@deprecated specifically)",
        "A_Content": "  UPDATE: I think is better, when we show DeprecationWarning only first time for each code line and when we can send some message:  import inspect import traceback import warnings import functools  import time   def deprecated(message: str = ''):     \"\"\"     This is a decorator which can be used to mark functions     as deprecated. It will result in a warning being emitted     when the function is used first time and filter is set for show DeprecationWarning.     \"\"\"     def decorator_wrapper(func):         @functools.wraps(func)         def function_wrapper(*args, **kwargs):             current_call_source = '|'.join(traceback.format_stack(inspect.currentframe()))             if current_call_source not in function_wrapper.last_call_source:                 warnings.warn(\"Function {} is now deprecated! {}\".format(func.__name__, message),                               category=DeprecationWarning, stacklevel=2)                 function_wrapper.last_call_source.add(current_call_source)              return func(*args, **kwargs)          function_wrapper.last_call_source = set()          return function_wrapper     return decorator_wrapper   @deprecated('You must use my_func2!') def my_func():     time.sleep(.1)     print('aaa')     time.sleep(.1)   def my_func2():     print('bbb')   warnings.simplefilter('always', DeprecationWarning)  # turn off filter print('before cycle') for i in range(5):     my_func() print('after cycle') my_func() my_func() my_func()   Result:  before cycle C:/Users/adr-0/OneDrive/Projects/Python/test/unit1.py:45: DeprecationWarning: Function my_func is now deprecated! You must use my_func2! aaa aaa aaa aaa aaa after cycle C:/Users/adr-0/OneDrive/Projects/Python/test/unit1.py:47: DeprecationWarning: Function my_func is now deprecated! You must use my_func2! aaa C:/Users/adr-0/OneDrive/Projects/Python/test/unit1.py:48: DeprecationWarning: Function my_func is now deprecated! You must use my_func2! aaa C:/Users/adr-0/OneDrive/Projects/Python/test/unit1.py:49: DeprecationWarning: Function my_func is now deprecated! You must use my_func2! aaa  Process finished with exit code 0   We can just click on the warning path and go to the line in PyCharm.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "decorator"
        ],
        "URL": "https://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to mark routines as deprecated, but apparently there's no standard library decorator for deprecation. I am aware of recipes for it and the warnings module, but my question is: why is there no standard library decorator for this (common) task ?   Additional question: are there standard decorators in the standard library at all ?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "decorators in the python standard lib (@deprecated specifically)",
        "A_Content": "  Augmenting this answer by Steven Vascellaro:  If you use Anaconda, first install deprecation package:  conda install -c conda-forge deprecation    Then paste the following on the top of the file  import deprecation  @deprecation.deprecated(deprecated_in=\"1.0\", removed_in=\"2.0\",                     current_version=__version__,                     details=\"Use the bar function instead\") def foo():     \"\"\"Do some stuff\"\"\"     return 1   See http://deprecation.readthedocs.io/ for the full documentation.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "decorator"
        ],
        "URL": "https://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to mark routines as deprecated, but apparently there's no standard library decorator for deprecation. I am aware of recipes for it and the warnings module, but my question is: why is there no standard library decorator for this (common) task ?   Additional question: are there standard decorators in the standard library at all ?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  Quoting from the Django migrations documentation:     The migration files for each app live in a migrations directory inside of that app, and are designed to be committed to, and distributed as part of, its codebase. You should be making them once on your development machine and then running the same migrations on your colleagues machines, your staging machines, and eventually your production machines.   If you follow this process, you shouldn't be getting any merge conflicts in the migration files.  To mitigate any issues you currently have, you should specify which repository or branch has the authoritative version of the migration files, and then use git's attribute mechanism to specify the merge strategy \"ours\" for these files.  This will tell git to always ignore external changes to these files and prefer the local version.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "71",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  No.  I've been over this many times and I can't, for the life of me, find a case where we need migrations in the repo.  As I see it, the definitive record of the schema is models.py. If I merge a change and someone else pulls it, all will be correct when they run makemigrations and migrate. There's no need to define what the strategy of \"ours\" is for migrations.  If we need to roll back, then we revert models and migrate. All good, no problems, ever.  No complaining that a field already exists, etc.  I wonder if anyone can give me a specific case where it's an advantage for me to have to merge another developer's migrations files before I can get to work. I know the docs say I should, so I assume it's so. But I've never encountered even one.  Anyone?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  You can follow the below process.  You can run makemigrations locally and this creates the migration file. Commit this new migration file to repo.   In my opinion you should not run makemigrations in production at all. You can run migrate in production and you will see the migrations are applied from the migration file that you committed from local. This way you can avoid all conflicts.  IN LOCAL ENV, to create the migration files,  python manage.py makemigrations  python manage.py migrate   Now commit these newly created files, something like below.  git add app/migrations/... git commit -m 'add migration files' app/migrations/...     IN PRODUCTION ENV, run only the below command.  python manage.py migrate      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  Quote from the 2018 docs, Django 2.0. (two separate commands = makemigrations and migrate)     The reason that there are separate commands to make and apply   migrations is because youll commit migrations to your version control   system and ship them with your app; they not only make your   development easier, theyre also useable by other developers and in   production.   https://docs.djangoproject.com/en/2.0/intro/tutorial02/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  The solution usually used, is that, before anything is merged into master, the developer must pull any remote changes. If there's a conflict in migration versions, he should rename his local migration (the remote one has been run by other devs, and, potentially, in production), to N+1.  During development it might be okay to just not-commit migrations (don't add an ignore though, just don't add them). But once you've gone into production, you'll need them in order to keep the schema in sync with model changes.  You then need to edit the file, and change the dependencies to the latest remote version.  This works for Django migrations, as well as other similar apps (sqlalchemy+alembic, RoR, etc).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  I can't imagine why you would be getting conflicts, unless you're editing the migrations somehow? That usually ends badly - if someone misses some intermediate commits then they won't be upgrading from the correct version, and their copy of the database will be corrupted.  The process that I follow is pretty simple - whenever you change the models for an app, you also commit a migration, and then that migration doesn't change - if you need something different in the model, then you change the model and commit a new migration alongside your changes.  In greenfield projects, you can often delete the migrations and start over from scratch with a 0001_ migration when you release, but if you have production code, then you can't (though you can squash migrations down into one).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  Feels like you'd need to adjust your git workflow, instead of ignoring conflicts.  Ideally, every new feature is developed in a different branch, and merged back with a pull request.  PR cannot be merged if there's a conflict, therefore who needs to merge his feature needs to resolve the conflict, migrations included.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  Short answer I propose excluding migrations in the repo. After code merge, just run ./manage.py makemigrations and you are all set.  Long answer I don't think you should put migrations files into repo. It will spoil the migration states in other person's dev environment and other prod and stage environment. (refer to Sugar Tang's comment for examples).   In my point of view, the purpose of Django migrations is to find gaps between previous model states and new model states, and then serialise the gap. If your model changes after code merge, you can simple do makemigrations to find out the gap. Why do you want to manually and carefully merge other migrations when you can achieve the same automatically and bug free? Django documentation says,     They*(migrations)*re designed to be mostly automatic   ; please keep it that way. To merge migrations manually, you have to fully understand what others have changed and any dependence of the changes. That's a lot of overhead and error prone. So tracking models file is sufficient.  It is a good topic on the workflow. I am open to other options.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Should I be adding the Django migration files in the .gitignore file?",
        "A_Content": "  Having a bunch of migration files in git is messy. There is only one file in migration folder that you should not ignore. That file is init.py file, If you ignore it, python will no longer look for submodules inside the directory, so any attempts to import the modules will fail. So the question should be how to ignore all migration files but init.py?  The solution is:  Add '0*.py' to .gitignore files and it does the job perfectly.   Hope this helps someone.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "git"
        ],
        "URL": "https://stackoverflow.com/questions/28035119/should-i-be-adding-the-django-migration-files-in-the-gitignore-file",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Should I be adding the Django migration files in the .gitignore file?   I've recently been getting a lot of git issues due to migration conflicts and was wondering if I should be marking migration files as ignore.   If so, how would I go about adding all of the migrations that I have in my apps, and adding them to the .gitignore file?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: reverse accessors for foreign keys clashing",
        "A_Content": "  The related_name would ensure that the fields were not conflicting with each other, but you have two models, each of which has both of those fields. You need to put the name of the concrete model in each one, which you can do with some special string substitution:   create_user = models.ForeignKey(User, related_name='%(class)s_requests_created')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/22538563/django-reverse-accessors-for-foreign-keys-clashing",
        "A_Votes": "96",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have two Django models which inherit from a base class:  - Request     - Inquiry     - Analysis   Request has two foreign keys to the built-in User model.  create_user = models.ForeignKey(User, related_name='requests_created') assign_user = models.ForeignKey(User, related_name='requests_assigned')   For some reason I'm getting the error  Reverse accessor for 'Analysis.assign_user' clashes with reverse accessor for 'Inquiry.assign_user'.  Everything I've read says that setting the related_name should prevent the clash, but I'm still getting the same error. Can anyone think of why this would be happening? Thanks!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Most suitable python library for Github API v3 [closed]",
        "A_Content": "  In the end, I ended up using PyGithub. It works well, and the author is really receptive for feedback and bug reports. :-)  (Adapted from my edit to the original question, for better visibility)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "api",
            "github"
        ],
        "URL": "https://stackoverflow.com/questions/10625190/most-suitable-python-library-for-github-api-v3",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am looking for a python library for the Github APIv3 suitable for me.  Background: I am a python noob with a background primarily rooted in Matlab and C++, and have recently learned to use python-matplotlib.  I found one library (python-github3) mentioned in the GH API docs. After playing around with it in ipython for an hour or two, I found it really unintuitive to explore/work with. I looked some more, and found there's quite a number of people at least attempting to write such a library. The more promising-looking (at a glance) are PyGithub and another python-github3, which apparently is different from the first one.  Before I spend the next days consecutively trying out library after library, I wanted to ask the SO community if there is an accepted, definitive, obvious choice for that library?  What I didn't like about the first library was the (to me) unintuitive way to get at data - some things you get as attributes, some you get as return value of a method, that return value is some complicated object which has to be paged and iterated through, etc.   In that regard, PyGithub looks more attractive at first glance - clearly drill down through an object hierarchy, and then arrive at the attribute containing what you want:  for repo in g.get_user().get_repos():     print repo.name  So, any pearls of wisdom to share? I know I don't have skills enough to quickly judge library quality, which is why I'm turning to the SO community.  edit: fwiw, I ended up using PyGithub. It works well, and the author is really receptive for feedback and bug reports. :-)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Most suitable python library for Github API v3 [closed]",
        "A_Content": "  Since you mentioned you are a beginner python programmer, I would suggest you to try to use the JSON API without any Github library first. It really isn't that difficult and it will help you a lot later in your programming life since same approach can be applied to any JSON API. Especially if it seems that trying out libraries will take days.   I'm not saying that some library isn't easier to use, I'm just saying the small extra effort to use the API directly might be worth it in the long run. At least it will help you understand why some of those libraries seem \"unintuitive\" (as you said).  Simple example to fetch creation time of django repository:  import requests import json r = requests.get('https://api.github.com/repos/django/django') if(r.ok):     repoItem = json.loads(r.text or r.content)     print \"Django repository created: \" + repoItem['created_at']   This is using the popular requests library. In your code you'll naturally need to handle the error cases too.  If you need access with authentication it will be a bit more complex.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "api",
            "github"
        ],
        "URL": "https://stackoverflow.com/questions/10625190/most-suitable-python-library-for-github-api-v3",
        "A_Votes": "66",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a python library for the Github APIv3 suitable for me.  Background: I am a python noob with a background primarily rooted in Matlab and C++, and have recently learned to use python-matplotlib.  I found one library (python-github3) mentioned in the GH API docs. After playing around with it in ipython for an hour or two, I found it really unintuitive to explore/work with. I looked some more, and found there's quite a number of people at least attempting to write such a library. The more promising-looking (at a glance) are PyGithub and another python-github3, which apparently is different from the first one.  Before I spend the next days consecutively trying out library after library, I wanted to ask the SO community if there is an accepted, definitive, obvious choice for that library?  What I didn't like about the first library was the (to me) unintuitive way to get at data - some things you get as attributes, some you get as return value of a method, that return value is some complicated object which has to be paged and iterated through, etc.   In that regard, PyGithub looks more attractive at first glance - clearly drill down through an object hierarchy, and then arrive at the attribute containing what you want:  for repo in g.get_user().get_repos():     print repo.name  So, any pearls of wisdom to share? I know I don't have skills enough to quickly judge library quality, which is why I'm turning to the SO community.  edit: fwiw, I ended up using PyGithub. It works well, and the author is really receptive for feedback and bug reports. :-)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Most suitable python library for Github API v3 [closed]",
        "A_Content": "  Documentation is horrible for PyGitHub, but the product is great.  Here is a quick sample for actually retrieving a file, changing it with a new comment at the beginning of the file and committing it back  from github import Github gh = Github(login_or_token='.....', base_url='...../api/v3') user = gh.get_user() repo = user.get_repo(\"RepoName\") file = repo.get_file_contents(\"/App/forms.py\") decoded_content = \"# Test \" + \"\\r\\n\" + file.decoded_content repo.update_file(\"/\"RepoName\"/forms.py\", \"Commit Comments\",decoded_content,   file.sha)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "api",
            "github"
        ],
        "URL": "https://stackoverflow.com/questions/10625190/most-suitable-python-library-for-github-api-v3",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a python library for the Github APIv3 suitable for me.  Background: I am a python noob with a background primarily rooted in Matlab and C++, and have recently learned to use python-matplotlib.  I found one library (python-github3) mentioned in the GH API docs. After playing around with it in ipython for an hour or two, I found it really unintuitive to explore/work with. I looked some more, and found there's quite a number of people at least attempting to write such a library. The more promising-looking (at a glance) are PyGithub and another python-github3, which apparently is different from the first one.  Before I spend the next days consecutively trying out library after library, I wanted to ask the SO community if there is an accepted, definitive, obvious choice for that library?  What I didn't like about the first library was the (to me) unintuitive way to get at data - some things you get as attributes, some you get as return value of a method, that return value is some complicated object which has to be paged and iterated through, etc.   In that regard, PyGithub looks more attractive at first glance - clearly drill down through an object hierarchy, and then arrive at the attribute containing what you want:  for repo in g.get_user().get_repos():     print repo.name  So, any pearls of wisdom to share? I know I don't have skills enough to quickly judge library quality, which is why I'm turning to the SO community.  edit: fwiw, I ended up using PyGithub. It works well, and the author is really receptive for feedback and bug reports. :-)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Most suitable python library for Github API v3 [closed]",
        "A_Content": "  Libsaas is great for beginners. It has a nice documentation and is easy to use, see example. Feel free to contribute.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "api",
            "github"
        ],
        "URL": "https://stackoverflow.com/questions/10625190/most-suitable-python-library-for-github-api-v3",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a python library for the Github APIv3 suitable for me.  Background: I am a python noob with a background primarily rooted in Matlab and C++, and have recently learned to use python-matplotlib.  I found one library (python-github3) mentioned in the GH API docs. After playing around with it in ipython for an hour or two, I found it really unintuitive to explore/work with. I looked some more, and found there's quite a number of people at least attempting to write such a library. The more promising-looking (at a glance) are PyGithub and another python-github3, which apparently is different from the first one.  Before I spend the next days consecutively trying out library after library, I wanted to ask the SO community if there is an accepted, definitive, obvious choice for that library?  What I didn't like about the first library was the (to me) unintuitive way to get at data - some things you get as attributes, some you get as return value of a method, that return value is some complicated object which has to be paged and iterated through, etc.   In that regard, PyGithub looks more attractive at first glance - clearly drill down through an object hierarchy, and then arrive at the attribute containing what you want:  for repo in g.get_user().get_repos():     print repo.name  So, any pearls of wisdom to share? I know I don't have skills enough to quickly judge library quality, which is why I'm turning to the SO community.  edit: fwiw, I ended up using PyGithub. It works well, and the author is really receptive for feedback and bug reports. :-)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  There is nothing \"dirty\" about using try-except clause. This is the pythonic way. ValueError will be raised by the .index method only, because it's the only code you have there!  To answer the comment: In Python, easier to ask forgiveness than to get permission philosophy is well established, and no index will not raise this type of error for any other issues. Not that I can think of any.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "48",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  thing_index = thing_list.index(elem) if elem in thing_list else -1   One line. Simple. No exceptions.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "36",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  The dict type has a get function, where if the key doesn't exist in the dictionary, the 2nd argument to get is the value that it should return.  Similarly there is setdefault, which returns the value in the dict if the key exists, otherwise it sets the value according to your default parameter and then returns your default parameter.  You could extend the list type to have a getindexdefault method.  class SuperDuperList(list):     def getindexdefault(self, elem, default):         try:             thing_index = self.index(elem)             return thing_index         except ValueError:             return default   Which could then be used like:  mylist = SuperDuperList([0,1,2]) index = mylist.getindexdefault( 'asdf', -1 )      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  There is nothing wrong with your code that uses ValueError. Here's yet another one-liner if you'd like to avoid exceptions:  thing_index = next((i for i, x in enumerate(thing_list) if x == thing), -1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  This issue is one of language philosophy. In Java for example there has always been a tradition that exceptions should really only be used in \"exceptional circumstances\" that is when errors have happened, rather than for flow control. In the beginning this was for performance reasons as Java exceptions were slow but now this has become the accepted style.  In contrast Python has always used exceptions to indicate normal program flow, like raising a ValueError as we are discussing here. There is nothing \"dirty\" about this in Python style and there are many more where that came from. An even more common example is StopIteration exception which is raised by an iterators next() method to signal that there are no further values.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  What about this:  otherfunction(thing_collection, thing)   Rather than expose something so implementation-dependent like a list index in a function interface, pass the collection and the thing and let otherfunction deal with the \"test for membership\" issues.  If otherfunction is written to be collection-type-agnostic, then it would probably start with:  if thing in thing_collection:     ... proceed with operation on thing   which will work if thing_collection is a list, tuple, set, or dict.  This is possibly clearer than:  if thing_index != MAGIC_VALUE_INDICATING_NOT_A_MEMBER:   which is the code you already have in otherfunction.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  I have the same issue with the \".index()\" method on lists. I have no issue with the fact that it throws an exception but I strongly disagree with the fact that it's a non-descriptive ValueError. I could understand if it would've been an IndexError, though.  I can see why returning \"-1\" would be an issue too because it's a valid index in Python. But realistically, I never expect a \".index()\" method to return a negative number.  Here goes a one liner (ok, it's a rather long line ...), goes through the list exactly once and returns \"None\" if the item isn't found. It would be trivial to rewrite it to return -1, should you so desire.  indexOf = lambda list, thing: \\             reduce(lambda acc, (idx, elem): \\                    idx if (acc is None) and elem == thing else acc, list, None)   How to use:  >>> indexOf([1,2,3], 4) >>> >>> indexOf([1,2,3], 1) 0 >>>      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  If you are doing this often then it is better to stove it away in a helper function:  def index_of(val, in_list):     try:         return in_list.index(val)     except ValueError:         return -1       ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  I don't know why you should think it is dirty... because of the exception? if you want a oneliner, here it is:  thing_index = thing_list.index(elem) if thing_list.count(elem) else -1   but i would advise against using it; I think Ross Rogers solution is the best, use an object to encapsulate your desiderd behaviour, don't try pushing the language to its limits at the cost of readability.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Best way to handle list.index(might-not-exist) in python?",
        "A_Content": "  I'd suggest:  if thing in thing_list:   list_index = -1 else:   list_index = thing_list.index(thing)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "find",
            "indexing"
        ],
        "URL": "https://stackoverflow.com/questions/2132718/best-way-to-handle-list-indexmight-not-exist-in-python",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code which looks something like this:  thing_index = thing_list.index(thing) otherfunction(thing_list, thing_index)   ok so that's simplified but you get the idea. Now thing might not actually be in the list, in which case I want to pass -1 as thing_index. In other languages this is what you'd expect index() to return if it couldn't find the element. In fact it throws a ValueError.  I could do this:  try:     thing_index = thing_list.index(thing) except ValueError:     thing_index = -1 otherfunction(thing_list, thing_index)   But this feels dirty, plus I don't know if ValueError could be raised for some other reason. I came up with the following solution based on generator functions, but it seems a little complex:  thing_index = ( [(i for i in xrange(len(thing_list)) if thing_list[i]==thing)] or [-1] )[0]   Is there a cleaner way to achieve the same thing? Let's assume the list isn't sorted.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Joining pairs of elements of a list - Python",
        "A_Content": "  You can use slice notation with steps:  >>> x = \"abcdefghijklm\" >>> x[0::2] #0. 2. 4... 'acegikm' >>> x[1::2] #1. 3. 5 .. 'bdfhjl' >>> [i+j for i,j in zip(x[::2], x[1::2])] # zip makes (0,1),(2,3) ... ['ab', 'cd', 'ef', 'gh', 'ij', 'kl']   Same logic applies for lists too. String lenght doesn't matter, because you're simply adding two strings together.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "join"
        ],
        "URL": "https://stackoverflow.com/questions/5850986/joining-pairs-of-elements-of-a-list-python",
        "A_Votes": "68",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I know that a list can be joined to make one long string as in:  x = ['a', 'b', 'c', 'd'] print ''.join(x)   Obviously this would output:  'abcd'   However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:  ['ab', 'cd']   Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be:   ['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Joining pairs of elements of a list - Python",
        "A_Content": "  Use an iterator.    List comprehension:  >>> si = iter(['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']) >>> [c+next(si, '') for c in si] ['abcde', 'fghijklmn', 'opqr']    Very efficient for memory usage. Exactly one traversal of s   Generator expression:  >>> si = iter(['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']) >>> pair_iter = (c+next(si, '') for c in si) >>> pair_iter # can be used in a for loop <generator object at 0x4ccaa8> >>> list(pair_iter)  ['abcde', 'fghijklmn', 'opqr']    use as an iterator   Using map, str.__add__, iter  >>> si = iter(['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']) >>> map(str.__add__, si, si) ['abcde', 'fghijklmn', 'opqr']   next(iterator[, default]) is available starting in Python 2.6     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "join"
        ],
        "URL": "https://stackoverflow.com/questions/5850986/joining-pairs-of-elements-of-a-list-python",
        "A_Votes": "29",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know that a list can be joined to make one long string as in:  x = ['a', 'b', 'c', 'd'] print ''.join(x)   Obviously this would output:  'abcd'   However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:  ['ab', 'cd']   Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be:   ['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Joining pairs of elements of a list - Python",
        "A_Content": "  just to be pythonic :-)  >>> x = ['a1sd','23df','aaa','ccc','rrrr', 'ssss', 'e', ''] >>> [x[i] + x[i+1] for i in range(0,len(x),2)] ['a1sd23df', 'aaaccc', 'rrrrssss', 'e']   in case the you want to be alarmed if the list length is odd you can try:  [x[i] + x[i+1] if not len(x) %2 else 'odd index' for i in range(0,len(x),2)]   Best of Luck     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "join"
        ],
        "URL": "https://stackoverflow.com/questions/5850986/joining-pairs-of-elements-of-a-list-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know that a list can be joined to make one long string as in:  x = ['a', 'b', 'c', 'd'] print ''.join(x)   Obviously this would output:  'abcd'   However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:  ['ab', 'cd']   Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be:   ['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Joining pairs of elements of a list - Python",
        "A_Content": "  Without building temporary lists:  >>> import itertools >>> s = 'abcdefgh' >>> si = iter(s) >>> [''.join(each) for each in itertools.izip(si, si)] ['ab', 'cd', 'ef', 'gh']   or:  >>> import itertools >>> s = 'abcdefgh' >>> si = iter(s) >>> map(''.join, itertools.izip(si, si)) ['ab', 'cd', 'ef', 'gh']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "join"
        ],
        "URL": "https://stackoverflow.com/questions/5850986/joining-pairs-of-elements-of-a-list-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know that a list can be joined to make one long string as in:  x = ['a', 'b', 'c', 'd'] print ''.join(x)   Obviously this would output:  'abcd'   However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:  ['ab', 'cd']   Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be:   ['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Joining pairs of elements of a list - Python",
        "A_Content": "  >>> lst =  ['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']  >>> print [lst[2*i]+lst[2*i+1] for i in range(len(lst)/2)] ['abcde', 'fghijklmn', 'opqr']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "join"
        ],
        "URL": "https://stackoverflow.com/questions/5850986/joining-pairs-of-elements-of-a-list-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know that a list can be joined to make one long string as in:  x = ['a', 'b', 'c', 'd'] print ''.join(x)   Obviously this would output:  'abcd'   However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:  ['ab', 'cd']   Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be:   ['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Joining pairs of elements of a list - Python",
        "A_Content": "  Well I would do it this way as I am no good with Regs..  CODE  t = '1. eat, food\\n\\ 7am\\n\\ 2. brush, teeth\\n\\ 8am\\n\\ 3. crack, eggs\\n\\ 1pm'.splitlines()  print [i+j for i,j in zip(t[::2],t[1::2])]   output:    ['1. eat, food   7am', '2. brush, teeth   8am', '3. crack, eggs   1pm']     Hope this helps :)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "list",
            "join"
        ],
        "URL": "https://stackoverflow.com/questions/5850986/joining-pairs-of-elements-of-a-list-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know that a list can be joined to make one long string as in:  x = ['a', 'b', 'c', 'd'] print ''.join(x)   Obviously this would output:  'abcd'   However, what I am trying to do is simply join the first and second strings in the list, then join the third and fourth and so on. In short, from the above example instead achieve an output of:  ['ab', 'cd']   Is there any simple way to do this? I should probably also mention that the lengths of the strings in the list will be unpredictable, as will the number of strings within the list, though the number of strings will always be even. So the original list could just as well be:   ['abcd', 'e', 'fg', 'hijklmn', 'opq', 'r']       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Extract traceback info from an exception object",
        "A_Content": "  The answer to this question depends on the version of Python you're using.  In Python 3  It's simple: exceptions come equipped with a __traceback__ attribute that contains the traceback. This attribute is also writable, and can be conveniently set using the with_traceback method of exceptions:   raise Exception(\"foo occurred\").with_traceback(tracebackobj)   These features are minimally described as part of the raise documentation.  All credit for this part of the answer should go to Vyctor, who first posted this information. I'm including it here only because this answer is stuck at the top, and Python 3 is becoming more common.  In Python 2  It's annoyingly complex. The trouble with tracebacks is that they have references to stack frames, and stack frames have references to the tracebacks that have references to stack frames that have references to... you get the idea. This causes problems for the garbage collector. (Thanks to ecatmur for first pointing this out.)  The nice way of solving this would be to surgically break the cycle after leaving the except clause, which is what Python 3 does. The Python 2 solution is much uglier: you are provided with an ad-hoc function,sys.exc_info(), which only works inside the except clause. It returns a tuple containing the exception, the exception type, and the traceback for whatever exception is currently being handled.   So if you are inside the except clause, you can use the output of sys.exec_info() along with the traceback module to do various useful things:  >>> import sys, traceback >>> def raise_exception(): ...     try: ...         raise Exception ...     except Exception: ...         ex_type, ex, tb = sys.exc_info() ...         traceback.print_tb(tb) ...     finally: ...         del tb ...  >>> raise_exception()   File \"<stdin>\", line 3, in raise_exception   But as your edit indicates, you're trying to get the traceback that would have been printed if your exception had not been handled, after it has already been handled. That's a much harder question. Unfortunately, sys.exc_info returns (None, None, None) when no exception is being handled. Other related sys attribues don't help either. sys.exc_traceback is deprecated and undefined when no exception is being handled; sys.last_traceback seems perfect, but it appears only to be defined during interactive sessions.  If you can control how the exception is raised, you might be able to use inspect and a custom exception to store some of the information. But I'm not entirely sure how that would work.  To tell the truth, catching and returning an exception is kind of an unusual thing to do. This might be a sign that you need to refactor anyway.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "debugging",
            "exception-handling"
        ],
        "URL": "https://stackoverflow.com/questions/11414894/extract-traceback-info-from-an-exception-object",
        "A_Votes": "55",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Given an Exception object (of unknown origin) is there way to obtain its traceback? I have code like this:  def stuff():    try:        .....        return useful    except Exception as e:        return e  result = stuff() if isinstance(result, Exception):     result.traceback <-- How?   How can I extract the traceback from the Exception object once I have it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Extract traceback info from an exception object",
        "A_Content": "  Since Python 3.0[PEP 3109] the built in class Exception has a __traceback__ attribute which contains a traceback object (with Python 3.2.3):    >>> try: ...     raise Exception() ... except Exception as e: ...     tb = e.__traceback__ ... >>> tb <traceback object at 0x00000000022A9208>   The problem is that after Googling __traceback__ for a while I found only few articles but none of them describes whether or why you should (not) use __traceback__.  However, the Python 3 documentation for raise says that:     A traceback object is normally created automatically when an exception is raised and attached to it as the __traceback__ attribute, which is writable.   So I assume it's meant to be used.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "debugging",
            "exception-handling"
        ],
        "URL": "https://stackoverflow.com/questions/11414894/extract-traceback-info-from-an-exception-object",
        "A_Votes": "40",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Given an Exception object (of unknown origin) is there way to obtain its traceback? I have code like this:  def stuff():    try:        .....        return useful    except Exception as e:        return e  result = stuff() if isinstance(result, Exception):     result.traceback <-- How?   How can I extract the traceback from the Exception object once I have it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Extract traceback info from an exception object",
        "A_Content": "  There's a very good reason the traceback is not stored in the exception; because the traceback holds references to its stack's locals, this would result in a circular reference and (temporary) memory leak until the circular GC kicks in.  (This is why you should never store the traceback in a local variable.)  About the only thing I can think of would be for you to monkeypatch stuff's globals so that when it thinks it's catching Exception it's actually catching a specialised type and the exception propagates to you as the caller:  module_containing_stuff.Exception = type(\"BogusException\", (Exception,), {}) try:     stuff() except Exception:     import sys     print sys.exc_info()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "debugging",
            "exception-handling"
        ],
        "URL": "https://stackoverflow.com/questions/11414894/extract-traceback-info-from-an-exception-object",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Given an Exception object (of unknown origin) is there way to obtain its traceback? I have code like this:  def stuff():    try:        .....        return useful    except Exception as e:        return e  result = stuff() if isinstance(result, Exception):     result.traceback <-- How?   How can I extract the traceback from the Exception object once I have it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Extract traceback info from an exception object",
        "A_Content": "  A way to get traceback as a string from an exception object in Python 3:  import traceback  # `e` is an exception object that you get from somewhere traceback_str = ''.join(traceback.format_tb(e.__traceback__))   traceback.format_tb(...) returns a list of strings. ''.join(...) joins them together. For more reference, please visit: https://docs.python.org/3/library/traceback.html#traceback.format_exc     ",
        "Language": "Python",
        "Tags": [
            "python",
            "debugging",
            "exception-handling"
        ],
        "URL": "https://stackoverflow.com/questions/11414894/extract-traceback-info-from-an-exception-object",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Given an Exception object (of unknown origin) is there way to obtain its traceback? I have code like this:  def stuff():    try:        .....        return useful    except Exception as e:        return e  result = stuff() if isinstance(result, Exception):     result.traceback <-- How?   How can I extract the traceback from the Exception object once I have it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "python: is it possible to attach a console into a running process",
        "A_Content": "  If you have access to the program's source-code, you can add this functionality relatively easily.   See Recipe 576515: Debugging a running python process by interrupting and providing an interactive prompt (Python)  To quote:     This provides code to allow any python   program which uses it to be   interrupted at the current point, and   communicated with via a normal python   interactive console. This allows the   locals, globals and associated program   state to be investigated, as well as   calling arbitrary functions and   classes.      To use, a process should import the   module, and call listen() at any point   during startup. To interrupt this   process, the script can be run   directly, giving the process Id of the   process to debug as the parameter.     Another implementation of roughly the same concept is provided by rconsole.  From the documentation:     rconsole is a remote Python console   with auto completion, which can be   used to inspect and modify the   namespace of a running script.      To invoke in a script do:   from rfoo.utils import rconsole rconsole.spawn_server()      To attach from a shell do:   $ rconsole      Security note: The rconsole listener   started with spawn_server() will   accept any local connection and may   therefore be insecure to use in shared   hosting or similar environments!      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/4163964/python-is-it-possible-to-attach-a-console-into-a-running-process",
        "A_Votes": "38",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.  It's better the process is running without being affected(of course performance can down a little bit)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "python: is it possible to attach a console into a running process",
        "A_Content": "  This will interrupt your process (unless you start it in a thread), but you can use the code module to start a Python console:  import code code.interact()   This will block until the user exits the interactive console by executing exit().  The code module is available in at least Python v2.6, probably others.  I tend to use this approach in combination with signals for my Linux work (for Windows, see below). I slap this at the top of my Python scripts:  import code import signal signal.signal(signal.SIGUSR2, lambda sig, frame: code.interact())   And then trigger it from a shell with kill -SIGUSR2 <PID>, where <PID> is the process ID. The process then stops whatever it is doing and presents a console:  Python 2.6.2 (r262:71600, Oct  9 2009, 17:53:52) [GCC 3.4.2] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. (InteractiveConsole) >>>   Generally from there I'll load the server-side component of a remote debugger like the excellent WinPDB.  Windows is not a POSIX-compliant OS, and so does not provide the same signals as Linux. However, Python v2.2 and above expose a Windows-specific signal SIGBREAK (triggered by pressing CTRL+Pause/Break). This does not interfere with normal CTRL+C (SIGINT) operation, and so is a handy alternative.  Therefore a portable, but slightly ugly, version of the above is:  import code import signal signal.signal(         vars(signal).get(\"SIGBREAK\") or vars(signal).get(\"SIGUSR2\"),         lambda sig, frame: code.interact()         )   Advantages of this approach:   No external modules (all standard Python stuff) Barely consumes any resources until triggered (2x import)     Here's the code I use in my production environment which will load the server-side of WinPDB (if available) and fall back to opening a Python console.  # Break into a Python console upon SIGUSR1 (Linux) or SIGBREAK (Windows: # CTRL+Pause/Break).  To be included in all production code, just in case. def debug_signal_handler(signal, frame):     del signal     del frame      try:         import rpdb2         print         print         print \"Starting embedded RPDB2 debugger. Password is 'foobar'\"         print         print         rpdb2.start_embedded_debugger(\"foobar\", True, True)         rpdb2.setbreak(depth=1)         return     except StandardError:         pass      try:         import code         code.interact()     except StandardError as ex:         print \"%r, returning to normal program flow\" % ex  import signal try:     signal.signal(             vars(signal).get(\"SIGBREAK\") or vars(signal).get(\"SIGUSR1\"),             debug_signal_handler             ) except ValueError:     # Typically: ValueError: signal only works in main thread     pass      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/4163964/python-is-it-possible-to-attach-a-console-into-a-running-process",
        "A_Votes": "52",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.  It's better the process is running without being affected(of course performance can down a little bit)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "python: is it possible to attach a console into a running process",
        "A_Content": "  Use pyrasite-shell. I can't believe it works so well, but it does. \"Give it a pid, get a shell\".  $ sudo pip install pyrasite $ echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope # If YAMA activated, see below. $ pyrasite-shell 16262 Pyrasite Shell 2.0 Connected to 'python my_script.py' Python 2.7.6 (default, Jun 22 2015, 17:58:13)  [GCC 4.8.2] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.  >>> globals() >>> print(db_session) >>> run_some_local_function() >>> some_existing_local_variable = 'new value'   This launches the python shell with access to the globals() and locals() variables of that running python process, and other wonderful things.  Only tested this personally on Ubuntu but seems to cater for OSX too.  Adapted from this answer.  Note: The line switching off the ptrace_scope property is only necessary for kernels/systems that have been built with CONFIG_SECURITY_YAMA on. Take care messing with ptrace_scope in sensitive environments because it could introduce certain security vulnerabilities. See here for details.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/4163964/python-is-it-possible-to-attach-a-console-into-a-running-process",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.  It's better the process is running without being affected(of course performance can down a little bit)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "python: is it possible to attach a console into a running process",
        "A_Content": "  Why not simply using the pdb module? It allows you to stop a script, inspect elements values, and execute the code line by line. And since it is built upon the Python interpreter, it also provides the features provided by the classic interpreter. To use it, just put these 2 lines in your code, where you wish to stop and inspect it:  import pdb pdb.set_trace()      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/4163964/python-is-it-possible-to-attach-a-console-into-a-running-process",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.  It's better the process is running without being affected(of course performance can down a little bit)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "python: is it possible to attach a console into a running process",
        "A_Content": "  Another possibility, without adding stuff to the python scripts, is described here:  https://wiki.python.org/moin/DebuggingWithGdb  Unfortunately, this solution also requires some forethought, at least to the extent that you need to be using a version of python with debugging symbols in it.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/4163964/python-is-it-possible-to-attach-a-console-into-a-running-process",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.  It's better the process is running without being affected(of course performance can down a little bit)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "python: is it possible to attach a console into a running process",
        "A_Content": "  Using PyCharm, I was getting a failure to connect to process in Ubuntu.  The fix for this is to disable YAMA.  For more info see askubuntu  echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/4163964/python-is-it-possible-to-attach-a-console-into-a-running-process",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I just want to see the state of the process, is it possible to attach a console into the process, so I can invoke functions inside the process and see some of the global variables.  It's better the process is running without being affected(of course performance can down a little bit)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Unit testing with django-celery?",
        "A_Content": "  Try setting:  BROKER_BACKEND = 'memory'   (Thanks to asksol's comment.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "unit-testing",
            "celery"
        ],
        "URL": "https://stackoverflow.com/questions/4055860/unit-testing-with-django-celery",
        "A_Votes": "41",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am trying to come up with a testing methodology for our django-celery project.  I have read the notes in the documentation, but it didn't give me a good idea of what to actually do.  I am not worried about testing the tasks in the actual daemons, just the functionality of my code.  Mainly I am wondering:   How can we bypass task.delay() during the test (I tried setting CELERY_ALWAYS_EAGER = True but it made no difference)? How do we use the test settings that are recommended (if that is the best way) without actually changing our settings.py? Can we still use manage.py test or do we have to use a custom runner?   Overall any hints or tips for testing with celery would be very helpful.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Unit testing with django-celery?",
        "A_Content": "  I like to use the override_settings decorator on tests which need celery results to complete.  from django.test import TestCase from django.test.utils import override_settings from myapp.tasks import mytask  class AddTestCase(TestCase):      @override_settings(CELERY_EAGER_PROPAGATES_EXCEPTIONS=True,                        CELERY_ALWAYS_EAGER=True,                        BROKER_BACKEND='memory')     def test_mytask(self):         result = mytask.delay()         self.assertTrue(result.successful())   If you want to apply this to all tests you can use the celery test runner as described at http://docs.celeryproject.org/en/2.5/django/unit-testing.html which basically sets these same settings except (BROKER_BACKEND = 'memory').   In settings:  TEST_RUNNER = 'djcelery.contrib.test_runner.CeleryTestSuiteRunner'   Look at the source for CeleryTestSuiteRunner and it's pretty clear what's happening.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "unit-testing",
            "celery"
        ],
        "URL": "https://stackoverflow.com/questions/4055860/unit-testing-with-django-celery",
        "A_Votes": "62",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am trying to come up with a testing methodology for our django-celery project.  I have read the notes in the documentation, but it didn't give me a good idea of what to actually do.  I am not worried about testing the tasks in the actual daemons, just the functionality of my code.  Mainly I am wondering:   How can we bypass task.delay() during the test (I tried setting CELERY_ALWAYS_EAGER = True but it made no difference)? How do we use the test settings that are recommended (if that is the best way) without actually changing our settings.py? Can we still use manage.py test or do we have to use a custom runner?   Overall any hints or tips for testing with celery would be very helpful.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Unit testing with django-celery?",
        "A_Content": "  Here's an excerpt from my testing base class that stubs out the apply_async method and records to the calls to it (which includes Task.delay.)  It's a little gross, but it's managed to fit my needs over the past few months I've been using it.  from django.test import TestCase from celery.task.base import Task # For recent versions, Task has been moved to celery.task.app: # from celery.app.task import Task # See http://docs.celeryproject.org/en/latest/reference/celery.app.task.html  class CeleryTestCaseBase(TestCase):      def setUp(self):         super(CeleryTestCaseBase, self).setUp()         self.applied_tasks = []          self.task_apply_async_orig = Task.apply_async          @classmethod         def new_apply_async(task_class, args=None, kwargs=None, **options):             self.handle_apply_async(task_class, args, kwargs, **options)          # monkey patch the regular apply_sync with our method         Task.apply_async = new_apply_async      def tearDown(self):         super(CeleryTestCaseBase, self).tearDown()          # Reset the monkey patch to the original method         Task.apply_async = self.task_apply_async_orig      def handle_apply_async(self, task_class, args=None, kwargs=None, **options):         self.applied_tasks.append((task_class, tuple(args), kwargs))      def assert_task_sent(self, task_class, *args, **kwargs):         was_sent = any(task_class == task[0] and args == task[1] and kwargs == task[2]                        for task in self.applied_tasks)         self.assertTrue(was_sent, 'Task not called w/class %s and args %s' % (task_class, args))      def assert_task_not_sent(self, task_class):         was_sent = any(task_class == task[0] for task in self.applied_tasks)         self.assertFalse(was_sent, 'Task was not expected to be called, but was.  Applied tasks: %s' %                 self.applied_tasks)   Here's an \"off the top of the head\" example of how you'd use it in your test cases:  mymodule.py  from my_tasks import SomeTask  def run_some_task(should_run):     if should_run:         SomeTask.delay(1, some_kwarg=2)   test_mymodule.py  class RunSomeTaskTest(CeleryTestCaseBase):     def test_should_run(self):         run_some_task(should_run=True)         self.assert_task_sent(SomeTask, 1, some_kwarg=2)      def test_should_not_run(self):         run_some_task(should_run=False)         self.assert_task_not_sent(SomeTask)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "unit-testing",
            "celery"
        ],
        "URL": "https://stackoverflow.com/questions/4055860/unit-testing-with-django-celery",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am trying to come up with a testing methodology for our django-celery project.  I have read the notes in the documentation, but it didn't give me a good idea of what to actually do.  I am not worried about testing the tasks in the actual daemons, just the functionality of my code.  Mainly I am wondering:   How can we bypass task.delay() during the test (I tried setting CELERY_ALWAYS_EAGER = True but it made no difference)? How do we use the test settings that are recommended (if that is the best way) without actually changing our settings.py? Can we still use manage.py test or do we have to use a custom runner?   Overall any hints or tips for testing with celery would be very helpful.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Unit testing with django-celery?",
        "A_Content": "  since I still see this come up in search results, settings override with   TEST_RUNNER = 'djcelery.contrib.test_runner.CeleryTestSuiteRunner'   worked for me as per Celery Docs     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "unit-testing",
            "celery"
        ],
        "URL": "https://stackoverflow.com/questions/4055860/unit-testing-with-django-celery",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am trying to come up with a testing methodology for our django-celery project.  I have read the notes in the documentation, but it didn't give me a good idea of what to actually do.  I am not worried about testing the tasks in the actual daemons, just the functionality of my code.  Mainly I am wondering:   How can we bypass task.delay() during the test (I tried setting CELERY_ALWAYS_EAGER = True but it made no difference)? How do we use the test settings that are recommended (if that is the best way) without actually changing our settings.py? Can we still use manage.py test or do we have to use a custom runner?   Overall any hints or tips for testing with celery would be very helpful.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Function chaining in Python",
        "A_Content": "  I don't know whether this is function chaining as much as it's callable chaining, but, since functions are callables I guess there's no harm done. Either way, there's two ways I can think of doing this:  Sub-classing int and defining __call__:  The first way would be with a custom int subclass that defines __call__ which returns a new instance of itself with the updated value:  class CustomInt(int):     def __call__(self, v):         return CustomInt(self + v)   Function add can now be defined to return a CustomInt instance, which, as a callable that returns an updated value of itself, can be called in succession:  >>> def add(v): ...    return CustomInt(v) >>> add(1) 1 >>> add(1)(2) 3 >>> add(1)(2)(3)(44)  # and so on.. 50   In addition, as an int subclass, the returned value retains the __repr__ and __str__ behavior of ints. For more complex operations though, you should define other dunders appropriately.   As @Caridorc noted in a comment, add could also be simply written as:  add = CustomInt    Renaming the class to add instead of CustomInt also works similarly.    Define a closure, requires extra call to yield value:  The only other way I can think of involves a nested function that requires an extra empty argument call in order to return the result. I'm not using nonlocal and opt for attaching attributes to the function objects to make it portable between Pythons:  def add(v):     def _inner_adder(val=None):           \"\"\"          if val is None we return _inner_adder.v          else we increment and return ourselves         \"\"\"         if val is None:                 return _inner_adder.v         _inner_adder.v += val         return _inner_adder     _inner_adder.v = v  # save value     return _inner_adder    This continuously returns itself (_inner_adder) which, if a val is supplied, increments it (_inner_adder += val) and if not, returns the value as it is. Like I mentioned, it requires an extra () call in order to return the incremented value:  >>> add(1)(2)() 3 >>> add(1)(2)(3)()  # and so on.. 6      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/39038358/function-chaining-in-python",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    On codewars.com I encountered the following task:     Create a function add that adds numbers together when called in succession. So add(1) should return 1, add(1)(2) should return 1+2, ...   While I'm familiar with the basics of Python, I've never encountered a function that is able to be called in such succession, i.e. a function f(x) that can be called as f(x)(y)(z).... Thus far, I'm not even sure how to interpret this notation.   As a mathematician, I'd suspect that f(x)(y) is a function that assigns to every x a function g_{x} and then returns g_{x}(y) and likewise for f(x)(y)(z).   Should this interpretation be correct, Python would allow me to dynamically create functions which seems very interesting to me. I've searched the web for the past hour, but wasn't able to find a lead in the right direction. Since I don't know how this programming concept is called, however, this may not be too surprising.  How do you call this concept and where can I read more about it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Function chaining in Python",
        "A_Content": "  You can hate me, but here is a one-liner :)  add = lambda v: type(\"\", (int,), {\"__call__\": lambda self, v: self.__class__(self + v)})(v)   Edit: Ok, how this works? The code is identical to answer of @Jim, but everything happens on a single line.   type can be used to construct new types: type(name, bases, dict) -> a new type. For name we provide empty string, as name is not really needed in this case. For bases (tuple) we provide an (int,), which is identical to inheriting int. dict are the class attributes, where we attach the __call__ lambda. self.__class__(self + v) is identical to return CustomInt(self + v) The new type is constructed and returned within the outer lambda.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/39038358/function-chaining-in-python",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    On codewars.com I encountered the following task:     Create a function add that adds numbers together when called in succession. So add(1) should return 1, add(1)(2) should return 1+2, ...   While I'm familiar with the basics of Python, I've never encountered a function that is able to be called in such succession, i.e. a function f(x) that can be called as f(x)(y)(z).... Thus far, I'm not even sure how to interpret this notation.   As a mathematician, I'd suspect that f(x)(y) is a function that assigns to every x a function g_{x} and then returns g_{x}(y) and likewise for f(x)(y)(z).   Should this interpretation be correct, Python would allow me to dynamically create functions which seems very interesting to me. I've searched the web for the past hour, but wasn't able to find a lead in the right direction. Since I don't know how this programming concept is called, however, this may not be too surprising.  How do you call this concept and where can I read more about it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Function chaining in Python",
        "A_Content": "  If you want to define a function to be called multiple times, first you need to return a callable object each time (for example a function) otherwise you have to create your own object by defining a __call__ attribute, in order for it to be callable.  The next point is that you need to preserve all the arguments, which in this case means you might want to use Coroutines or a recursive function. But note that Coroutines are much more optimized/flexible than recursive functions, specially for such tasks.  Here is a sample function using Coroutines, that preserves the latest state of itself. Note that it can't be called multiple times since the return value is an integer which is not callable, but you might think about turning this into your expected object ;-).  def add():     current = yield     while True:         value = yield current         current = value + current   it = add() next(it) print(it.send(10)) print(it.send(2)) print(it.send(4))  10 12 16      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/39038358/function-chaining-in-python",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    On codewars.com I encountered the following task:     Create a function add that adds numbers together when called in succession. So add(1) should return 1, add(1)(2) should return 1+2, ...   While I'm familiar with the basics of Python, I've never encountered a function that is able to be called in such succession, i.e. a function f(x) that can be called as f(x)(y)(z).... Thus far, I'm not even sure how to interpret this notation.   As a mathematician, I'd suspect that f(x)(y) is a function that assigns to every x a function g_{x} and then returns g_{x}(y) and likewise for f(x)(y)(z).   Should this interpretation be correct, Python would allow me to dynamically create functions which seems very interesting to me. I've searched the web for the past hour, but wasn't able to find a lead in the right direction. Since I don't know how this programming concept is called, however, this may not be too surprising.  How do you call this concept and where can I read more about it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Function chaining in Python",
        "A_Content": "  The pythonic way to do this would be to use dynamic arguments:  def add(*args):     return sum(args)   This is not the answer you're looking for, and you may know this, but I thought I would give it anyway because if someone was wondering about doing this not out of curiosity but for work. They should probably have the \"right thing to do\" answer.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/39038358/function-chaining-in-python",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    On codewars.com I encountered the following task:     Create a function add that adds numbers together when called in succession. So add(1) should return 1, add(1)(2) should return 1+2, ...   While I'm familiar with the basics of Python, I've never encountered a function that is able to be called in such succession, i.e. a function f(x) that can be called as f(x)(y)(z).... Thus far, I'm not even sure how to interpret this notation.   As a mathematician, I'd suspect that f(x)(y) is a function that assigns to every x a function g_{x} and then returns g_{x}(y) and likewise for f(x)(y)(z).   Should this interpretation be correct, Python would allow me to dynamically create functions which seems very interesting to me. I've searched the web for the past hour, but wasn't able to find a lead in the right direction. Since I don't know how this programming concept is called, however, this may not be too surprising.  How do you call this concept and where can I read more about it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Function chaining in Python",
        "A_Content": "  Simply:  class add(int):    def __call__(self, n):       return add(self + n)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/39038358/function-chaining-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    On codewars.com I encountered the following task:     Create a function add that adds numbers together when called in succession. So add(1) should return 1, add(1)(2) should return 1+2, ...   While I'm familiar with the basics of Python, I've never encountered a function that is able to be called in such succession, i.e. a function f(x) that can be called as f(x)(y)(z).... Thus far, I'm not even sure how to interpret this notation.   As a mathematician, I'd suspect that f(x)(y) is a function that assigns to every x a function g_{x} and then returns g_{x}(y) and likewise for f(x)(y)(z).   Should this interpretation be correct, Python would allow me to dynamically create functions which seems very interesting to me. I've searched the web for the past hour, but wasn't able to find a lead in the right direction. Since I don't know how this programming concept is called, however, this may not be too surprising.  How do you call this concept and where can I read more about it?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  Exceptions are not conditionals.  The conditional version is clearer.  That's natural: this is straightforward flow control, which is what conditionals are designed for, not exceptions.  The exception version is primarily used as an optimization when doing these lookups in a loop: for some algorithms it allows eliminating tests from inner loops.  It doesn't have that benefit here.  It has the small advantage that it avoids having to say \"blah\" twice, but if you're doing a lot of these you should probably have a helper move_key function anyway.  In general, I'd strongly recommend sticking with the conditional version by default unless you have a specific reason not to.  Conditionals are the obvious way to do this, which is usually a strong recommendation to prefer one solution over another.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "58",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  There is also a third way that avoids both exceptions and double-lookup, which can be important if the lookup is expensive:  value = B.get(\"blah\", None) if value is None:      A[\"blah\"] = value   In case you expect the dictionary to contain None values, you can use some more esoteric constants like NotImplemented, Ellipsis or make a new one:  MyConst = object() def update_key(A, B, key):     value = B.get(key, MyConst)     if value is not MyConst:          A[key] = value   Anyway, using update() is the most readable option for me:  a.update((k, b[k]) for k in (\"foo\", \"bar\", \"blah\") if k in b)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  From what I understand, you want to update dict A with key,value pairs from dict B  update is a better choice.  A.update(B)   Example:  >>> A = {'a':1, 'b': 2, 'c':3} >>> B = {'d': 2, 'b':5, 'c': 4} >>> A.update(B) >>> A {'a': 1, 'c': 4, 'b': 5, 'd': 2} >>>       ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  Direct quote from Python performance wiki:     Except for the first time, each time a word is seen the if statement's test fails. If you are counting a large number of words, many will probably occur multiple times. In a situation where the initialization of a value is only going to occur once and the augmentation of that value will occur many times it is cheaper to use a try statement.   So it seems that both options are viable depending from situation. For more details you might like to check this link out: Try-except-performance      ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  I think the general rule here is will A[\"blah\"] normally exist, if so try-except is good if not then use if \"blah\" in b:  I think \"try\" is cheap in time but \"except\" is more expensive.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  I think the second example is what you should go for unless this code makes sense:  try:     A[\"foo\"] = B[\"foo\"]     A[\"bar\"] = B[\"bar\"]     A[\"baz\"] = B[\"baz\"] except KeyError:     pass   Keep in mind that code will abort as soon as there is a key that isn't in B.  If this code makes sense, then you should use the exception method, otherwise use the test method.  In my opinion, because it's shorter and clearly expresses the intent, it's a lot easier to read than the exception method.  Of course, the people telling you to use update are correct.  If you are using a version of Python that supports dictionary comprehensions, I would strongly prefer this code:  updateset = {'foo', 'bar', 'baz'} A.update({k: B[k] for k in updateset if k in B})      ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  The rule in other languages is to reserve exceptions for exceptional conditions, i.e. errors that don't occur in regular use. Don't know how that rule applies to Python, as StopIteration shouldn't exist by that rule.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  Personally, I lean towards the second method (but using has_key):  if B.has_key(\"blah\"):   A[\"blah\"] = B[\"blah\"]   That way, each assignment operation is only two lines (instead of 4 with try/except), and any exceptions that get thrown will be real errors or things you've missed (instead of just trying to access keys that aren't there).  As it turns out (see the comments on your question), has_key is deprecated - so I guess it's better written as  if \"blah\" in B:   A[\"blah\"] = B[\"blah\"]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Python: if key in dict vs. try/except",
        "A_Content": "  Why not justed do this :  def try_except(x,col):     try:         return x[col]     except:         return None  list(map(lambda x: try_except(x,'blah'),A))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "idioms"
        ],
        "URL": "https://stackoverflow.com/questions/4512557/python-if-key-in-dict-vs-try-except",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a question about idioms and readability, and there seems to be a clash of Python philosophies for this particular case:  I want to build dictionary A from dictionary B. If a specific key does not exist in B, then do nothing and continue on.  Which way is better?  try:     A[\"blah\"] = B[\"blah\"] except KeyError:     pass   or   if \"blah\" in B:     A[\"blah\"] = B[\"blah\"]   \"Do and ask for forgiveness\" vs. \"simplicity and explicitness\".  Which is better and why?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to write to an existing excel file without overwriting data (using pandas)?",
        "A_Content": "  Pandas docs says it uses openpyxl for xlsx files. Quick look through the code in ExcelWriter gives a clue that something like this might work out:  import pandas from openpyxl import load_workbook  book = load_workbook('Masterfile.xlsx') writer = pandas.ExcelWriter('Masterfile.xlsx', engine='openpyxl')  writer.book = book writer.sheets = dict((ws.title, ws) for ws in book.worksheets)  data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "excel",
            "python-2.7",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas",
        "A_Votes": "94",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I use pandas to write to excel file in the following fashion:  import pandas  writer = pandas.ExcelWriter('Masterfile.xlsx')   data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()   Masterfile.xlsx already consists of number of different tabs.  Pandas correctly writes to \"Main\" sheet, unfortunately it also deletes all other tabs.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to write to an existing excel file without overwriting data (using pandas)?",
        "A_Content": "  With openpyxlversion 2.4.0 and pandasversion 0.19.2, the process @ski came up with gets a bit simpler:  import pandas from openpyxl import load_workbook  with pandas.ExcelWriter('Masterfile.xlsx', engine='openpyxl') as writer:     writer.book = load_workbook('Masterfile.xlsx')     data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2']) #That's it!      ",
        "Language": "Python",
        "Tags": [
            "python",
            "excel",
            "python-2.7",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use pandas to write to excel file in the following fashion:  import pandas  writer = pandas.ExcelWriter('Masterfile.xlsx')   data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()   Masterfile.xlsx already consists of number of different tabs.  Pandas correctly writes to \"Main\" sheet, unfortunately it also deletes all other tabs.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to write to an existing excel file without overwriting data (using pandas)?",
        "A_Content": "  Here is a helper function:  def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,                        truncate_sheet=False,                         **to_excel_kwargs):     \"\"\"     Append a DataFrame [df] to existing Excel file [filename]     into [sheet_name] Sheet.     If [filename] doesn't exist, then this function will create it.      Parameters:       filename : File path or existing ExcelWriter                  (Example: '/path/to/file.xlsx')       df : dataframe to save to workbook       sheet_name : Name of sheet which will contain DataFrame.                    (default: 'Sheet1')       startrow : upper left cell row to dump data frame.                  Per default (startrow=None) calculate the last row                  in the existing DF and write to the next row...       truncate_sheet : truncate (remove and recreate) [sheet_name]                        before writing DataFrame to Excel file       to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`                         [can be dictionary]      Returns: None     \"\"\"     from openpyxl import load_workbook      # ignore [engine] parameter if it was passed     if 'engine' in to_excel_kwargs:         to_excel_kwargs.pop('engine')      writer = pd.ExcelWriter(filename, engine='openpyxl')      try:         # try to open an existing workbook         writer.book = load_workbook(filename)          # get the last row in the existing Excel sheet         # if it was not specified explicitly         if startrow is None and sheet_name in writer.book.sheetnames:             startrow = writer.book[sheet_name].max_row          # truncate sheet         if truncate_sheet and sheet_name in writer.book.sheetnames:             # index of [sheet_name] sheet             idx = writer.book.sheetnames.index(sheet_name)             # remove [sheet_name]             writer.book.remove(writer.book.worksheets[idx])             # create an empty sheet [sheet_name] using old index             writer.book.create_sheet(sheet_name, idx)          # copy existing sheets         writer.sheets = {ws.title:ws for ws in writer.book.worksheets}     except FileNotFoundError:         # file does not exist yet, we will create it         pass      if startrow is None:         startrow = 0      # write out the new sheet     df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)      # save the workbook     writer.save()   NOTE: for Pandas < 0.21.0, replace sheet_name with sheetname!  Usage examples:  append_df_to_excel('d:/temp/test.xlsx', df)  append_df_to_excel('d:/temp/test.xlsx', df, header=None, index=False)  append_df_to_excel('d:/temp/test.xlsx', df, sheet_name='Sheet2', index=False)  append_df_to_excel('d:/temp/test.xlsx', df, sheet_name='Sheet2', index=False, startrow=25)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "excel",
            "python-2.7",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use pandas to write to excel file in the following fashion:  import pandas  writer = pandas.ExcelWriter('Masterfile.xlsx')   data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()   Masterfile.xlsx already consists of number of different tabs.  Pandas correctly writes to \"Main\" sheet, unfortunately it also deletes all other tabs.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to write to an existing excel file without overwriting data (using pandas)?",
        "A_Content": "  Old question, but I am guessing some people still search for this - so...  I find this method nice because all worksheets are loaded into a dictionary of sheet name and dataframe pairs, created by pandas with the sheetname=None option.  It is simple to add, delete or modify worksheets between reading the spreadsheet into the dict format and writing it back from the dict.  For me the xlsxwriter works better than openpyxl for this particular task in terms of speed and format.  Note: future versions of pandas (0.21.0+) will change the \"sheetname\" parameter to \"sheet_name\".  # read a single or multi-sheet excel file # (returns dict of sheetname(s), dataframe(s)) ws_dict = pd.read_excel(excel_file_path,                         sheetname=None)  # all worksheets are accessible as dataframes.  # easy to change a worksheet as a dataframe: mod_df = ws_dict['existing_worksheet']  # do work on mod_df...then reassign ws_dict['existing_worksheet'] = mod_df  # add a dataframe to the workbook as a new worksheet with # ws name, df as dict key, value: ws_dict['new_worksheet'] = some_other_dataframe  # when done, write dictionary back to excel... # xlsxwriter honors datetime and date formats # (only included as example)... with pd.ExcelWriter(excel_file_path,                     engine='xlsxwriter',                     datetime_format='yyyy-mm-dd',                     date_format='yyyy-mm-dd') as writer:      for ws_name, df_sheet in ws_dict.items():         df_sheet.to_excel(writer, sheet_name=ws_name)   For the example in the 2013 question:  ws_dict = pd.read_excel('Masterfile.xlsx',                         sheetname=None)  ws_dict['Main'] = data_filtered[['Diff1', 'Diff2']]  with pd.ExcelWriter('Masterfile.xlsx',                     engine='xlsxwriter') as writer:      for ws_name, df_sheet in ws_dict.items():         df_sheet.to_excel(writer, sheet_name=ws_name)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "excel",
            "python-2.7",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use pandas to write to excel file in the following fashion:  import pandas  writer = pandas.ExcelWriter('Masterfile.xlsx')   data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()   Masterfile.xlsx already consists of number of different tabs.  Pandas correctly writes to \"Main\" sheet, unfortunately it also deletes all other tabs.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to write to an existing excel file without overwriting data (using pandas)?",
        "A_Content": "  I know this is an older thread, but this is the first item you find when searching, and the above solutions don't work if you need to retain charts in a workbook that you already have created. In that case, xlwings is a better option - it allows you to write to the excel book and keeps the charts/chart data.  simple example:  import xlwings as xw import pandas as pd  #create DF months = ['2017-01','2017-02','2017-03','2017-04','2017-05','2017-06','2017-07','2017-08','2017-09','2017-10','2017-11','2017-12'] value1 = [x * 5+5 for x in range(len(months))] df = pd.DataFrame(value1, index = months, columns = ['value1']) df['value2'] = df['value1']+5 df['value3'] = df['value2']+5  #load workbook that has a chart in it wb = xw.Book('C:\\\\data\\\\bookwithChart.xlsx')  ws = wb.sheets['chartData']  ws.range('A1').options(index=False).value = df  wb = xw.Book('C:\\\\data\\\\bookwithChart_updated.xlsx')  xw.apps[0].quit()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "excel",
            "python-2.7",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use pandas to write to excel file in the following fashion:  import pandas  writer = pandas.ExcelWriter('Masterfile.xlsx')   data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()   Masterfile.xlsx already consists of number of different tabs.  Pandas correctly writes to \"Main\" sheet, unfortunately it also deletes all other tabs.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to write to an existing excel file without overwriting data (using pandas)?",
        "A_Content": "  def append_sheet_to_master(self, master_file_path, current_file_path, sheet_name):     try:         master_book = load_workbook(master_file_path)         master_writer = pandas.ExcelWriter(master_file_path, engine='openpyxl')         master_writer.book = master_book         master_writer.sheets = dict((ws.title, ws) for ws in master_book.worksheets)         current_frames = pandas.ExcelFile(current_file_path).parse(pandas.ExcelFile(current_file_path).sheet_names[0],                                                                header=None,                                                                index_col=None)         current_frames.to_excel(master_writer, sheet_name, index=None, header=False)          master_writer.save()     except Exception as e:         raise e   This works perfectly fine only thing is that formatting of the master file(file to which we add new sheet) is lost.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "excel",
            "python-2.7",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use pandas to write to excel file in the following fashion:  import pandas  writer = pandas.ExcelWriter('Masterfile.xlsx')   data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()   Masterfile.xlsx already consists of number of different tabs.  Pandas correctly writes to \"Main\" sheet, unfortunately it also deletes all other tabs.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to write to an existing excel file without overwriting data (using pandas)?",
        "A_Content": "  writer = pd.ExcelWriter('prueba1.xlsx'engine='openpyxl',keep_date_col=True)   The \"keep_date_col\" hope help you     ",
        "Language": "Python",
        "Tags": [
            "python",
            "excel",
            "python-2.7",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use pandas to write to excel file in the following fashion:  import pandas  writer = pandas.ExcelWriter('Masterfile.xlsx')   data_filtered.to_excel(writer, \"Main\", cols=['Diff1', 'Diff2'])  writer.save()   Masterfile.xlsx already consists of number of different tabs.  Pandas correctly writes to \"Main\" sheet, unfortunately it also deletes all other tabs.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "In Python, what does dict.pop(a,b) mean?",
        "A_Content": "  The pop method of dicts (like self.data, i.e. {'a':'aaa','b':'bbb','c':'ccc'}, here) takes two arguments -- see the docs  The second argument, default, is what pop returns if the first argument, key, is absent. (If you call pop with just one argument, key, it raises an exception if that key's absent).  In your example, print b.pop('a',{'b':'bbb'}), this is irrelevant because 'a' is a key in b.data. But if you repeat that line...:  b=a() print b.pop('a',{'b':'bbb'}) print b.pop('a',{'b':'bbb'}) print b.data   you'll see it makes a difference: the first pop removes the 'a' key, so in the second pop the default argument is actually returned (since 'a' is now absent from b.data).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary"
        ],
        "URL": "https://stackoverflow.com/questions/1990802/in-python-what-does-dict-popa-b-mean",
        "A_Votes": "97",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    class a(object):     data={'a':'aaa','b':'bbb','c':'ccc'}     def pop(self, key, *args):             return self.data.pop(key, *args)#what is this mean.  b=a() print b.pop('a',{'b':'bbb'}) print b.data   self.data.pop(key, *args) ------ why is there a second argument?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "In Python, what does dict.pop(a,b) mean?",
        "A_Content": "  So many questions here. I see at least two, maybe three:   What does pop(a,b) do?/Why are there a second argument? What is *args being used for?     The first question is trivially answered in the Python Standard Library reference:     pop(key[, default])      If key is in the dictionary, remove it and return its value, else return default.   If default is not given and key is not in the dictionary, a KeyError is raised.     The second question is covered in the Python Language Reference:     If the form *identifier is present,   it is initialized to a tuple receiving   any excess positional parameters,   defaulting to the empty tuple. If the   form **identifier is present, it is   initialized to a new dictionary   receiving any excess keyword   arguments, defaulting to a new empty   dictionary.   In other words, the pop function takes at least two arguments. The first two get assigned the names self and key; and the rest are stuffed into a tuple called args.  What's happening on the next line when *args is passed along in the call to self.data.pop is the inverse of this - the tuple *args is expanded to of positional parameters which get passed along. This is explained in the Python Language Reference:     If the syntax *expression appears in   the function call, expression must   evaluate to a sequence. Elements from   this sequence are treated as if they   were additional positional arguments   In short, a.pop() wants to be flexible and accept any number of positional parameters, so that it can pass this unknown number of positional parameters on to self.data.pop().  This gives you flexibility; data happens to be a dict right now, and so self.data.pop() takes either one or two parameters; but if you changed data to be a type which took 19 parameters for a call to self.data.pop() you wouldn't have to change class a at all. You'd still have to change any code that called a.pop() to pass the required 19 parameters though.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary"
        ],
        "URL": "https://stackoverflow.com/questions/1990802/in-python-what-does-dict-popa-b-mean",
        "A_Votes": "23",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    class a(object):     data={'a':'aaa','b':'bbb','c':'ccc'}     def pop(self, key, *args):             return self.data.pop(key, *args)#what is this mean.  b=a() print b.pop('a',{'b':'bbb'}) print b.data   self.data.pop(key, *args) ------ why is there a second argument?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "In Python, what does dict.pop(a,b) mean?",
        "A_Content": "  def func(*args):      pass   When you define a function this way, *args will be array of arguments passed to the function. This allows your function to work without knowing ahead of time how many arguments are going to be passed to it.   You do this with keyword arguments too, using **kwargs:  def func2(**kwargs):      pass   See: Arbitrary argument lists    In your case, you've defined a class which is acting like a dictionary. The dict.pop method is defined as pop(key[, default]).   Your method doesn't use the default parameter. But, by defining your method with *args and passing *args to dict.pop(), you are allowing the caller to use the default parameter.  In other words, you should be able to use your class's pop method like dict.pop:   my_a = a() value1 = my_a.pop('key1')       # throw an exception if key1 isn't in the dict value2 = my_a.pop('key2', None) # return None if key2 isn't in the dict      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary"
        ],
        "URL": "https://stackoverflow.com/questions/1990802/in-python-what-does-dict-popa-b-mean",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    class a(object):     data={'a':'aaa','b':'bbb','c':'ccc'}     def pop(self, key, *args):             return self.data.pop(key, *args)#what is this mean.  b=a() print b.pop('a',{'b':'bbb'}) print b.data   self.data.pop(key, *args) ------ why is there a second argument?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "In Python, what does dict.pop(a,b) mean?",
        "A_Content": "  >>> def func(a, *args, **kwargs): ...   print 'a %s, args %s, kwargs %s' % (a, args, kwargs) ...  >>> func('one', 'two', 'three', four='four', five='five') a one, args ('two', 'three'), kwargs {'four': 'four', 'five': 'five'}     >>> def anotherfunct(beta, *args): ...   print 'beta %s, args %s' % (beta, args) ...  >>> def func(a, *args, **kwargs): ...   anotherfunct(a, *args) ...  >>> func('one', 'two', 'three', four='four', five='five') beta one, args ('two', 'three') >>>       ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary"
        ],
        "URL": "https://stackoverflow.com/questions/1990802/in-python-what-does-dict-popa-b-mean",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    class a(object):     data={'a':'aaa','b':'bbb','c':'ccc'}     def pop(self, key, *args):             return self.data.pop(key, *args)#what is this mean.  b=a() print b.pop('a',{'b':'bbb'}) print b.data   self.data.pop(key, *args) ------ why is there a second argument?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "remove pytz timezone",
        "A_Content": "  To remove a timezone (tzinfo) from a datetime object:  # dt_tz is a datetime.datetime object dt = dt_tz.replace(tzinfo=None)   If you are using a library like arrow, then you can remove timezone by simply converting an arrow object to to a datetime object, then doing the same thing as the example above.  # <Arrow [2014-10-09T10:56:09.347444-07:00]> arrowObj = arrow.get('2014-10-09T10:56:09.347444-07:00')  # datetime.datetime(2014, 10, 9, 10, 56, 9, 347444, tzinfo=tzoffset(None, -25200)) tmpDatetime = arrowObj.datetime  # datetime.datetime(2014, 10, 9, 10, 56, 9, 347444) tmpDatetime = tmpDatetime.replace(tzinfo=None)   Why would you do this? One example is that mysql does not support timezones with its DATETIME type. So using ORM's like sqlalchemy will simply remove the timezone when you give it a datetime.datetime object to insert into the database. The solution is to convert your datetime.datetime object to UTC (so everything in your database is UTC since it can't specify timezone) then either insert it into the database (where the timezone is removed anyway) or remove it yourself. Also note that you cannot compare datetime.datetime objects where one is timezone aware and another is timezone naive.  ############################################################################## # MySQL example! where MySQL doesn't support timezones with its DATETIME type! ##############################################################################  arrowObj = arrow.get('2014-10-09T10:56:09.347444-07:00')  arrowDt = arrowObj.to(\"utc\").datetime  # inserts datetime.datetime(2014, 10, 9, 17, 56, 9, 347444, tzinfo=tzutc()) insertIntoMysqlDatabase(arrowDt)  # returns datetime.datetime(2014, 10, 9, 17, 56, 9, 347444) dbDatetimeNoTz = getFromMysqlDatabase()  # cannot compare timzeone aware and timezone naive dbDatetimeNoTz == arrowDt # False, or TypeError on python versions before 3.3  # compare datetimes that are both aware or both naive work however dbDatetimeNoTz == arrowDt.replace(tzinfo=None) # True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "timezone",
            "pytz"
        ],
        "URL": "https://stackoverflow.com/questions/10944047/remove-pytz-timezone",
        "A_Votes": "130",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is there a simple way to remove the timezone from a pytz datetime object? e.g. reconstructing dt from dt_tz in this example:  >>> import datetime >>> import pytz >>> dt = datetime.datetime.now() >>> dt datetime.datetime(2012, 6, 8, 9, 27, 32, 601000) >>> dt_tz = pytz.utc.localize(dt) >>> dt_tz datetime.datetime(2012, 6, 8, 9, 27, 32, 601000, tzinfo=<UTC>)      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  My favorite way of doing this is to use the DJANGO_SETTINGS_MODULE environment variable and use two (or more) settings files, e.g. production_settings.py and test_settings.py.   You can then use a bootstrap script or a process manager to load the correct settings (by setting the environment). If you're using a virtualenv, you could even hack this into the virtualenv's activate script.  You can take advantage of the PYTHONPATH variable to store the settings in a completely different location (e.g. on a production server, storing them in /etc/ makes sense)  this allows for easier deployment as you totally separate configuration from application files (which is a good thing).  Setting DJANGO_SETTINGS_MODULE using a Process Manager  If you're not fond of writing a bootstrap script that sets the environment (and there are very good reasons to feel that way!), I would recommend using a process manager:   Supervisor lets you pass environment variables to managed processes using a program's environment configuration key. Honcho (a pure-Python equivalent of Ruby's Foreman) lets you define environment variables in an \"environment\" (.env) file.   Hacking bin/activate to set DJANGO_SETTINGS_MODULE  If using a virtualenv, you append this to your bin/activate script:  DJANGO_SETTINGS_MODULE=\"production_settings\" export DJANGO_SETTINGS_MODULE   And on your test server:  DJANGO_SETTINGS_MODULE=\"test_settings\" export DJANGO_SETTINGS_MODULE      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "72",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  I usually have one settings file per environment, and a shared settings file:  /myproject/   settings.production.py   settings.development.py   shared_settings.py   Each of my environment files has:  try:     from shared_settings import * except ImportError:     pass   This allows me to override shared settings if necessary (by adding the modifications below that stanza).  I then select which settings files to use by linking it in to settings.py:  ln -s settings.development.py settings.py      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "28",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  By default use production settings, but create a file called settings_dev.py in the same folder as your settings.py file. Add overrides there, such as DEBUG=True.  On the computer that will be used for development, add this to your ~/.bashrc file:  export DJANGO_DEVELOPMENT=true   At the bottom of your settings.py file, add the following.  # Override production variables if DJANGO_DEVELOPMENT env variable is set if os.environ.get('DJANGO_DEVELOPMENT') is not None:     from settings_dev import *    (Note that importing * should generally be avoided in Python, but this is a unique circumstance)  By default the production servers will not override anything. Done!  Compared to the other answers, this one is simpler because it doesn't require updating PYTHONPATH, or setting DJANGO_SETTINGS_MODULE which only allows you to work on one django project at a time.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  Create multiple settings*.py files, extrapolating the variables that need to change per environment. Then at the end of your master settings.py file:  try:   from settings_dev import * except ImportError:   pass   You keep the separate settings_* files for each stage.  At the top of your settings_dev.py file, add this:  import sys globals().update(vars(sys.modules['settings']))   To import variables that you need to modify.  This wiki entry has more ideas on how to split your settings.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  Here is the approach we use :   a settings module to split settings into multiple files for readability ; a .env.json file to store credentials and parameters that we want excluded from our git repository, or that are environment specific ; an env.py file to read the .env.json file   Considering the following structure :  ... .env.json           # the file containing all specific credentials and parameters .gitignore          # the .gitignore file to exclude `.env.json` project_name/       # project dir (the one which django-admin.py creates)   accounts/         # project's apps     __init__.py     ...   ...   env.py            # the file to load credentials   settings/     __init__.py     # main settings file     database.py     # database conf     storage.py      # storage conf     ... venv                # virtualenv ...   With .env.json like :  {     \"debug\": false,     \"allowed_hosts\": [\"mydomain.com\"],     \"django_secret_key\": \"my_very_long_secret_key\",     \"db_password\": \"my_db_password\",     \"db_name\": \"my_db_name\",     \"db_user\": \"my_db_user\",     \"db_host\": \"my_db_host\", }   And project_name/env.py :  <!-- language: lang-python --> import json import os   def get_credentials():     env_file_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))     with open(os.path.join(env_file_dir, '.env.json'), 'r') as f:         creds = json.loads(f.read())     return creds   credentials = get_credentials()   We can have the following settings:  <!-- language: lang-py --> # project_name/settings/__init__.py from project_name.env import credentials from project_name.settings.database import * from project_name.settings.storage import * ...  SECRET_KEY = credentials.get('django_secret_key')  DEBUG = credentials.get('debug')  ALLOWED_HOSTS = credentials.get('allowed_hosts', [])  INSTALLED_APPS = [     'django.contrib.admin',     'django.contrib.auth',     'django.contrib.contenttypes',     'django.contrib.sessions',     'django.contrib.messages',     'django.contrib.staticfiles',      ... ]  if DEBUG:     INSTALLED_APPS += ['debug_toolbar']  ...  # project_name/settings/database.py from project_name.env import credentials  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql_psycopg2',         'NAME': credentials.get('db_name', ''),         'USER': credentials.get('db_user', ''),         'HOST': credentials.get('db_host', ''),         'PASSWORD': credentials.get('db_password', ''),         'PORT': '5432',     } }   the benefits of this solution are :   user specific credentials and configurations for local development without modifying the git repository ; environment specific configuration, you can have for example three different environments with three different .env.json like dev, stagging and production ; credentials are not in the repository   I hope this helps, just let me know if you see any caveats with this solution.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  I use the awesome django-configurations, and all the settings are stored in my settings.py:  from configurations import Configuration  class Base(Configuration):     # all the base settings here...     BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))     ...  class Develop(Base):     # development settings here...     DEBUG = True      ...  class Production(Base):     # production settings here...     DEBUG = False   To configure the Django project I just followed the docs.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  I use the folloring file structure:  project/    ...    settings/    settings/common.py    settings/local.py    settings/prod.py    settings/__init__.py -> local.py   So __init__.py is a link (ln in unix or mklink in windows) to local.py or can be to prod.py so the configuration is still in the project.settings module is clean and organized, and if you want to use a particular config you can use the environment variable DJANGO_SETTINGS_MODULE to project.settings.prod if you need to run a command for production environment.  In the files prod.py and local.py:  from .shared import *  DATABASE = {     ... }   and the shared.py file keeps as global without specific configs.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  If you want to keep 1 settings file, and your development operating system is different than your production operating system, you can put this at the bottom of your settings.py:  from sys import platform if platform == \"linux\" or platform == \"linux2\":     # linux     # some special setting here for when I'm on my prod server elif platform == \"darwin\":     # OS X     # some special setting here for when I'm developing on my mac elif platform == \"win32\":     # Windows...     # some special setting here for when I'm developing on my pc   Read more: How do I check the operating system in Python?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  building off cs01's answer:  if you're having problems with the environment variable, set its value to a string (e.g. I did DJANGO_DEVELOPMENT=\"true\").  I also changed cs01's file workflow as follows:  #settings.py import os if os.environ.get('DJANGO_DEVELOPMENT') is not None:     from settings_dev import *  else:     from settings_production import * #settings_dev.py development settings go here #settings_production.py production settings go here   This way, Django doesn't have to read through the entirety of a settings file before running the appropriate settings file. This solution comes in handy if your production file needs stuff that's only on your production server.  Note: in Python 3, imported files need to have a . appended (e.g. from .settings_dev import *)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  This seems to have been answered, however a method which I use as combined with version control is the following:  Setup a env.py file in the same directory as settings on my local development environment that I also add to .gitignore:  env.py:  #!usr/bin/python  DJANGO_ENV = True ALLOWED_HOSTS = ['127.0.0.1', 'dev.mywebsite.com']   .gitignore:  mywebsite/env.py   settings.py:  if os.path.exists(os.getcwd() + '/env.py'):     #env.py is excluded using the .gitignore file - when moving to production we can automatically set debug mode to off:     from env import * else:     DJANGO_ENV = False  DEBUG = DJANGO_ENV   I just find this works and is far more elegant - with env.py it is easy to see our local environment variables and we can handle all of this without multiple settings.py files or the likes. This methods allows for all sorts of local environment variables to be used that we wouldn't want set on our production server. Utilising the .gitignore via version control we are also keeping everything seamlessly integrated.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Django: How to manage development and production settings?",
        "A_Content": "  This is my solution, with different environements for dev, test and prod  import socket  [...]  DEV_PC = 'PC059' host_name = socket.gethostname()  if host_name == DEV_PC:    #do something    pass elif [...]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/10664244/django-how-to-manage-development-and-production-settings",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have been developing a basic app. Now at the deployment stage it has become clear I have need for both a local settings and production settings.  It would be great to know the following:   How best to deal with development and production settings.  How to keep apps such as django-debug-toolbar only in a development environment. Any other tips and best practices for development and deployment settings.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  >>> myseries[myseries == 7] 3    7 dtype: int64 >>> myseries[myseries == 7].index[0] 3   Though I admit that there should be a better way to do that, but this at least avoids iterating and looping through the object and moves it to the C level.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "101",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  Converting to an Index, you can use get_loc  In [1]: myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])  In [3]: Index(myseries).get_loc(7) Out[3]: 3  In [4]: Index(myseries).get_loc(10) KeyError: 10   Duplicate handling  In [5]: Index([1,1,2,2,3,4]).get_loc(2) Out[5]: slice(2, 4, None)   Will return a boolean array if non-contiguous returns  In [6]: Index([1,1,2,1,3,2,4]).get_loc(2) Out[6]: array([False, False,  True, False, False,  True, False], dtype=bool)   Uses a hashtable internally, so fast  In [7]: s = Series(randint(0,10,10000))  In [9]: %timeit s[s == 5] 1000 loops, best of 3: 203 s per loop  In [12]: i = Index(s)  In [13]: %timeit i.get_loc(5) 1000 loops, best of 3: 226 s per loop   As Viktor points out, there is a one-time creation overhead to creating an index (its incurred when you actually DO something with the index, e.g. the is_unique)  In [2]: s = Series(randint(0,10,10000))  In [3]: %timeit Index(s) 100000 loops, best of 3: 9.6 s per loop  In [4]: %timeit Index(s).is_unique 10000 loops, best of 3: 140 s per loop      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  In [92]: (myseries==7).argmax() Out[92]: 3   This works if you know 7 is there in advance. You can check this with (myseries==7).any()    Another approach (very similar to the first answer) that also accounts for multiple 7's (or none) is   In [122]: myseries = pd.Series([1,7,0,7,5], index=['a','b','c','d','e']) In [123]: list(myseries[myseries==7].index) Out[123]: ['b', 'd']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  Another way to do this, although equally unsatisfying is:  s = pd.Series([1,3,0,7,5],index=[0,1,2,3,4])  list(s).index(7)   returns:     3  On time tests using a current dataset I'm working with (consider it random):  [64]:    %timeit pd.Index(article_reference_df.asset_id).get_loc('100000003003614') 10000 loops, best of 3: 60.1 s per loop  In [66]: %timeit article_reference_df.asset_id[article_reference_df.asset_id == '100000003003614'].index[0] 1000 loops, best of 3: 255 s per loop   In [65]: %timeit list(article_reference_df.asset_id).index('100000003003614') 100000 loops, best of 3: 14.5 s per loop      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  If you use numpy, you can get an array of the indecies that your value is found:  import numpy as np import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) np.where(myseries == 7)   This returns a one element tuple containing an array of the indecies where 7 is the value in myseries:  (array([3], dtype=int64),)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  you can use Series.idxmax()   >>> import pandas as pd >>> myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) >>> myseries.idxmax() 3 >>>       ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  Reference Viktor Kerkez (Aug 20 '13 at 5:52) Jonathan Eunice (Nov 7 '16 at 14:03)  >>> myseries[myseries == 7] 3    7 dtype: int64 >>> myseries[myseries == 7].index   # using index[0] specifies the output of the first occurrence only.  Using index without adding the element index will give you indexes all occurrences if the series had more then one 7 there.  It still presumes you know which number you are looking for.   3       ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Find element's index in pandas Series",
        "A_Content": "  Often your value occurs at multiple indices:  >>> myseries = pd.Series([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1]) >>> myseries.index[myseries == 1] Int64Index([3, 4, 5, 6, 10, 11], dtype='int64')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)  I.e., I'd like something like:  import pandas as pd myseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4]) print myseries.find(7) # should output 3   Certainly, it is possible to define such a method with a loop:  def find(s, el):     for i in s.index:         if s[i] == el:              return i     return None  print find(myseries, 7)   but I assume there should be a better way. Is there?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  Absolutely (for the example you provided).  Tuples are first class citizens in Python  There is a builtin function divmod() that does exactly that.  q, r = divmod(x, y) # ((x - x%y)/y, x%y) Invariant: div*y + mod == x   There are other examples: zip, enumerate, dict.items.   for i, e in enumerate([1, 3, 3]):     print \"index=%d, element=%s\" % (i, e)  # reverse keys and values in a dictionary d = dict((v, k) for k, v in adict.items()) # or  d = dict(zip(adict.values(), adict.keys()))   BTW, parentheses are not necessary most of the time. Citation from Python Library Reference:      Tuples may be constructed in a number of ways:         Using a pair of parentheses to denote the empty tuple: ()   Using a trailing comma for a singleton tuple: a, or (a,)   Separating items with commas: a, b, c or (a, b, c)   Using the tuple() built-in: tuple() or tuple(iterable)      Functions should serve single purpose  Therefore they should return a single object. In your case this object is a tuple. Consider tuple as an ad-hoc compound data structure. There are languages where almost every single function returns multiple values (list in Lisp).  Sometimes it is sufficient to return (x, y) instead of Point(x, y).  Named tuples  With the introduction of named tuples in Python 2.6 it is preferable in many cases to return named tuples instead of plain tuples.  >>> import collections >>> Point = collections.namedtuple('Point', 'x y') >>> x, y = Point(0, 1) >>> p = Point(x, y) >>> x, y, p (0, 1, Point(x=0, y=1)) >>> p.x, p.y, p[0], p[1] (0, 1, 0, 1) >>> for i in p: ...   print(i) ... 0 1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "97",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  Firstly, note that Python allows for the following (no need for the parenthesis):  q, r = divide(22, 7)   Regarding your question, there's no hard and fast rule either way. For simple (and usually contrived) examples, it may seem that it's always possible for a given function to have a single purpose, resulting in a single value. However, when using Python for real-world applications, you quickly run into many cases where returning multiple values is necessary, and results in cleaner code.  So, I'd say do whatever makes sense, and don't try to conform to an artificial convention. Python supports multiple return values, so use it when appropriate.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  The example you give is actually a python builtin function, called divmod. So someone, at some point in time, thought that it was pythonic enough to include in the core functionality.  To me, if it makes the code cleaner, it is pythonic. Compare these two code blocks:  seconds = 1234 minutes, seconds = divmod(seconds, 60) hours, minutes = divmod(minutes, 60)  seconds = 1234 minutes = seconds / 60 seconds = seconds % 60 hours = minutes / 60 minutes = minutes % 60      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  Yes, returning multiple values (i.e., a tuple) is definitely pythonic.  As others have pointed out, there are plenty of examples in the Python standard library, as well as in well-respected Python projects.  Two additional comments:   Returning multiple values is sometimes very, very useful.  Take, for example, a method that optionally handles an event (returning some value in doing so) and also returns success or failure.  This might arise in a chain of responsibility pattern.  In other cases, you want to return multiple, closely linked pieces of data---as in the example given.  In this setting, returning multiple values is akin to returning a single instance of an anonymous class with several member variables. Python's handling of method arguments necessitates the ability to directly return multiple values.  In C++, for example, method arguments can be passed by reference, so you can assign output values to them, in addition to the formal return value.  In Python, arguments are passed \"by reference\" (but in the sense of Java, not C++).  You can't assign new values to method arguments and have it reflected outside method scope.  For example:  // C++ void test(int& arg) {     arg = 1; }  int foo = 0; test(foo); // foo is now 1!   Compare with:  # Python def test(arg):     arg = 1  foo = 0 test(foo) # foo is still 0       ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  It's definitely pythonic. The fact that you can return multiple values from a function the boilerplate you would have in a language like C where you need to define a struct for every combination of types you return somewhere.  However, if you reach the point where you are returning something crazy like 10 values from a single function, you should seriously consider bundling them in a class because at that point it gets unwieldy.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  Returning a tuple is cool. Also note the new namedtuple which was added in python 2.6 which may make this more palatable for you: http://docs.python.org/dev/library/collections.html#collections.namedtuple     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  I'm fairly new to Python, but the tuple technique seems very pythonic to me.  However, I've had another idea that may enhance readability.  Using a dictionary allows access to the different values by name rather than position.  For example:  def divide(x, y):     return {'quotient': x/y, 'remainder':x%y }  answer = divide(22, 7) print answer['quotient'] print answer['remainder']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  OT: RSRE's Algol68 has the curious \"/:=\" operator. eg.  INT quotient:=355, remainder; remainder := (quotient /:= 113);   Giving a quotient of 3, and a remainder of 16.   Note: typically the value of \"(x/:=y)\" is discarded as quotient \"x\" is assigned by reference, but in RSRE's case the returned value is the remainder.  c.f. Integer Arithmetic - Algol68     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Is it pythonic for a function to return multiple values?",
        "A_Content": "  It's fine to return multiple values using a tuple for simple functions such as divmod. If it makes the code readable, it's Pythonic.  If the return value starts to become confusing, check whether the function is doing too much and split it if it is. If a big tuple is being used like an object, make it an object. Also, consider using named tuples, which will be part of the standard library in Python 2.6.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "function",
            "return-value",
            "multiple-return-values"
        ],
        "URL": "https://stackoverflow.com/questions/61605/is-it-pythonic-for-a-function-to-return-multiple-values",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, you can have a function return multiple values.  Here's a contrived example:  def divide(x, y):     quotient = x/y     remainder = x % y     return quotient, remainder    (q, r) = divide(22, 7)   This seems very useful, but it looks like it can also be abused (\"Well..function X already computes what we need as an intermediate value.  Let's have X return that value also\").  When should you draw the line and define a different method?       ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get value from form field in django framework?",
        "A_Content": "  Using a form in a view pretty much explains it.     The standard pattern for processing a form in a view looks like this:   def contact(request):     if request.method == 'POST': # If the form has been submitted...         form = ContactForm(request.POST) # A form bound to the POST data         if form.is_valid(): # All validation rules pass             # Process the data in form.cleaned_data             # ...              print form.cleaned_data['my_form_field_name']              return HttpResponseRedirect('/thanks/') # Redirect after POST     else:         form = ContactForm() # An unbound form      return render_to_response('contact.html', {         'form': form,     })      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/4706255/how-to-get-value-from-form-field-in-django-framework",
        "A_Votes": "91",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How to get value from form field in django framework? I mean, in views not in templates...     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get value from form field in django framework?",
        "A_Content": "  Take your pick:  def my_view(request):      if request.method == 'POST':         print request.POST.get('my_field')          form = MyForm(request.POST)          print form['my_field'].value()         print form.data['my_field']          if form.is_valid():              print form.cleaned_data['my_field']             print form.instance.my_field              form.save()             print form.instance.id  # now this one can access id/pk   Note: the field is accessed as soon as it's available.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/4706255/how-to-get-value-from-form-field-in-django-framework",
        "A_Votes": "28",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to get value from form field in django framework? I mean, in views not in templates...     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get value from form field in django framework?",
        "A_Content": "  You can do this after you validate your data.  if myform.is_valid():   data = myform.cleaned_data   field = data['field']   Also, read the django docs. They are perfect.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/4706255/how-to-get-value-from-form-field-in-django-framework",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to get value from form field in django framework? I mean, in views not in templates...     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get value from form field in django framework?",
        "A_Content": "  I use django 1.7+ and python 2.7+, the solution above dose not work.  And the input value in the form can be got use POST as below (use the same form above):  if form.is_valid():   data = request.POST.get('my_form_field_name')   print data   Hope this helps.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/4706255/how-to-get-value-from-form-field-in-django-framework",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to get value from form field in django framework? I mean, in views not in templates...     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get value from form field in django framework?",
        "A_Content": "  To retrieve data from form which send post request you can do it like this  def login_view(request):     if(request.POST):         login_data = request.POST.dict()         username = login_data.get(\"username\")         password = login_data.get(\"password\")         user_type = login_data.get(\"user_type\")         print(user_type, username, password)         return HttpResponse(\"This is a post request\")     else:         return render(request, \"base.html\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/4706255/how-to-get-value-from-form-field-in-django-framework",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to get value from form field in django framework? I mean, in views not in templates...     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Detect & Record Audio in Python",
        "A_Content": "  As a follow up to Nick Fortescue's answer, here's a more complete example of how to record from the microphone and process the resulting data:  from sys import byteorder from array import array from struct import pack  import pyaudio import wave  THRESHOLD = 500 CHUNK_SIZE = 1024 FORMAT = pyaudio.paInt16 RATE = 44100  def is_silent(snd_data):     \"Returns 'True' if below the 'silent' threshold\"     return max(snd_data) < THRESHOLD  def normalize(snd_data):     \"Average the volume out\"     MAXIMUM = 16384     times = float(MAXIMUM)/max(abs(i) for i in snd_data)      r = array('h')     for i in snd_data:         r.append(int(i*times))     return r  def trim(snd_data):     \"Trim the blank spots at the start and end\"     def _trim(snd_data):         snd_started = False         r = array('h')          for i in snd_data:             if not snd_started and abs(i)>THRESHOLD:                 snd_started = True                 r.append(i)              elif snd_started:                 r.append(i)         return r      # Trim to the left     snd_data = _trim(snd_data)      # Trim to the right     snd_data.reverse()     snd_data = _trim(snd_data)     snd_data.reverse()     return snd_data  def add_silence(snd_data, seconds):     \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"     r = array('h', [0 for i in xrange(int(seconds*RATE))])     r.extend(snd_data)     r.extend([0 for i in xrange(int(seconds*RATE))])     return r  def record():     \"\"\"     Record a word or words from the microphone and      return the data as an array of signed shorts.      Normalizes the audio, trims silence from the      start and end, and pads with 0.5 seconds of      blank sound to make sure VLC et al can play      it without getting chopped off.     \"\"\"     p = pyaudio.PyAudio()     stream = p.open(format=FORMAT, channels=1, rate=RATE,         input=True, output=True,         frames_per_buffer=CHUNK_SIZE)      num_silent = 0     snd_started = False      r = array('h')      while 1:         # little endian, signed short         snd_data = array('h', stream.read(CHUNK_SIZE))         if byteorder == 'big':             snd_data.byteswap()         r.extend(snd_data)          silent = is_silent(snd_data)          if silent and snd_started:             num_silent += 1         elif not silent and not snd_started:             snd_started = True          if snd_started and num_silent > 30:             break      sample_width = p.get_sample_size(FORMAT)     stream.stop_stream()     stream.close()     p.terminate()      r = normalize(r)     r = trim(r)     r = add_silence(r, 0.5)     return sample_width, r  def record_to_file(path):     \"Records from the microphone and outputs the resulting data to 'path'\"     sample_width, data = record()     data = pack('<' + ('h'*len(data)), *data)      wf = wave.open(path, 'wb')     wf.setnchannels(1)     wf.setsampwidth(sample_width)     wf.setframerate(RATE)     wf.writeframes(data)     wf.close()  if __name__ == '__main__':     print(\"please speak a word into the microphone\")     record_to_file('demo.wav')     print(\"done - result written to demo.wav\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "wav",
            "audio-recording"
        ],
        "URL": "https://stackoverflow.com/questions/892199/detect-record-audio-in-python",
        "A_Votes": "88",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.  I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.  Just can't quite get my head around it, can anyone get me started with a basic example.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Detect & Record Audio in Python",
        "A_Content": "  I believe the WAVE module does not support recording, just processing existing files. You might want to look at PyAudio for actually recording. WAV is about the world's simplest file format. In paInt16 you just get a signed integer representing a level, and closer to 0 is quieter. I can't remember if WAV files are high byte first or low byte, but something like this ought to work (sorry, I'm not really a python programmer:  from array import array  # you'll probably want to experiment on threshold # depends how noisy the signal threshold = 10  max_value = 0  as_ints = array('h', data) max_value = max(as_ints) if max_value > threshold:     # not silence   PyAudio code for recording kept for reference:  import pyaudio import sys  chunk = 1024 FORMAT = pyaudio.paInt16 CHANNELS = 1 RATE = 44100 RECORD_SECONDS = 5  p = pyaudio.PyAudio()  stream = p.open(format=FORMAT,                 channels=CHANNELS,                  rate=RATE,                  input=True,                 output=True,                 frames_per_buffer=chunk)  print \"* recording\" for i in range(0, 44100 / chunk * RECORD_SECONDS):     data = stream.read(chunk)     # check for silence here by comparing the level with 0 (or some threshold) for      # the contents of data.     # then write data or not to a file  print \"* done\"  stream.stop_stream() stream.close() p.terminate()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "wav",
            "audio-recording"
        ],
        "URL": "https://stackoverflow.com/questions/892199/detect-record-audio-in-python",
        "A_Votes": "42",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.  I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.  Just can't quite get my head around it, can anyone get me started with a basic example.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Detect & Record Audio in Python",
        "A_Content": "  Thanks to cryo for improved version that I based my tested code below:  #Instead of adding silence at start and end of recording (values=0) I add the original audio . This makes audio sound more natural as volume is >0. See trim() #I also fixed issue with the previous code - accumulated silence counter needs to be cleared once recording is resumed.  from array import array from struct import pack from sys import byteorder import copy import pyaudio import wave  THRESHOLD = 500  # audio levels not normalised. CHUNK_SIZE = 1024 SILENT_CHUNKS = 3 * 44100 / 1024  # about 3sec FORMAT = pyaudio.paInt16 FRAME_MAX_VALUE = 2 ** 15 - 1 NORMALIZE_MINUS_ONE_dB = 10 ** (-1.0 / 20) RATE = 44100 CHANNELS = 1 TRIM_APPEND = RATE / 4  def is_silent(data_chunk):     \"\"\"Returns 'True' if below the 'silent' threshold\"\"\"     return max(data_chunk) < THRESHOLD  def normalize(data_all):     \"\"\"Amplify the volume out to max -1dB\"\"\"     # MAXIMUM = 16384     normalize_factor = (float(NORMALIZE_MINUS_ONE_dB * FRAME_MAX_VALUE)                         / max(abs(i) for i in data_all))      r = array('h')     for i in data_all:         r.append(int(i * normalize_factor))     return r  def trim(data_all):     _from = 0     _to = len(data_all) - 1     for i, b in enumerate(data_all):         if abs(b) > THRESHOLD:             _from = max(0, i - TRIM_APPEND)             break      for i, b in enumerate(reversed(data_all)):         if abs(b) > THRESHOLD:             _to = min(len(data_all) - 1, len(data_all) - 1 - i + TRIM_APPEND)             break      return copy.deepcopy(data_all[_from:(_to + 1)])  def record():     \"\"\"Record a word or words from the microphone and      return the data as an array of signed shorts.\"\"\"      p = pyaudio.PyAudio()     stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, output=True, frames_per_buffer=CHUNK_SIZE)      silent_chunks = 0     audio_started = False     data_all = array('h')      while True:         # little endian, signed short         data_chunk = array('h', stream.read(CHUNK_SIZE))         if byteorder == 'big':             data_chunk.byteswap()         data_all.extend(data_chunk)          silent = is_silent(data_chunk)          if audio_started:             if silent:                 silent_chunks += 1                 if silent_chunks > SILENT_CHUNKS:                     break             else:                  silent_chunks = 0         elif not silent:             audio_started = True                    sample_width = p.get_sample_size(FORMAT)     stream.stop_stream()     stream.close()     p.terminate()      data_all = trim(data_all)  # we trim before normalize as threshhold applies to un-normalized wave (as well as is_silent() function)     data_all = normalize(data_all)     return sample_width, data_all  def record_to_file(path):     \"Records from the microphone and outputs the resulting data to 'path'\"     sample_width, data = record()     data = pack('<' + ('h' * len(data)), *data)      wave_file = wave.open(path, 'wb')     wave_file.setnchannels(CHANNELS)     wave_file.setsampwidth(sample_width)     wave_file.setframerate(RATE)     wave_file.writeframes(data)     wave_file.close()  if __name__ == '__main__':     print(\"Wait in silence to begin recording; wait in silence to terminate\")     record_to_file('demo.wav')     print(\"done - result written to demo.wav\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "wav",
            "audio-recording"
        ],
        "URL": "https://stackoverflow.com/questions/892199/detect-record-audio-in-python",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.  I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.  Just can't quite get my head around it, can anyone get me started with a basic example.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Detect & Record Audio in Python",
        "A_Content": "  The pyaudio website has many examples that are pretty short and clear: http://people.csail.mit.edu/hubert/pyaudio/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "wav",
            "audio-recording"
        ],
        "URL": "https://stackoverflow.com/questions/892199/detect-record-audio-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.  I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.  Just can't quite get my head around it, can anyone get me started with a basic example.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Detect & Record Audio in Python",
        "A_Content": "  import pyaudio import wave from array import array  FORMAT=pyaudio.paInt16 CHANNELS=2 RATE=44100 CHUNK=1024 RECORD_SECONDS=15 FILE_NAME=\"RECORDING.wav\"  audio=pyaudio.PyAudio() #instantiate the pyaudio  #recording prerequisites stream=audio.open(format=FORMAT,channels=CHANNELS,                    rate=RATE,                   input=True,                   frames_per_buffer=CHUNK)  #starting recording frames=[]  for i in range(0,int(RATE/CHUNK*RECORD_SECONDS)):     data=stream.read(CHUNK)     data_chunk=array('h',data)     vol=max(data_chunk)     if(vol>=500):         print(\"something said\")         frames.append(data)     else:         print(\"nothing\")     print(\"\\n\")   #end of recording stream.stop_stream() stream.close() audio.terminate() #writing to file wavfile=wave.open(FILE_NAME,'wb') wavfile.setnchannels(CHANNELS) wavfile.setsampwidth(audio.get_sample_size(FORMAT)) wavfile.setframerate(RATE) wavfile.writeframes(b''.join(frames))#append frames recorded to file wavfile.close()   I think this will help.It is a simple script which will check if there is a silence or not.If silence is detected it will not record otherwise it will record.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "wav",
            "audio-recording"
        ],
        "URL": "https://stackoverflow.com/questions/892199/detect-record-audio-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.  I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.  Just can't quite get my head around it, can anyone get me started with a basic example.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Detect & Record Audio in Python",
        "A_Content": "  You might want to look at csounds, also.  It has several API's, including Python.  It might be able to interact with an A-D interface and gather sound samples.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "wav",
            "audio-recording"
        ],
        "URL": "https://stackoverflow.com/questions/892199/detect-record-audio-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to capture audio clips as WAV files that I can then pass to another bit of python for processing. The problem is that I need to determine when there is audio present and then record it, stop when it goes silent and then pass that file to the processing module.  I'm thinking it should be possible with the wave module to detect when there is pure silence and discard it then as soon as something other than silence is detected start recording, then when the line goes silent again stop the recording.  Just can't quite get my head around it, can anyone get me started with a basic example.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get one value from a generator in Python?",
        "A_Content": "  Yes, or next(gen) in 2.6+.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "generator"
        ],
        "URL": "https://stackoverflow.com/questions/2419770/how-to-get-one-value-from-a-generator-in-python",
        "A_Votes": "90",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Very basic question - how to get one value from a generator in Python?  So far I found I can get one by writing gen.next(). I just want to make sure this is the right way?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get one value from a generator in Python?",
        "A_Content": "  In Python <= 2.5, use gen.next(). This will work for all Python 2.x versions, but not Python 3.x  In Python >= 2.6, use next(gen). This is a built in function, and is clearer. It will also work in Python 3.  Both of these end up calling a specially named function, next(), which can be overridden by subclassing. In Python 3, however, this function has been renamed to __next__(), to be consistent with other special functions.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "generator"
        ],
        "URL": "https://stackoverflow.com/questions/2419770/how-to-get-one-value-from-a-generator-in-python",
        "A_Votes": "46",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Very basic question - how to get one value from a generator in Python?  So far I found I can get one by writing gen.next(). I just want to make sure this is the right way?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get one value from a generator in Python?",
        "A_Content": "  This is the correct way to do it.  You can also use next(gen).  http://docs.python.org/library/functions.html#next     ",
        "Language": "Python",
        "Tags": [
            "python",
            "generator"
        ],
        "URL": "https://stackoverflow.com/questions/2419770/how-to-get-one-value-from-a-generator-in-python",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Very basic question - how to get one value from a generator in Python?  So far I found I can get one by writing gen.next(). I just want to make sure this is the right way?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get one value from a generator in Python?",
        "A_Content": "  Use (for python 3)  next(generator)   Here is an example  def fun(x):     n = 0     while n < x:         yield n         n += 1 z = fun(10) next(z) next(z)   should print  0 1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "generator"
        ],
        "URL": "https://stackoverflow.com/questions/2419770/how-to-get-one-value-from-a-generator-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Very basic question - how to get one value from a generator in Python?  So far I found I can get one by writing gen.next(). I just want to make sure this is the right way?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How to get one value from a generator in Python?",
        "A_Content": "  In python 3 you don't have gen.next(), but you still can use next(gen). A bit bizarre if you ask me but that's how it is.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "generator"
        ],
        "URL": "https://stackoverflow.com/questions/2419770/how-to-get-one-value-from-a-generator-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Very basic question - how to get one value from a generator in Python?  So far I found I can get one by writing gen.next(). I just want to make sure this is the right way?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "method of iterating over sqlalchemy model's defined columns?",
        "A_Content": "  You could use the following function:  def __unicode__(self):     return \"[%s(%s)]\" % (self.__class__.__name__, ', '.join('%s=%s' % (k, self.__dict__[k]) for k in sorted(self.__dict__) if '_sa_' != k[:4]))   It will exclude SA magic attributes, but will not exclude the relations. So basically it might load the dependencies, parents, children etc, which is definitely not desirable.  But it is actually much easier because if you inherit from Base, you have a __table__ attribute, so that you can do:  for c in JobStatus.__table__.columns:     print c  for c in JobStatus.__table__.foreign_keys:     print c   See How to discover table properties from SQLAlchemy mapped object - similar question.  Edit by Mike: Please see functions such as Mapper.c and Mapper.mapped_table.  If using 0.8 and higher also see Mapper.attrs and related functions.  Example for Mapper.attrs:  from sqlalchemy import inspect mapper = inspect(JobStatus) for column in mapper.attrs:     print column.key      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/2537471/method-of-iterating-over-sqlalchemy-models-defined-columns",
        "A_Votes": "57",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've been trying to figure out how to iterate over the list of columns defined in a SqlAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.dict since it contains a lot of SA specific items.   Anyone know of a way to just get the id, and desc names from the following?  class JobStatus(Base):     __tablename__ = 'jobstatus'      id = Column(Integer, primary_key=True)     desc = Column(Unicode(20))   In this small case I could easily create a:  def logme(self):     return {'id': self.id, 'desc': self.desc}   but I'd prefer something that was auto generating for larger objects.   Thanks for the assistance.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "method of iterating over sqlalchemy model's defined columns?",
        "A_Content": "  You can get the list of defined properties from the mapper. For your case you're interested in only ColumnProperty objects.  from sqlalchemy.orm import class_mapper import sqlalchemy  def attribute_names(cls):     return [prop.key for prop in class_mapper(cls).iterate_properties         if isinstance(prop, sqlalchemy.orm.ColumnProperty)]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/2537471/method-of-iterating-over-sqlalchemy-models-defined-columns",
        "A_Votes": "55",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been trying to figure out how to iterate over the list of columns defined in a SqlAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.dict since it contains a lot of SA specific items.   Anyone know of a way to just get the id, and desc names from the following?  class JobStatus(Base):     __tablename__ = 'jobstatus'      id = Column(Integer, primary_key=True)     desc = Column(Unicode(20))   In this small case I could easily create a:  def logme(self):     return {'id': self.id, 'desc': self.desc}   but I'd prefer something that was auto generating for larger objects.   Thanks for the assistance.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "method of iterating over sqlalchemy model's defined columns?",
        "A_Content": "  I realise that this is an old question, but I've just come across the same requirement and would like to offer an alternative solution to future readers.  As Josh notes, full SQL field names will be returned by JobStatus.__table__.columns, so rather than the original field name  id, you will get jobstatus.id. Not as useful as it could be.  The solution to obtaining a list of field names as they were originally defined is to look the _data attribute on the column object, which contains the full data. If we look at JobStatus.__table__.columns._data, it looks like this:  {'desc': Column('desc', Unicode(length=20), table=<jobstatus>),  'id': Column('id', Integer(), table=<jobstatus>, primary_key=True, nullable=False)}   From here you can simply call JobStatus.__table__.columns._data.keys() which gives you a nice, clean list:  ['id', 'desc']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/2537471/method-of-iterating-over-sqlalchemy-models-defined-columns",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been trying to figure out how to iterate over the list of columns defined in a SqlAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.dict since it contains a lot of SA specific items.   Anyone know of a way to just get the id, and desc names from the following?  class JobStatus(Base):     __tablename__ = 'jobstatus'      id = Column(Integer, primary_key=True)     desc = Column(Unicode(20))   In this small case I could easily create a:  def logme(self):     return {'id': self.id, 'desc': self.desc}   but I'd prefer something that was auto generating for larger objects.   Thanks for the assistance.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "method of iterating over sqlalchemy model's defined columns?",
        "A_Content": "  self.__table__.columns will \"only\" give you the columns defined in that particular class, i.e. without inherited ones.  if you need all, use self.__mapper__.columns.  in your example i'd probably use something like this:  class JobStatus(Base):      ...      def __iter__(self):         values = vars(self)         for attr in self.__mapper__.columns.keys():             if attr in values:                 yield attr, values[attr]      def logme(self):         return dict(self)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/2537471/method-of-iterating-over-sqlalchemy-models-defined-columns",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been trying to figure out how to iterate over the list of columns defined in a SqlAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.dict since it contains a lot of SA specific items.   Anyone know of a way to just get the id, and desc names from the following?  class JobStatus(Base):     __tablename__ = 'jobstatus'      id = Column(Integer, primary_key=True)     desc = Column(Unicode(20))   In this small case I could easily create a:  def logme(self):     return {'id': self.id, 'desc': self.desc}   but I'd prefer something that was auto generating for larger objects.   Thanks for the assistance.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "method of iterating over sqlalchemy model's defined columns?",
        "A_Content": "  To get an as_dict method on all of my classes I used a Mixin class which uses the technics described by Ants Aasma.  class BaseMixin(object):                                                                                                                                                                                  def as_dict(self):                                                                                                                                                                                        result = {}                                                                                                                                                                                           for prop in class_mapper(self.__class__).iterate_properties:                                                                                                                                              if isinstance(prop, ColumnProperty):                                                                                                                                                                      result[prop.key] = getattr(self, prop.key)                                                                                                                                                    return result   And then use it like this in your classes  class MyClass(BaseMixin, Base):     pass   That way you can invoke the following on an instance of MyClass.  > myclass = MyClass() > myclass.as_dict()   Hope this helps.    I've played arround with this a bit further, I actually needed to render my instances as dict as the form of a HAL object with it's links to related objects. So I've added this little magic down here, which will crawl over all properties of the class same as the above, with the difference that I will crawl deeper into Relaionship properties and generate links for these automatically.  Please note that this will only work for relationships have a single primary key  from sqlalchemy.orm import class_mapper, ColumnProperty from functools import reduce   def deepgetattr(obj, attr):     \"\"\"Recurses through an attribute chain to get the ultimate value.\"\"\"     return reduce(getattr, attr.split('.'), obj)   class BaseMixin(object):     def as_dict(self):         IgnoreInstrumented = (             InstrumentedList, InstrumentedDict, InstrumentedSet         )         result = {}         for prop in class_mapper(self.__class__).iterate_properties:             if isinstance(getattr(self, prop.key), IgnoreInstrumented):                 # All reverse relations are assigned to each related instances                 # we don't need to link these, so we skip                 continue             if isinstance(prop, ColumnProperty):                 # Add simple property to the dictionary with its value                 result[prop.key] = getattr(self, prop.key)             if isinstance(prop, RelationshipProperty):                 # Construct links relaions                 if 'links' not in result:                     result['links'] = {}                  # Get value using nested class keys                 value = (                     deepgetattr(                         self, prop.key + \".\" + prop.mapper.primary_key[0].key                     )                 )                 result['links'][prop.key] = {}                 result['links'][prop.key]['href'] = (                     \"/{}/{}\".format(prop.key, value)                 )         return result      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/2537471/method-of-iterating-over-sqlalchemy-models-defined-columns",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been trying to figure out how to iterate over the list of columns defined in a SqlAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.dict since it contains a lot of SA specific items.   Anyone know of a way to just get the id, and desc names from the following?  class JobStatus(Base):     __tablename__ = 'jobstatus'      id = Column(Integer, primary_key=True)     desc = Column(Unicode(20))   In this small case I could easily create a:  def logme(self):     return {'id': self.id, 'desc': self.desc}   but I'd prefer something that was auto generating for larger objects.   Thanks for the assistance.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "method of iterating over sqlalchemy model's defined columns?",
        "A_Content": "  I know this is an old question, but what about:  class JobStatus(Base):      ...      def columns(self):         return [col for col in dir(self) if isinstance(col, db.Column)]   Then, to get column names: jobStatus.columns()  That would return ['id', 'desc']  Then you can loop through, and do stuff with the columns and values:  for col in jobStatus.colums():     doStuff(getattr(jobStatus, col))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/2537471/method-of-iterating-over-sqlalchemy-models-defined-columns",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been trying to figure out how to iterate over the list of columns defined in a SqlAlchemy model. I want it for writing some serialization and copy methods to a couple of models. I can't just iterate over the obj.dict since it contains a lot of SA specific items.   Anyone know of a way to just get the id, and desc names from the following?  class JobStatus(Base):     __tablename__ = 'jobstatus'      id = Column(Integer, primary_key=True)     desc = Column(Unicode(20))   In this small case I could easily create a:  def logme(self):     return {'id': self.id, 'desc': self.desc}   but I'd prefer something that was auto generating for larger objects.   Thanks for the assistance.      ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Install particular version with easy_install",
        "A_Content": "  I believe the way to specify a version would be like this:  easy_install lxml==2.2.8   I (and most other Python users I suspect) stopped using easy_install and started using pip some time ago, so a solution in those terms is:  easy_install pip pip install lxml==2.2.8   (pip has several benefits, including an uninstall command)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "version",
            "easy-install"
        ],
        "URL": "https://stackoverflow.com/questions/3833011/install-particular-version-with-easy-install",
        "A_Votes": "129",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm trying to install lxml. I've had a look at the website, and version 2.2.8 looked reasonable to me but when I did easy_install lxml, it installed version 2.3.beta1 which is not really what I want I presume.  What is the best way to fix this and how can I force easy_install to install a particular version?  (Mac os x 10.6)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Install particular version with easy_install",
        "A_Content": "  From the easy_install documentation:  easy_install PackageName==1.2.3     ",
        "Language": "Python",
        "Tags": [
            "python",
            "version",
            "easy-install"
        ],
        "URL": "https://stackoverflow.com/questions/3833011/install-particular-version-with-easy-install",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install lxml. I've had a look at the website, and version 2.2.8 looked reasonable to me but when I did easy_install lxml, it installed version 2.3.beta1 which is not really what I want I presume.  What is the best way to fix this and how can I force easy_install to install a particular version?  (Mac os x 10.6)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Install particular version with easy_install",
        "A_Content": "  You should do something like this:  easy_install \"lxml==2.2.8\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "version",
            "easy-install"
        ],
        "URL": "https://stackoverflow.com/questions/3833011/install-particular-version-with-easy-install",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install lxml. I've had a look at the website, and version 2.2.8 looked reasonable to me but when I did easy_install lxml, it installed version 2.3.beta1 which is not really what I want I presume.  What is the best way to fix this and how can I force easy_install to install a particular version?  (Mac os x 10.6)     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Why can't I call read() twice on an open file?",
        "A_Content": "  Calling read() reads through the entire file and leaves the read cursor at the end of the file (with nothing more to read). If you are looking to read a certain number of lines at a time you could use  readline(), readlines() or iterate through lines with for line in handle:.  To answer your question directly, once a file has been read, with read() you can use seek(0) to return the read cursor to the start of the file (docs are here). If you know the file isn't going to be too large, you can also save the read() output to a variable, using it in your findall expressions.  Ps. Dont forget to close the file after you are done with it ;)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "io"
        ],
        "URL": "https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file",
        "A_Votes": "108",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?  Here's the code  f = f.open()  # get the year match = re.search(r'Popularity in (\\d+)', f.read())  if match:   print match.group(1)  # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', f.read())  if matches:   # matches is always None   Of course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Why can't I call read() twice on an open file?",
        "A_Content": "  yeah, as above...  i'll write just an example:  >>> a = open('file.txt') >>> a.read() #output >>> a.seek(0) >>> a.read() #same output      ",
        "Language": "Python",
        "Tags": [
            "python",
            "io"
        ],
        "URL": "https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?  Here's the code  f = f.open()  # get the year match = re.search(r'Popularity in (\\d+)', f.read())  if match:   print match.group(1)  # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', f.read())  if matches:   # matches is always None   Of course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Why can't I call read() twice on an open file?",
        "A_Content": "  Everyone who has answered this question so far is absolutely right - read() moves through the file, so after you've called it, you can't call it again.  What I'll add is that in your particular case, you don't need to seek back to the start or reopen the file, you can just store the text that you've read in a local variable, and use it twice, or as many times as you like, in your program:  f = f.open() text = f.read() # read the file into a local variable # get the year match = re.search(r'Popularity in (\\d+)', text) if match:   print match.group(1) # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', text) if matches:   # matches will now not always be None      ",
        "Language": "Python",
        "Tags": [
            "python",
            "io"
        ],
        "URL": "https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?  Here's the code  f = f.open()  # get the year match = re.search(r'Popularity in (\\d+)', f.read())  if match:   print match.group(1)  # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', f.read())  if matches:   # matches is always None   Of course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Why can't I call read() twice on an open file?",
        "A_Content": "  The read pointer moves to after the last read byte/character. Use the seek() method to rewind the read pointer to the beginning.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "io"
        ],
        "URL": "https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?  Here's the code  f = f.open()  # get the year match = re.search(r'Popularity in (\\d+)', f.read())  if match:   print match.group(1)  # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', f.read())  if matches:   # matches is always None   Of course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Why can't I call read() twice on an open file?",
        "A_Content": "  Every open file has an associated position. When you read() you read from that position. For example read(10) reads the first 10 bytes from a newly opened file, then another read(10) reads the next 10 bytes. read() without arguments reads all of the contents of the file, leaving the file position at the end of the file. Next time you call read() there is nothing to read.  You can use seek to move the file position. Or probably better in your case would be to do one read() and keep the result for both searches.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "io"
        ],
        "URL": "https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?  Here's the code  f = f.open()  # get the year match = re.search(r'Popularity in (\\d+)', f.read())  if match:   print match.group(1)  # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', f.read())  if matches:   # matches is always None   Of course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Why can't I call read() twice on an open file?",
        "A_Content": "  read() consumes. So, you could reset the file, or seek to the start before re-reading. Or, if it suites your task, you can use read(n) to consume only n bytes.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "io"
        ],
        "URL": "https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?  Here's the code  f = f.open()  # get the year match = re.search(r'Popularity in (\\d+)', f.read())  if match:   print match.group(1)  # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', f.read())  if matches:   # matches is always None   Of course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Why can't I call read() twice on an open file?",
        "A_Content": "  I always find the read method something of a walk down a dark alley. You go down a bit and stop but if you are not counting your steps you are not sure how far along you are. Seek gives the solution by repositioning, the other option is Tell which returns the position along the file. May be the Python file api can combine read and seek into a read_from(position,bytes) to make it simpler - till that happens you should read this page.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "io"
        ],
        "URL": "https://stackoverflow.com/questions/3906137/why-cant-i-call-read-twice-on-an-open-file",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    For an exercise I'm doing, I'm trying to read the contents of a given file twice using the read() method. Strangely, when I call it the second time, it doesn't seem to return the file content as a string?  Here's the code  f = f.open()  # get the year match = re.search(r'Popularity in (\\d+)', f.read())  if match:   print match.group(1)  # get all the names matches = re.findall(r'<td>(\\d+)</td><td>(\\w+)</td><td>(\\w+)</td>', f.read())  if matches:   # matches is always None   Of course I know that this is not the most efficient or best way, this is not the point here. The point is, why can't I call read() twice? Do I have to reset the file handle? Or close / reopen the file in order to do that?     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  Depending on what you are doing, system() or popen() may be perfect.  Use system() if the Python script has no output, or if you want the Python script's output to go directly to the browser.  Use popen() if you want to write data to the Python script's standard input, or read data from the Python script's standard output in php.  popen() will only let you read or write, but not both.  If you want both, check out proc_open(), but with two way communication between programs you need to be careful to avoid deadlocks, where each program is waiting for the other to do something.  If you want to pass user supplied data to the Python script, then the big thing to be careful about is command injection.  If you aren't careful, your user could send you data like \"; evilcommand ;\" and make your program execute arbitrary commands against your will.  escapeshellarg() and escapeshellcmd() can help with this, but personally I like to remove everything that isn't a known good character, using something like  preg_replace('/[^a-zA-Z0-9]/', '', $str)      ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "98",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  The backquote operator will also allow you to run python scripts using similar syntax to above  In a python file called python.py:  hello = \"hello\" world = \"world\" print hello + \" \" + world   In a php file called python.php:  $python = `python python.py`; echo $python;      ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  There's also a PHP extension: Pip - Python in PHP, which I've never tried but had it bookmarked for just such an occasion     ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  You can run a python script via php, and outputs on browser.  Basically you have to call the python script this way:  $command = \"python /path/to/python_script.py 2>&1\"; $pid = popen( $command,\"r\"); while( !feof( $pid ) ) {  echo fread($pid, 256);  flush();  ob_flush();  usleep(100000); } pclose($pid);   Note: if you run any time.sleep() in you python code, it will not outputs the results on browser.  For full codes working, visit How to execute python script from php and show output on browser     ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  I do this kind of thing all the time for quick-and-dirty scripts.  It's quite common to have a CGI or PHP script that just uses system/popen to call some external program.  Just be extra careful if your web server is open to the internet at large.  Be sure to sanitize your GET/POST input in this case so as to not allow attackers to run arbitrary commands on your machine.     ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  Your call_python_file.php should look like this:  <?php     $item='Everything is awesome!!';     $tmp = exec(\"py.py $item\");     echo $tmp; ?>   This executes the python script and outputs the result to the browser. While in your python script the (sys.argv[1:]) variable will bring in all your arguments. To display the argv as a string for wherever your php is pulling from so if you want to do a text area:  import sys  list1 = ' '.join(sys.argv[1:])  def main():   print list1  if __name__ == '__main__':     main()      ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  Note that if you are using a virtual environment (as in shared hosting) then you must adjust your path to python, e.g: /home/user/mypython/bin/python ./cgi-bin/test.py     ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Calling Python in PHP",
        "A_Content": "  If you want to execute your Python script in PHP, it's necessary to do this command in your php script:  exec('your script python.py')      ",
        "Language": "Python",
        "Tags": [
            "php",
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/166944/calling-python-in-php",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python script I recently wrote that I call using the command line with some options. I now want a very thin web interface to call this script locally on my Mac.  I don't want to go through the minor trouble of installing mod_python or mod_wsgi on my Mac, so I was just going to do a system() or popen() from PHP to call the Python script.  Any better ideas? Thanks in advance!     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Get name of current class?",
        "A_Content": "  obj.__class__.__name__ will get you any objects name, so you can do this:  class Clazz():     def getName(self):         return self.__class__.__name__   Usage:  >>> c = Clazz() >>> c.getName() 'Clazz'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/6943182/get-name-of-current-class",
        "A_Votes": "116",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I get the name of the class I am currently in?  Example:  def get_input(class_name):     [do things]     return class_name_result  class foo():     input = get_input([class name goes here])   Due to the nature of the program I am interfacing with (vistrails), I cannot use __init__() to initialize input.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Get name of current class?",
        "A_Content": "  Within the body of a class, the class name isn't defined yet, so it is not available.  Can you not simply type the name of the class?  Maybe you need to say more about the problem so we can find a solution for you.  I would create a metaclass to do this work for you.  It's invoked at class creation time (conceptually at the very end of the class: block), and can manipulate the class being created.  I haven't tested this:  class InputAssigningMetaclass(type):     def __new__(cls, name, bases, attrs):         cls.input = get_input(name)         return super(MyType, cls).__new__(cls, name, bases, newattrs)  class MyBaseFoo(object):     __metaclass__ = InputAssigningMetaclass  class foo(MyBaseFoo):     # etc, no need to create 'input'  class foo2(MyBaseFoo):     # etc, no need to create 'input'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/6943182/get-name-of-current-class",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I get the name of the class I am currently in?  Example:  def get_input(class_name):     [do things]     return class_name_result  class foo():     input = get_input([class name goes here])   Due to the nature of the program I am interfacing with (vistrails), I cannot use __init__() to initialize input.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Get name of current class?",
        "A_Content": "  You can access it by the class' private attributes:  cls_name = self.__class__.__name__   EDIT:  As said by Ned Batcheler, this wouldn't work in the class body, but it would in a method.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/6943182/get-name-of-current-class",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I get the name of the class I am currently in?  Example:  def get_input(class_name):     [do things]     return class_name_result  class foo():     input = get_input([class name goes here])   Due to the nature of the program I am interfacing with (vistrails), I cannot use __init__() to initialize input.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Get name of current class?",
        "A_Content": "  EDIT: Yes, you can; but you have to cheat: The currently running class name is present on the call stack, and the traceback module allows you to access the stack.  >>> import traceback >>> def get_input(class_name): ...     return class_name.encode('rot13') ...  >>> class foo(object): ...      _name = traceback.extract_stack()[-1][2] ...     input = get_input(_name) ...  >>>  >>> foo.input 'sbb'   However, I wouldn't do this;  My original answer is still my own preference as a solution.  Original answer:  probably the very simplest solution is to use a decorator, which is similar to Ned's answer involving metaclasses, but less powerful (decorators are capable of black magic, but metaclasses are capable of ancient, occult black magic)  >>> def get_input(class_name): ...     return class_name.encode('rot13') ...  >>> def inputize(cls): ...     cls.input = get_input(cls.__name__) ...     return cls ...  >>> @inputize ... class foo(object): ...     pass ...  >>> foo.input 'sbb' >>>       ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/6943182/get-name-of-current-class",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I get the name of the class I am currently in?  Example:  def get_input(class_name):     [do things]     return class_name_result  class foo():     input = get_input([class name goes here])   Due to the nature of the program I am interfacing with (vistrails), I cannot use __init__() to initialize input.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "Get name of current class?",
        "A_Content": "  import sys  def class_meta(frame):     class_context = '__module__' in frame.f_locals     assert class_context, 'Frame is not a class context'      module_name = frame.f_locals['__module__']     class_name = frame.f_code.co_name     return module_name, class_name  def print_class_path():     print('%s.%s' % class_meta(sys._getframe(1)))  class MyClass(object):     print_class_path()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/6943182/get-name-of-current-class",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do I get the name of the class I am currently in?  Example:  def get_input(class_name):     [do things]     return class_name_result  class foo():     input = get_input([class name goes here])   Due to the nature of the program I am interfacing with (vistrails), I cannot use __init__() to initialize input.     ",
        "Q_Votes": "73"
    },
    {
        "Q_Title": "How can you get the SSH return code using Paramiko?",
        "A_Content": "  SSHClient is a simple wrapper class around the more lower-level functionality in Paramiko.  The API documentation lists a recv_exit_status() method on the Channel class.  A very simple demonstration script:  $ cat sshtest.py import paramiko import getpass  pw = getpass.getpass()  client = paramiko.SSHClient() client.set_missing_host_key_policy(paramiko.WarningPolicy()) client.connect('127.0.0.1', password=pw)  while True:     cmd = raw_input(\"Command to run: \")     if cmd == \"\":         break     chan = client.get_transport().open_session()     print \"running '%s'\" % cmd     chan.exec_command(cmd)     print \"exit status: %s\" % chan.recv_exit_status()  client.close()  $ python sshtest.py Password:  Command to run: true running 'true' exit status: 0 Command to run: false running 'false' exit status: 1 Command to run:  $      ",
        "Language": "Python",
        "Tags": [
            "python",
            "ssh",
            "paramiko"
        ],
        "URL": "https://stackoverflow.com/questions/3562403/how-can-you-get-the-ssh-return-code-using-paramiko",
        "A_Votes": "43",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    client = paramiko.SSHClient() stdin, stdout, stderr = client.exec_command(command)   Is there any way to get the command return code?  It's hard to parse all stdout/stderr and know whether the command finished successfully or not.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How can you get the SSH return code using Paramiko?",
        "A_Content": "  Much easier example that doesn't involve invoking the channel class directly:  import paramiko  client = paramiko.SSHClient() client.set_missing_host_key_policy(paramiko.AutoAddPolicy()) client.connect('blahblah.com')  stdin, stdout, stderr = client.exec_command(\"uptime\") print stdout.channel.recv_exit_status()    # status is 0  stdin, stdout, stderr = client.exec_command(\"oauwhduawhd\") print stdout.channel.recv_exit_status()    # status is 127      ",
        "Language": "Python",
        "Tags": [
            "python",
            "ssh",
            "paramiko"
        ],
        "URL": "https://stackoverflow.com/questions/3562403/how-can-you-get-the-ssh-return-code-using-paramiko",
        "A_Votes": "215",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    client = paramiko.SSHClient() stdin, stdout, stderr = client.exec_command(command)   Is there any way to get the command return code?  It's hard to parse all stdout/stderr and know whether the command finished successfully or not.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How can you get the SSH return code using Paramiko?",
        "A_Content": "  Thanks for JanC, I added some modification for the example and tested in Python3, it really useful for me.  import paramiko import getpass  pw = getpass.getpass()  client = paramiko.SSHClient() client.set_missing_host_key_policy(paramiko.WarningPolicy()) #client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  def start():     try :         client.connect('127.0.0.1', port=22, username='ubuntu', password=pw)         return True     except Exception as e:         #client.close()         print(e)         return False  while start():     key = True     cmd = input(\"Command to run: \")     if cmd == \"\":         break     chan = client.get_transport().open_session()     print(\"running '%s'\" % cmd)     chan.exec_command(cmd)     while key:         if chan.recv_ready():             print(\"recv:\\n%s\" % chan.recv(4096).decode('ascii'))         if chan.recv_stderr_ready():             print(\"error:\\n%s\" % chan.recv_stderr(4096).decode('ascii'))         if chan.exit_status_ready():             print(\"exit status: %s\" % chan.recv_exit_status())             key = False             client.close() client.close()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "ssh",
            "paramiko"
        ],
        "URL": "https://stackoverflow.com/questions/3562403/how-can-you-get-the-ssh-return-code-using-paramiko",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    client = paramiko.SSHClient() stdin, stdout, stderr = client.exec_command(command)   Is there any way to get the command return code?  It's hard to parse all stdout/stderr and know whether the command finished successfully or not.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How can you get the SSH return code using Paramiko?",
        "A_Content": "  In my case, output buffering was the problem. Because of buffering, the outputs from the application doesn't come out non-blocking way. You can find the answer about how to print output without buffering in here: Disable output buffering. For short, just run python with -u option like this:  > python -u script.py     ",
        "Language": "Python",
        "Tags": [
            "python",
            "ssh",
            "paramiko"
        ],
        "URL": "https://stackoverflow.com/questions/3562403/how-can-you-get-the-ssh-return-code-using-paramiko",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    client = paramiko.SSHClient() stdin, stdout, stderr = client.exec_command(command)   Is there any way to get the command return code?  It's hard to parse all stdout/stderr and know whether the command finished successfully or not.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  When I run your code I get this error:  UnboundLocalError: local variable '_total' referenced before assignment   This problem is caused by this line:  _total += PRICE_RANGES[key][0]   The documentation about Scopes and Namespaces  says this:     A special quirk of Python is that  if no global statement is in effect  assignments to names always go into the innermost scope. Assignments do not copy data  they just bind names to objects.   So since the line is effectively saying:  _total = _total + PRICE_RANGES[key][0]   it creates _total in the namespace of recurse().  Since _total is then new and unassigned you can't use it in the addition.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "51",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  Here's an illustration that gets to the essence of David's answer.   def outer():     a = 0     b = 1      def inner():         print a         print b         #b = 4      inner()  outer()   With the statement b = 4 commented out, this code outputs 0 1, just what you'd expect.  But if you uncomment that line, on the line print b, you get the error  UnboundLocalError: local variable 'b' referenced before assignment   It seems mysterious that the presence of b = 4 might somehow make b disappear on the lines that precede it. But the text David quotes explains why: during static analysis, the interpreter determines that b is assigned to in inner, and that it is therefore a local variable of inner. The print line attempts to print the b in that inner scope before it has been assigned.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "113",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  In Python 3, you can use the nonlocal statement to access non-local, non-global scopes.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "87",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  Rather than declaring a special object or map or array, one can also use a function attribute. This makes the scoping of the variable really clear.  def sumsquares(x,y):   def addsquare(n):     sumsquares.total += n*n    sumsquares.total = 0   addsquare(x)   addsquare(y)   return sumsquares.total   Of course this attribute belongs to the function (defintion), and not to the function call. So one must be mindful of threading and recursion.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  This is a variation of redman's solution, but using a proper namespace instead of an array to encapsulate the variable:  def foo():     class local:         counter = 0     def bar():         print(local.counter)         local.counter += 1     bar()     bar()     bar()  foo() foo()   I'm not sure if using a class object this way is considered an ugly hack or a proper coding technique in the python community, but it works fine in python 2.x and 3.x (tested with 2.7.3 and 3.2.3). I'm also unsure about the run-time efficiency of this solution.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  You probably have gotten the answer to your question. But i wanted to indicate a way i ussually get around this and that is by using lists. For instance, if i want to do this:   X=0 While X<20:     Do something. ..     X+=1   I would instead do this:  X=[0] While X<20:    Do something....    X[0]+=1   This way X is never a local variable     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  More from a philosophical point of view, one answer might be \"if you're having namespace problems, give it a namespace of its very own!\"  Providing it in its own class not only allows you to encapsulate the problem but also makes testing easier, eliminates those pesky globals, and reduces the need to shovel variables around between various top-level functions (doubtless there'll be more than just get_order_total).  Preserving the OP's code to focus on the essential change,  class Order(object):   PRICE_RANGES = {                   64:(25, 0.35),                   32:(13, 0.40),                   16:(7, 0.45),                   8:(4, 0.5)                   }     def __init__(self):     self._total = None    def get_order_total(self, quantity):       self._total = 0       _i = self.PRICE_RANGES.iterkeys()       def recurse(_i):           try:               key = _i.next()               if quantity % key != quantity:                   self._total += self.PRICE_RANGES[key][0]               return recurse(_i)            except StopIteration:               return (key, quantity % key)        res = recurse(_i)  #order = Order() #order.get_order_total(100)   As a PS, one hack which is a variant on the list idea in another answer, but perhaps clearer,  def outer():   order = {'total': 0}    def inner():     order['total'] += 42    inner()    return order['total']  print outer()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  While I used to use @redman's list-based approach, it's not optimal in terms of readability.  Here is a modified @Hans' approach, except I use an attribute of the inner function, rather than the outer. This should be more compatible with recursion, and maybe even multithreading:  def outer(recurse=2):     if 0 == recurse:         return      def inner():         inner.attribute += 1      inner.attribute = 0     inner()     inner()     outer(recurse-1)     inner()     print \"inner.attribute =\", inner.attribute  outer() outer()   This prints:  inner.attribute = 3 inner.attribute = 3 inner.attribute = 3 inner.attribute = 3   If I s/inner.attribute/outer.attribute/g, we get:  outer.attribute = 3 outer.attribute = 4 outer.attribute = 3 outer.attribute = 4   So, indeed, it seems better to make them the inner function's attributes.  Also, it seems sensible in terms of readability: because then the variable conceptually relates to the inner function, and this notation reminds the reader that the variable is shared between the scopes of the inner and the outer functions. A slight downside for the readability is that the inner.attribute may only be set syntactically after the def inner(): ....     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  >>> def get_order_total(quantity):     global PRICE_RANGES      total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):     print locals()     print globals()         try:             key = _i.next()             if quantity % key != quantity:                 total += PRICE_RANGES[key][0]             return recurse(_i)         except StopIteration:             return (key, quantity % key)     print 'main function', locals(), globals()      res = recurse(_i)   >>> get_order_total(20) main function {'total': 0, 'recurse': <function recurse at 0xb76baed4>, '_i': <dictionary-keyiterator object at 0xb6473e64>, 'quantity': 20} {'__builtins__': <module '__builtin__' (built-in)>, 'PRICE_RANGES': {64: (25, 0.34999999999999998), 32: (13, 0.40000000000000002), 16: (7, 0.45000000000000001), 8: (4, 0.5)}, '__package__': None, 's': <function s at 0xb646adf4>, 'get_order_total': <function get_order_total at 0xb646ae64>, '__name__': '__main__', '__doc__': None} {'recurse': <function recurse at 0xb76baed4>, '_i': <dictionary-keyiterator object at 0xb6473e64>, 'quantity': 20} {'__builtins__': <module '__builtin__' (built-in)>, 'PRICE_RANGES': {64: (25, 0.34999999999999998), 32: (13, 0.40000000000000002), 16: (7, 0.45000000000000001), 8: (4, 0.5)}, '__package__': None, 's': <function s at 0xb646adf4>, 'get_order_total': <function get_order_total at 0xb646ae64>, '__name__': '__main__', '__doc__': None} {'recurse': <function recurse at 0xb76baed4>, '_i': <dictionary-keyiterator object at 0xb6473e64>, 'quantity': 20} {'__builtins__': <module '__builtin__' (built-in)>, 'PRICE_RANGES': {64: (25, 0.34999999999999998), 32: (13, 0.40000000000000002), 16: (7, 0.45000000000000001), 8: (4, 0.5)}, '__package__': None, 's': <function s at 0xb646adf4>, 'get_order_total': <function get_order_total at 0xb646ae64>, '__name__': '__main__', '__doc__': None} {'recurse': <function recurse at 0xb76baed4>, '_i': <dictionary-keyiterator object at 0xb6473e64>, 'quantity': 20} {'__builtins__': <module '__builtin__' (built-in)>, 'PRICE_RANGES': {64: (25, 0.34999999999999998), 32: (13, 0.40000000000000002), 16: (7, 0.45000000000000001), 8: (4, 0.5)}, '__package__': None, 's': <function s at 0xb646adf4>, 'get_order_total': <function get_order_total at 0xb646ae64>, '__name__': '__main__', '__doc__': None}  Traceback (most recent call last):   File \"<pyshell#32>\", line 1, in <module>     get_order_total(20)   File \"<pyshell#31>\", line 18, in get_order_total     res = recurse(_i)   File \"<pyshell#31>\", line 13, in recurse     return recurse(_i)   File \"<pyshell#31>\", line 13, in recurse     return recurse(_i)   File \"<pyshell#31>\", line 12, in recurse     total += PRICE_RANGES[key][0] UnboundLocalError: local variable 'total' referenced before assignment >>>    as you see, total is in the local scope of the main function, but it's not in the local scope of recurse (obviously) but neither it is in the global scope, 'cause it's defined only in the local scope of get_order_total     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python nested functions variable scoping",
        "A_Content": "  My way around...  def outer():  class Cont(object):     var1 = None     @classmethod     def inner(cls, arg):         cls.var1 = arg   Cont.var1 = \"Before\" print Cont.var1 Cont.inner(\"After\") print Cont.var1  outer()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "scope"
        ],
        "URL": "https://stackoverflow.com/questions/5218895/python-nested-functions-variable-scoping",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've read almost all the other questions about the topic, but my code still doesn't work.  I think I'm missing something about python variable scope.  Here is my code:  PRICE_RANGES = {                 64:(25, 0.35),                 32:(13, 0.40),                 16:(7, 0.45),                 8:(4, 0.5)                 }  def get_order_total(quantity):     global PRICE_RANGES     _total = 0     _i = PRICE_RANGES.iterkeys()     def recurse(_i):         try:             key = _i.next()             if quantity % key != quantity:                 _total += PRICE_RANGES[key][0]             return recurse(_i)          except StopIteration:             return (key, quantity % key)      res = recurse(_i)   And I get      \"global name '_total' is not defined\"   I know the problem is on the _total assignment, but I can't understand why. Shouldn't recurse() have access to the parent function's variables?  Can someone explain to me what I'm missing about python variable scope?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to dynamically create a derived type in the Python C-API",
        "A_Content": "  I encountered the same problem when I was modifying an extension to be compatible with Python 3, and found this page when I was trying to solve it.  I did eventually solve it by reading the source code for the Python interpreter, PEP 0384 and the documentation for the C-API.  Setting the Py_TPFLAGS_HEAPTYPE flag tells the interpreter to recast your PyTypeObject as PyHeapTypeObject, which contains additional members that must also be allocated. At some point the interpreter attempts to refer to these extra members and, if you leave them unallocated, it will cause a segfault.  Python 3.2 introduced the C structures PyType_Slot and PyType_Spec and the C function PyType_FromSpec that simplify the creation of dynamic types. In a nutshell, you use PyType_Slot and PyType_Spec to specify the tp_* members of the PyTypeObject and then call PyType_FromSpec to do the dirty work of allocating and initialising the memory.  From PEP 0384, we have:  typedef struct{   int slot;    /* slot id, see below */   void *pfunc; /* function pointer */ } PyType_Slot;  typedef struct{   const char* name;   int basicsize;   int itemsize;   int flags;   PyType_Slot *slots; /* terminated by slot==0. */ } PyType_Spec;  PyObject* PyType_FromSpec(PyType_Spec*);   (The above isn't a literal copy from PEP 0384, which also includes const char *doc as a member of PyType_Spec. But that member doesn't appear in the source code.)  To use these in the original example, assume we have a C structure, BrownNoddy, that extends the C structure for the base class Noddy. Then we would have:  PyType_Slot slots[] = {     { Py_tp_doc, \"BrownNoddy objects\" },     { Py_tp_base, &NoddyType },     { Py_tp_new, BrownNoddy_new },     { 0 }, }; PyType_Spec spec = { \"noddy.BrownNoddy\", sizeof(BrownNoddy), 0,                       Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, slots }; PyTypeObject *BrownNoddyType = (PyTypeObject *)PyType_FromSpec(&spec);   This should do everything in the original code, including calling PyType_Ready, plus what is necessary for creating a dynamic type, including setting Py_TPFLAGS_HEAPTYPE, and allocating and initialising the extra memory for a PyHeapTypeObject.  I hope that's helpful.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "derived-class",
            "python-2.x",
            "python-c-api"
        ],
        "URL": "https://stackoverflow.com/questions/8066438/how-to-dynamically-create-a-derived-type-in-the-python-c-api",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume we have the type Noddy as defined in the tutorial on writing C extension modules for Python.  Now we want to create a derived type, overwriting only the __new__() method of Noddy.  Currently I use the following approach (error checking stripped for readability):  PyTypeObject *BrownNoddyType =     (PyTypeObject *)PyType_Type.tp_alloc(&PyType_Type, 0); BrownNoddyType->tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE; BrownNoddyType->tp_name = \"noddy.BrownNoddy\"; BrownNoddyType->tp_doc = \"BrownNoddy objects\"; BrownNoddyType->tp_base = &NoddyType; BrownNoddyType->tp_new = BrownNoddy_new; PyType_Ready(BrownNoddyType);   This works, but I'm not sure if it is The Right Way To Do It.  I would have expected that I have to set the Py_TPFLAGS_HEAPTYPE flag, too, because I dynamically allocate the type object on the heap, but doing so leads to a segfault in the interpreter.  I also thought about explicitly calling type() using PyObject_Call() or similar, but I discarded the idea.  I would need to wrap the function BrownNoddy_new() in a Python function object and create a dictionary mapping __new__ to this function object, which seems silly.  What is the best way to go about this?  Is my approach correct? Is there an interface function I missed?  Update  There are two threads on a related topic on the python-dev mailing list (1) (2).  From these threads and a few experiments I deduce that I shouldn't set Py_TPFLAGS_HEAPTYPE unless the type is allocated by a call to type().  There are different recommendations in these threads whether it is better to allocate the type manually or to call type().  I'd be happy with the latter if only I knew what the recommended way to wrap the C function that is supposed to go in the tp_new slot is.  For regular methods this step would be easy -- I could just use PyDescr_NewMethod() to get a suitable wrapper object. I don't know how to create such a wrapper object for my __new__() method, though -- maybe I need the undocumented function PyCFunction_New() to create such a wrapper object.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to dynamically create a derived type in the Python C-API",
        "A_Content": "  I apologize up front if this answer is terrible, but you can find an implementation of this idea in PythonQt, in particular I think the following files might be useful references:   PythonQtClassInfo.cpp PythonQtClassInfo.h PythonQtClassWrapper.cpp PythonQtClassWrapper.h   This fragment from PythonQtClassWrapper_init jumps out at me as being somewhat interesting:  static int PythonQtClassWrapper_init(PythonQtClassWrapper* self, PyObject* args, PyObject* kwds) {   // call the default type init   if (PyType_Type.tp_init((PyObject *)self, args, kwds) < 0) {     return -1;   }    // if we have no CPP class information, try our base class   if (!self->classInfo()) {     PyTypeObject*  superType = ((PyTypeObject *)self)->tp_base;      if (!superType || (superType->ob_type != &PythonQtClassWrapper_Type)) {       PyErr_Format(PyExc_TypeError, \"type %s is not derived from PythonQtClassWrapper\", ((PyTypeObject*)self)->tp_name);       return -1;     }      // take the class info from the superType     self->_classInfo = ((PythonQtClassWrapper*)superType)->classInfo();   }    return 0; }   It's worth noting that PythonQt does use a wrapper generator, so it's not exactly in line with what you're asking for, but personally I think trying to outsmart the vtable isn't the most optimal design. Basically, there are many different C++ wrapper generators for Python and people use them for a good reason - they're documented, there are examples floating around in search results and on stack overflow. If you hand roll a solution for this that nobody's seen before, it'll be that much harder for them to debug if they run into problems. Even if it's closed-source, the next guy who has to maintain it will be scratching his head and you'll have to explain it to every new person who comes along.  Once you get a code generator working, all you need to do is maintain the underlying C++ code, you don't have to update or modify your extension code by hand. (Which is probably not too far away from the tempting solution you went with)  The proposed solution is an example of breaking the type-safety that the newly introduced PyCapsule provides a bit more protection against (when used as directed).  So, while its possible it might not be the best long term choice to implement derived/subclasses this way, but rather wrap the code and let the vtable do what it does best and when the new guy has questions you can just point him at the documentation for whatever solution fits best.  This is just my opinion though. :D     ",
        "Language": "Python",
        "Tags": [
            "python",
            "derived-class",
            "python-2.x",
            "python-c-api"
        ],
        "URL": "https://stackoverflow.com/questions/8066438/how-to-dynamically-create-a-derived-type-in-the-python-c-api",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume we have the type Noddy as defined in the tutorial on writing C extension modules for Python.  Now we want to create a derived type, overwriting only the __new__() method of Noddy.  Currently I use the following approach (error checking stripped for readability):  PyTypeObject *BrownNoddyType =     (PyTypeObject *)PyType_Type.tp_alloc(&PyType_Type, 0); BrownNoddyType->tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE; BrownNoddyType->tp_name = \"noddy.BrownNoddy\"; BrownNoddyType->tp_doc = \"BrownNoddy objects\"; BrownNoddyType->tp_base = &NoddyType; BrownNoddyType->tp_new = BrownNoddy_new; PyType_Ready(BrownNoddyType);   This works, but I'm not sure if it is The Right Way To Do It.  I would have expected that I have to set the Py_TPFLAGS_HEAPTYPE flag, too, because I dynamically allocate the type object on the heap, but doing so leads to a segfault in the interpreter.  I also thought about explicitly calling type() using PyObject_Call() or similar, but I discarded the idea.  I would need to wrap the function BrownNoddy_new() in a Python function object and create a dictionary mapping __new__ to this function object, which seems silly.  What is the best way to go about this?  Is my approach correct? Is there an interface function I missed?  Update  There are two threads on a related topic on the python-dev mailing list (1) (2).  From these threads and a few experiments I deduce that I shouldn't set Py_TPFLAGS_HEAPTYPE unless the type is allocated by a call to type().  There are different recommendations in these threads whether it is better to allocate the type manually or to call type().  I'd be happy with the latter if only I knew what the recommended way to wrap the C function that is supposed to go in the tp_new slot is.  For regular methods this step would be easy -- I could just use PyDescr_NewMethod() to get a suitable wrapper object. I don't know how to create such a wrapper object for my __new__() method, though -- maybe I need the undocumented function PyCFunction_New() to create such a wrapper object.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to dynamically create a derived type in the Python C-API",
        "A_Content": "  One way to try and understand how to do this is to create a version of it using SWIG.  See what it produces and see if it matches or is done a different way.  From what I can tell the people who have been writing SWIG have an in depth understanding of extending Python.  Can't hurt to see how they do things at any rate. It may help you understand this problem.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "derived-class",
            "python-2.x",
            "python-c-api"
        ],
        "URL": "https://stackoverflow.com/questions/8066438/how-to-dynamically-create-a-derived-type-in-the-python-c-api",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume we have the type Noddy as defined in the tutorial on writing C extension modules for Python.  Now we want to create a derived type, overwriting only the __new__() method of Noddy.  Currently I use the following approach (error checking stripped for readability):  PyTypeObject *BrownNoddyType =     (PyTypeObject *)PyType_Type.tp_alloc(&PyType_Type, 0); BrownNoddyType->tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE; BrownNoddyType->tp_name = \"noddy.BrownNoddy\"; BrownNoddyType->tp_doc = \"BrownNoddy objects\"; BrownNoddyType->tp_base = &NoddyType; BrownNoddyType->tp_new = BrownNoddy_new; PyType_Ready(BrownNoddyType);   This works, but I'm not sure if it is The Right Way To Do It.  I would have expected that I have to set the Py_TPFLAGS_HEAPTYPE flag, too, because I dynamically allocate the type object on the heap, but doing so leads to a segfault in the interpreter.  I also thought about explicitly calling type() using PyObject_Call() or similar, but I discarded the idea.  I would need to wrap the function BrownNoddy_new() in a Python function object and create a dictionary mapping __new__ to this function object, which seems silly.  What is the best way to go about this?  Is my approach correct? Is there an interface function I missed?  Update  There are two threads on a related topic on the python-dev mailing list (1) (2).  From these threads and a few experiments I deduce that I shouldn't set Py_TPFLAGS_HEAPTYPE unless the type is allocated by a call to type().  There are different recommendations in these threads whether it is better to allocate the type manually or to call type().  I'd be happy with the latter if only I knew what the recommended way to wrap the C function that is supposed to go in the tp_new slot is.  For regular methods this step would be easy -- I could just use PyDescr_NewMethod() to get a suitable wrapper object. I don't know how to create such a wrapper object for my __new__() method, though -- maybe I need the undocumented function PyCFunction_New() to create such a wrapper object.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to *actually* read CSV data in TensorFlow?",
        "A_Content": "  I think you are mixing up imperative and graph-construction parts here. The operation tf.train.shuffle_batch creates a new queue node, and a single node can be used to process the entire dataset. So I think you are hanging because you created a bunch of shuffle_batch queues in your for loop and didn't start queue runners for them.   Normal input pipeline usage looks like this:   Add nodes like shuffle_batch to input pipeline (optional, to prevent unintentional graph modification) finalize graph   --- end of graph construction, beginning of imperative programming --   tf.start_queue_runners while(True): session.run()   To be more scalable (to avoid Python GIL), you could generate all of your data using TensorFlow pipeline. However, if performance is not critical, you can hook up a numpy array to an input pipeline by using slice_input_producer. Here's an example with some Print nodes to see what's going on (messages in Print go to stdout when node is run)  tf.reset_default_graph()  num_examples = 5 num_features = 2 data = np.reshape(np.arange(num_examples*num_features), (num_examples, num_features)) print data  (data_node,) = tf.slice_input_producer([tf.constant(data)], num_epochs=1, shuffle=False) data_node_debug = tf.Print(data_node, [data_node], \"Dequeueing from data_node \") data_batch = tf.batch([data_node_debug], batch_size=2) data_batch_debug = tf.Print(data_batch, [data_batch], \"Dequeueing from data_batch \")  sess = tf.InteractiveSession() sess.run(tf.initialize_all_variables()) tf.get_default_graph().finalize() tf.start_queue_runners()  try:   while True:     print sess.run(data_batch_debug) except tf.errors.OutOfRangeError as e:   print \"No more inputs.\"   You should see something like this  [[0 1]  [2 3]  [4 5]  [6 7]  [8 9]] [[0 1]  [2 3]] [[4 5]  [6 7]] No more inputs.   The \"8, 9\" numbers didn't fill up the full batch, so they didn't get produced. Also tf.Print are printed to sys.stdout, so they show up in separately in Terminal for me.  PS: a minimal of connecting batch to a manually initialized queue is in github issue 2193  Also, for debugging purposes you might want to set timeout on your session so that your IPython notebook doesn't hang on empty queue dequeues. I use this helper function for my sessions  def create_session():   config = tf.ConfigProto(log_device_placement=True)   config.gpu_options.per_process_gpu_memory_fraction=0.3 # don't hog all vRAM   config.operation_timeout_in_ms=60000   # terminate on long hangs   # create interactive session to register a default session   sess = tf.InteractiveSession(\"\", config=config)   return sess   Scalability Notes:   tf.constant inlines copy of your data into the Graph. There's a fundamental limit of 2GB on size of Graph definition so that's an upper limit on size of data You could get around that limit by using v=tf.Variable and saving the data into there by running v.assign_op with a tf.placeholder on right-hand side and feeding numpy array to the placeholder (feed_dict) That still creates two copies of data, so to save memory  you could make your own version of slice_input_producer which operates on numpy arrays, and uploads rows one at a time using feed_dict      ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/37091899/how-to-actually-read-csv-data-in-tensorflow",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm relatively new to the world of TensorFlow, and pretty perplexed by how you'd actually read CSV data into a usable example/label tensors in TensorFlow. The example from the TensorFlow tutorial on reading CSV data is pretty fragmented and only gets you part of the way to being able to train on CSV data.  Here's my code that I've pieced together, based off that CSV tutorial:  from __future__ import print_function import tensorflow as tf  def file_len(fname):     with open(fname) as f:         for i, l in enumerate(f):             pass     return i + 1  filename = \"csv_test_data.csv\"  # setup text reader file_length = file_len(filename) filename_queue = tf.train.string_input_producer([filename]) reader = tf.TextLineReader(skip_header_lines=1) _, csv_row = reader.read(filename_queue)  # setup CSV decoding record_defaults = [[0],[0],[0],[0],[0]] col1,col2,col3,col4,col5 = tf.decode_csv(csv_row, record_defaults=record_defaults)  # turn features back into a tensor features = tf.stack([col1,col2,col3,col4])  print(\"loading, \" + str(file_length) + \" line(s)\\n\") with tf.Session() as sess:   tf.initialize_all_variables().run()    # start populating filename queue   coord = tf.train.Coordinator()   threads = tf.train.start_queue_runners(coord=coord)    for i in range(file_length):     # retrieve a single instance     example, label = sess.run([features, col5])     print(example, label)    coord.request_stop()   coord.join(threads)   print(\"\\ndone loading\")   And here is an brief example from the CSV file I'm loading - pretty basic data - 4 feature columns, and 1 label column:  0,0,0,0,0 0,15,0,0,0 0,30,0,0,0 0,45,0,0,0   All the code above does is print each example from the CSV file, one by one, which, while nice, is pretty darn useless for training.  What I'm struggling with here is how you'd actually turn those individual examples, loaded one-by-one, into a training dataset. For example, here's a notebook I was working on in the Udacity Deep Learning course. I basically want to take the CSV data I'm loading, and plop it into something like train_dataset and train_labels:  def reformat(dataset, labels):   dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)   # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]   labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)   return dataset, labels train_dataset, train_labels = reformat(train_dataset, train_labels) valid_dataset, valid_labels = reformat(valid_dataset, valid_labels) test_dataset, test_labels = reformat(test_dataset, test_labels) print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape)   I've tried using tf.train.shuffle_batch, like this, but it just inexplicably hangs:    for i in range(file_length):     # retrieve a single instance     example, label = sess.run([features, colRelevant])     example_batch, label_batch = tf.train.shuffle_batch([example, label], batch_size=file_length, capacity=file_length, min_after_dequeue=10000)     print(example, label)   So to sum up, here are my questions:   What am I missing about this process?   It feels like there is some key intuition that I'm missing about how to properly build an input pipeline.  Is there a way to avoid having to know the length of the CSV file?   It feels pretty inelegant to have to know the number of lines you want to process (the for i in range(file_length) line of code above)      Edit: As soon as Yaroslav pointed out that I was likely mixing up imperative and graph-construction parts here, it started to become clearer. I was able to pull together the following code, which I think is closer to what would typically done when training a model from CSV (excluding any model training code):  from __future__ import print_function import numpy as np import tensorflow as tf import math as math import argparse  parser = argparse.ArgumentParser() parser.add_argument('dataset') args = parser.parse_args()  def file_len(fname):     with open(fname) as f:         for i, l in enumerate(f):             pass     return i + 1  def read_from_csv(filename_queue):   reader = tf.TextLineReader(skip_header_lines=1)   _, csv_row = reader.read(filename_queue)   record_defaults = [[0],[0],[0],[0],[0]]   colHour,colQuarter,colAction,colUser,colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)   features = tf.stack([colHour,colQuarter,colAction,colUser])     label = tf.stack([colLabel])     return features, label  def input_pipeline(batch_size, num_epochs=None):   filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)     example, label = read_from_csv(filename_queue)   min_after_dequeue = 10000   capacity = min_after_dequeue + 3 * batch_size   example_batch, label_batch = tf.train.shuffle_batch(       [example, label], batch_size=batch_size, capacity=capacity,       min_after_dequeue=min_after_dequeue)   return example_batch, label_batch  file_length = file_len(args.dataset) - 1 examples, labels = input_pipeline(file_length, 1)  with tf.Session() as sess:   tf.initialize_all_variables().run()    # start populating filename queue   coord = tf.train.Coordinator()   threads = tf.train.start_queue_runners(coord=coord)    try:     while not coord.should_stop():       example_batch, label_batch = sess.run([examples, labels])       print(example_batch)   except tf.errors.OutOfRangeError:     print('Done training, epoch reached')   finally:     coord.request_stop()    coord.join(threads)       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to *actually* read CSV data in TensorFlow?",
        "A_Content": "  Or you could try this, the code loads the Iris dataset into tensorflow using pandas and numpy and a simple one neuron output is printed in the session. Hope it helps for a basic understanding.... [ I havent added the way of one hot decoding labels].  import tensorflow as tf  import numpy import pandas as pd df=pd.read_csv('/home/nagarjun/Desktop/Iris.csv',usecols = [0,1,2,3,4],skiprows = [0],header=None) d = df.values l = pd.read_csv('/home/nagarjun/Desktop/Iris.csv',usecols = [5] ,header=None) labels = l.values data = numpy.float32(d) labels = numpy.array(l,'str') #print data, labels  #tensorflow x = tf.placeholder(tf.float32,shape=(150,5)) x = data w = tf.random_normal([100,150],mean=0.0, stddev=1.0, dtype=tf.float32) y = tf.nn.softmax(tf.matmul(w,x))  with tf.Session() as sess:     print sess.run(y)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/37091899/how-to-actually-read-csv-data-in-tensorflow",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm relatively new to the world of TensorFlow, and pretty perplexed by how you'd actually read CSV data into a usable example/label tensors in TensorFlow. The example from the TensorFlow tutorial on reading CSV data is pretty fragmented and only gets you part of the way to being able to train on CSV data.  Here's my code that I've pieced together, based off that CSV tutorial:  from __future__ import print_function import tensorflow as tf  def file_len(fname):     with open(fname) as f:         for i, l in enumerate(f):             pass     return i + 1  filename = \"csv_test_data.csv\"  # setup text reader file_length = file_len(filename) filename_queue = tf.train.string_input_producer([filename]) reader = tf.TextLineReader(skip_header_lines=1) _, csv_row = reader.read(filename_queue)  # setup CSV decoding record_defaults = [[0],[0],[0],[0],[0]] col1,col2,col3,col4,col5 = tf.decode_csv(csv_row, record_defaults=record_defaults)  # turn features back into a tensor features = tf.stack([col1,col2,col3,col4])  print(\"loading, \" + str(file_length) + \" line(s)\\n\") with tf.Session() as sess:   tf.initialize_all_variables().run()    # start populating filename queue   coord = tf.train.Coordinator()   threads = tf.train.start_queue_runners(coord=coord)    for i in range(file_length):     # retrieve a single instance     example, label = sess.run([features, col5])     print(example, label)    coord.request_stop()   coord.join(threads)   print(\"\\ndone loading\")   And here is an brief example from the CSV file I'm loading - pretty basic data - 4 feature columns, and 1 label column:  0,0,0,0,0 0,15,0,0,0 0,30,0,0,0 0,45,0,0,0   All the code above does is print each example from the CSV file, one by one, which, while nice, is pretty darn useless for training.  What I'm struggling with here is how you'd actually turn those individual examples, loaded one-by-one, into a training dataset. For example, here's a notebook I was working on in the Udacity Deep Learning course. I basically want to take the CSV data I'm loading, and plop it into something like train_dataset and train_labels:  def reformat(dataset, labels):   dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)   # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]   labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)   return dataset, labels train_dataset, train_labels = reformat(train_dataset, train_labels) valid_dataset, valid_labels = reformat(valid_dataset, valid_labels) test_dataset, test_labels = reformat(test_dataset, test_labels) print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape)   I've tried using tf.train.shuffle_batch, like this, but it just inexplicably hangs:    for i in range(file_length):     # retrieve a single instance     example, label = sess.run([features, colRelevant])     example_batch, label_batch = tf.train.shuffle_batch([example, label], batch_size=file_length, capacity=file_length, min_after_dequeue=10000)     print(example, label)   So to sum up, here are my questions:   What am I missing about this process?   It feels like there is some key intuition that I'm missing about how to properly build an input pipeline.  Is there a way to avoid having to know the length of the CSV file?   It feels pretty inelegant to have to know the number of lines you want to process (the for i in range(file_length) line of code above)      Edit: As soon as Yaroslav pointed out that I was likely mixing up imperative and graph-construction parts here, it started to become clearer. I was able to pull together the following code, which I think is closer to what would typically done when training a model from CSV (excluding any model training code):  from __future__ import print_function import numpy as np import tensorflow as tf import math as math import argparse  parser = argparse.ArgumentParser() parser.add_argument('dataset') args = parser.parse_args()  def file_len(fname):     with open(fname) as f:         for i, l in enumerate(f):             pass     return i + 1  def read_from_csv(filename_queue):   reader = tf.TextLineReader(skip_header_lines=1)   _, csv_row = reader.read(filename_queue)   record_defaults = [[0],[0],[0],[0],[0]]   colHour,colQuarter,colAction,colUser,colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)   features = tf.stack([colHour,colQuarter,colAction,colUser])     label = tf.stack([colLabel])     return features, label  def input_pipeline(batch_size, num_epochs=None):   filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)     example, label = read_from_csv(filename_queue)   min_after_dequeue = 10000   capacity = min_after_dequeue + 3 * batch_size   example_batch, label_batch = tf.train.shuffle_batch(       [example, label], batch_size=batch_size, capacity=capacity,       min_after_dequeue=min_after_dequeue)   return example_batch, label_batch  file_length = file_len(args.dataset) - 1 examples, labels = input_pipeline(file_length, 1)  with tf.Session() as sess:   tf.initialize_all_variables().run()    # start populating filename queue   coord = tf.train.Coordinator()   threads = tf.train.start_queue_runners(coord=coord)    try:     while not coord.should_stop():       example_batch, label_batch = sess.run([examples, labels])       print(example_batch)   except tf.errors.OutOfRangeError:     print('Done training, epoch reached')   finally:     coord.request_stop()    coord.join(threads)       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to *actually* read CSV data in TensorFlow?",
        "A_Content": "  You can use latest tf.data API :  dataset = tf.contrib.data.make_csv_dataset(filepath) iterator = dataset.make_initializable_iterator() columns = iterator.get_next() with tf.Session() as sess:    sess.run([iteator.initializer])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv",
            "tensorflow"
        ],
        "URL": "https://stackoverflow.com/questions/37091899/how-to-actually-read-csv-data-in-tensorflow",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm relatively new to the world of TensorFlow, and pretty perplexed by how you'd actually read CSV data into a usable example/label tensors in TensorFlow. The example from the TensorFlow tutorial on reading CSV data is pretty fragmented and only gets you part of the way to being able to train on CSV data.  Here's my code that I've pieced together, based off that CSV tutorial:  from __future__ import print_function import tensorflow as tf  def file_len(fname):     with open(fname) as f:         for i, l in enumerate(f):             pass     return i + 1  filename = \"csv_test_data.csv\"  # setup text reader file_length = file_len(filename) filename_queue = tf.train.string_input_producer([filename]) reader = tf.TextLineReader(skip_header_lines=1) _, csv_row = reader.read(filename_queue)  # setup CSV decoding record_defaults = [[0],[0],[0],[0],[0]] col1,col2,col3,col4,col5 = tf.decode_csv(csv_row, record_defaults=record_defaults)  # turn features back into a tensor features = tf.stack([col1,col2,col3,col4])  print(\"loading, \" + str(file_length) + \" line(s)\\n\") with tf.Session() as sess:   tf.initialize_all_variables().run()    # start populating filename queue   coord = tf.train.Coordinator()   threads = tf.train.start_queue_runners(coord=coord)    for i in range(file_length):     # retrieve a single instance     example, label = sess.run([features, col5])     print(example, label)    coord.request_stop()   coord.join(threads)   print(\"\\ndone loading\")   And here is an brief example from the CSV file I'm loading - pretty basic data - 4 feature columns, and 1 label column:  0,0,0,0,0 0,15,0,0,0 0,30,0,0,0 0,45,0,0,0   All the code above does is print each example from the CSV file, one by one, which, while nice, is pretty darn useless for training.  What I'm struggling with here is how you'd actually turn those individual examples, loaded one-by-one, into a training dataset. For example, here's a notebook I was working on in the Udacity Deep Learning course. I basically want to take the CSV data I'm loading, and plop it into something like train_dataset and train_labels:  def reformat(dataset, labels):   dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)   # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]   labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)   return dataset, labels train_dataset, train_labels = reformat(train_dataset, train_labels) valid_dataset, valid_labels = reformat(valid_dataset, valid_labels) test_dataset, test_labels = reformat(test_dataset, test_labels) print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape)   I've tried using tf.train.shuffle_batch, like this, but it just inexplicably hangs:    for i in range(file_length):     # retrieve a single instance     example, label = sess.run([features, colRelevant])     example_batch, label_batch = tf.train.shuffle_batch([example, label], batch_size=file_length, capacity=file_length, min_after_dequeue=10000)     print(example, label)   So to sum up, here are my questions:   What am I missing about this process?   It feels like there is some key intuition that I'm missing about how to properly build an input pipeline.  Is there a way to avoid having to know the length of the CSV file?   It feels pretty inelegant to have to know the number of lines you want to process (the for i in range(file_length) line of code above)      Edit: As soon as Yaroslav pointed out that I was likely mixing up imperative and graph-construction parts here, it started to become clearer. I was able to pull together the following code, which I think is closer to what would typically done when training a model from CSV (excluding any model training code):  from __future__ import print_function import numpy as np import tensorflow as tf import math as math import argparse  parser = argparse.ArgumentParser() parser.add_argument('dataset') args = parser.parse_args()  def file_len(fname):     with open(fname) as f:         for i, l in enumerate(f):             pass     return i + 1  def read_from_csv(filename_queue):   reader = tf.TextLineReader(skip_header_lines=1)   _, csv_row = reader.read(filename_queue)   record_defaults = [[0],[0],[0],[0],[0]]   colHour,colQuarter,colAction,colUser,colLabel = tf.decode_csv(csv_row, record_defaults=record_defaults)   features = tf.stack([colHour,colQuarter,colAction,colUser])     label = tf.stack([colLabel])     return features, label  def input_pipeline(batch_size, num_epochs=None):   filename_queue = tf.train.string_input_producer([args.dataset], num_epochs=num_epochs, shuffle=True)     example, label = read_from_csv(filename_queue)   min_after_dequeue = 10000   capacity = min_after_dequeue + 3 * batch_size   example_batch, label_batch = tf.train.shuffle_batch(       [example, label], batch_size=batch_size, capacity=capacity,       min_after_dequeue=min_after_dequeue)   return example_batch, label_batch  file_length = file_len(args.dataset) - 1 examples, labels = input_pipeline(file_length, 1)  with tf.Session() as sess:   tf.initialize_all_variables().run()    # start populating filename queue   coord = tf.train.Coordinator()   threads = tf.train.start_queue_runners(coord=coord)    try:     while not coord.should_stop():       example_batch, label_batch = sess.run([examples, labels])       print(example_batch)   except tf.errors.OutOfRangeError:     print('Done training, epoch reached')   finally:     coord.request_stop()    coord.join(threads)       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Advanced PDF Parsing Using Python (extracting text without tables, etc): What's the Best Library? [closed]",
        "A_Content": "  You can also take a look at PDFMiner, an other PDF parser in Python.  The particularity of PDFMiner that can interest you is that you can control how it regroup text parts when doing the extracting. You do this by specifing the space between lines, words, characters, etc. So, maybe by tweeking this you can achieve what you want (that depends of the variability of your documents). PDFMiner can also give you the location of the text in the page, it can extract data by Object ID and other stuff. So dig in PDFMiner and be creative!  But your problem is really not an easy one to solve because, in a PDF, the text is not continous, but made from a lot of small groups of characters positioned absolutely in the page. The focus of PDF is to keep the layout intact. It's not content oriented but presentation oriented.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pdf",
            "parsing",
            "text-extraction",
            "information-extraction"
        ],
        "URL": "https://stackoverflow.com/questions/1848464/advanced-pdf-parsing-using-python-extracting-text-without-tables-etc-whats",
        "A_Votes": "53",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm looking for a PDF library which will allow me to extract the text from a PDF document. I've looked at PyPDF, and this can extract the text from a PDF document very nicely. The problem with this is that if there are tables in the document, the text in the tables is extracted in-line with the rest of the document text. This can be problematic because it produces sections of text that aren't useful and look garbled (for instance, lots of numbers mashed together).  I'm looking for something that's a bit more advanced. I'd like to extract the text from a PDF document, excluding any tables and special formatting. Is there a library out there that does this? Or am I forced to do some post-processing on the output text to get rid of these sections?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Advanced PDF Parsing Using Python (extracting text without tables, etc): What's the Best Library? [closed]",
        "A_Content": "  That's a difficult problem to solve since visually similar PDFs may have a wildly differing structure depending on how they were produced. In the worst case the library would need to basically act like an OCR. On the other hand, the PDF may contain sufficient structure and metadata for easy removal of tables and figures, which the library can be tailored to take advantage of.  I'm pretty sure there are no open source tools which solve your problem for a wide variety of PDFs, but I remember having heard of commercial software claiming to do exactly what you ask for. I'm sure you'll run into them while googling.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pdf",
            "parsing",
            "text-extraction",
            "information-extraction"
        ],
        "URL": "https://stackoverflow.com/questions/1848464/advanced-pdf-parsing-using-python-extracting-text-without-tables-etc-whats",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking for a PDF library which will allow me to extract the text from a PDF document. I've looked at PyPDF, and this can extract the text from a PDF document very nicely. The problem with this is that if there are tables in the document, the text in the tables is extracted in-line with the rest of the document text. This can be problematic because it produces sections of text that aren't useful and look garbled (for instance, lots of numbers mashed together).  I'm looking for something that's a bit more advanced. I'd like to extract the text from a PDF document, excluding any tables and special formatting. Is there a library out there that does this? Or am I forced to do some post-processing on the output text to get rid of these sections?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is virtualenv recommended for django production server?",
        "A_Content": "  I would do it that way if you ever think you'll run more than one project on the webserver. As soon as you have two projects you run the risk of a future upgrade of any python package breaking the other site.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "deployment"
        ],
        "URL": "https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server",
        "A_Votes": "44",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have always been using virtualenv for testing my app in localhost since I have isolated environment and can safely test new release of packages.  Now It comes the time when I have to deploy my app to a production server. I am wondering if I should also use virtualenv for production server or just normal installation should do. Since it's production server I can always use the correct version that I tested in the dev server (under virtual-env)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is virtualenv recommended for django production server?",
        "A_Content": "     Is virtualenv recommended for django production server?   Yes, it makes your project not depend on certain aspects of the system environment and also it allows you to make the deployment process more clear and configurable.  I use fabric, pip and virtualenv to deploy all my Django projects.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "deployment"
        ],
        "URL": "https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have always been using virtualenv for testing my app in localhost since I have isolated environment and can safely test new release of packages.  Now It comes the time when I have to deploy my app to a production server. I am wondering if I should also use virtualenv for production server or just normal installation should do. Since it's production server I can always use the correct version that I tested in the dev server (under virtual-env)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is virtualenv recommended for django production server?",
        "A_Content": "  Yes, I think you should use virtualenv to deploy it into production. It makes things a lot easier and cleaner for you, especially if you plan on deploying multiple services, e.g. django based websites or other python projects. You don't want each of them to be polluting the global python environment with their packages.  I think virtualenv will help you manage all your dependencies cleanly.  To update your production env all you need to do is to:  pip -r name_of_your_requirements_file.txt   I use virtualenvs in production, and you can use uWSGI to serve the applications, with Cherokee as a web server.  To use your virtualenv in production, you will need to add its path to your PYTHONPATH.  For example if your env has the path \"/home/www/my_project/env/\", the path to add would be:  /home/www/env/lib/python2.7/site-packages/   You can set this up in many different ways, but if you're generating your FCGI or uWSGI interface through manage.py, simply add the following at the very top of your manage.py (before the rest):  import os my_virtualenv_path = \"/home/www/my_project/env/lib/python2.7/site-packages/\" # Add it to your PYTHONPATH os.path.append(my_virtualenv_path)   You can adapt this to your setup, just in case you could also do the following in the shell:  export PYTHONPATH:$PYTHONPATH:/home/www/my_project/env/lib/python2.7/site-packages/   You will also need to add the directory that contains your settings.py file to the PYTHONPATH, so Django will be able to discover it. Just proceed in a similar manner to do so.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "deployment"
        ],
        "URL": "https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have always been using virtualenv for testing my app in localhost since I have isolated environment and can safely test new release of packages.  Now It comes the time when I have to deploy my app to a production server. I am wondering if I should also use virtualenv for production server or just normal installation should do. Since it's production server I can always use the correct version that I tested in the dev server (under virtual-env)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is virtualenv recommended for django production server?",
        "A_Content": "  In most cases I would agree it is best to have a virtualenv even if it doesn't seem like you need it when you first set up the server. That said if you are using some kind of cloud service and spinning up servers for a specific task for a short time then I don't see the point of using a virtualenv.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "deployment"
        ],
        "URL": "https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have always been using virtualenv for testing my app in localhost since I have isolated environment and can safely test new release of packages.  Now It comes the time when I have to deploy my app to a production server. I am wondering if I should also use virtualenv for production server or just normal installation should do. Since it's production server I can always use the correct version that I tested in the dev server (under virtual-env)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is virtualenv recommended for django production server?",
        "A_Content": "  I think its a good indication that its a fully supported production solution when uwsgi directly supports it with the vhost flag: http://projects.unbit.it/uwsgi/wiki/VirtualHosting     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "deployment"
        ],
        "URL": "https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have always been using virtualenv for testing my app in localhost since I have isolated environment and can safely test new release of packages.  Now It comes the time when I have to deploy my app to a production server. I am wondering if I should also use virtualenv for production server or just normal installation should do. Since it's production server I can always use the correct version that I tested in the dev server (under virtual-env)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Is virtualenv recommended for django production server?",
        "A_Content": "  Using Docker containers for both development and production deployment is pretty popular now, so if you're considering following this trend, you won't need virtualenv anymore.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "deployment"
        ],
        "URL": "https://stackoverflow.com/questions/9337149/is-virtualenv-recommended-for-django-production-server",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have always been using virtualenv for testing my app in localhost since I have isolated environment and can safely test new release of packages.  Now It comes the time when I have to deploy my app to a production server. I am wondering if I should also use virtualenv for production server or just normal installation should do. Since it's production server I can always use the correct version that I tested in the dev server (under virtual-env)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When to create a new app (with startapp) in Django?",
        "A_Content": "  James Bennett has a wonderful set of slides on how to organize reusable apps in Django.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've googled around for this, but I still have trouble relating to what Django defines as \"apps\".   Should I create a new app for each piece of functionality in a site, even though it uses models from the main project?   Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the \"main project\" or other apps?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When to create a new app (with startapp) in Django?",
        "A_Content": "  I prefer to think of Django applications as reusable modules or components than as \"applications\".   This helps me encapsulate and decouple certain features from one another, improving re-usability should I decide to share a particular \"app\" with the community at large, and maintainability.  My general approach is to bucket up specific features or feature sets into \"apps\" as though I were going to release them publicly. The hard part here is figuring out how big each bucket is.   A good trick I use is to imagine how my apps would be used if they were released publicly. This often encourages me to shrink the buckets and more clearly define its \"purpose\".     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've googled around for this, but I still have trouble relating to what Django defines as \"apps\".   Should I create a new app for each piece of functionality in a site, even though it uses models from the main project?   Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the \"main project\" or other apps?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When to create a new app (with startapp) in Django?",
        "A_Content": "  I tend to create new applications for each logically separate set of models. e.g.:   User Profiles Forum Posts Blog posts      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've googled around for this, but I still have trouble relating to what Django defines as \"apps\".   Should I create a new app for each piece of functionality in a site, even though it uses models from the main project?   Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the \"main project\" or other apps?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When to create a new app (with startapp) in Django?",
        "A_Content": "  Here is the updated presentation on 6 September 2008.  DjangoCon 2008: Reusable Apps @7:53  Slide: Reusable_apps.pdf     Taken from the slide      Should this be its own application?         Is it completely unrelated to the apps focus?   Is it orthogonal to whatever else Im doing?   Will I need similar functionality on other sites?         If any of them is \"Yes\"? Then best to break it into a    separate application.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've googled around for this, but I still have trouble relating to what Django defines as \"apps\".   Should I create a new app for each piece of functionality in a site, even though it uses models from the main project?   Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the \"main project\" or other apps?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When to create a new app (with startapp) in Django?",
        "A_Content": "  The rule I follow is it should be a new app if I want to reuse the functionality in a different project.  If it needs deep understanding of the models in your project, it's probably more cohesive to stick it with the models.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've googled around for this, but I still have trouble relating to what Django defines as \"apps\".   Should I create a new app for each piece of functionality in a site, even though it uses models from the main project?   Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the \"main project\" or other apps?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When to create a new app (with startapp) in Django?",
        "A_Content": "  An 'app' could be many different things, it all really comes down to taste. For example, let's say you are building a blog. Your app could be the entire blog, or you could have an 'admin' app, a 'site' app for all of the public views, an 'rss' app, a 'services' app so developers can interface with the blog in their own ways, etc.  I personally would make the blog itself the app, and break out the functionality within it. The blog could then be reused rather easily in other websites.  The nice thing about Django is that it will recognize any models.py file within any level of your directory tree as a file containing Django models. So breaking your functionality out into smaller 'sub apps' within an 'app' itself won't make anything more difficult.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/64237/when-to-create-a-new-app-with-startapp-in-django",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've googled around for this, but I still have trouble relating to what Django defines as \"apps\".   Should I create a new app for each piece of functionality in a site, even though it uses models from the main project?   Do you guys have good rule of thumb of when to split off a new app, and when to keep functionality together with the \"main project\" or other apps?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Difference between coroutine and future/task in Python 3.5?",
        "A_Content": "  A comment by Vincent linked to https://github.com/python/asyncio/blob/master/asyncio/tasks.py#L346, which shows that wait() wraps the coroutines in ensure_future() for you!  In other words, we do need a future, and coroutines will be silently transformed into them.  I'll update this answer when I find a definitive explanation of how to batch coroutines/futures.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-asyncio",
            "python-3.5"
        ],
        "URL": "https://stackoverflow.com/questions/34753401/difference-between-coroutine-and-future-task-in-python-3-5",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Let's say we have a dummy function:  async def foo(arg):     result = await some_remote_call(arg)     return result.upper()   What's the difference between:  coros = [] for i in range(5):     coros.append(foo(i))  loop = get_event_loop() loop.run_until_complete(wait(coros))   And:  from asyncio import ensure_future  futures = [] for i in range(5):     futures.append(ensure_future(foo(i)))  loop = get_event_loop() loop.run_until_complete(wait(futures))   Note: The example returns a result, but this isn't the focus of the question. When return value matters, use gather() instead of wait().  Regardless of return value, I'm looking for clarity on ensure_future(). wait(coros) and wait(futures) both run the coroutines, so when and why should a coroutine be wrapped in ensure_future?  Basically, what's the Right Way (tm) to run a bunch of non-blocking operations using Python 3.5's async?  For extra credit, what if I want to batch the calls? For example, I need to call some_remote_call(...) 1000 times, but I don't want to crush the web server/database/etc with 1000 simultaneous connections. This is doable with a thread or process pool, but is there a way to do this with asyncio?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Difference between coroutine and future/task in Python 3.5?",
        "A_Content": "  A coroutine is a generator function that can both yield values and accept values from the outside. The benefit of using a coroutine is that we can pause the execution of a function and resume it later. In case of a network operation, it makes sense to pause the execution of a function while we're waiting for the response. We can use the time to run some other functions.   A future is like the Promise objects from Javascript. It is like a place holder for a value that will be materialized in the future. In the above mentioned case, while waiting on network I/O, a function can give us a container, a promise that it will fill the container with the value when the operation completes. We hold on to the future object and when it's fulfilled, we can call a method on it to retrieve the actual result.   Direct Answer: You don't need ensure_future if you don't need the results. They are good if you need the results or retrieve exceptions occured.   Extra Credits: I would choose run_in_executor and pass an Executor instance to control the number of max workers.   Explanations and Sample codes  In the first example, you are using coroutines. The wait function takes a bunch of coroutines and combines them together. So wait() finishes when all the coroutines are exhausted (completed/finished returning all the values).   loop = get_event_loop() #  loop.run_until_complete(wait(coros))   The run_until_complete method would make sure that the loop is alive until the execution is finished. Please notice how you are not getting the results of the async execution in this case.   In the second example, you are using the ensure_future function to wrap a coroutine and return a Task object which is a kind of Future. The coroutine is scheduled to be executed in the main event loop when you call ensure_future. The returned future/task object doesn't yet have a value but over time, when the network operations finish, the future object will hold the result of the operation.   from asyncio import ensure_future  futures = [] for i in range(5):     futures.append(ensure_future(foo(i)))  loop = get_event_loop() loop.run_until_complete(wait(futures))   So in this example, we're doing the same thing except we're using futures instead of just using coroutines.   Let's look at an example on how to use asyncio/coroutines/futures:  import asyncio   async def slow_operation():     await asyncio.sleep(1)     return 'Future is done!'   def got_result(future):     print(future.result())      # We have result, so let's stop     loop.stop()   loop = asyncio.get_event_loop() task = loop.create_task(slow_operation()) task.add_done_callback(got_result)  # We run forever loop.run_forever()   Here, we have used the create_task method on the loop object. ensure_future would schedule the task in the main event loop. This method enables us to schedule a coroutine on a loop we choose.   We also see the concept of adding a callback using the add_done_callback method on the task object.   A Task is done when the coroutine returns a value, raises an exception or gets cancelled. There are methods to check these incidents.   I have written some blog posts on these topics which might help:   http://masnun.com/2015/11/13/python-generators-coroutines-native-coroutines-and-async-await.html http://masnun.com/2015/11/20/python-asyncio-future-task-and-the-event-loop.html http://masnun.com/2015/12/07/python-3-using-blocking-functions-or-codes-with-asyncio.html   Of course, you can find more details on the official manual: https://docs.python.org/3/library/asyncio.html     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-asyncio",
            "python-3.5"
        ],
        "URL": "https://stackoverflow.com/questions/34753401/difference-between-coroutine-and-future-task-in-python-3-5",
        "A_Votes": "68",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Let's say we have a dummy function:  async def foo(arg):     result = await some_remote_call(arg)     return result.upper()   What's the difference between:  coros = [] for i in range(5):     coros.append(foo(i))  loop = get_event_loop() loop.run_until_complete(wait(coros))   And:  from asyncio import ensure_future  futures = [] for i in range(5):     futures.append(ensure_future(foo(i)))  loop = get_event_loop() loop.run_until_complete(wait(futures))   Note: The example returns a result, but this isn't the focus of the question. When return value matters, use gather() instead of wait().  Regardless of return value, I'm looking for clarity on ensure_future(). wait(coros) and wait(futures) both run the coroutines, so when and why should a coroutine be wrapped in ensure_future?  Basically, what's the Right Way (tm) to run a bunch of non-blocking operations using Python 3.5's async?  For extra credit, what if I want to batch the calls? For example, I need to call some_remote_call(...) 1000 times, but I don't want to crush the web server/database/etc with 1000 simultaneous connections. This is doable with a thread or process pool, but is there a way to do this with asyncio?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Numpy: Should I use newaxis or None?",
        "A_Content": "  None is allowed because numpy.newaxis is merely an alias for None.  In [1]: import numpy  In [2]: numpy.newaxis is None Out[2]: True   The authors probably chose it because they needed a convenient constant, and None was available.  As for why you should prefer newaxis over None: mainly it's because it's more explicit, and partly because someday the numpy authors might change it to something other than None.  (They're not planning to, and probably won't, but there's no good reason to prefer None.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/944863/numpy-should-i-use-newaxis-or-none",
        "A_Votes": "83",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In numpy one can use the 'newaxis' object in the slicing syntax to create an axis of length one, e.g.:  import numpy as np print np.zeros((3,5))[:,np.newaxis,:].shape # shape will be (3,1,5)   The documentation states that one can also use None instead of newaxis, the effect is exactly the same.  Is there any reason to choose one over the other? Is there any general preference or style guide? My impression is that newaxis is more popular, probably because it is more explicit. So is there any reason why None is allowed?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When are .pyc files refreshed?",
        "A_Content": "  The .pyc files are created (and possibly overwritten) only when that python file is imported by some other script. If the import is called, Python checks to see if the .pyc file's internal timestamp matches the corresponding .py file.  If it does, it loads the .pyc; if it does not or if the .pyc does not yet exist, Python compiles the .py file into a .pyc and loads it.  What do you mean by \"stricter checking\"?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals",
            "pyc"
        ],
        "URL": "https://stackoverflow.com/questions/15839555/when-are-pyc-files-refreshed",
        "A_Votes": "63",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I understand that \".pyc\" files are compiled versions of the plain-text \".py\" files, created at runtime to make programs run faster. However I have observed a few things:   Upon modification of \"py\" files, program behavior changes. This indicates that the \"py\" files are compiled or at least go though some sort of hashing process or compare time stamps in order to tell whether or not they should be re-compiled. Upon deleting all \".pyc\" files (rm *.pyc) sometimes program behavior will change. Which would indicate that they are not being compiled on update of \".py\"s.   Questions:   How do they decide when to be compiled? Is there a way to ensure that they have stricter checking during development?      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "When are .pyc files refreshed?",
        "A_Content": "  .pyc files generated whenever the corresponding code elements are imported, and updated if the corresponding code files have been updated.  If the .pyc files are deleted, they will be automatically regenerated.  However, they are not automatically deleted when the corresponding code files are deleted.  This can cause some really fun bugs during file-level refactors.  First of all, you can end up pushing code that only works on your machine and on no one else's.  If you have dangling references to files you deleted, these will still work locally if you don't manually delete the relevant .pyc files because .pyc files can be used in imports.  This is compounded with the fact that a properly configured version control system will only push .py files to the central repository, not .pyc files, meaning that your code can pass the \"import test\" (does everything import okay) just fine and not work on anyone else's computer.  Second, you can have some pretty terrible bugs if you turn packages into modules.  When you convert a package (a folder with an __init__.py file) into a module (a .py file), the .pyc files that once represented that package remain.  In particular, the __init__.pyc remains.  So, if you have the package foo with some code that doesn't matter, then later delete that package and create a file foo.py with some function def bar(): pass and run:  from foo import bar   you get:  ImportError: cannot import name bar   because python is still using the old .pyc files from the foo package, none of which define bar.  This can be especially problematic on a web server, where totally functioning code can break because of .pyc files.  As a result of both of these reasons (and possibly others), your deployment code and testing code should delete .pyc files, such as with the following line of bash:  find . -name '*.pyc' -delete   Also, as of python 2.6, you can run python with the -B flag to not use .pyc files.  See How to avoid .pyc files? for more details.  See also: How do I remove all .pyc files from a project?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals",
            "pyc"
        ],
        "URL": "https://stackoverflow.com/questions/15839555/when-are-pyc-files-refreshed",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I understand that \".pyc\" files are compiled versions of the plain-text \".py\" files, created at runtime to make programs run faster. However I have observed a few things:   Upon modification of \"py\" files, program behavior changes. This indicates that the \"py\" files are compiled or at least go though some sort of hashing process or compare time stamps in order to tell whether or not they should be re-compiled. Upon deleting all \".pyc\" files (rm *.pyc) sometimes program behavior will change. Which would indicate that they are not being compiled on update of \".py\"s.   Questions:   How do they decide when to be compiled? Is there a way to ensure that they have stricter checking during development?      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Differences between numpy.random and random.random in Python",
        "A_Content": "  You have made many correct observations already!  Unless you'd like to seed both of the random generators, it's probably simpler in the long run to choose one generator or the other.   For numpy.random.seed(), the main difficulty is that it is not thread-safe - that is, it's not safe to use if you have many different threads of execution, because it's not guaranteed to work if two different threads are executing the function at the same time. If you're not using threads, and if you can reasonably expect that you won't need to rewrite your program this way in the future, numpy.random.seed() should be fine. (As a side note Python threads actually have a number of problems so that would actually pretty reasonable.) If there's any reason to suspect that you may need threads in the future, it's much safer in the long run to do as suggested, and to make a local instance of the numpy.random.Random class. As far as I can tell, random.random.seed() is thread-safe (or at least, I haven't found any evidence to the contrary).  The numpy.random library contains a few extra probability distributions commonly used in scientific research, as well as a couple of convenience functions for generating arrays of random data. The random.random library is a little more lightweight, and should be fine if you're not doing scientific research or other kinds of work in statistics.  Otherwise, they both use the Mersenne twister sequence to generate their random numbers, and they're both completely deterministic - that is, if you know a few key bits of information, it's possible to predict with absolute certainty what number will come next. For this reason, neither is suitable for any serious cryptographic uses. But because the sequence is so very very long, both are fine for generating random numbers in everyday programs. This is also the reason for the necessity to seed the random value - if you start in the same place each time, you'll always get the same sequence of random numbers!  As a side note, if you do need cryptographic level randomness, you should use the secrets module, or something like Crypto.Random if you're using a Python version earlier than Python 3.6.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "random-seed"
        ],
        "URL": "https://stackoverflow.com/questions/7029993/differences-between-numpy-random-and-random-random-in-python",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a big script in Python. I inspired myself in other people's code so I ended up using the numpy.random module for some things (for example for creating an array of random numbers taken from a binomial distribution) and in other places I use the module random.random.  Can someone please tell me the major differences between the two? Looking at the doc webpage for each of the two it seems to me that numpy.random just has more methods, but I am unclear about how the generation of the random numbers is different.  The reason why I am asking is because I need to seed my main program for debugging purposes. But it doesn't work unless I use the same random number generator in all the modules that I am importing, is this correct?  Also, I read here, in another post, a discussion about NOT using numpy.random.seed(), but I didn't really understand why this was such a bad idea. I would really appreciate if someone explain me why this is the case.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Differences between numpy.random and random.random in Python",
        "A_Content": "  From Python for Data Analysis, the module numpy.random supplements the Python random with functions for efficiently generating whole arrays of sample values from many kinds of probability distributions.  By contrast, Python's built-in random module only samples one value at a time, while numpy.random can generate very large sample faster. Using IPython magic function %timeit one can see which module performs faster:  In [1]: from random import normalvariate In [2]: N = 1000000  In [3]: %timeit samples = [normalvariate(0, 1) for _ in xrange(N)] 1 loop, best of 3: 963 ms per loop  In [4]: %timeit np.random.normal(size=N) 10 loops, best of 3: 38.5 ms per loop      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "random-seed"
        ],
        "URL": "https://stackoverflow.com/questions/7029993/differences-between-numpy-random-and-random-random-in-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a big script in Python. I inspired myself in other people's code so I ended up using the numpy.random module for some things (for example for creating an array of random numbers taken from a binomial distribution) and in other places I use the module random.random.  Can someone please tell me the major differences between the two? Looking at the doc webpage for each of the two it seems to me that numpy.random just has more methods, but I am unclear about how the generation of the random numbers is different.  The reason why I am asking is because I need to seed my main program for debugging purposes. But it doesn't work unless I use the same random number generator in all the modules that I am importing, is this correct?  Also, I read here, in another post, a discussion about NOT using numpy.random.seed(), but I didn't really understand why this was such a bad idea. I would really appreciate if someone explain me why this is the case.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Differences between numpy.random and random.random in Python",
        "A_Content": "  The source of the seed and the distribution profile used are going to affect the outputs - if you are looking for cryptgraphic randomness, seeding from os.urandom() will get nearly real random bytes from device chatter (ie ethernet or disk) (ie /dev/random on BSD)  this will avoid you giving a seed and so generating determinisitic random numbers.  However the random calls then allow you to fit the numbers to a distribution (what I call scientific random ness - eventually all you want is a bell curve distribution of random numbers, numpy is best at delviering this.  SO yes, stick with one generator, but decide what random you want - random, but defitniely from a distrubtuion curve, or as random as you can get without a quantum device.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "random-seed"
        ],
        "URL": "https://stackoverflow.com/questions/7029993/differences-between-numpy-random-and-random-random-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a big script in Python. I inspired myself in other people's code so I ended up using the numpy.random module for some things (for example for creating an array of random numbers taken from a binomial distribution) and in other places I use the module random.random.  Can someone please tell me the major differences between the two? Looking at the doc webpage for each of the two it seems to me that numpy.random just has more methods, but I am unclear about how the generation of the random numbers is different.  The reason why I am asking is because I need to seed my main program for debugging purposes. But it doesn't work unless I use the same random number generator in all the modules that I am importing, is this correct?  Also, I read here, in another post, a discussion about NOT using numpy.random.seed(), but I didn't really understand why this was such a bad idea. I would really appreciate if someone explain me why this is the case.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  Thanks to gpilotino for giving me the push into the right direction for implementing this.  I noticed the question's code is using a datetime to figure out when its live . So I used the DateFieldFilterSpec and subclassed it.  from django.db import models from django.contrib.admin.filterspecs import FilterSpec, ChoicesFilterSpec,DateFieldFilterSpec from django.utils.encoding import smart_unicode from django.utils.translation import ugettext as _ from datetime import datetime  class IsLiveFilterSpec(DateFieldFilterSpec):     \"\"\"     Adds filtering by future and previous values in the admin     filter sidebar. Set the is_live_filter filter in the model field attribute     'is_live_filter'.    my_model_field.is_live_filter = True     \"\"\"      def __init__(self, f, request, params, model, model_admin):         super(IsLiveFilterSpec, self).__init__(f, request, params, model,                                                model_admin)         today = datetime.now()         self.links = (             (_('Any'), {}),             (_('Yes'), {'%s__lte' % self.field.name: str(today),                        }),             (_('No'), {'%s__gte' % self.field.name: str(today),                     }),          )       def title(self):         return \"Is Live\"  # registering the filter FilterSpec.filter_specs.insert(0, (lambda f: getattr(f, 'is_live_filter', False),                                IsLiveFilterSpec))   To use you can put the above code into a filters.py, and import it in the model you want to add the filter to      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "57",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  you have to write a custom FilterSpec (not documentend anywhere). Look here for an example:  http://www.djangosnippets.org/snippets/1051/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  In current django development version there is the support for custom filters: https://docs.djangoproject.com/en/dev/ref/contrib/admin/#django.contrib.admin.ModelAdmin.list_filter     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  You can't, unfortunately. Currently non-field items can not be used as list_filter entries.  Note that your admin class wouldn't have worked even if it was a field, as a single-item tuple needs a comma: ('is_live',)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  Just a sidenote: You can use the deafult ticks on Django admin more easily like this:  def is_live(self):     if self.when_to_publish is not None:         if ( self.when_to_publish < datetime.now() ):             return True     else:         return False  is_live.boolean = True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  Not an optimal way (CPU-wise) but simple and will work, so I do it this way (for my small database). My Django version is 1.6.  In admin.py:  class IsLiveFilter(admin.SimpleListFilter):     title = 'Live'     parameter_name = 'islive'     def lookups(self, request, model_admin):         return (             ('1', 'islive'),         )     def queryset(self, request, queryset):         if self.value():             array = []             for element in queryset:                 if element.is_live.__call__() == True:                     q_array.append(element.id)             return queryset.filter(pk__in=q_array)   ...  class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = (IsLiveFilter)   Key idea here is to access custom fields in a QuerySet via __call__() function.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  The user supplies goods to some countries postage free. I wanted to filter those countries:   All - all countries, Yes - postage free, No - charged postage.  The main answer for this question did not work for me (Django 1.3) I think because there was no field_path parameter provided in the __init__ method. Also it subclassed DateFieldFilterSpec. The postage field is a FloatField  from django.contrib.admin.filterspecs import FilterSpec  class IsFreePostage(FilterSpec):      def __init__(self, f, request, params, model, model_admin, field_path=None):         super(IsFreePostage, self).__init__(f, request, params, model,             model_admin, field_path)          self.removes = {             'Yes': ['postage__gt'],             'No': ['postage__exact'],             'All': ['postage__exact', 'postage__gt'] }          self.links = (             ('All', {}),             ('Yes', {'postage__exact': 0}),             ('No', {'postage__gt': 0}))          if request.GET.has_key('postage__exact'):             self.ttl = 'Yes'         elif request.GET.has_key('postage__gt'):             self.ttl = 'No'         else:             self.ttl = 'All'      def choices(self, cl):         for title, param_dict in self.links:             yield {'selected': title == self.ttl,                    'query_string': cl.get_query_string(param_dict,                        self.removes[title]),                    'display': title}     def title(self):         return 'Free Postage'  FilterSpec.filter_specs.insert(0,     (lambda f: getattr(f, 'free_postage', False), IsFreePostage))   In self.links we supply dicts. used to construct HTTP query strings like ?postage__exact=0 for each of the possible filters. Filters I think are cumulative so if there was a previous request for 'No' and now we have a request for 'Yes' we have to remove the  'No' query. self.removes specifies what needs to be removed for each query. The choices method constructs the query strings, says which filter has been selected and sets the displayed name of the filter.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Custom Filter in Django Admin on Django 1.3 or below",
        "A_Content": "  Here is the answer and implemented the custom filter as simple as possible this might help   Django admin date range filter     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/991926/custom-filter-in-django-admin-on-django-1-3-or-below",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I add a custom filter to django admin (the filters that appear on the right side of a model dashboard)?  I know its easy to include a filter based on a field of that model, but what about a \"calculated\" field like this:  class NewsItem(models.Model):     headline = models.CharField(max_length=4096, blank=False)     byline_1 = models.CharField(max_length=4096, blank=True)     dateline = models.DateTimeField(help_text=_(\"date/time that appears on article\"))     body_copy = models.TextField(blank=False)      when_to_publish = models.DateTimeField(verbose_name=\"When to publish\",  blank=True, null=True)      # HOW CAN I HAVE \"is_live\" as part of the admin filter?  It's a calculated state!!     def is_live(self):         if self.when_to_publish is not None:             if ( self.when_to_publish < datetime.now() ):                 return \"\"\" <img alt=\"True\" src=\"/media/img/admin/icon-yes.gif\"/> \"\"\"         else:             return \"\"\" <img alt=\"False\" src=\"/media/img/admin/icon-no.gif\"/> \"\"\"            is_live.allow_tags = True     class NewsItemAdmin(admin.ModelAdmin):     form = NewsItemAdminForm     list_display = ('headline', 'id', 'is_live')     list_filter = ('is_live')  #  how can i make this work??      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Homebrew brew doctor warning about /Library/Frameworks/Python.framework, even with brew's Python installed",
        "A_Content": "  I had the same problem. When I upgraded python3 through Homebrew, I started getting this:  -bash: python3: command not found   I had the same conflict with Python somehow being installed in /Library/Framework/Python.framework. I just did a brew link overwrite and everything is working fine now. There is some info about what to do with the Python version in the /Library/Framework/Python.framework here.  I guess you could try deleting that version as the link suggests, just make sure that version isn't being used. When I got into the Python.framework directory I was seeing some EPD version of Python, which I think is Enthought. You could delete it, but I if it isn't causing you any problems besides the unsightly Homebrew warning message, then I think you should just ignore it for now.   Update:  I did delete the Python.framework directory which, through some poking around inside that directory, I started seeing a few old versions of Python that I didn't install with Homebrew. One was from Enthought, and another was a distribution of Python3.3. I think some of these installs in the Framework directory are user installs. I installed R on my system, and there is also an R.framework directory, so I think most of these are user installs. After I deleted the directory, I just had to call brew prune to remove the old symlinks. I checked both brew versions of python 2.7.6 and 3.3.4, and they seem to be in good working order with all of my installed packages. I guess I leave the decision to remove that directory, or python version, to your discretion.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "macos",
            "python-2.7",
            "homebrew",
            "brew-doctor"
        ],
        "URL": "https://stackoverflow.com/questions/22255579/homebrew-brew-doctor-warning-about-library-frameworks-python-framework-even-wi",
        "A_Votes": "33",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    When I ran Homebrew's brew doctor (Mac OS X 10.9.2), I get the following warning message:     Warning: Python is installed at /Library/Frameworks/Python.framework      Homebrew only supports building against the System-provided Python or   a brewed Python. In particular, Pythons installed to /Library can   interfere with other software installs.   Therefore, I ran brew install and followed the steps provided in the installation's caveats output to install Homebrew's version of Python. Running which python confirms that Homebrew's version of it is indeed at the top of my PATH. Output is /usr/local/bin/python.  Despite all this, when I rerun brew doctor, I am still getting the same warning message. How do I suppress this warning? Do I need to delete the /Library/Frameworks/Python.framework directory from my computer? Am I just supposed to ignore it? Is there a different application on my computer that may be causing this warning to emit?  Note that I don't have any applications in particular that are running into errors due to this warning from brew doctor. Also note that this warning message didn't always print out when I ran brew doctor, it was something that started to appear recently. Also, I am using Python 2.7 on my computer, trying to stay away from Python 3.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Homebrew brew doctor warning about /Library/Frameworks/Python.framework, even with brew's Python installed",
        "A_Content": "  I also received this message.   Something, sometime installed      /Library/Frameworks/Python.framework   on my machine (the folder date was about 4 years old).  I've chosen to remove it.  Please note that the Apple provided framework lives in      /System/Library/Frameworks/Python.framework/      ",
        "Language": "Python",
        "Tags": [
            "python",
            "macos",
            "python-2.7",
            "homebrew",
            "brew-doctor"
        ],
        "URL": "https://stackoverflow.com/questions/22255579/homebrew-brew-doctor-warning-about-library-frameworks-python-framework-even-wi",
        "A_Votes": "35",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I ran Homebrew's brew doctor (Mac OS X 10.9.2), I get the following warning message:     Warning: Python is installed at /Library/Frameworks/Python.framework      Homebrew only supports building against the System-provided Python or   a brewed Python. In particular, Pythons installed to /Library can   interfere with other software installs.   Therefore, I ran brew install and followed the steps provided in the installation's caveats output to install Homebrew's version of Python. Running which python confirms that Homebrew's version of it is indeed at the top of my PATH. Output is /usr/local/bin/python.  Despite all this, when I rerun brew doctor, I am still getting the same warning message. How do I suppress this warning? Do I need to delete the /Library/Frameworks/Python.framework directory from my computer? Am I just supposed to ignore it? Is there a different application on my computer that may be causing this warning to emit?  Note that I don't have any applications in particular that are running into errors due to this warning from brew doctor. Also note that this warning message didn't always print out when I ran brew doctor, it was something that started to appear recently. Also, I am using Python 2.7 on my computer, trying to stay away from Python 3.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Homebrew brew doctor warning about /Library/Frameworks/Python.framework, even with brew's Python installed",
        "A_Content": "  per this thread, enter this command:  sudo rm -rf /Library/Frameworks/Python.framework  because there are multiple installations of Python on your computer, and this removes the one that may cause additional problems in the future.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "macos",
            "python-2.7",
            "homebrew",
            "brew-doctor"
        ],
        "URL": "https://stackoverflow.com/questions/22255579/homebrew-brew-doctor-warning-about-library-frameworks-python-framework-even-wi",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I ran Homebrew's brew doctor (Mac OS X 10.9.2), I get the following warning message:     Warning: Python is installed at /Library/Frameworks/Python.framework      Homebrew only supports building against the System-provided Python or   a brewed Python. In particular, Pythons installed to /Library can   interfere with other software installs.   Therefore, I ran brew install and followed the steps provided in the installation's caveats output to install Homebrew's version of Python. Running which python confirms that Homebrew's version of it is indeed at the top of my PATH. Output is /usr/local/bin/python.  Despite all this, when I rerun brew doctor, I am still getting the same warning message. How do I suppress this warning? Do I need to delete the /Library/Frameworks/Python.framework directory from my computer? Am I just supposed to ignore it? Is there a different application on my computer that may be causing this warning to emit?  Note that I don't have any applications in particular that are running into errors due to this warning from brew doctor. Also note that this warning message didn't always print out when I ran brew doctor, it was something that started to appear recently. Also, I am using Python 2.7 on my computer, trying to stay away from Python 3.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Homebrew brew doctor warning about /Library/Frameworks/Python.framework, even with brew's Python installed",
        "A_Content": "  You can use this solution as I've put below  sudo rm -rf /Library/Frameworks/Python.framework  brew doctor   brew prune  brew update   That combo fixed it for me, even thought this error usually doesn't cause any major problems its just was annoying me to see them pop up under brew doctor     ",
        "Language": "Python",
        "Tags": [
            "python",
            "macos",
            "python-2.7",
            "homebrew",
            "brew-doctor"
        ],
        "URL": "https://stackoverflow.com/questions/22255579/homebrew-brew-doctor-warning-about-library-frameworks-python-framework-even-wi",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I ran Homebrew's brew doctor (Mac OS X 10.9.2), I get the following warning message:     Warning: Python is installed at /Library/Frameworks/Python.framework      Homebrew only supports building against the System-provided Python or   a brewed Python. In particular, Pythons installed to /Library can   interfere with other software installs.   Therefore, I ran brew install and followed the steps provided in the installation's caveats output to install Homebrew's version of Python. Running which python confirms that Homebrew's version of it is indeed at the top of my PATH. Output is /usr/local/bin/python.  Despite all this, when I rerun brew doctor, I am still getting the same warning message. How do I suppress this warning? Do I need to delete the /Library/Frameworks/Python.framework directory from my computer? Am I just supposed to ignore it? Is there a different application on my computer that may be causing this warning to emit?  Note that I don't have any applications in particular that are running into errors due to this warning from brew doctor. Also note that this warning message didn't always print out when I ran brew doctor, it was something that started to appear recently. Also, I am using Python 2.7 on my computer, trying to stay away from Python 3.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Homebrew brew doctor warning about /Library/Frameworks/Python.framework, even with brew's Python installed",
        "A_Content": "  Removing directories manually can be a nightmare, but fortunately 'brew' can take care of that. Once you are done with the removal, put this:  brew doctor   The above command will list the broken system links. In order to get rid of these broken symlinks, put this:  brew prune   Check with 'brew doctor' once more to ensure no links are broken. Your system will then be ready to brew.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "macos",
            "python-2.7",
            "homebrew",
            "brew-doctor"
        ],
        "URL": "https://stackoverflow.com/questions/22255579/homebrew-brew-doctor-warning-about-library-frameworks-python-framework-even-wi",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I ran Homebrew's brew doctor (Mac OS X 10.9.2), I get the following warning message:     Warning: Python is installed at /Library/Frameworks/Python.framework      Homebrew only supports building against the System-provided Python or   a brewed Python. In particular, Pythons installed to /Library can   interfere with other software installs.   Therefore, I ran brew install and followed the steps provided in the installation's caveats output to install Homebrew's version of Python. Running which python confirms that Homebrew's version of it is indeed at the top of my PATH. Output is /usr/local/bin/python.  Despite all this, when I rerun brew doctor, I am still getting the same warning message. How do I suppress this warning? Do I need to delete the /Library/Frameworks/Python.framework directory from my computer? Am I just supposed to ignore it? Is there a different application on my computer that may be causing this warning to emit?  Note that I don't have any applications in particular that are running into errors due to this warning from brew doctor. Also note that this warning message didn't always print out when I ran brew doctor, it was something that started to appear recently. Also, I am using Python 2.7 on my computer, trying to stay away from Python 3.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Homebrew brew doctor warning about /Library/Frameworks/Python.framework, even with brew's Python installed",
        "A_Content": "  I had installed Python 3 from Homebrew and then another Python 3 directly from the Python website.  I deleted /Library/Frameworks/Python.framework. I did not add any PATH or PYTHONPATH variables in .bash_profile. Then I checked the following in the shell:  ~$ python Python 2.7.10 (default, Feb  7 2017, 00:08:15) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> quit()  ~$ python3 Python 3.6.4 (default, Mar  9 2018, 23:15:12) [GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> quit()  ~$   In addition, I checked the following:  ~$ whereis python /usr/bin/python  ~$ whereis python3  ~$ which python /usr/bin/python  ~$ which python3 /usr/local/bin/python3  ~$ which pip /usr/local/bin/pip  ~$ which pip3 /usr/local/bin/pip3  ~$ whereis pip  ~$ whereis pip3   I was a bit worried that whereis python3, whereis pip, and whereis pip3 returned nothing, but since there was no problem launching Python, I'd think it's safe to delete /Library/Frameworks/Python.framework. Ask me again in a month.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "macos",
            "python-2.7",
            "homebrew",
            "brew-doctor"
        ],
        "URL": "https://stackoverflow.com/questions/22255579/homebrew-brew-doctor-warning-about-library-frameworks-python-framework-even-wi",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I ran Homebrew's brew doctor (Mac OS X 10.9.2), I get the following warning message:     Warning: Python is installed at /Library/Frameworks/Python.framework      Homebrew only supports building against the System-provided Python or   a brewed Python. In particular, Pythons installed to /Library can   interfere with other software installs.   Therefore, I ran brew install and followed the steps provided in the installation's caveats output to install Homebrew's version of Python. Running which python confirms that Homebrew's version of it is indeed at the top of my PATH. Output is /usr/local/bin/python.  Despite all this, when I rerun brew doctor, I am still getting the same warning message. How do I suppress this warning? Do I need to delete the /Library/Frameworks/Python.framework directory from my computer? Am I just supposed to ignore it? Is there a different application on my computer that may be causing this warning to emit?  Note that I don't have any applications in particular that are running into errors due to this warning from brew doctor. Also note that this warning message didn't always print out when I ran brew doctor, it was something that started to appear recently. Also, I am using Python 2.7 on my computer, trying to stay away from Python 3.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Distributed task queues (Ex. Celery) vs crontab scripts",
        "A_Content": "  It depends what you want your tasks to be doing, if you need to distribute them, and how you want to manage them.  A crontab is capable of executing a script every N intervals. It runs, and then returns. Essentially you get a single execution each interval. You could just direct a crontab to execute a django management command and get access to the entire django environment, so celery doesn't really help you there.  What celery brings to the table, with the help of a message queue, is distributed tasks. Many servers can join the pool of workers and each receive a work item without fear of double handling. It's also possible to execute a task as soon as it is ready. With cron, you're limited to a minimum of one minute.  As an example, imagine you've just launched a new web application and you're receiving hundreds of sign ups that require an email to be sent to each user. Sending an email may take a long time (comparatively) so you decide that you'll handle activation emails via tasks.  If you were using cron, you'd need to ensure that every minute cron is able to process all of the emails that need to be sent. If you have several servers you now need to make sure that you aren't sending multiple activation emails to the same user - you need some kind of synchronization.  With celery, you add a task to the queue. You may have several workers per server so you've already scaled ahead of a cronjob. You may also have several servers allowing you to scale even more. Synchronization is handled as part of the 'queue'.  You can use celery as a cron replacement but that's not really its primary use. It is used for farming out asynchronous tasks across a distributed cluster.  And of course, celery has a big list of features that cron does not.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "celery"
        ],
        "URL": "https://stackoverflow.com/questions/16232572/distributed-task-queues-ex-celery-vs-crontab-scripts",
        "A_Votes": "107",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm having trouble understanding the purpose of 'distributed task queues'. For example, python's celery library.   I know that in celery, the python framework, you can set timed windows for functions to get executed. However, that can also be easily done in a linux crontab directed at a python script.   And as far as I know, and shown from my own django-celery webapps, celery consumes much more RAM memory than just setting up a raw crontab. Few hundred MB difference for a relatively small app.  Can someone please help me with this distinction? Perhaps a high level explanation of how task queues / crontabs work in general would be nice also.  Thank you.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Finding the index of elements based on a condition using python list comprehension",
        "A_Content": "   In Python, you wouldn't use indexes for this at all, but just deal with the values[value for value in a if value > 2]. Usually dealing with indexes means you're not doing something the best way. If you do need an API similar to Matlab's, you would use numpy, a package for multidimensional arrays and numerical math in Python which is heavily inspired by Matlab. You would be using a numpy array instead of a list.  >>> import numpy >>> a = numpy.array([1, 2, 3, 1, 2, 3]) >>> a array([1, 2, 3, 1, 2, 3]) >>> numpy.where(a > 2) (array([2, 5]),) >>> a > 2 array([False, False,  True, False, False,  True], dtype=bool) >>> a[numpy.where(a > 2)] array([3, 3]) >>> a[a > 2] array([3, 3])       ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/7270321/finding-the-index-of-elements-based-on-a-condition-using-python-list-comprehensi",
        "A_Votes": "54",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The following Python code appears to be very long winded when coming from a Matlab background  >>> a = [1, 2, 3, 1, 2, 3] >>> [index for index,value in enumerate(a) if value > 2] [2, 5]   When in Matlab I can write:  >> a = [1, 2, 3, 1, 2, 3]; >> find(a>2) ans =      3     6   Is there a short hand method of writing this in Python, or do I just stick with the long version?    Thank you for all the suggestions and explanation of the rationale for Python's syntax.  After finding the following on the numpy website, I think I have found a solution I like:  http://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays  Applying the information from that website to my problem above, would give the following:  >>> from numpy import array >>> a = array([1, 2, 3, 1, 2, 3]) >>> b = a>2  array([False, False, True, False, False, True], dtype=bool) >>> r = array(range(len(b))) >>> r(b) [2, 5]   The following should then work (but I haven't got a Python interpreter on hand to test it):  class my_array(numpy.array):     def find(self, b):         r = array(range(len(b)))         return r(b)   >>> a = my_array([1, 2, 3, 1, 2, 3]) >>> a.find(a>2) [2, 5]      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Finding the index of elements based on a condition using python list comprehension",
        "A_Content": "  Another way:  >>> [i for i in range(len(a)) if a[i] > 2] [2, 5]   In general, remember that while find is a ready-cooked function, list comprehensions are a general, and thus very powerful solution. Nothing prevents you from writing a find function in Python and use it later as you wish. I.e.:  >>> def find_indices(lst, condition): ...   return [i for i, elem in enumerate(lst) if condition(elem)] ...  >>> find_indices(a, lambda e: e > 2) [2, 5]   Note that I'm using lists here to mimic Matlab. It would be more Pythonic to use generators and iterators.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/7270321/finding-the-index-of-elements-based-on-a-condition-using-python-list-comprehensi",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The following Python code appears to be very long winded when coming from a Matlab background  >>> a = [1, 2, 3, 1, 2, 3] >>> [index for index,value in enumerate(a) if value > 2] [2, 5]   When in Matlab I can write:  >> a = [1, 2, 3, 1, 2, 3]; >> find(a>2) ans =      3     6   Is there a short hand method of writing this in Python, or do I just stick with the long version?    Thank you for all the suggestions and explanation of the rationale for Python's syntax.  After finding the following on the numpy website, I think I have found a solution I like:  http://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays  Applying the information from that website to my problem above, would give the following:  >>> from numpy import array >>> a = array([1, 2, 3, 1, 2, 3]) >>> b = a>2  array([False, False, True, False, False, True], dtype=bool) >>> r = array(range(len(b))) >>> r(b) [2, 5]   The following should then work (but I haven't got a Python interpreter on hand to test it):  class my_array(numpy.array):     def find(self, b):         r = array(range(len(b)))         return r(b)   >>> a = my_array([1, 2, 3, 1, 2, 3]) >>> a.find(a>2) [2, 5]      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Finding the index of elements based on a condition using python list comprehension",
        "A_Content": "  For me it works well:  >>> import numpy as np >>> a = np.array([1, 2, 3, 1, 2, 3]) >>> np.where(a > 2)[0] [2 5]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/7270321/finding-the-index-of-elements-based-on-a-condition-using-python-list-comprehensi",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The following Python code appears to be very long winded when coming from a Matlab background  >>> a = [1, 2, 3, 1, 2, 3] >>> [index for index,value in enumerate(a) if value > 2] [2, 5]   When in Matlab I can write:  >> a = [1, 2, 3, 1, 2, 3]; >> find(a>2) ans =      3     6   Is there a short hand method of writing this in Python, or do I just stick with the long version?    Thank you for all the suggestions and explanation of the rationale for Python's syntax.  After finding the following on the numpy website, I think I have found a solution I like:  http://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays  Applying the information from that website to my problem above, would give the following:  >>> from numpy import array >>> a = array([1, 2, 3, 1, 2, 3]) >>> b = a>2  array([False, False, True, False, False, True], dtype=bool) >>> r = array(range(len(b))) >>> r(b) [2, 5]   The following should then work (but I haven't got a Python interpreter on hand to test it):  class my_array(numpy.array):     def find(self, b):         r = array(range(len(b)))         return r(b)   >>> a = my_array([1, 2, 3, 1, 2, 3]) >>> a.find(a>2) [2, 5]      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Finding the index of elements based on a condition using python list comprehension",
        "A_Content": "  Maybe another question is, \"what are you going to do with those indices once you get them?\"  If you are going to use them to create another list, then in Python, they are an unnecessary middle step.  If you want all the values that match a given condition, just use the builtin filter:  matchingVals = filter(lambda x : x>2, a)   Or write your own list comprhension:  matchingVals = [x for x in a if x > 2]   If you want to remove them from the list, then the Pythonic way is not to necessarily remove from the list, but write a list comprehension as if you were creating a new list, and assigning back in-place using the listvar[:] on the left-hand-side:  a[:] = [x for x in a if x <= 2]   Matlab supplies find because its array-centric model works by selecting items using their array indices. You can do this in Python, certainly, but the more Pythonic way is using iterators and generators, as already mentioned by @EliBendersky.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/7270321/finding-the-index-of-elements-based-on-a-condition-using-python-list-comprehensi",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The following Python code appears to be very long winded when coming from a Matlab background  >>> a = [1, 2, 3, 1, 2, 3] >>> [index for index,value in enumerate(a) if value > 2] [2, 5]   When in Matlab I can write:  >> a = [1, 2, 3, 1, 2, 3]; >> find(a>2) ans =      3     6   Is there a short hand method of writing this in Python, or do I just stick with the long version?    Thank you for all the suggestions and explanation of the rationale for Python's syntax.  After finding the following on the numpy website, I think I have found a solution I like:  http://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays  Applying the information from that website to my problem above, would give the following:  >>> from numpy import array >>> a = array([1, 2, 3, 1, 2, 3]) >>> b = a>2  array([False, False, True, False, False, True], dtype=bool) >>> r = array(range(len(b))) >>> r(b) [2, 5]   The following should then work (but I haven't got a Python interpreter on hand to test it):  class my_array(numpy.array):     def find(self, b):         r = array(range(len(b)))         return r(b)   >>> a = my_array([1, 2, 3, 1, 2, 3]) >>> a.find(a>2) [2, 5]      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Finding the index of elements based on a condition using python list comprehension",
        "A_Content": "  Even if it's a late answer: I think this is still a very good question and IMHO Python (without additional libraries or toolkits like numpy) is still lacking a convenient method to access the indices of list elements according to a manually defined filter.  You could manually define a function, which provides that functionality:  def indices(list, filtr=lambda x: bool(x)):     return [i for i,x in enumerate(list) if filtr(x)]  print(indices([1,0,3,5,1], lambda x: x==1))   Yields: [0, 4]  In my imagination the perfect way would be making a child class of list and adding the indices function as class method. In this way only the filter method would be needed:  class MyList(list):     def __init__(self, *args):         list.__init__(self, *args)     def indices(self, filtr=lambda x: bool(x)):         return [i for i,x in enumerate(self) if filtr(x)]  my_list = MyList([1,0,3,5,1]) my_list.indices(lambda x: x==1)   I elaborated a bit more on that topic here: http://tinyurl.com/jajrr87     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/7270321/finding-the-index-of-elements-based-on-a-condition-using-python-list-comprehensi",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    The following Python code appears to be very long winded when coming from a Matlab background  >>> a = [1, 2, 3, 1, 2, 3] >>> [index for index,value in enumerate(a) if value > 2] [2, 5]   When in Matlab I can write:  >> a = [1, 2, 3, 1, 2, 3]; >> find(a>2) ans =      3     6   Is there a short hand method of writing this in Python, or do I just stick with the long version?    Thank you for all the suggestions and explanation of the rationale for Python's syntax.  After finding the following on the numpy website, I think I have found a solution I like:  http://docs.scipy.org/doc/numpy/user/basics.indexing.html#boolean-or-mask-index-arrays  Applying the information from that website to my problem above, would give the following:  >>> from numpy import array >>> a = array([1, 2, 3, 1, 2, 3]) >>> b = a>2  array([False, False, True, False, False, True], dtype=bool) >>> r = array(range(len(b))) >>> r(b) [2, 5]   The following should then work (but I haven't got a Python interpreter on hand to test it):  class my_array(numpy.array):     def find(self, b):         r = array(range(len(b)))         return r(b)   >>> a = my_array([1, 2, 3, 1, 2, 3]) >>> a.find(a>2) [2, 5]      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  Give the full path of image with forward slash. It solved the error for me.  E.g.  import numpy as np import cv2  img = cv2.imread('C:/Python34/images/2015-05-27-191152.jpg') gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   Also, if you give 0 in second parameter while loading image using cv2.imread than no need to convert image using cvtColor, it is already loaded as grayscale image eg.  import numpy as np import cv2  gray = cv2.imread('C:/Python34/images/2015-05-27-191152.jpg',0)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "82",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  Please Set As Below  img = cv2.imread('2015-05-27-191152.jpg',1)     // Change Flag As 1 For Color Image                                                 //or O for Gray Image So It image is                                                  //already gray      ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  img = cv2.imread('2015-05-27-191152.jpg',0)   The above line of code reads your image in grayscale color model, because of the 0 appended at the end. And if you again try to convert an already gray image to gray image it will show that error.  So either use above style or try undermentioned code:  img = cv2.imread('2015-05-27-191152.jpg') gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  Only pass name of the image, no need of 0:  img=cv2.imread('sample.jpg')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  First thing you should check is that whether the image exists in the root directory or not. This is mostly due to image with height = 0. Which means that cv2.imread(imageName) is not reading the image.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  I've had this error message show up, for completely unrelated reasons to the flags 0 or 1 mentionned in the other answers. You might be seeing it too because cv2.imread will not error out if the path string you pass it is not an image:  In [1]: import cv2    ...: img = cv2.imread('asdfasdf')  # This is clearly not an image file    ...: gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    ...:  OpenCV Error: Assertion failed (scn == 3 || scn == 4) in cv::cvtColor, file C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp, line 10638 --------------------------------------------------------------------------- error                                     Traceback (most recent call last) <ipython-input-4-19408d38116b> in <module>()       1 import cv2       2 img = cv2.imread('asdfasdf')  # This is clearly not an image file ----> 3 gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  error: C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:10638: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor   So you're seeing a cvtColor failure when it's in fact a silent imread error. Keep that in mind next time you go wasting an hour of your life with that cryptic metaphor.  Solution  You might need to check that your path string represents a valid file before passing it to cv2.imread:    import os   def read_img(path):     \"\"\"Given a path to an image file, returns a cv2 array      str -> np.ndarray\"\"\"     if os.path.isfile(path):         return cv2.imread(path)     else:         raise ValueError('Path provided is not a valid file: {}'.format(path))   path = '2015-05-27-191152.jpg' img = read_img(path) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   Written this way, your code will fail gracefully.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  This answer if for the people experiencing the same problem trying to accessing the camera.   import numpy as np import cv2  cap = cv2.VideoCapture(0)  while(True):     # Capture frame-by-frame     ret, frame = cap.read()      # Our operations on the frame come here     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)     # Display the resulting frame     cv2.imshow('frame',gray)     if cv2.waitKey(1) & 0xFF == ord('q'):         break  # When everything done, release the capture cap.release() cv2.destroyAllWindows()   Using Linux:  if you are trying to access the camera from your computer most likely there is a permission issue, try running the python script with sudo it should fix it.  sudo python python_script.py   To test if the camera is accessible run the following command.   ffmpeg -f v4l2 -framerate 25 -video_size 640x480 -i /dev/video0 output.mkv       ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  This code  for those who are experiencing the same problem trying to accessing the camera could be written with a safety check.  if ret is True:    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  else:    continue  OR  in case you want to close the camera/ discontinue  if there will be some problem with the frame itself   if ret is True:    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  else:    break  For reference https://github.com/HackerShackOfficial/AI-Smart-Mirror/issues/36     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   The above error is the result of an invalid image name or if the file does not exists in the local directory.  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   Also if you are using the 2nd argument of cv2.imread() as '0', then it is already converted into a grayscaled image.  The difference between converting the image by passing 0 as parameter and applying the following:   img = cv2.cvtCOLOR(img, cv2.COLOR_BGR2GRAY)    is that, in the case img = cv2.cvtCOLOR(img, cv2.COLOR_BGR2GRAY), the images are 8uC1 and 32sC1 type images.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  Here is what i observed when I used my own image sets in .jpg format. In the sample script available in Opencv doc, note that it has the undistort and crop the image lines as below:  # undistort dst = cv2.undistort(img, mtx, dist, None, newcameramtx)  # crop the image x,y,w,h = roi dst = dst[y:y+h, x:x+w] cv2.imwrite('calibresult.jpg',dst)   So, when we run the code for the first time, it executes the line cv2.imwrite('calibresult.jpg',dst) saving a image calibresult.jpg in the current directory. So, when I ran the code for the next time, along with my sample image sets that I used for calibrating the camera in jpg format, the code also tried to consider this newly added image calibresult.jpg due to which the error popped out  error: C:\\builds\\master_PackSlaveAddon-win64-vc12-static\\opencv\\modules\\imgproc\\src\\color.cpp:7456: error: (-215) scn == 3 || scn == 4 in function cv::ipp_cvtColor   What I did was: I simply deleted that newly generated image after each run or alternatively changed the type of the image to say png or tiff type. That solved the problem. Check if you are inputting and writing calibresult of the same type. If so, just change the type.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  On OS X I realised, that while cv2.imread can deal with \"filename.jpg\" it can not process \"file.name.jpg\". Being a novice to python, I can't yet propose a solution, but as Franois Leblanc wrote, it is more of a silent imread error.  So it has a problem with an additional dot in the filename and propabely other signs as well, as with \" \" (Space) or \"%\" and so on.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  i think it because cv2.imread cannot read .jpg picture, you need to change .jpg to.png`.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  I also found if your webcam didnt close right or something is using it, then CV2 will give this same error. I had to restart my pc to get it to work again.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  The simplest solution for removing that error was running the command   cap.release() cv2.closeAllWindows()   That worked for me and also sometimes restarting the kernel was required because of old processes running in the background.  If the image isn't in the working directory then also it wont be working for that try to place the image file in pwd in the same folder as there is code else provide the full path to the image file or folder.  To avoid this problem in future try to code with exceptional handling  so that if incorrect termination happens for some random reason the capturing device would get released after the program gets over.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "open cv error: (-215) scn == 3 || scn == 4 in function cvtColor",
        "A_Content": "  2015-05-27-191152.jpg << Looking back at your image format, I occasionally confused between .png and .jpg and encountered the same error.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "opencv",
            "photo"
        ],
        "URL": "https://stackoverflow.com/questions/30506126/open-cv-error-215-scn-3-scn-4-in-function-cvtcolor",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently in Ubuntu 14.04, using python 2.7 and cv2.   When I run this code:   import numpy as np import cv2  img = cv2.imread('2015-05-27-191152.jpg',0) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   it returns:   File \"face_detection.py\", line 11, in <module>     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.error: /home/arthurckl/Desktop/opencv-3.0.0-rc1/modules/imgproc/src/color.cpp:7564: error: (-215) scn == 3 || scn == 4 in function cvtColor   I already searched here and one answer said that I could be loading my photo the wrong way, because it should have 3 dimensions: rows, columns and depth.  When I print the img.shape it returns only two numbers, so I must be doing it wrong. But I don't know the right way to load my photo.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Where should virtualenvs be created?",
        "A_Content": "  Many people use the virtualenvwrapper tool, which keeps all virtualenvs in the same place (the ~/.virtualenvs directory) and allows shortcuts for creating and keeping them there. For example, you might do:  mkvirtualenv djangoproject   and then later:  workon djangoproject   It's probably a bad idea to keep the virtualenv directory in the project itself, since you don't want to distribute it (it might be specific to your computer or operating system). Instead, keep a requirements.txt file using pip:  pip freeze > requirements.txt   and distribute that. This will allow others using your project to reinstall all the same requirements into their virtualenv with:  pip install -r requirements.txt      ",
        "Language": "Python",
        "Tags": [
            "python",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/12184846/where-should-virtualenvs-be-created",
        "A_Votes": "93",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm confused as to where I should put my virtualenvs.  With my first django project, I created the project with the command, \"django-admin.py startproject djangoproject\".  I then cd'd into the djangoproject directory and ran the command, \"virtualenv env\" which created the virtual environment directory at the same level as the inner \"djangoproject\" directory.  Is this the wrong place in which to create the virtualenv for this particular project?  I'm getting the impression that most people keep all their virtualenvs together in an entirely different directory, e.g. ~/virtualenvs, and then use virtualenvwrapper to switch back and forth between them.  Is there a correct way to do this?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Where should virtualenvs be created?",
        "A_Content": "  Changing the location of the virtualenv directory breaks it  This is a major advantage of putting the directory outside of the repository tree, e.g. under ~/.virtualenvs with virutalenvwrapper.  Otherwise, if you keep it in the project tree, moving the project location will break the virtualenv.  See: Renaming a virtualenv folder without breaking it  There is --relocatable but it is known to not be perfect.  Another minor advantage: you don't have to .gitignore it.  If it weren't for that, I'd just leave my virtualenvs gitignored in the project tree itself to keep related stuff close together.  This is fine since you you will likely never reuse a given virtualenv across projects.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/12184846/where-should-virtualenvs-be-created",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm confused as to where I should put my virtualenvs.  With my first django project, I created the project with the command, \"django-admin.py startproject djangoproject\".  I then cd'd into the djangoproject directory and ran the command, \"virtualenv env\" which created the virtual environment directory at the same level as the inner \"djangoproject\" directory.  Is this the wrong place in which to create the virtualenv for this particular project?  I'm getting the impression that most people keep all their virtualenvs together in an entirely different directory, e.g. ~/virtualenvs, and then use virtualenvwrapper to switch back and forth between them.  Is there a correct way to do this?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Where should virtualenvs be created?",
        "A_Content": "  The generally accepted place to put them is the same place that the default installation of virtualenvwrapper puts them: ~/.virtualenvs  Related: virtualenvwrapper is an excellent tool that provides shorthands for the common virtualenv commands. http://www.doughellmann.com/projects/virtualenvwrapper/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/12184846/where-should-virtualenvs-be-created",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm confused as to where I should put my virtualenvs.  With my first django project, I created the project with the command, \"django-admin.py startproject djangoproject\".  I then cd'd into the djangoproject directory and ran the command, \"virtualenv env\" which created the virtual environment directory at the same level as the inner \"djangoproject\" directory.  Is this the wrong place in which to create the virtualenv for this particular project?  I'm getting the impression that most people keep all their virtualenvs together in an entirely different directory, e.g. ~/virtualenvs, and then use virtualenvwrapper to switch back and forth between them.  Is there a correct way to do this?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Where should virtualenvs be created?",
        "A_Content": "  If you use pyenv install Python, then pyenv-virtualenv will be a best practice. If set .python-version file, it can auto activate or deactivate virtual env when you change work folder. Pyenv-virtualenv also put all virtual env into $HOME/.pyenv/versions folder.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/12184846/where-should-virtualenvs-be-created",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm confused as to where I should put my virtualenvs.  With my first django project, I created the project with the command, \"django-admin.py startproject djangoproject\".  I then cd'd into the djangoproject directory and ran the command, \"virtualenv env\" which created the virtual environment directory at the same level as the inner \"djangoproject\" directory.  Is this the wrong place in which to create the virtualenv for this particular project?  I'm getting the impression that most people keep all their virtualenvs together in an entirely different directory, e.g. ~/virtualenvs, and then use virtualenvwrapper to switch back and forth between them.  Is there a correct way to do this?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  This works for me:  Receive  import socket import struct  MCAST_GRP = '224.1.1.1' MCAST_PORT = 5007  sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP) sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) sock.bind(('', MCAST_PORT))  # use MCAST_GRP instead of '' to listen only                              # to MCAST_GRP, not all groups on MCAST_PORT mreq = struct.pack(\"4sl\", socket.inet_aton(MCAST_GRP), socket.INADDR_ANY)  sock.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)  while True:   print sock.recv(10240)   Send  import socket  MCAST_GRP = '224.1.1.1' MCAST_PORT = 5007  sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP) sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 2) sock.sendto(\"robot\", (MCAST_GRP, MCAST_PORT))   It is based off the examples from http://wiki.python.org/moin/UdpCommunication which didn't work.  My system is... Linux 2.6.31-15-generic #50-Ubuntu SMP Tue Nov 10 14:54:29 UTC 2009 i686 GNU/Linux Python 2.6.4     ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "83",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  Multicast sender that broadcasts to a multicast group:  #!/usr/bin/env python  import socket import struct  def main():   MCAST_GRP = '224.1.1.1'   MCAST_PORT = 5007   sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)   sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 32)   sock.sendto('Hello World!', (MCAST_GRP, MCAST_PORT))  if __name__ == '__main__':   main()   Multicast receiver that reads from a multicast group and prints hex data to the console:  #!/usr/bin/env python  import socket import binascii  def main():   MCAST_GRP = '224.1.1.1'    MCAST_PORT = 5007   sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)   try:     sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)   except AttributeError:     pass   sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, 32)    sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_LOOP, 1)    sock.bind((MCAST_GRP, MCAST_PORT))   host = socket.gethostbyname(socket.gethostname())   sock.setsockopt(socket.SOL_IP, socket.IP_MULTICAST_IF, socket.inet_aton(host))   sock.setsockopt(socket.SOL_IP, socket.IP_ADD_MEMBERSHIP,                     socket.inet_aton(MCAST_GRP) + socket.inet_aton(host))    while 1:     try:       data, addr = sock.recvfrom(1024)     except socket.error, e:       print 'Expection'       hexdata = binascii.hexlify(data)       print 'Data = %s' % hexdata  if __name__ == '__main__':   main()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  better use:  sock.bind((MCAST_GRP, MCAST_PORT))   instead of:  sock.bind(('', MCAST_PORT))   ... because if you want to listen to multiple mcast groups on the same port, you'll get all messages on all listeners     ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  In order to Join multicast group Python uses native OS socket interface. Due to portability and stability of Python environment many of socket options are directly forwarded to native socket setsockopt call. Multicast mode of operation such as joining and dropping group membership can be accomplished by setsockopt only.  Basic program for receiving multicast IP packet can look like:  from socket import *  multicast_port  = 55555 multicast_group = \"224.1.1.1\" interface_ip    = \"10.11.1.43\"  s = socket(AF_INET, SOCK_DGRAM ) s.bind((\"\", multicast_port )) mreq = inet_aton(multicast_group) + inet_aton(interface_ip) s.setsockopt(IPPROTO_IP, IP_ADD_MEMBERSHIP, str(mreq))  while 1:     print s.recv(1500)   Firstly it creates socket, binds it and triggers triggers multicast group joining by issuing setsockopt. At very end it receives packets forever.   Sending multicast IP frames is straight forward. If you have single NIC in your system sending such packets does not differ from usual UDP frames sending. All you need to take care of is just set correct destination IP address in sendto() method.   I noticed that lot of examples around Internet works by accident in fact. Even on official python documentation. Issue for all of them are using struct.pack incorrectly. Please be advised that typical example uses 4sl as format and it is not aligned with actual OS socket interface structure.  I will try to describe what happen underneath the hood when exercising setsockopt call for python socket object.   Python forwards setsockopt method call to native C socket interface. Linux socket documentation (see man 7 ip) introduces two forms of ip_mreqn structure for IP_ADD_MEMBERSHIP option. Shortest is form is 8 bytes long and longer is 12 bytes long. Above example generates 8 byte setsockopt call where fist for bytes defines multicast_group and second interface_ip.       ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  Have a look at py-multicast. Network module can check if an interface supports multicast (on Linux at least).   import multicast from multicast import network  receiver = multicast.MulticastUDPReceiver (\"eth0\", \"238.0.0.1\", 1234 ) data = receiver.read() receiver.close()  config = network.ifconfig() print config['eth0'].addresses # ['10.0.0.1'] print config['eth0'].multicast #True - eth0 supports multicast print config['eth0'].up #True - eth0 is up   Perhaps problems with not seeing IGMP, were caused by an interface not supporting multicast?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  There is a framework to do this from http://twistedmatrix.com/trac/. Here is the example https://twistedmatrix.com/documents/12.2.0/core/howto/udp.html     ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  To make the client code (from tolomea) work on Solaris you need to pass the ttl value for the IP_MULTICAST_TTL socket option as an unsigned char. Otherwise you will get an error.  This worked for me on Solaris 10 and 11:   import socket import struct  MCAST_GRP = '224.1.1.1' MCAST_PORT = 5007 ttl = struct.pack('B', 2)  sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP) sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ttl) sock.sendto(\"robot\", (MCAST_GRP, MCAST_PORT))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  tolomea's answer worked for me. I hacked it into socketserver.UDPServer too:  class ThreadedMulticastServer(socketserver.ThreadingMixIn, socketserver.UDPServer):     def __init__(self, *args):         super().__init__(*args)         self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)         self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)         self.socket.bind((MCAST_GRP, MCAST_PORT))         mreq = struct.pack('4sl', socket.inet_aton(MCAST_GRP), socket.INADDR_ANY)         self.socket.setsockopt(socket.IPPROTO_IP, socket.IP_ADD_MEMBERSHIP, mreq)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Multicast in Python",
        "A_Content": "  Multicast traffic is no different than regular UDP except for the IP address. Take a look at the standard socket library. You may be able to find something that builds on socket and is easier to use.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "multicast"
        ],
        "URL": "https://stackoverflow.com/questions/603852/multicast-in-python",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you send and receive UDP Multicast in Python?  Is there a standard library to do so?       ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - manually install package using virtualenv",
        "A_Content": "  I typically would extract the program to a temporary folder, then from that folder, run the setup.py using the direct path to the virtualenv python instance.  eg if your virtualenv is in /home/username/virtualpy, use this (from your temporary folder)  /home/username/virtualpy/bin/python setup.py install   This should install it to your virtualenv site package folder.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "installation",
            "virtualenv",
            "pip",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/5979513/python-manually-install-package-using-virtualenv",
        "A_Votes": "105",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a python program I want to install into my virtualenv - it's a zip package that I need to unzip and then run a setup.py program - but my question is more regarding how to get these unzipped files into my virtualenv so that the package gets installed into the virtualenv's site-packages folder?  I can also install from inside my virtualenv using pip install <package name>, but for some reason, the package that PIP downloads is out of date.  So - can someone tell me a few easy steps for installing a package manually?  So far I have the basic commands to load up the Virtualenv:  -bash-3.2$ source ~/.bashrc -bash-3.2$ workon test (test)-bash-3.2$ //Now I'm working on my virtualenv, but where do I go after this??   So - does it matter where I unzip the python package/program to - or should I be logged in to the virtualenv first before unzipping? After I load up the virtualenv and I'm inside using it with the 'workon test' command, will any python package I install, regardless of the directory I find it, install itself into the proper virtualenv's site-packages folder?  Option 1 is to unzip the python program into /home/username/tmp - then log into my virtualenv, navigate to that folder and run the setup.py program - assuming that the virtualenv will transfer all relevant files to it's own site-packages folder.  OR scenario 2 is to unzip the files directly into site-packages, and run it from there (after logging in to the virtualenv), etc  Thank you for helping a Python clutz with this!      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - manually install package using virtualenv",
        "A_Content": "  well when you switch to the virtual environment. you should type   which python   and if it returns the path where your virtual environment exists then its okay you can directly run this command.  $ python setup.py build $ python setup.py install   but if it gives the global level path which is not your virtualenv's path then you should try using  $ ~/.virtualenv/python-env/bin/python setup.py build $ ~/.virtualenv/python-env/bin/python setup.py install      ",
        "Language": "Python",
        "Tags": [
            "python",
            "installation",
            "virtualenv",
            "pip",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/5979513/python-manually-install-package-using-virtualenv",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a python program I want to install into my virtualenv - it's a zip package that I need to unzip and then run a setup.py program - but my question is more regarding how to get these unzipped files into my virtualenv so that the package gets installed into the virtualenv's site-packages folder?  I can also install from inside my virtualenv using pip install <package name>, but for some reason, the package that PIP downloads is out of date.  So - can someone tell me a few easy steps for installing a package manually?  So far I have the basic commands to load up the Virtualenv:  -bash-3.2$ source ~/.bashrc -bash-3.2$ workon test (test)-bash-3.2$ //Now I'm working on my virtualenv, but where do I go after this??   So - does it matter where I unzip the python package/program to - or should I be logged in to the virtualenv first before unzipping? After I load up the virtualenv and I'm inside using it with the 'workon test' command, will any python package I install, regardless of the directory I find it, install itself into the proper virtualenv's site-packages folder?  Option 1 is to unzip the python program into /home/username/tmp - then log into my virtualenv, navigate to that folder and run the setup.py program - assuming that the virtualenv will transfer all relevant files to it's own site-packages folder.  OR scenario 2 is to unzip the files directly into site-packages, and run it from there (after logging in to the virtualenv), etc  Thank you for helping a Python clutz with this!      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get __name__ of calling function's module in Python",
        "A_Content": "  Check out the inspect module:  inspect.stack() will return the stack information.  Inside a function, inspect.stack()[1] will return your caller's stack.  From there, you can get more information about the caller's function name, module, etc.  See the docs for details:  http://docs.python.org/library/inspect.html  Also, Doug Hellmann has a nice writeup of the inspect module in his PyMOTW series:  http://pymotw.com/2/inspect/index.html#module-inspect  EDIT: Here's some code which does what you want, I think:  def info(msg):     frm = inspect.stack()[1]     mod = inspect.getmodule(frm[0])     print '[%s] %s' % (mod.__name__, msg)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "stack-trace",
            "introspection"
        ],
        "URL": "https://stackoverflow.com/questions/1095543/get-name-of-calling-functions-module-in-python",
        "A_Votes": "95",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Suppose myapp/foo.py contains:  def info(msg):     caller_name = ????     print '[%s] %s' % (caller_name, msg)   And myapp/bar.py contains:  import foo foo.info('Hello') # => [myapp.bar] Hello   I want caller_name to be set to the __name__ attribute of the calling functions' module (which is 'myapp.foo') in this case. How can this be done?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get __name__ of calling function's module in Python",
        "A_Content": "  Confronted with a similar problem, I have found that sys._current_frames() from the sys module contains interesting information that can help you, without the need to import inspect, at least in specific use cases.  >>> sys._current_frames() {4052: <frame object at 0x03200C98>}   You can then \"move up\" using f_back :  >>> f = sys._current_frames().values()[0] >>> # for python3: f = list(sys._current_frames().values())[0]  >>> print f.f_back.f_globals['__file__'] '/base/data/home/apps/apricot/1.6456165165151/caller.py'  >>> print f.f_back.f_globals['__name__'] '__main__'   For the filename you can also use f.f_back.f_code.co_filename, as suggested by Mark Roddy above. I am not sure of the limits and caveats of this method (multiple threads will most likely be a problem) but I intend to use it in my case.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "stack-trace",
            "introspection"
        ],
        "URL": "https://stackoverflow.com/questions/1095543/get-name-of-calling-functions-module-in-python",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Suppose myapp/foo.py contains:  def info(msg):     caller_name = ????     print '[%s] %s' % (caller_name, msg)   And myapp/bar.py contains:  import foo foo.info('Hello') # => [myapp.bar] Hello   I want caller_name to be set to the __name__ attribute of the calling functions' module (which is 'myapp.foo') in this case. How can this be done?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Get __name__ of calling function's module in Python",
        "A_Content": "  I don't recommend do this, but you can accomplish your goal with the following method:  def caller_name():     frame=inspect.currentframe()     frame=frame.f_back.f_back     code=frame.f_code     return code.co_filename   Then update your existing method as follows:  def info(msg):     caller = caller_name()     print '[%s] %s' % (caller, msg)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "stack-trace",
            "introspection"
        ],
        "URL": "https://stackoverflow.com/questions/1095543/get-name-of-calling-functions-module-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Suppose myapp/foo.py contains:  def info(msg):     caller_name = ????     print '[%s] %s' % (caller_name, msg)   And myapp/bar.py contains:  import foo foo.info('Hello') # => [myapp.bar] Hello   I want caller_name to be set to the __name__ attribute of the calling functions' module (which is 'myapp.foo') in this case. How can this be done?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Convert generator object to list for debugging [duplicate]",
        "A_Content": "  Simply call list on the generator.  lst = list(gen) lst   Be aware that this affects the generator which will not return any further items.  You also cannot directly call list in IPython, as it conflicts with a command for listing lines of code.  Tested on this file:  def gen():     yield 1     yield 2     yield 3     yield 4     yield 5 import ipdb ipdb.set_trace()  g1 = gen()  text = \"aha\" + \"bebe\"  mylst = range(10, 20)   which when run:  $ python code.py  > /home/javl/sandbox/so/debug/code.py(10)<module>()       9  ---> 10 g1 = gen()      11   ipdb> n > /home/javl/sandbox/so/debug/code.py(12)<module>()      11  ---> 12 text = \"aha\" + \"bebe\"      13   ipdb> lst = list(g1) ipdb> lst [1, 2, 3, 4, 5] ipdb> q Exiting Debugger.   General method for escaping function/variable/debugger name conflicts  There are debugger commands p and pp that will print and prettyprint any expression following them.  So you could use it as follows:  $ python code.py  > /home/javl/sandbox/so/debug/code.py(10)<module>()       9  ---> 10 g1 = gen()      11   ipdb> n > /home/javl/sandbox/so/debug/code.py(12)<module>()      11  ---> 12 text = \"aha\" + \"bebe\"      13   ipdb> p list(g1) [1, 2, 3, 4, 5] ipdb> c   There is also an exec command, called by prefixing your expression with !, which forces debugger to take your expression as Python one.  ipdb> !list(g1) []   For more details see help p, help pp and help exec when in debugger.  ipdb> help exec (!) statement Execute the (one-line) statement in the context of the current stack frame. The exclamation point can be omitted unless the first word of the statement resembles a debugger command. To assign to a global variable you must always prefix the command with a 'global' command, e.g.: (Pdb) global list_options; list_options = ['-l']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "generator",
            "ipdb"
        ],
        "URL": "https://stackoverflow.com/questions/24130745/convert-generator-object-to-list-for-debugging",
        "A_Votes": "115",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              Fastest way to convert an iterator to a list                                        1 answer                                          When I'm debugging in Python using IPython, I sometimes hit a break-point and I want to examine a variable that is currently a generator. The simplest way I can think of doing this is converting it to a list, but I'm not clear on what's an easy way of doing this in one line in ipdb, since I'm so new to Python.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python how to write to a binary file?",
        "A_Content": "  This is exactly what bytearray is for:  newFileByteArray = bytearray(newFileBytes) newFile.write(newFileByteArray)   If you're using Python 3.x, you can use bytes instead (and probably ought to, as it signals your intention better). But in Python 2.x, that won't work, because bytes is just an alias for str. As usual, showing with the interactive interpreter is easier than explaining with text, so let me just do that.  Python 3.x:  >>> bytearray(newFileBytes) bytearray(b'{\\x03\\xff\\x00d') >>> bytes(newFileBytes) b'{\\x03\\xff\\x00d'   Python 2.x:  >>> bytearray(newFileBytes) bytearray(b'{\\x03\\xff\\x00d') >>> bytes(newFileBytes) '[123, 3, 255, 0, 100]'      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/18367007/python-how-to-write-to-a-binary-file",
        "A_Votes": "80",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a list of bytes as integers, which is something like  [120, 3, 255, 0, 100]   How can I write this list to a file as binary?  Would this work?  newFileBytes = [123, 3, 255, 0, 100] # make file newFile = open(\"filename.txt\", \"wb\") # write to file newFile.write(newFileBytes)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python how to write to a binary file?",
        "A_Content": "  Use struct.pack to convert the integer values into binary bytes, then write the bytes. E.g.  newFile.write(struct.pack('5B', *newFileBytes))   However I would never give a binary file a .txt extension.  The benefit of this method is that it works for other types as well, for example if any of the values were greater than 255 you could use '5i' for the format instead to get full 32-bit integers.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/18367007/python-how-to-write-to-a-binary-file",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of bytes as integers, which is something like  [120, 3, 255, 0, 100]   How can I write this list to a file as binary?  Would this work?  newFileBytes = [123, 3, 255, 0, 100] # make file newFile = open(\"filename.txt\", \"wb\") # write to file newFile.write(newFileBytes)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python how to write to a binary file?",
        "A_Content": "  To convert from integers < 256 to binary, use the chr function.  So you're looking at doing the following.  newFileBytes=[123,3,255,0,100] newfile=open(path,'wb') newfile.write((''.join(chr(i) for i in newFileBytes)).encode('ascii'))      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/18367007/python-how-to-write-to-a-binary-file",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of bytes as integers, which is something like  [120, 3, 255, 0, 100]   How can I write this list to a file as binary?  Would this work?  newFileBytes = [123, 3, 255, 0, 100] # make file newFile = open(\"filename.txt\", \"wb\") # write to file newFile.write(newFileBytes)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python how to write to a binary file?",
        "A_Content": "  You can use the following code example using Python 3 syntax:  from struct import pack with open(\"foo.bin\", \"wb\") as file:   file.write(pack(\"<IIIII\", *bytearray([120, 3, 255, 0, 100])))   Here is shell one-liner:  python -c $'from struct import pack\\nwith open(\"foo.bin\", \"wb\") as file: file.write(pack(\"<IIIII\", *bytearray([120, 3, 255, 0, 100])))'      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/18367007/python-how-to-write-to-a-binary-file",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of bytes as integers, which is something like  [120, 3, 255, 0, 100]   How can I write this list to a file as binary?  Would this work?  newFileBytes = [123, 3, 255, 0, 100] # make file newFile = open(\"filename.txt\", \"wb\") # write to file newFile.write(newFileBytes)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python how to write to a binary file?",
        "A_Content": "  As of Python 3.2+, you can also accomplish this using the to_bytes native int method:  newFileBytes = [123, 3, 255, 0, 100] # make file newFile = open(\"filename.txt\", \"wb\") # write to file for byte in newFileBytes:     newFile.write(byte.to_bytes(1, byteorder='big'))   I.e., each single call to to_bytes in this case creates a string of length 1, with its characters arranged in big-endian order (which is trivial for length-1 strings), which represents the integer value byte. You can also shorten the last two lines into a single one:  newFile.write(''.join([byte.to_bytes(1, byteorder='big') for byte in newFileBytes]))      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/18367007/python-how-to-write-to-a-binary-file",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of bytes as integers, which is something like  [120, 3, 255, 0, 100]   How can I write this list to a file as binary?  Would this work?  newFileBytes = [123, 3, 255, 0, 100] # make file newFile = open(\"filename.txt\", \"wb\") # write to file newFile.write(newFileBytes)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python how to write to a binary file?",
        "A_Content": "  Use pickle, like this: import pickle  Your code would look like this:  import pickle mybytes = [120, 3, 255, 0, 100] with open(\"bytesfile\", \"wb\") as mypicklefile:     pickle.dump(mybytes, mypicklefile)   To read the data back, use the pickle.load method     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/18367007/python-how-to-write-to-a-binary-file",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of bytes as integers, which is something like  [120, 3, 255, 0, 100]   How can I write this list to a file as binary?  Would this work?  newFileBytes = [123, 3, 255, 0, 100] # make file newFile = open(\"filename.txt\", \"wb\") # write to file newFile.write(newFileBytes)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I install an old version of Django on virtualenv?",
        "A_Content": "  There was never a Django 1.0.7. The 1.0 series only went up to 1.0.4. You can see all the releases in the tags section of the Django code repository.  However to answer your question, don't use easy_install, use pip. (If it's not already installed, do easy_install pip, then never touch easy_install again). Now you can do:  pip install Django==1.0.4      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "setuptools",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/3220280/how-do-i-install-an-old-version-of-django-on-virtualenv",
        "A_Votes": "117",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    This may sound like a stupid question, since the very purpose of virtualenv is to this exactly: Installing some specific version of a package (in this case Django) inside the virtual environment. But it's exactly what I want to do, and I can't figure it out.  I'm on Windows XP, and I created the virtual environment successfully, and I'm able to run it, but how am I supposed to install the Django version I want into it? I mean, I know to use the newly-created easy_install script, but how do I make it install Django 1.0.7? If I do easy_install django, it will install the latest version. I tried putting the version number 1.0.7 into this command in various ways, but nothing worked.  How do I do this?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I install an old version of Django on virtualenv?",
        "A_Content": "  +1 on the previous poster's reply: use pip if you can. But, in a pinch, the easiest way is to install an older version would be to download the tarball from the downloads page or, if you have subversion installed, do an svn export of the release you want (they are all tagged here).   Once you have the version of Django you want, just run the following command inside the django directory:  python setup.py install   This will install that version of Django in your virtualenv.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "setuptools",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/3220280/how-do-i-install-an-old-version-of-django-on-virtualenv",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may sound like a stupid question, since the very purpose of virtualenv is to this exactly: Installing some specific version of a package (in this case Django) inside the virtual environment. But it's exactly what I want to do, and I can't figure it out.  I'm on Windows XP, and I created the virtual environment successfully, and I'm able to run it, but how am I supposed to install the Django version I want into it? I mean, I know to use the newly-created easy_install script, but how do I make it install Django 1.0.7? If I do easy_install django, it will install the latest version. I tried putting the version number 1.0.7 into this command in various ways, but nothing worked.  How do I do this?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I install an old version of Django on virtualenv?",
        "A_Content": "  +1 for already mentioned solutions.  I just wanna add another solution.  To install a specific version of Django (say 1.10.x),    Clone the Django repo from Github.  git clone https://github.com/django/django.git Go into the directory and checkout to the specific branch.  cd django  git checkout origin/stable/1.10.x Run install command.   python setup.py install      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "setuptools",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/3220280/how-do-i-install-an-old-version-of-django-on-virtualenv",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may sound like a stupid question, since the very purpose of virtualenv is to this exactly: Installing some specific version of a package (in this case Django) inside the virtual environment. But it's exactly what I want to do, and I can't figure it out.  I'm on Windows XP, and I created the virtual environment successfully, and I'm able to run it, but how am I supposed to install the Django version I want into it? I mean, I know to use the newly-created easy_install script, but how do I make it install Django 1.0.7? If I do easy_install django, it will install the latest version. I tried putting the version number 1.0.7 into this command in various ways, but nothing worked.  How do I do this?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Why does ActivePython exist?",
        "A_Content": "  It's a packaging, or \"distribution\", of Python, with some extras -- not (anywhere) quite as \"Sumo\" as Enthought's HUGE distro of \"Python plus everything\", but still in a similar vein (and it first appeared much earlier).  I don't think you're missing anything in particular, except perhaps the fact that David Ascher (Python enthusiast and my coauthor in the Python Cookbook) used to be CTO at ActiveState (and so no doubt internally pushed Python to go with other dynamic languages ActiveState focuses on) but he's gone now (he's CEO at the Mozilla-owned firm that deals with email and similar forms of communication -- ThunderBird and the like, in terms of programs).  No doubt some firms prefer to purchase a distribution with commercially available support contracts, like ActivePython, just because that's the way some purchasing departments in several enterprises (and/or their IT depts) are used to work. Unless you care about such issues, I don't think you're missing anything by giving ActiveState's Python distro a pass;-).  [[I feel similarly about costly Enterprise distros of Linux, vs. Debian or Ubuntu or the like -- but then I'm not in Purchasing, nor in an IT department, nor do I work for a very traditional enterprise anyway;-)]]     ",
        "Language": "Python",
        "Tags": [
            "python",
            "activestate",
            "activepython"
        ],
        "URL": "https://stackoverflow.com/questions/1352528/why-does-activepython-exist",
        "A_Votes": "46",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    What's ActivePython actually about?   From what I've read it's just standard Python with openssl and pyWin32 (on Win). No big deal I guess, I could install them in matter of minutes, and most people don't need them anyway. All other mentioned libraries (zlib, bzip2, sqlite3, Tkinter, ElementTree, ctypes, multiprocessing) are part of core Python distribution.   Next up, the tag-line \"ActivePython is the industry-standard Python distribution\", isn't core Python distribution \"industry-standard\" (whatever that means?)?   And the weirdest thing, is that ActiveState bundles it with crappy PythonWin, and not their own most-awesome Python editor/IDE, Komodo. What gives?  I actually never got to installing ActivePython, so maybe I don't know something, but it seems pretty irrelevant, and I see the name quite often on forums or here.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Why does ActivePython exist?",
        "A_Content": "  ActiveState has a long tradition contributing Windows support to Python, Tcl, and Perl: by hiring key developers (like Mark Hammond, for some time), by fixing bugs specific to Windows, and having employees contribute fixes back, and by being sponsors of the Python Software Foundation.  While it is true that the distribution they produce is fairly similar to mine, it's as RichieHindle says: you can get paid support from ActiveState (but not from me).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "activestate",
            "activepython"
        ],
        "URL": "https://stackoverflow.com/questions/1352528/why-does-activepython-exist",
        "A_Votes": "32",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's ActivePython actually about?   From what I've read it's just standard Python with openssl and pyWin32 (on Win). No big deal I guess, I could install them in matter of minutes, and most people don't need them anyway. All other mentioned libraries (zlib, bzip2, sqlite3, Tkinter, ElementTree, ctypes, multiprocessing) are part of core Python distribution.   Next up, the tag-line \"ActivePython is the industry-standard Python distribution\", isn't core Python distribution \"industry-standard\" (whatever that means?)?   And the weirdest thing, is that ActiveState bundles it with crappy PythonWin, and not their own most-awesome Python editor/IDE, Komodo. What gives?  I actually never got to installing ActivePython, so maybe I don't know something, but it seems pretty irrelevant, and I see the name quite often on forums or here.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Why does ActivePython exist?",
        "A_Content": "  The main feature is that you can buy a paid support contract for it.  Why does Red Hat Enterprise Linux exist when you can compile everything yourself?  8-)  For many businesses, the combination of proven Open Source software and a support contract from people who build, package and test that software, is an excellent proposition.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "activestate",
            "activepython"
        ],
        "URL": "https://stackoverflow.com/questions/1352528/why-does-activepython-exist",
        "A_Votes": "29",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's ActivePython actually about?   From what I've read it's just standard Python with openssl and pyWin32 (on Win). No big deal I guess, I could install them in matter of minutes, and most people don't need them anyway. All other mentioned libraries (zlib, bzip2, sqlite3, Tkinter, ElementTree, ctypes, multiprocessing) are part of core Python distribution.   Next up, the tag-line \"ActivePython is the industry-standard Python distribution\", isn't core Python distribution \"industry-standard\" (whatever that means?)?   And the weirdest thing, is that ActiveState bundles it with crappy PythonWin, and not their own most-awesome Python editor/IDE, Komodo. What gives?  I actually never got to installing ActivePython, so maybe I don't know something, but it seems pretty irrelevant, and I see the name quite often on forums or here.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Why does ActivePython exist?",
        "A_Content": "  I've been using ActivePerl for years and when I made the switch to Python, I very naturally downloaded ActivePython. Never had any problems with the Active* distributions - they're robust, come with a few useful libraries that the vanilla core Python doesn't have. They also come bundled with a .CHM Python documentation compilation that's very useful.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "activestate",
            "activepython"
        ],
        "URL": "https://stackoverflow.com/questions/1352528/why-does-activepython-exist",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's ActivePython actually about?   From what I've read it's just standard Python with openssl and pyWin32 (on Win). No big deal I guess, I could install them in matter of minutes, and most people don't need them anyway. All other mentioned libraries (zlib, bzip2, sqlite3, Tkinter, ElementTree, ctypes, multiprocessing) are part of core Python distribution.   Next up, the tag-line \"ActivePython is the industry-standard Python distribution\", isn't core Python distribution \"industry-standard\" (whatever that means?)?   And the weirdest thing, is that ActiveState bundles it with crappy PythonWin, and not their own most-awesome Python editor/IDE, Komodo. What gives?  I actually never got to installing ActivePython, so maybe I don't know something, but it seems pretty irrelevant, and I see the name quite often on forums or here.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Why does ActivePython exist?",
        "A_Content": "  Here is an email to python-list I wrote on this a long while ago:  https://mail.python.org/pipermail/python-list/2007-July/456660.html    Mostly those details are still true. Also, all the other responses I've seen to this question are fair.  Note that as of release 2.6.3.7 ActivePython includes PyPM (similar to PPM for ActivePerl) to help with installing Python packages -- the hoped for benefit over \"easy_install\" and \"pip\" (and others) to be the installation of popular binary packages.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "activestate",
            "activepython"
        ],
        "URL": "https://stackoverflow.com/questions/1352528/why-does-activepython-exist",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's ActivePython actually about?   From what I've read it's just standard Python with openssl and pyWin32 (on Win). No big deal I guess, I could install them in matter of minutes, and most people don't need them anyway. All other mentioned libraries (zlib, bzip2, sqlite3, Tkinter, ElementTree, ctypes, multiprocessing) are part of core Python distribution.   Next up, the tag-line \"ActivePython is the industry-standard Python distribution\", isn't core Python distribution \"industry-standard\" (whatever that means?)?   And the weirdest thing, is that ActiveState bundles it with crappy PythonWin, and not their own most-awesome Python editor/IDE, Komodo. What gives?  I actually never got to installing ActivePython, so maybe I don't know something, but it seems pretty irrelevant, and I see the name quite often on forums or here.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Difference between except: and except Exception as e: in Python",
        "A_Content": "  In the second you can access the attributes of the exception object:  >>> def catch(): ...     try: ...         asd() ...     except Exception as e: ...         print e.message, e.args ...  >>> catch() global name 'asd' is not defined (\"global name 'asd' is not defined\",)   But it doesn't catch BaseException or the system-exiting exceptions SystemExit, KeyboardInterrupt and GeneratorExit:  >>> def catch(): ...     try: ...         raise BaseException() ...     except Exception as e: ...         print e.message, e.args ...  >>> catch() Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"<stdin>\", line 3, in catch BaseException   Which a bare except does:  >>> def catch(): ...     try: ...         raise BaseException() ...     except: ...         pass ...  >>> catch() >>>    See the Built-in Exceptions section of the docs and the Errors and Exceptions section of the tutorial for more info.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/18982610/difference-between-except-and-except-exception-as-e-in-python",
        "A_Votes": "86",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Both the following snippets of code do the same thing. They catch every exception and execute the code in the except: block  Snippet 1 -   try:     #some code that may throw an exception except:     #exception handling code   Snippet 2 -   try:     #some code that may throw an exception except Exception as e:     #exception handling code   What is exactly the difference in both the constructs?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Difference between except: and except Exception as e: in Python",
        "A_Content": "  except:   accepts all exceptions, whereas  except Exception as e:   only accepts exceptions that you're meant to catch.  Here's an example of one that you're not meant to catch:  >>> try: ...     input() ... except: ...     pass ...  >>> try: ...     input() ... except Exception as e: ...     pass ...  Traceback (most recent call last):   File \"<stdin>\", line 2, in <module> KeyboardInterrupt   The first one silenced the KeyboardInterrupt!  Here's a quick list:  issubclass(BaseException, BaseException) #>>> True issubclass(BaseException, Exception) #>>> False   issubclass(KeyboardInterrupt, BaseException) #>>> True issubclass(KeyboardInterrupt, Exception) #>>> False   issubclass(SystemExit, BaseException) #>>> True issubclass(SystemExit, Exception) #>>> False   If you want to catch any of those, it's best to do  except BaseException:   to point out that you know what you're doing.    All exceptions stem from BaseException, and those you're meant to catch day-to-day (those that'll be thrown for the programmer) inherit too from Exception.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/18982610/difference-between-except-and-except-exception-as-e-in-python",
        "A_Votes": "31",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Both the following snippets of code do the same thing. They catch every exception and execute the code in the except: block  Snippet 1 -   try:     #some code that may throw an exception except:     #exception handling code   Snippet 2 -   try:     #some code that may throw an exception except Exception as e:     #exception handling code   What is exactly the difference in both the constructs?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Difference between except: and except Exception as e: in Python",
        "A_Content": "  There are differences with some exceptions, e.g. KeyboardInterrupt.  Reading PEP8:     A bare except: clause will catch SystemExit and KeyboardInterrupt   exceptions, making it harder to interrupt a program with Control-C,   and can disguise other problems. If you want to catch all exceptions   that signal program errors, use except Exception: (bare except is   equivalent to except BaseException:).      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/18982610/difference-between-except-and-except-exception-as-e-in-python",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Both the following snippets of code do the same thing. They catch every exception and execute the code in the except: block  Snippet 1 -   try:     #some code that may throw an exception except:     #exception handling code   Snippet 2 -   try:     #some code that may throw an exception except Exception as e:     #exception handling code   What is exactly the difference in both the constructs?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Difference between except: and except Exception as e: in Python",
        "A_Content": "  Using the second form gives you a variable (named based upon the as clause, in your example e) in the except block scope with the exception object bound to it so you can use the infomration in the exception (type, message, stack trace, etc) to handle the exception in a more specially tailored manor.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/18982610/difference-between-except-and-except-exception-as-e-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Both the following snippets of code do the same thing. They catch every exception and execute the code in the except: block  Snippet 1 -   try:     #some code that may throw an exception except:     #exception handling code   Snippet 2 -   try:     #some code that may throw an exception except Exception as e:     #exception handling code   What is exactly the difference in both the constructs?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "What's the best way to assert for numpy.array equality?",
        "A_Content": "  check out the assert functions in numpy.testing, e.g.  assert_array_equal  for floating point arrays equality test might fail and assert_almost_equal is more reliable.  update  A few versions ago numpy obtained assert_allclose which is now my favorite since it allows us to specify both absolute and relative error and doesn't require decimal rounding as the closeness criterion.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/3302949/whats-the-best-way-to-assert-for-numpy-array-equality",
        "A_Votes": "92",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to make some unittests for my app, and I need to compare two arrays. Since array.__eq__ returns a new array (so TestCase.assertEqual fails), what is the best way to assert for equality?  Currently I'm using  self.assertTrue((arr1 == arr2).all())   but I don't really like it :\\     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "What's the best way to assert for numpy.array equality?",
        "A_Content": "  I think (arr1 == arr2).all() looks pretty nice. But you could use:  numpy.allclose(arr1, arr2)   but it's not quite the same.   An alternative, almost the same as your example is:  numpy.alltrue(arr1 == arr2)   Note that scipy.array is actually a reference numpy.array. That makes it easier to find the documentation.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/3302949/whats-the-best-way-to-assert-for-numpy-array-equality",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to make some unittests for my app, and I need to compare two arrays. Since array.__eq__ returns a new array (so TestCase.assertEqual fails), what is the best way to assert for equality?  Currently I'm using  self.assertTrue((arr1 == arr2).all())   but I don't really like it :\\     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "What's the best way to assert for numpy.array equality?",
        "A_Content": "  I find that using self.assertEqual(arr1.tolist(), arr2.tolist()) is the easiest way of comparing arrays with unittest.  I agree it's not the prettiest solution and it's probably not the fastest but it's probably more uniform with the rest of your test cases, you get all the unittest error description and it's really simple to implement.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/3302949/whats-the-best-way-to-assert-for-numpy-array-equality",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to make some unittests for my app, and I need to compare two arrays. Since array.__eq__ returns a new array (so TestCase.assertEqual fails), what is the best way to assert for equality?  Currently I'm using  self.assertTrue((arr1 == arr2).all())   but I don't really like it :\\     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "What's the best way to assert for numpy.array equality?",
        "A_Content": "  Since Python 3.2 you can use assertSequenceEqual(array1.tolist(), array2.tolist()).  This has the added value of showing you the exact items in which the arrays differ.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/3302949/whats-the-best-way-to-assert-for-numpy-array-equality",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to make some unittests for my app, and I need to compare two arrays. Since array.__eq__ returns a new array (so TestCase.assertEqual fails), what is the best way to assert for equality?  Currently I'm using  self.assertTrue((arr1 == arr2).all())   but I don't really like it :\\     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "What's the best way to assert for numpy.array equality?",
        "A_Content": "  In my tests I use this:  try:     numpy.testing.assert_array_equal(arr1, arr2)     res = True except AssertionError as err:     res = False     print (err) self.assertTrue(res)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/3302949/whats-the-best-way-to-assert-for-numpy-array-equality",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to make some unittests for my app, and I need to compare two arrays. Since array.__eq__ returns a new array (so TestCase.assertEqual fails), what is the best way to assert for equality?  Currently I'm using  self.assertTrue((arr1 == arr2).all())   but I don't really like it :\\     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python 2.7 on Ubuntu",
        "A_Content": "  I did it with pythonbrew on my Ubuntu 10.10 machine.  $ python -V Python 2.6.6 $ curl -kL https://raw.github.com/utahta/pythonbrew/master/pythonbrew-install | bash $ . $HOME/.pythonbrew/etc/bashrc $ pythonbrew install 2.7.1 $ pythonbrew switch 2.7.1 Switched to Python-2.7.1 $ python -V Python 2.7.1   I also used it to install Python 3.2.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "ubuntu",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/5233536/python-2-7-on-ubuntu",
        "A_Votes": "108",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python.   My question is, how can I install 2.7 and run it side by side with 2.6?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python 2.7 on Ubuntu",
        "A_Content": "  I recently backported Python 2.7 to Debian squeeze. Since Ubuntu 10.10 is newer than Debian squeeze, if you can do it on squeeze, you can certainly do it on Ubuntu. I don't have access to a Ubuntu 10.10 system. If I set one up, I'll test on it, and update this answer. So, here instead is a brief sketch of what I did on Debian.  First, a general and obvious comment, but something that is easily overlooked. One should not take the listed build dependencies of a Debian package too seriously. They may be far more specific than needed. For example, software like Python, which is designed to be portable and run over a wide array of systems, is unlikely to build depend on very specific versions of software. The runtime dependencies can be adjusted as well, but this should be done with more caution. However, runtime dependencies are mostly generated dynamically based on software that is already on this system, so usually that is not a big issue.  apt-cache policy python2.7 python2.7:   Installed: 2.7.2-8   Candidate: 2.7.2-8   Version table:      2.7.2-12 0          50 http://debian.csail.mit.edu/debian/ unstable/main i386 Packages      2.7.2-8 0          50 http://debian.csail.mit.edu/debian/ testing/main i386 Packages  *** 2.7.2-8 0         100 /var/lib/dpkg/status   Selecting the testing version we get  apt-get source python2.7=2.7.2-8   Looking at debian/control, we see the following build dependency lines.  Build-Depends: debhelper (>= 5), quilt, autoconf, libreadline-dev, libtinfo-dev, libncursesw5-dev (>= 5.3), tk8.5-dev, zlib1g-dev, blt-dev (>= 2.4z), libssl-dev, libexpat1-dev, sharutils, libbz2-dev, libbluetooth-dev [linux-any], locales [!armel !avr32 !hppa !ia64 !mipsel], libsqlite3-dev, libffi-dev (>= 3.0.5), mime-support, libgpm2 [linux-any], netbase, lsb-release, bzip2, libdb4.8-dev, gdb, python, help2man Build-Depends-Indep: python-sphinx Build-Conflicts: tcl8.3-dev, tk8.3-dev, tcl8.4-dev, tk8.4-dev, python2.7-xml, python-xml, autoconf2.13, libncurses5-dev  Most of this is easily satisfied on squeeze. With the handy utility apt-show-versions we get on my machine  apt-show-versions debhelper quilt autoconf libreadline-dev libtinfo-dev libncursesw5-dev tk8.5-dev zlib1g-dev blt-dev \\ libssl-dev libexpat1-dev sharutils libbz2-dev libbluetooth-dev locales libsqlite3-dev \\ libffi-dev mime-support libgpm2 netbase lsb-release bzip2 libdb4.8-dev gdb python help2man python-sphinx  autoconf/squeeze uptodate 2.67-2 blt-dev/squeeze uptodate 2.4z-4.2 bzip2/squeeze uptodate 1.0.5-6 debhelper/squeeze-backports uptodate 8.9.13~bpo60+1 gdb/squeeze uptodate 7.0.1-2+b1 help2man/squeeze uptodate 1.38.2 libbluetooth-dev/squeeze uptodate 4.66-3 libbz2-dev/squeeze uptodate 1.0.5-6 libdb4.8-dev/squeeze uptodate 4.8.30-2 libexpat1-dev/squeeze uptodate 2.0.1-7 libffi-dev/squeeze uptodate 3.0.9-3 libgpm2/squeeze uptodate 1.20.4-3.3 libncursesw5-dev/squeeze uptodate 5.7+20100313-5 libreadline-dev/squeeze uptodate 6.1-3 libsqlite3-dev/squeeze uptodate 3.7.3-1 libssl-dev/squeeze uptodate 0.9.8o-4squeeze5 libtinfo-dev not installed locales/squeeze uptodate 2.11.2-10 lsb-release/squeeze uptodate 3.2-23.2squeeze1 mime-support/squeeze uptodate 3.48-1 netbase/squeeze uptodate 4.45 python/squeeze uptodate 2.6.6-3+squeeze6 python-sphinx/squeeze-backports uptodate 1.0.8+dfsg-2~bpo60+1 quilt/squeeze uptodate 0.48-7 sharutils/squeeze uptodate 1:4.9-1 tk8.5-dev/squeeze uptodate 8.5.8-1 zlib1g-dev/squeeze uptodate 1:1.2.3.4.dfsg-3   We see that everything except libtinfo-dev is available in squeeze. I do have the squeeze backport versions of debhelper and python-sphinx, but both of these are also available for debian squeeze in versions satisfying the build requirements.  Observe also that I have libncurses5-dev installed  apt-show-versions libncurses5-dev  libncurses5-dev/squeeze uptodate 5.7+20100313-5   Both of these packages correspond to the source package curses 5.7+20100313-5. Observe that libtinfo-dev in fact replaces libncurses5-dev.  apt-cache show libtinfo-dev  Package: libtinfo-dev Source: ncurses Version: 5.9-4 Installed-Size: 279 Maintainer: Craig Small <csmall@debian.org> Architecture: i386 Replaces: libncurses5-dev (<< 5.9-3) Depends: libtinfo5 (= 5.9-4)   One would not expect python 2.7 to develop on such a specific version of curses, and in fact it doesn't. However, if you try to build the packages without satisfying the dependency you get  debuild -uc -us  dpkg-checkbuilddeps: Unmet build dependencies: libtinfo-dev dpkg-checkbuilddeps: Build conflicts: libncurses5-dev debuild: fatal error at line 1289: You do not appear to have all build dependencies properly met. You can use mk-build-deps to generate a dummy package which Depends on all the required packages, or you can install them manually using dpkg or apt using the error messages just above this message.   So, it is necessary to edit debian/control. Note that you also need to similarly edit the file debian/control.in, otherwise the control file will be incorrectly regenerated from control.in. The simplest thing to do is just remove libncurses5-dev from the Build-Conflicts line and libtinfo-dev from the Build-Depends line, and then run debuild -uc -us again. If you are going to have this package installed alongside the standard default Python 2.6 packages on Debian squeeze, you also need to remove the two lines  Conflicts: python-profiler (<= 2.7.1-2) Replaces: python-profiler (<= 2.7.1-2)   Those lines are there because 2.7 includes the python-profiler functionality. If 2.7 is the default python, then python-profiler is no longer necessary.  However, if one is installing 2.7 as a non-default Python, that reasoning does not apply, and python-profiler is still needed by 2.6.  This should build successfully, and result in the following list of binary packages.  ls -lah *.deb  -rw-r--r-- 1 faheem staff 289K Jan 12 02:33 idle-python2.7_2.7.2-8_all.deb -rw-r--r-- 1 faheem staff 1.1M Jan 12 02:34 libpython2.7_2.7.2-8_i386.deb -rw-r--r-- 1 faheem staff 2.5M Jan 12 02:34 python2.7_2.7.2-8_i386.deb -rw-r--r-- 1 faheem staff  12M Jan 12 02:34 python2.7-dbg_2.7.2-8_i386.deb -rw-r--r-- 1 faheem staff 4.9M Jan 12 02:34 python2.7-dev_2.7.2-8_i386.deb -rw-r--r-- 1 faheem staff 6.0M Jan 12 02:33 python2.7-doc_2.7.2-8_all.deb -rw-r--r-- 1 faheem staff 692K Jan 12 02:33 python2.7-examples_2.7.2-8_all.deb -rw-r--r-- 1 faheem staff 1.7M Jan 12 02:34 python2.7-minimal_2.7.2-8_i386.deb   Finally, one can install the binary packages with  dpkg -i python2.7-minimal_2.7.2-8_i386.deb python2.7_2.7.2-8_i386.deb python2.7-dev_2.7.2-8_i386.deb libpython2.7_2.7.2-8_i386.deb    Sometimes dpkg can be a little difficult about satisfying dependencies when they are all installed at once, so you might have to run apt-get -f install afterwards if you get dependency errors, or alternatively install the packages in smaller groups.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "ubuntu",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/5233536/python-2-7-on-ubuntu",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python.   My question is, how can I install 2.7 and run it side by side with 2.6?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python 2.7 on Ubuntu",
        "A_Content": "  Well if the only thing you need is argparse (saw that in one of your comments!) you could just do :  pip install argparse   This is not exactly an answer to the exact question :-) , but indeed if you are only missing a few feature, many 2.7 features actually come from independent projects and/or some compatibility packages can be found, eg:   argparse Which implementation of OrderedDict should be used in python2.6? : OrderedDict on pypi you should be able to replace memoryview with buffer   The list of 2.7 novelties is admittedly longer, but most of the other new features are probably not a big miss, and in exchange you do not mess around with multiple python installations on your box. Otherwise go with pythonbrew :-)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "ubuntu",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/5233536/python-2-7-on-ubuntu",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python.   My question is, how can I install 2.7 and run it side by side with 2.6?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python 2.7 on Ubuntu",
        "A_Content": "  ubuntu 12.04  Install dependencies:  $ sudo apt-get install python-software-properties   Add the repo:  $ sudo add-apt-repository ppa:fkrull/deadsnakes   Update the repo index:  $ sudo apt-get update   Install Python 3.3:  $ sudo apt-get install python3.3   ubuntu 12.04 > more  Installing the dependencies:  $ sudo apt-get install build-essential libsqlite3-dev sqlite3 bzip2 libbz2-dev   Download and compile python:  $ wget http://python.org/ftp/python/3.3.0/Python-3.3.0.tar.bz2  $ tar jxf ./Python-3.3.0.tar.bz2  $ cd ./Python-3.3.0  $ ./configure --prefix=/opt/python3.3  $ make && sudo make install   Some nice touches to install a py command by creating a symlink:  $ mkdir ~/bin $ ln -s /opt/python3.3/bin/python ~/bin/py      ",
        "Language": "Python",
        "Tags": [
            "python",
            "ubuntu",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/5233536/python-2-7-on-ubuntu",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python.   My question is, how can I install 2.7 and run it side by side with 2.6?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python 2.7 on Ubuntu",
        "A_Content": "  Just download Python 2.7 from http://www.python.org/download/releases/2.7.1/ and build it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "ubuntu",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/5233536/python-2-7-on-ubuntu",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python.   My question is, how can I install 2.7 and run it side by side with 2.6?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python 2.7 on Ubuntu",
        "A_Content": "  You can use virtualenv to create distinct Python environments. Just being newsy, but what does Python 2.7 have that you need?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "ubuntu",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/5233536/python-2-7-on-ubuntu",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python.   My question is, how can I install 2.7 and run it side by side with 2.6?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python 2.7 on Ubuntu",
        "A_Content": "  Another option is to install ActivePython if you do not want to compile things yourself. It includes a binary package manager as well.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "ubuntu",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/5233536/python-2-7-on-ubuntu",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am new to Python and am working on a Linux machine (Ubuntu 10.10). It is running python 2.6, but I'd like to run 2.7 as it has features I want to use. I have been urged to not install 2.7 and set that as my default python.   My question is, how can I install 2.7 and run it side by side with 2.6?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python closure: Write to variable in parent scope",
        "A_Content": "  Problem: This is because Python's scoping rules are demented. The presence of the += assignment operator marks the target, num_converted, as local to the enclosing function's scope, and there is no sound way in Python 2.x to access just one scoping level out from there. Only the global keyword can lift variable references out of the current scope, and it takes you straight to the top.  Fix: Turn num_converted into a single-element array.  num_converted = [0] def convert_variables(m):     name = m.group(1)     num_converted[0] += 1     return '<%%= %s %%>' % name      ",
        "Language": "Python",
        "Tags": [
            "python",
            "closures",
            "scope",
            "python-2.x"
        ],
        "URL": "https://stackoverflow.com/questions/4851463/python-closure-write-to-variable-in-parent-scope",
        "A_Votes": "81",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have the following code inside a function:  stored_blocks = {} def replace_blocks(m):     block = m.group(0)     block_hash = sha1(block)     stored_blocks[block_hash] = block     return '{{{%s}}}' % block_hash  num_converted = 0 def convert_variables(m):     name = m.group(1)     num_converted += 1     return '<%%= %s %%>' % name  fixed = MATCH_DECLARE_NEW.sub('', template) fixed = MATCH_PYTHON_BLOCK.sub(replace_blocks, fixed) fixed = MATCH_FORMAT.sub(convert_variables, fixed)   Adding elements to stored_blocks works fine, but I cannot increase num_converted in the second subfunction:     UnboundLocalError: local variable 'num_converted' referenced before assignment   I could use global but global variables are ugly and I really don't need that variable to be global at all.  So I'm curious how I can write to a variable in the parent function's scope. nonlocal num_converted would probably do the job, but I need a solution that works with Python 2.x.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python closure: Write to variable in parent scope",
        "A_Content": "  (see below for the edited answer)  You can use something like:  def convert_variables(m):     name = m.group(1)     convert_variables.num_converted += 1     return '<%%= %s %%>' % name  convert_variables.num_converted = 0   This way, num_converted works as a C-like \"static\" variable of the convert_variable method     (edited)  def convert_variables(m):     name = m.group(1)     convert_variables.num_converted = convert_variables.__dict__.get(\"num_converted\", 0) + 1     return '<%%= %s %%>' % name   This way, you don't need to initialize the counter in the main procedure.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "closures",
            "scope",
            "python-2.x"
        ],
        "URL": "https://stackoverflow.com/questions/4851463/python-closure-write-to-variable-in-parent-scope",
        "A_Votes": "28",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code inside a function:  stored_blocks = {} def replace_blocks(m):     block = m.group(0)     block_hash = sha1(block)     stored_blocks[block_hash] = block     return '{{{%s}}}' % block_hash  num_converted = 0 def convert_variables(m):     name = m.group(1)     num_converted += 1     return '<%%= %s %%>' % name  fixed = MATCH_DECLARE_NEW.sub('', template) fixed = MATCH_PYTHON_BLOCK.sub(replace_blocks, fixed) fixed = MATCH_FORMAT.sub(convert_variables, fixed)   Adding elements to stored_blocks works fine, but I cannot increase num_converted in the second subfunction:     UnboundLocalError: local variable 'num_converted' referenced before assignment   I could use global but global variables are ugly and I really don't need that variable to be global at all.  So I'm curious how I can write to a variable in the parent function's scope. nonlocal num_converted would probably do the job, but I need a solution that works with Python 2.x.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python closure: Write to variable in parent scope",
        "A_Content": "  Using the global keyword is fine. If you write:  num_converted = 0 def convert_variables(m):     global num_converted     name = m.group(1)     num_converted += 1     return '<%%= %s %%>' % name   ... num_converted  doesn't become a \"global variable\" (i.e. it doesn't become visible in any other unexpected places), it just means it can be modified inside convert_variables. That seems to be exactly what you want.  To put it another way, num_converted is already a global variable. All the global num_converted syntax does is tell Python \"inside this function, don't create a local num_converted variable, instead, use the existing global one.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "closures",
            "scope",
            "python-2.x"
        ],
        "URL": "https://stackoverflow.com/questions/4851463/python-closure-write-to-variable-in-parent-scope",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code inside a function:  stored_blocks = {} def replace_blocks(m):     block = m.group(0)     block_hash = sha1(block)     stored_blocks[block_hash] = block     return '{{{%s}}}' % block_hash  num_converted = 0 def convert_variables(m):     name = m.group(1)     num_converted += 1     return '<%%= %s %%>' % name  fixed = MATCH_DECLARE_NEW.sub('', template) fixed = MATCH_PYTHON_BLOCK.sub(replace_blocks, fixed) fixed = MATCH_FORMAT.sub(convert_variables, fixed)   Adding elements to stored_blocks works fine, but I cannot increase num_converted in the second subfunction:     UnboundLocalError: local variable 'num_converted' referenced before assignment   I could use global but global variables are ugly and I really don't need that variable to be global at all.  So I'm curious how I can write to a variable in the parent function's scope. nonlocal num_converted would probably do the job, but I need a solution that works with Python 2.x.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python closure: Write to variable in parent scope",
        "A_Content": "  What about using a class instance to hold the state?  You instantiate a class and pass instance methods to subs and those functions would have a reference to self...     ",
        "Language": "Python",
        "Tags": [
            "python",
            "closures",
            "scope",
            "python-2.x"
        ],
        "URL": "https://stackoverflow.com/questions/4851463/python-closure-write-to-variable-in-parent-scope",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code inside a function:  stored_blocks = {} def replace_blocks(m):     block = m.group(0)     block_hash = sha1(block)     stored_blocks[block_hash] = block     return '{{{%s}}}' % block_hash  num_converted = 0 def convert_variables(m):     name = m.group(1)     num_converted += 1     return '<%%= %s %%>' % name  fixed = MATCH_DECLARE_NEW.sub('', template) fixed = MATCH_PYTHON_BLOCK.sub(replace_blocks, fixed) fixed = MATCH_FORMAT.sub(convert_variables, fixed)   Adding elements to stored_blocks works fine, but I cannot increase num_converted in the second subfunction:     UnboundLocalError: local variable 'num_converted' referenced before assignment   I could use global but global variables are ugly and I really don't need that variable to be global at all.  So I'm curious how I can write to a variable in the parent function's scope. nonlocal num_converted would probably do the job, but I need a solution that works with Python 2.x.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python closure: Write to variable in parent scope",
        "A_Content": "  I have couple of remarks.  First, one application for such nested functions comes up when dealing with raw callbacks, as are used in libraries like xml.parsers.expat.  (That the library authors chose this approach may be objectionable, but ... there are reasons to use it nonetheless.)  Second: within a class, there are much nicer alternatives to the array (num_converted[0]).  I suppose this is what Sebastjan was talking about.  class MainClass:     _num_converted = 0     def outer_method( self ):         def convert_variables(m):             name = m.group(1)             self._num_converted += 1             return '<%%= %s %%>' % name   It's still odd enough to merit a comment in the code... But the variable is at least local to the class.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "closures",
            "scope",
            "python-2.x"
        ],
        "URL": "https://stackoverflow.com/questions/4851463/python-closure-write-to-variable-in-parent-scope",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code inside a function:  stored_blocks = {} def replace_blocks(m):     block = m.group(0)     block_hash = sha1(block)     stored_blocks[block_hash] = block     return '{{{%s}}}' % block_hash  num_converted = 0 def convert_variables(m):     name = m.group(1)     num_converted += 1     return '<%%= %s %%>' % name  fixed = MATCH_DECLARE_NEW.sub('', template) fixed = MATCH_PYTHON_BLOCK.sub(replace_blocks, fixed) fixed = MATCH_FORMAT.sub(convert_variables, fixed)   Adding elements to stored_blocks works fine, but I cannot increase num_converted in the second subfunction:     UnboundLocalError: local variable 'num_converted' referenced before assignment   I could use global but global variables are ugly and I really don't need that variable to be global at all.  So I'm curious how I can write to a variable in the parent function's scope. nonlocal num_converted would probably do the job, but I need a solution that works with Python 2.x.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python closure: Write to variable in parent scope",
        "A_Content": "  Modified from: https://stackoverflow.com/a/40690954/819544   You can leverage the inspect module to access the calling scope's globals dict and write into that. That means this trick can even be leveraged to access the calling scope from a nested function defined in an imported submodule.  import inspect   def get_globals(scope_level=0):     return dict(inspect.getmembers(inspect.stack()[scope_level][0]))[\"f_globals\"]  num_converted = 0 def foobar():     get_globals(0)['num_converted'] += 1  foobar() print(num_converted)  # 1   Play with the scope_level argument as needed. Setting scope_level=1 works for a function defined in a submodule, scope_level=2 for the inner function defined in a decorator in a submodule, etc.  NB: Just because you can do this, doesn't mean you should.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "closures",
            "scope",
            "python-2.x"
        ],
        "URL": "https://stackoverflow.com/questions/4851463/python-closure-write-to-variable-in-parent-scope",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following code inside a function:  stored_blocks = {} def replace_blocks(m):     block = m.group(0)     block_hash = sha1(block)     stored_blocks[block_hash] = block     return '{{{%s}}}' % block_hash  num_converted = 0 def convert_variables(m):     name = m.group(1)     num_converted += 1     return '<%%= %s %%>' % name  fixed = MATCH_DECLARE_NEW.sub('', template) fixed = MATCH_PYTHON_BLOCK.sub(replace_blocks, fixed) fixed = MATCH_FORMAT.sub(convert_variables, fixed)   Adding elements to stored_blocks works fine, but I cannot increase num_converted in the second subfunction:     UnboundLocalError: local variable 'num_converted' referenced before assignment   I could use global but global variables are ugly and I really don't need that variable to be global at all.  So I'm curious how I can write to a variable in the parent function's scope. nonlocal num_converted would probably do the job, but I need a solution that works with Python 2.x.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to organize a relatively large Flask application?",
        "A_Content": "  I have created a Flask boilerplate project called \"Fbone\", please feel free to check it out and fork :)  Fbone (Flask bone) is a Flask (Python microframework) template/bootstrap/boilerplate application.  Overview   Well designed for big project using blueprint. Integrate with hottest frontend framework: jQuery / html5boilerplate / bootstrap. Backed by the famous SQLalchemy. Implement tricky \"remember me\" by flask-login. Handle web forms by flask-wtform. Unit testing with flask-testing and nose. Easily deploy via fabric and mod_wsgi (example included). i18n by flask-babel   btw, I just found this wiki on building a large project with Flask useful, pls check it!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask"
        ],
        "URL": "https://stackoverflow.com/questions/9395587/how-to-organize-a-relatively-large-flask-application",
        "A_Votes": "80",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm building my first Flask app and I can't figure out a good, clean Pythonic way of organizing my application. I don't want to have everything in a single .py file as in their example. I would like to have each part of my app in a separate module. What would be a good way to organize things?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to organize a relatively large Flask application?",
        "A_Content": "  Flask 0.7 implements Blueprints. They are great for using the route decorator without importing the main application object.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask"
        ],
        "URL": "https://stackoverflow.com/questions/9395587/how-to-organize-a-relatively-large-flask-application",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm building my first Flask app and I can't figure out a good, clean Pythonic way of organizing my application. I don't want to have everything in a single .py file as in their example. I would like to have each part of my app in a separate module. What would be a good way to organize things?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to organize a relatively large Flask application?",
        "A_Content": "  Make sure to read Matt Wright's wonderful post on the subject.  The post features:   A description of a structure for large flask projects An example application on Github A description of best design practices in general when it comes to large web apps, like the MVC pattern, App factories, Services and Data Migration to name a few (most interesting feature IMHO).      ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask"
        ],
        "URL": "https://stackoverflow.com/questions/9395587/how-to-organize-a-relatively-large-flask-application",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm building my first Flask app and I can't figure out a good, clean Pythonic way of organizing my application. I don't want to have everything in a single .py file as in their example. I would like to have each part of my app in a separate module. What would be a good way to organize things?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to organize a relatively large Flask application?",
        "A_Content": "  I'm working on a (by my standards) big Flask project (5000 lines of Python code and it's only half-finished). The customer wants the project to be modular, so I took this apporach:  My folder structure looks like this:   __init__.py  modules.yml  config  controllers     ...  lib: Common functions I use often     ...  models     ...  static: All static files     css     img     js  templates: Jinja2 templates      ...   In modules.yml I define my modules including name and URL. This way the customer is able to enable/disable modules without touching a single Python file. In addition, I generate the menus based on the modules list. By convention every module has it its own Python-module in controllers/ that will load its model from models/. Every controller defines a Blueprint stored as the controller's name. E.g. for a user module, I have in controllers/user.py:  # Module name is 'user', thus save Blueprint as 'user' variable user = Blueprint('user', __name__)  @user.route('/user/') def index():     pass   This way, I can read the modules.yml in my __init__.py and load and register all enabled modules dynamically:  # Import modules for module in modules:      # Get module name from 'url' setting, exculde leading slash     modname = module['url'][1:]      try:         # from project.controllers.<modname> import <modname>         mod = __import__(             'project.controllers.' + modname, None, None, modname         )     except Exception as e:         # Log exceptions here         # [...]      mod = getattr(mod, modname)  # Get blueprint from module     app.register_blueprint(mod, url_prefix=module['url'])   I hope, this can be some inspiration for you :)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask"
        ],
        "URL": "https://stackoverflow.com/questions/9395587/how-to-organize-a-relatively-large-flask-application",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm building my first Flask app and I can't figure out a good, clean Pythonic way of organizing my application. I don't want to have everything in a single .py file as in their example. I would like to have each part of my app in a separate module. What would be a good way to organize things?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to organize a relatively large Flask application?",
        "A_Content": "  I worked on a social network built on top of Flask. The special thing about my project was that the server is purely serving API endpoints and the frontend is a one-page Backbone app. The Flask structure I took is the following:        app         api            auth.py            ...         app.py         common            constants.py            helpers.py            response.py            ...         config.py         extensions.py         frontend            controllers.py         static            ...         templates            app.html            ...         users             UserConstants.py             UserForms.py             UserHelpers.py             UserModels.py             __init__.py      alembic     |    version         ...      tests         ...   You can read the more in-depth post I wrote on the topic here. I found it to be much more intuitive to separate different functional areas to its own folder.   I worked on the code a while ago and open sourced it completely! You can check it out on github.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask"
        ],
        "URL": "https://stackoverflow.com/questions/9395587/how-to-organize-a-relatively-large-flask-application",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm building my first Flask app and I can't figure out a good, clean Pythonic way of organizing my application. I don't want to have everything in a single .py file as in their example. I would like to have each part of my app in a separate module. What would be a good way to organize things?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to organize a relatively large Flask application?",
        "A_Content": "  I have created a Flask app yapper from scratch and integrated it with gulp for both frontend and backend development. It is a simple blog engine but can be easily modified for developing according to requirements. It is well structured using Blueprints.  Checkout the project page yapper     ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask"
        ],
        "URL": "https://stackoverflow.com/questions/9395587/how-to-organize-a-relatively-large-flask-application",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm building my first Flask app and I can't figure out a good, clean Pythonic way of organizing my application. I don't want to have everything in a single .py file as in their example. I would like to have each part of my app in a separate module. What would be a good way to organize things?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Case Insensitive Flask-SQLAlchemy Query",
        "A_Content": "  You can do it by using either the lower or upper functions in your filter:  from sqlalchemy import func user = models.User.query.filter(func.lower(User.username) == func.lower(\"GaNyE\")).first()   Another option is to do searching using ilike instead of like:  .query.filter(Model.column.ilike(\"ganye\"))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask",
            "flask-sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/16573095/case-insensitive-flask-sqlalchemy-query",
        "A_Votes": "134",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm using Flask-SQLAlchemy to query from a database of users; however, while  user = models.User.query.filter_by(username=\"ganye\").first()   will return  <User u'ganye'>   doing  user = models.User.query.filter_by(username=\"GANYE\").first()   returns  None   I'm wondering if there's a way to query the database in a case insensitive way, so that the second example will still return  <User u'ganye'>      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Case Insensitive Flask-SQLAlchemy Query",
        "A_Content": "  Improving on @plaes's answer, this one will make the query shorter if you specify just the column(s) you need:  user = models.User.query.with_entities(models.User.username).\\ filter(models.User.username.ilike(\"%ganye%\")).all()   The above example is very useful in case one needs to use Flask's jsonify for AJAX purposes and then in your javascript access it using data.result:  from flask import jsonify jsonify(result=user)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "flask",
            "flask-sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/16573095/case-insensitive-flask-sqlalchemy-query",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm using Flask-SQLAlchemy to query from a database of users; however, while  user = models.User.query.filter_by(username=\"ganye\").first()   will return  <User u'ganye'>   doing  user = models.User.query.filter_by(username=\"GANYE\").first()   returns  None   I'm wondering if there's a way to query the database in a case insensitive way, so that the second example will still return  <User u'ganye'>      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to migrate back from initial migration in Django 1.7?",
        "A_Content": "  You can do the same with Django 1.7+ also:  python manage.py migrate <app> zero   This clears <app> from migration history and drops all tables of <app>  See django docs for more info.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-1.7",
            "django-migrations"
        ],
        "URL": "https://stackoverflow.com/questions/25606879/how-to-migrate-back-from-initial-migration-in-django-1-7",
        "A_Votes": "130",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I created a new app with some models and now I noticed that some of the models are poorly thought out. As I haven't committed the code the sensible thing would be to migrate the database to last good state and redo the migration with better models. In this case the last good state is database where the new app doesn't exist.  How can I migrate back from initial migration in Django 1.7?  In South one could do:  python manage.py migrate <app> zero   Which would clear <app> from migration history and drop all tables of <app>.   How to do this with Django 1.7 migrations?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to migrate back from initial migration in Django 1.7?",
        "A_Content": "  you can also use the version number:  python manage.py migrate <app> 0002   Source: https://docs.djangoproject.com/en/1.7/ref/django-admin/#django-admin-migrate     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-1.7",
            "django-migrations"
        ],
        "URL": "https://stackoverflow.com/questions/25606879/how-to-migrate-back-from-initial-migration-in-django-1-7",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I created a new app with some models and now I noticed that some of the models are poorly thought out. As I haven't committed the code the sensible thing would be to migrate the database to last good state and redo the migration with better models. In this case the last good state is database where the new app doesn't exist.  How can I migrate back from initial migration in Django 1.7?  In South one could do:  python manage.py migrate <app> zero   Which would clear <app> from migration history and drop all tables of <app>.   How to do this with Django 1.7 migrations?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do you set your pythonpath in an already-created virtualenv?",
        "A_Content": "  If you want to change the PYTHONPATH used in a virtualenv, you can add the following line to your virtualenv's bin/activate file:  export PYTHONPATH=\"/the/path/you/want\"   This way, the new PYTHONPATH will be set each time you use this virtualenv.  EDIT: (to answer @RamRachum's comment)  To have it restored to its original value on deactivate, you could add  export OLD_PYTHONPATH=\"$PYTHONPATH\"   before the previously mentioned line, and add the following line to your bin/postdeactivate script.  export PYTHONPATH=\"$OLD_PYTHONPATH\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "linux",
            "unix",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/4757178/how-do-you-set-your-pythonpath-in-an-already-created-virtualenv",
        "A_Votes": "99",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    What file do I edit, and how? I created a virtual environment.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do you set your pythonpath in an already-created virtualenv?",
        "A_Content": "  The comment by @s29 should be an answer:  One way to add a directory to the virtual environment is to install virtualenvwrapper (which is useful for many things) and then do  mkvirtualenv myenv workon myenv add2virtualenv . #for current directory add2virtualenv ~/my/path   If you want to remove these path edit the file myenvhomedir/lib/python2.7/site-packages/_virtualenv_path_extensions.pth  Documentation on virtualenvwrapper can be found at http://virtualenvwrapper.readthedocs.org/en/latest/  Specific documentation on this feature can be found at http://virtualenvwrapper.readthedocs.org/en/latest/command_ref.html?highlight=add2virtualenv     ",
        "Language": "Python",
        "Tags": [
            "python",
            "linux",
            "unix",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/4757178/how-do-you-set-your-pythonpath-in-an-already-created-virtualenv",
        "A_Votes": "55",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What file do I edit, and how? I created a virtual environment.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do you set your pythonpath in an already-created virtualenv?",
        "A_Content": "  You can create a .pth file that contains the directory to search for, and place it in the site-packages directory. E.g.:  cd $(python -c \"from distutils.sysconfig import get_python_lib; print(get_python_lib())\") echo /some/library/path > some-library.pth   The effect is the same as adding /some/library/path to sys.path, and remain local to the virtualenv setup.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "linux",
            "unix",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/4757178/how-do-you-set-your-pythonpath-in-an-already-created-virtualenv",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What file do I edit, and how? I created a virtual environment.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do you set your pythonpath in an already-created virtualenv?",
        "A_Content": "  I modified my activate script to source the file .virtualenvrc, if it exists in the current directory, and to save/restore PYTHONPATH on activate/deactivate.  You can find the patched activate script here.. It's a drop-in replacement for the activate script created by virtualenv 1.11.6.  Then I added something like this to my .virtualenvrc:  export PYTHONPATH=\"${PYTHONPATH:+$PYTHONPATH:}/some/library/path\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "linux",
            "unix",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/4757178/how-do-you-set-your-pythonpath-in-an-already-created-virtualenv",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What file do I edit, and how? I created a virtual environment.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do you set your pythonpath in an already-created virtualenv?",
        "A_Content": "  After initializing your vertualenv(1. cd venv and 2. source bin/activate).  just set or change your python path by entering command following-  export PYTHONPATH='/home/django/srmvenv/lib/python3.4'  for checking python path enter 1)  type  python then  >> import sys >> sys.path   Thanks you...     ",
        "Language": "Python",
        "Tags": [
            "python",
            "linux",
            "unix",
            "virtualenv"
        ],
        "URL": "https://stackoverflow.com/questions/4757178/how-do-you-set-your-pythonpath-in-an-already-created-virtualenv",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What file do I edit, and how? I created a virtual environment.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "AttributeError('str' object has no attribute 'read')",
        "A_Content": "  The problem is that for json.load you should pass a file like object with a read function defined. So either you use json.load(reponse) or json.loads(response.read()).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "urllib2",
            "attributeerror",
            "canonical-quickly"
        ],
        "URL": "https://stackoverflow.com/questions/11174024/attributeerrorstr-object-has-no-attribute-read",
        "A_Votes": "125",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In Python I'm getting an error:  Exception:  (<type 'exceptions.AttributeError'>, AttributeError(\"'str' object has no attribute 'read'\",), <traceback object at 0x1543ab8>)   Given python code:  def getEntries (self, sub):     url = 'http://www.reddit.com/'     if (sub != ''):         url += 'r/' + sub      request = urllib2.Request (url +          '.json', None, {'User-Agent' : 'Reddit desktop client by /user/RobinJ1995/'})     response = urllib2.urlopen (request)     jsonofabitch = response.read ()      return json.load (jsonofabitch)['data']['children']   What does this error mean and what did I do to cause it?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "AttributeError('str' object has no attribute 'read')",
        "A_Content": "  AttributeError(\"'str' object has no attribute 'read'\",)   This means exactly what it says: something tried to find a .read attribute on the object that you gave it, and you gave it an object of type str (i.e., you gave it a string).  The error occurred here:  json.load (jsonofabitch)['data']['children']   Well, you aren't looking for read anywhere, so it must happen in the json.load function that you called (as indicated by the full traceback). That is because json.load is trying to .read the thing that you gave it, but you gave it jsonofabitch, which currently names a string (which you created by calling .read on the response).  Solution: don't call .read yourself; the function will do this, and is expecting you to give it the response directly so that it can do so.  You could also have figured this out by reading the built-in Python documentation for the function (try help(json.load), or for the entire module (try help(json)), or by checking the documentation for those functions on http://docs.python.org .     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "urllib2",
            "attributeerror",
            "canonical-quickly"
        ],
        "URL": "https://stackoverflow.com/questions/11174024/attributeerrorstr-object-has-no-attribute-read",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python I'm getting an error:  Exception:  (<type 'exceptions.AttributeError'>, AttributeError(\"'str' object has no attribute 'read'\",), <traceback object at 0x1543ab8>)   Given python code:  def getEntries (self, sub):     url = 'http://www.reddit.com/'     if (sub != ''):         url += 'r/' + sub      request = urllib2.Request (url +          '.json', None, {'User-Agent' : 'Reddit desktop client by /user/RobinJ1995/'})     response = urllib2.urlopen (request)     jsonofabitch = response.read ()      return json.load (jsonofabitch)['data']['children']   What does this error mean and what did I do to cause it?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "AttributeError('str' object has no attribute 'read')",
        "A_Content": "  If you get a python error like this:  AttributeError: 'str' object has no attribute 'some_method'   You probably poisoned your object accidentally by overwriting your object with a string.  How to reproduce this error in python with a few lines of code:  #!/usr/bin/env python import json def foobar(json):     msg = json.loads(json)  foobar('{\"batman\": \"yes\"}')   Run it, which prints:  AttributeError: 'str' object has no attribute 'loads'   But change the name of the variablename, and it works fine:  #!/usr/bin/env python import json def foobar(jsonstring):     msg = json.loads(jsonstring)  foobar('{\"batman\": \"yes\"}')   This error is caused when you tried to run a method within a string.  String has a few methods, but not the one you are invoking.  So stop trying to invoke a method which String does not define and start looking for where you poisoned your object.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "urllib2",
            "attributeerror",
            "canonical-quickly"
        ],
        "URL": "https://stackoverflow.com/questions/11174024/attributeerrorstr-object-has-no-attribute-read",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python I'm getting an error:  Exception:  (<type 'exceptions.AttributeError'>, AttributeError(\"'str' object has no attribute 'read'\",), <traceback object at 0x1543ab8>)   Given python code:  def getEntries (self, sub):     url = 'http://www.reddit.com/'     if (sub != ''):         url += 'r/' + sub      request = urllib2.Request (url +          '.json', None, {'User-Agent' : 'Reddit desktop client by /user/RobinJ1995/'})     response = urllib2.urlopen (request)     jsonofabitch = response.read ()      return json.load (jsonofabitch)['data']['children']   What does this error mean and what did I do to cause it?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Passing a list of kwargs?",
        "A_Content": "  Yes. You do it like this:  def method(**kwargs):   print kwargs  keywords = {'keyword1': 'foo', 'keyword2': 'bar'} method(keyword1='foo', keyword2='bar') method(**keywords)   Running this in Python confirms these produce identical results:  {'keyword2': 'bar', 'keyword1': 'foo'} {'keyword2': 'bar', 'keyword1': 'foo'}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "kwargs"
        ],
        "URL": "https://stackoverflow.com/questions/1496346/passing-a-list-of-kwargs",
        "A_Votes": "127",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Can I pass a list of kwargs to a method for brevity? This is what i'm attempting to do:  def method(**kwargs):     #do something  keywords = (keyword1 = 'foo', keyword2 = 'bar') method(keywords)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Passing a list of kwargs?",
        "A_Content": "  As others have pointed out, you can do what you want by passing a dict. There are various ways to construct a dict. One that preserves the keyword=value style you attempted is to use the dict built-in:  keywords = dict(keyword1 = 'foo', keyword2 = 'bar')   Note the versatility of dict; all of these produce the same result:  >>> kw1 = dict(keyword1 = 'foo', keyword2 = 'bar') >>> kw2 = dict({'keyword1':'foo', 'keyword2':'bar'}) >>> kw3 = dict([['keyword1', 'foo'], ['keyword2', 'bar']]) >>> kw4 = dict(zip(('keyword1', 'keyword2'), ('foo', 'bar'))) >>> assert kw1 == kw2 == kw3 == kw4 >>>       ",
        "Language": "Python",
        "Tags": [
            "python",
            "kwargs"
        ],
        "URL": "https://stackoverflow.com/questions/1496346/passing-a-list-of-kwargs",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can I pass a list of kwargs to a method for brevity? This is what i'm attempting to do:  def method(**kwargs):     #do something  keywords = (keyword1 = 'foo', keyword2 = 'bar') method(keywords)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Passing a list of kwargs?",
        "A_Content": "  Do you mean a dict?  Sure you can:  def method(**kwargs):     #do something  keywords = {keyword1: 'foo', keyword2: 'bar'} method(**keywords)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "kwargs"
        ],
        "URL": "https://stackoverflow.com/questions/1496346/passing-a-list-of-kwargs",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can I pass a list of kwargs to a method for brevity? This is what i'm attempting to do:  def method(**kwargs):     #do something  keywords = (keyword1 = 'foo', keyword2 = 'bar') method(keywords)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Passing a list of kwargs?",
        "A_Content": "  So when I've come here I was looking for a way to pass several **kwargs in one function - for later use in further functions. Because this, not that surprisingly, doesn't work:  def func1(**f2_x, **f3_x):      ...   With some own 'experimental' coding I came to the obviously way how to do it:  def func3(f3_a, f3_b):     print \"--func3--\"     print f3_a     print f3_b def func2(f2_a, f2_b):     print \"--func2--\"     print f2_a     print f2_b  def func1(f1_a, f1_b, f2_x={},f3_x={}):     print \"--func1--\"     print f1_a     print f1_b     func2(**f2_x)     func3(**f3_x)  func1('aaaa', 'bbbb', {'f2_a':1, 'f2_b':2}, {'f3_a':37, 'f3_b':69})   This prints as expected:  --func1-- aaaa bbbb --func2-- 1 2 --func3-- 37 69      ",
        "Language": "Python",
        "Tags": [
            "python",
            "kwargs"
        ],
        "URL": "https://stackoverflow.com/questions/1496346/passing-a-list-of-kwargs",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can I pass a list of kwargs to a method for brevity? This is what i'm attempting to do:  def method(**kwargs):     #do something  keywords = (keyword1 = 'foo', keyword2 = 'bar') method(keywords)      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Overriding urllib2.HTTPError or urllib.error.HTTPError and reading response HTML anyway",
        "A_Content": "  The HTTPError is a file-like object.  You can catch it and then read its contents.  try:     resp = urllib2.urlopen(url)     contents = resp.read() except urllib2.HTTPError, error:     contents = error.read()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "urllib",
            "http-error"
        ],
        "URL": "https://stackoverflow.com/questions/2233687/overriding-urllib2-httperror-or-urllib-error-httperror-and-reading-response-html",
        "A_Votes": "135",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I receive a 'HTTP Error 500: Internal Server Error' response, but I still want to read the data inside the error HTML.  With Python 2.6, I normally fetch a page using:  import urllib2 url = \"http://google.com\" data = urllib2.urlopen(url) data = data.read()   When attempting to use this on the failing URL, I get the exception urllib2.HTTPError:  urllib2.HTTPError: HTTP Error 500: Internal Server Error   How can I fetch such error pages (with or without urllib2), all while they are returning Internal Server Errors?  Note that with Python 3, the corresponding exception is urllib.error.HTTPError.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Overriding urllib2.HTTPError or urllib.error.HTTPError and reading response HTML anyway",
        "A_Content": "  If you mean you want to read the body of the 500:  request = urllib2.Request(url, data, headers) try:         resp = urllib2.urlopen(request)         print resp.read() except urllib2.HTTPError, error:         print \"ERROR: \", error.read()   In your case, you don't need to build up the request.  Just do  try:         resp = urllib2.urlopen(url)         print resp.read() except urllib2.HTTPError, error:         print \"ERROR: \", error.read()   so, you don't override urllib2.HTTPError, you just handle the exception.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "urllib",
            "http-error"
        ],
        "URL": "https://stackoverflow.com/questions/2233687/overriding-urllib2-httperror-or-urllib-error-httperror-and-reading-response-html",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I receive a 'HTTP Error 500: Internal Server Error' response, but I still want to read the data inside the error HTML.  With Python 2.6, I normally fetch a page using:  import urllib2 url = \"http://google.com\" data = urllib2.urlopen(url) data = data.read()   When attempting to use this on the failing URL, I get the exception urllib2.HTTPError:  urllib2.HTTPError: HTTP Error 500: Internal Server Error   How can I fetch such error pages (with or without urllib2), all while they are returning Internal Server Errors?  Note that with Python 3, the corresponding exception is urllib.error.HTTPError.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Overriding urllib2.HTTPError or urllib.error.HTTPError and reading response HTML anyway",
        "A_Content": "  alist=['http://someurl.com']  def testUrl():     errList=[]     for URL in alist:         try:             urllib2.urlopen(URL)         except urllib2.URLError, err:             (err.reason != 200)             errList.append(URL+\" \"+str(err.reason))             return URL+\" \"+str(err.reason)     return \"\".join(errList)  testUrl()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "urllib",
            "http-error"
        ],
        "URL": "https://stackoverflow.com/questions/2233687/overriding-urllib2-httperror-or-urllib-error-httperror-and-reading-response-html",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I receive a 'HTTP Error 500: Internal Server Error' response, but I still want to read the data inside the error HTML.  With Python 2.6, I normally fetch a page using:  import urllib2 url = \"http://google.com\" data = urllib2.urlopen(url) data = data.read()   When attempting to use this on the failing URL, I get the exception urllib2.HTTPError:  urllib2.HTTPError: HTTP Error 500: Internal Server Error   How can I fetch such error pages (with or without urllib2), all while they are returning Internal Server Errors?  Note that with Python 3, the corresponding exception is urllib.error.HTTPError.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Default value for field in Django model",
        "A_Content": "  Set editable to False and default to your default value.  http://docs.djangoproject.com/en/dev/ref/models/fields/#editable  b = models.CharField(max_length=7, default='0000000', editable=False)   Also, your id field is unnecessary.  Django will add it automatically.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/755857/default-value-for-field-in-django-model",
        "A_Votes": "109",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Suppose I have a model:  class SomeModel(models.Model):     id = models.AutoField(primary_key=True)     a = models.CharField(max_length=10)     b = models.CharField(max_length=7)   Currently I am using the default admin to create/edit objects of this type. How do I remove the field b from the admin so that each object cannot be created with a value, and rather will receive a default value of 0000000?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Default value for field in Django model",
        "A_Content": "  You can set the default like this:  b = models.CharField(max_length=7,default=\"foobar\")   and then you can hide the field with your model's Admin class like this:  class SomeModelAdmin(admin.ModelAdmin):     exclude = (\"b\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/755857/default-value-for-field-in-django-model",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Suppose I have a model:  class SomeModel(models.Model):     id = models.AutoField(primary_key=True)     a = models.CharField(max_length=10)     b = models.CharField(max_length=7)   Currently I am using the default admin to create/edit objects of this type. How do I remove the field b from the admin so that each object cannot be created with a value, and rather will receive a default value of 0000000?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Default value for field in Django model",
        "A_Content": "  You can also use a callable in the default field, such as:  b = models.CharField(max_length=7, default=foo)   And then define the callable:  def foo():     return 'bar'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "django-admin"
        ],
        "URL": "https://stackoverflow.com/questions/755857/default-value-for-field-in-django-model",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Suppose I have a model:  class SomeModel(models.Model):     id = models.AutoField(primary_key=True)     a = models.CharField(max_length=10)     b = models.CharField(max_length=7)   Currently I am using the default admin to create/edit objects of this type. How do I remove the field b from the admin so that each object cannot be created with a value, and rather will receive a default value of 0000000?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  I got the same error and ended up using a pre-built distribution of numpy available in SourceForge  (similarly, a distribution of matplotlib can be obtained).  Builds for both 32-bit 2.7 and 3.3/3.4 are available. PyCharm detected them straight away, of course.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  I was able to fix this on Windows 7 64-bit running Python 3.4.3 by running the set command at a command prompt to determine the existing Visual Studio tools environment variable; in my case it was VS140COMNTOOLS for Visual Studio Community 2015.  Then run the following (substituting the variable on the right-hand side if yours has a different name):  set VS100COMNTOOLS=%VS140COMNTOOLS%   This allowed me to install the PyCrypto module that was previously giving me the same error as the OP.  For a more permanent solution, add this environment variable to your Windows environment via Control Panel (\"Edit the system environment variables\"), though you might need to use the actual path instead of the variable substitution.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "50",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  Python 3.3 and later now uses the 2010 compiler. To best way to solve the issue is to just install Visual C++ Express 2010 for free.   Now comes the harder part for 64 bit users and to be honest I just moved to 32 bit but 2010 express doesn't come with a 64 bit compiler (you get a new error, ValueError: ['path'] ) so you have to install Microsoft SDK 7.1 and follow the directions here to get the 64 bit compiler working with python: Python PIP has issues with path for MS Visual Studio 2010 Express for 64-bit install on Windows 7  It may just be easier for you to use the 32 bit version for now. In addition to getting the compiler working, you can bypass the need to compile many modules by getting the binary wheel file from this locaiton http://www.lfd.uci.edu/~gohlke/pythonlibs/  Just download the .whl file you need, shift + right click the download folder and select \"open command window here\" and run   pip install module-name.whl    I used that method on 64 bit 3.4.3 before I broke down and decided to just get a working compiler for pip compiles modules from source by default, which is why the binary wheel files work and having pip build from source doesn't.  People getting this (vcvarsall.bat) error on Python 2.7 can instead install \"Microsoft Visual C++ Compiler for Python 2.7\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "31",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  I have encountered this problem twice. First time I used VS 2013 and the second time I used VS 2015 with different solution. The first solution on VS 2013 and python 2.7 is:   Click win+R Enter SET VS90COMNTOOLS=%VS120COMNTOOLS% Close all windows Enter pip install again   Now, one year later, I have found an easier method to fix it. This time I use VS 2015 and python 3.4.   Right click on My Computer. Click Properties Advanced system settings Environment variables Add New system variable Enter VS100COMNTOOLSto the variable name Enter the value of VS140COMNTOOLSto the new variable. Close all windows   Now I'm sure you will ask some question what is the VSXXXCOMNTOOLSand what should I do if I use VS2008 or other compiler.  There is a file python\\Lib\\distutils\\msvc9compiler.py, beginning on line 216 we see  def find_vcvarsall(version):     \"\"\"Find the vcvarsall.bat file     At first it tries to find the productdir of VS 2010 in the registry. If     that fails it falls back to the VS100COMNTOOLS env var.     \"\"\"   It means that you must give the productdir of VS 2010 for it, so if you are using  python 2.x and    Visual Studio 2010 (VS10):SET VS90COMNTOOLS=%VS100COMNTOOLS% Visual Studio 2012 (VS11):SET VS90COMNTOOLS=%VS110COMNTOOLS% Visual Studio 2013 (VS12):SET VS90COMNTOOLS=%VS120COMNTOOLS% Visual Studio 2015 (VS15):SET VS90COMNTOOLS=%VS140COMNTOOLS%   or if you are using python 3.x and    Visual Studio 2010 (VS10):SET VS100COMNTOOLS=%VS100COMNTOOLS% Visual Studio 2012 (VS11):SET VS100COMNTOOLS=%VS110COMNTOOLS% Visual Studio 2013 (VS12):SET VS100COMNTOOLS=%VS120COMNTOOLS% Visual Studio 2015 (VS15):SET VS100COMNTOOLS=%VS140COMNTOOLS%   And it's the same as adding a new system variable. See the second ways.  Update:Sometimes,it still doesn't work.Check your path,ensure that contains VSxxxCOMNTOOLS     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "23",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  VS 2010 Express is no longer linked to any VS Express pages (that I found). I did find this link to the ISO which I used and it fixed the errors mentioned here.  http://download.microsoft.com/download/1/E/5/1E5F1C0A-0D5B-426A-A603-1798B951DDAE/VS2010Express1.iso  Note: Also make sure  you have x86 everything (Python + Postgresql) or you'll get other errors. I did not try x64 everything.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  you can download .whl in LFD . Then use \"pip install ***.whl\" in CMD     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  Tried to install lxml, grab and other extensions, which requires VS 10.0+ and get the same issue. I find own way to solve this problem(Windows 10 x64, Python 3.4+):   Install Visual C++ 2010 Express (download). (Do not install Microsoft Visual Studio 2010 Service Pack 1 ) Remove all the Microsoft Visual C++ 2010 Redistributable packages from Control Panel\\Programs and Features. If you don't do those then the install is going to fail with an obscure \"Fatal error during installation\" error. Install offline version of Windows SDK for Visual Studio 2010 (v7.1) (download). This is required for 64bit extensions.  Windows has builtin mounting for ISOs. Just mount the ISO and run Setup\\SDKSetup.exe instead of setup.exe. Create a vcvars64.bat file in C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\VC\\bin\\amd64 that contains:  CALL \"C:\\Program Files\\Microsoft SDKs\\Windows\\v7.1\\Bin\\SetEnv.cmd\" /x64 Find extension on this site, then put them into the python folder, and install .whl extension with pip:  python -m pip install extensionname.whl Enjoy      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat) [duplicate]",
        "A_Content": "  you have to check your pip package to be updated to the latest version in your pycharm and then install numpy package.  in settings -> project:progLangComp -> Project Interpreter there is a table of packages and their current version (just labelled as Version) and their latest version (labelled as Latest). Pip current version number should be the same as latest version. If you see a blue arrow in front of pip, you have to update it to the latest then trying to install numpy or any other packages that you couldn't install, for me it was pandas which I wanted to install.  enter image description here     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pycharm"
        ],
        "URL": "https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Cannot find vcvarsall.bat when running a Python script                                        17 answers                                          Im trying to install numpy with PyCharm but i keep getting this error:      error: Microsoft Visual C++ 10.0 is required (Unable to find   vcvarsall.bat).   Can someone please explain to me exactly what i have to do to fix this error(and as simple and detailed as possible)? im using python 3.4.2 (i know this has been answered before but i did not understand it).     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: print a generator expression?",
        "A_Content": "  Quick answer:  Doing list() around a generator expression is (almost) exactly equivalent to having  [] brackets around it. So yeah, you can do  >>> list((x for x in string.letters if x in (y for y in \"BigMan on campus\")))   But you can just as well do  >>> [x for x in string.letters if x in (y for y in \"BigMan on campus\")]   Yes, that will turn the generator expression into a list comprehension. It's the same thing and calling list() on it. So the way to make a generator expression into a list is to put brackets around it.  Detailed explanation:  A generator expression is a \"naked\" for expression. Like so:  x*x for x in range(10)   Now, you can't stick that on a line by itself, you'll get a syntax error. But you can put parenthesis around it.  >>> (x*x for x in range(10)) <generator object <genexpr> at 0xb7485464>   This is sometimes called a generator comprehension, although I think the official name still is generator expression, there isn't really any difference, the parenthesis are only there to make the syntax valid. You do not need them if you are passing it in as the only parameter to a function for example:  >>> sorted(x*x for x in range(10)) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]   Basically all the other comprehensions available in Python 3 and Python 2.7 is just syntactic sugar around a generator expression. Set comprehensions:  >>> {x*x for x in range(10)} {0, 1, 4, 81, 64, 9, 16, 49, 25, 36}  >>> set(x*x for x in range(10)) {0, 1, 4, 81, 64, 9, 16, 49, 25, 36}   Dict comprehensions:  >>> dict((x, x*x) for x in range(10)) {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}  >>> {x: x*x for x in range(10)} {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81}   And list comprehensions under Python 3:  >>> list(x*x for x in range(10)) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]  >>> [x*x for x in range(10)] [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]   Under Python 2, list comprehensions is not just syntactic sugar. But the only difference is that x will under Python 2 leak into the namespace.  >>> x 9   While under Python 3 you'll get   >>> x Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> NameError: name 'x' is not defined   This means that the best way to get a nice printout of the content of your generator expression in Python is to make a list comprehension out of it! However, this will obviously not work if you already have a generator object. Doing that will just make a list of one generator:  >>> foo = (x*x for x in range(10)) >>> [foo] [<generator object <genexpr> at 0xb7559504>]   In that case you will need to call list():  >>> list(foo) [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]   Although this works, but is kinda stupid:  >>> [x for x in foo] [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/5164642/python-print-a-generator-expression",
        "A_Votes": "113",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In the Python shell, if I enter a list comprehension such as:  >>> [x for x in string.letters if x in [y for y in \"BigMan on campus\"]]   I get a nicely printed result:  ['a', 'c', 'g', 'i', 'm', 'n', 'o', 'p', 's', 'u', 'B', 'M']   Same for a dictionary comprehension:  >>> {x:x*2 for x in range(1,10)} {1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12, 7: 14, 8: 16, 9: 18}   If I enter a generator expression, I get not such a friendly response:   >>> (x for x in string.letters if x in (y for y in \"BigMan on campus\")) <generator object <genexpr> at 0x1004a0be0>   I know I can do this:  >>> for i in _: print i, a c g i m n o p s u B M   Other than that (or writing a helper function) can I easily evaluate and print that generator object in the interactive shell?      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: print a generator expression?",
        "A_Content": "  You can just wrap the expression in a call to list:  >>> list(x for x in string.letters if x in (y for y in \"BigMan on campus\")) ['a', 'c', 'g', 'i', 'm', 'n', 'o', 'p', 's', 'u', 'B', 'M']      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/5164642/python-print-a-generator-expression",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In the Python shell, if I enter a list comprehension such as:  >>> [x for x in string.letters if x in [y for y in \"BigMan on campus\"]]   I get a nicely printed result:  ['a', 'c', 'g', 'i', 'm', 'n', 'o', 'p', 's', 'u', 'B', 'M']   Same for a dictionary comprehension:  >>> {x:x*2 for x in range(1,10)} {1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12, 7: 14, 8: 16, 9: 18}   If I enter a generator expression, I get not such a friendly response:   >>> (x for x in string.letters if x in (y for y in \"BigMan on campus\")) <generator object <genexpr> at 0x1004a0be0>   I know I can do this:  >>> for i in _: print i, a c g i m n o p s u B M   Other than that (or writing a helper function) can I easily evaluate and print that generator object in the interactive shell?      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: print a generator expression?",
        "A_Content": "  Unlike a list or a dictionary, a generator can be infinite. Doing this wouldn't work:  def gen():     x = 0     while True:         yield x         x += 1 g1 = gen() list(g1)   # never ends   Also, reading a generator changes it, so there's not a perfect way to view it. To see a sample of the generator's output, you could do  g1 = gen() [g1.next() for i in range(10)]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/5164642/python-print-a-generator-expression",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In the Python shell, if I enter a list comprehension such as:  >>> [x for x in string.letters if x in [y for y in \"BigMan on campus\"]]   I get a nicely printed result:  ['a', 'c', 'g', 'i', 'm', 'n', 'o', 'p', 's', 'u', 'B', 'M']   Same for a dictionary comprehension:  >>> {x:x*2 for x in range(1,10)} {1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12, 7: 14, 8: 16, 9: 18}   If I enter a generator expression, I get not such a friendly response:   >>> (x for x in string.letters if x in (y for y in \"BigMan on campus\")) <generator object <genexpr> at 0x1004a0be0>   I know I can do this:  >>> for i in _: print i, a c g i m n o p s u B M   Other than that (or writing a helper function) can I easily evaluate and print that generator object in the interactive shell?      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: print a generator expression?",
        "A_Content": "  Or you can always map over an iterator, without the need to build an intermediate list:  >>> _ = map(sys.stdout.write, (x for x in string.letters if x in (y for y in \"BigMan on campus\"))) acgimnopsuBM      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/5164642/python-print-a-generator-expression",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In the Python shell, if I enter a list comprehension such as:  >>> [x for x in string.letters if x in [y for y in \"BigMan on campus\"]]   I get a nicely printed result:  ['a', 'c', 'g', 'i', 'm', 'n', 'o', 'p', 's', 'u', 'B', 'M']   Same for a dictionary comprehension:  >>> {x:x*2 for x in range(1,10)} {1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12, 7: 14, 8: 16, 9: 18}   If I enter a generator expression, I get not such a friendly response:   >>> (x for x in string.letters if x in (y for y in \"BigMan on campus\")) <generator object <genexpr> at 0x1004a0be0>   I know I can do this:  >>> for i in _: print i, a c g i m n o p s u B M   Other than that (or writing a helper function) can I easily evaluate and print that generator object in the interactive shell?      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python: print a generator expression?",
        "A_Content": "  >>> list(x for x in string.letters if x in (y for y in \"BigMan on campus\")) ['a', 'c', 'g', 'i', 'm', 'n', 'o', 'p', 's', 'u', 'B', 'M']      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/5164642/python-print-a-generator-expression",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In the Python shell, if I enter a list comprehension such as:  >>> [x for x in string.letters if x in [y for y in \"BigMan on campus\"]]   I get a nicely printed result:  ['a', 'c', 'g', 'i', 'm', 'n', 'o', 'p', 's', 'u', 'B', 'M']   Same for a dictionary comprehension:  >>> {x:x*2 for x in range(1,10)} {1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12, 7: 14, 8: 16, 9: 18}   If I enter a generator expression, I get not such a friendly response:   >>> (x for x in string.letters if x in (y for y in \"BigMan on campus\")) <generator object <genexpr> at 0x1004a0be0>   I know I can do this:  >>> for i in _: print i, a c g i m n o p s u B M   Other than that (or writing a helper function) can I easily evaluate and print that generator object in the interactive shell?      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  Iterating over strings is unfortunately rather slow in Python. Regular expressions are over an order of magnitude faster for this kind of thing. You just have to build the character class yourself. The unicodedata module is quite helpful for this, especially the unicodedata.category() function. See Unicode Character Database for descriptions of the categories.  import unicodedata, re  all_chars = (unichr(i) for i in xrange(0x110000)) control_chars = ''.join(c for c in all_chars if unicodedata.category(c) == 'Cc') # or equivalently and much more efficiently control_chars = ''.join(map(unichr, range(0,32) + range(127,160)))  control_char_re = re.compile('[%s]' % re.escape(control_chars))  def remove_control_chars(s):     return control_char_re.sub('', s)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "67",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  As far as I know, the most pythonic/efficient method would be:  import string  filtered_string = filter(lambda x: x in string.printable, myStr)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "56",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  In Python 3,  def filter_nonprintable(text):     import string     # Get the difference of all ASCII characters from the set of printable characters     nonprintable = set([chr(i) for i in range(128)]).difference(string.printable)     # Use translate to remove all non-printable characters     return text.translate({ord(character):None for character in nonprintable})   See this StackOverflow post on removing punctuation for how .translate() compares to regex & .replace()     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  You could try setting up a filter using the unicodedata.category() function:  printable = Set('Lu', 'Ll', ...) def filter_non_printable(str):   return ''.join(c for c in str if unicodedata.category(c) in printable)   See the Unicode database character properties for the available categories     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  This function uses list comprehensions and str.join, so it runs in linear time instead of O(n^2):  from curses.ascii import isprint  def printable(input):     return ''.join(char for char in input if isprint(char))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  The best I've come up with now is (thanks to the python-izers above)   def filter_non_printable(str):   return ''.join([c for c in str if ord(c) > 31 or ord(c) == 9])   This is the only way I've found out that works with Unicode characters/strings  Any better options?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  The one below performs faster than the others above. Take a look  ''.join([x if x in string.printable else '' for x in Str])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "  To remove 'whitespace',  import re t = \"\"\" \\n\\t<p>&nbsp;</p>\\n\\t<p>&nbsp;</p>\\n\\t<p>&nbsp;</p>\\n\\t<p>&nbsp;</p>\\n\\t<p> \"\"\" pat = re.compile(r'[\\t\\n]') print(pat.sub(\"\", t))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Stripping non printable characters from a string in python",
        "A_Content": "     In Python there's no POSIX regex classes   There are when using the regex library: https://pypi.org/project/regex/  It is well maintained and supports Unicode regex, Posix regex and many more. The usage (method signatures) is very similar to Python's re.  From the documentation:     [[:alpha:]]; [[:^alpha:]]      POSIX character classes are supported. These   are normally treated as an alternative form of \\p{...}.   (I'm not affiliated, just a user.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "non-printable"
        ],
        "URL": "https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use to run  $s =~ s/[^[:print:]]//g;   on Perl to get rid of non printable characters.   In Python there's no POSIX regex classes, and I can't write [:print:] having it mean what I want. I know of no way in Python to detect if a character is printable or not.   What would you do?   EDIT: It has to support Unicode characters as well. The string.printable way will happily strip them out of the output.  curses.ascii.isprint will return false for any unicode character.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Matplotlib (pyplot) savefig outputs blank image",
        "A_Content": "  First, what happens when T0 is not None? I would test that, then I would adjust the values I pass to plt.subplot(); maybe try values 131, 132, and 133, or values that depend whether or not T0 exists.  Second, after plt.show() is called, a new figure is created. To deal with this, you can   Call plt.savefig('tessstttyyy.png', dpi=100) before you call plt.show() Save the figure before you show() by calling plt.gcf() for \"get current figure\", then you can call savefig() on this Figure object at any time.   For example:   fig1 = plt.gcf() plt.show() plt.draw() fig1.savefig('tessstttyyy.png', dpi=100)   In your code, 'tesssttyyy.png' is blank because it is saving the new figure, to which nothing has been plotted.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "matplotlib",
            "figure"
        ],
        "URL": "https://stackoverflow.com/questions/9012487/matplotlib-pyplot-savefig-outputs-blank-image",
        "A_Votes": "128",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am trying to save plots I make using matplotlib; however, the images are saving blank.  Here is my code:  plt.subplot(121) plt.imshow(dataStack, cmap=mpl.cm.bone)  plt.subplot(122) y = copy.deepcopy(tumorStack) y = np.ma.masked_where(y == 0, y)  plt.imshow(dataStack, cmap=mpl.cm.bone) plt.imshow(y, cmap=mpl.cm.jet_r, interpolation='nearest')  if T0 is not None:     plt.subplot(123)     plt.imshow(T0, cmap=mpl.cm.bone)      #plt.subplot(124)     #Autozoom  #else:     #plt.subplot(124)     #Autozoom  plt.show() plt.draw() plt.savefig('tessstttyyy.png', dpi=100)   And tessstttyyy.png is blank (also tried with .jpg)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Matplotlib (pyplot) savefig outputs blank image",
        "A_Content": "  plt.show() should come after plt.savefig()     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "matplotlib",
            "figure"
        ],
        "URL": "https://stackoverflow.com/questions/9012487/matplotlib-pyplot-savefig-outputs-blank-image",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am trying to save plots I make using matplotlib; however, the images are saving blank.  Here is my code:  plt.subplot(121) plt.imshow(dataStack, cmap=mpl.cm.bone)  plt.subplot(122) y = copy.deepcopy(tumorStack) y = np.ma.masked_where(y == 0, y)  plt.imshow(dataStack, cmap=mpl.cm.bone) plt.imshow(y, cmap=mpl.cm.jet_r, interpolation='nearest')  if T0 is not None:     plt.subplot(123)     plt.imshow(T0, cmap=mpl.cm.bone)      #plt.subplot(124)     #Autozoom  #else:     #plt.subplot(124)     #Autozoom  plt.show() plt.draw() plt.savefig('tessstttyyy.png', dpi=100)   And tessstttyyy.png is blank (also tried with .jpg)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Matplotlib (pyplot) savefig outputs blank image",
        "A_Content": "  let's me give a more detail example:  import numpy as np import matplotlib.pyplot as plt   def draw_result(lst_iter, lst_loss, lst_acc, title):     plt.plot(lst_iter, lst_loss, '-b', label='loss')     plt.plot(lst_iter, lst_acc, '-r', label='accuracy')      plt.xlabel(\"n iteration\")     plt.legend(loc='upper left')     plt.title(title)     plt.savefig(title+\".png\")  # should before plt.show method      plt.show()   def test_draw():     lst_iter = range(100)     lst_loss = [0.01 * i + 0.01 * i ** 2 for i in xrange(100)]     # lst_loss = np.random.randn(1, 100).reshape((100, ))     lst_acc = [0.01 * i - 0.01 * i ** 2 for i in xrange(100)]     # lst_acc = np.random.randn(1, 100).reshape((100, ))     draw_result(lst_iter, lst_loss, lst_acc, \"sgd_method\")   if __name__ == '__main__':     test_draw()        ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "matplotlib",
            "figure"
        ],
        "URL": "https://stackoverflow.com/questions/9012487/matplotlib-pyplot-savefig-outputs-blank-image",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am trying to save plots I make using matplotlib; however, the images are saving blank.  Here is my code:  plt.subplot(121) plt.imshow(dataStack, cmap=mpl.cm.bone)  plt.subplot(122) y = copy.deepcopy(tumorStack) y = np.ma.masked_where(y == 0, y)  plt.imshow(dataStack, cmap=mpl.cm.bone) plt.imshow(y, cmap=mpl.cm.jet_r, interpolation='nearest')  if T0 is not None:     plt.subplot(123)     plt.imshow(T0, cmap=mpl.cm.bone)      #plt.subplot(124)     #Autozoom  #else:     #plt.subplot(124)     #Autozoom  plt.show() plt.draw() plt.savefig('tessstttyyy.png', dpi=100)   And tessstttyyy.png is blank (also tried with .jpg)     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to calculate cumulative normal distribution in Python",
        "A_Content": "  Here's an example:  >>> from scipy.stats import norm >>> norm.cdf(1.96) 0.9750021048517795 >>> norm.cdf(-1.96) 0.024997895148220435   In other words, approximately 95% of the standard normal interval lies within two standard deviations, centered on a standard mean of zero.  If you need the inverse CDF:  >>> norm.ppf(norm.cdf(1.96)) array(1.9599999999999991)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/809362/how-to-calculate-cumulative-normal-distribution-in-python",
        "A_Votes": "93",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a function in Numpy or Scipy (or any rigorous Python library) that will give me the cumulative normal distribution function in Python.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to calculate cumulative normal distribution in Python",
        "A_Content": "  It may be too late to answer the question but since Google still leads people here, I decide to write my solution here.  That is, since Python 2.7, the math library has integrated the error function math.erf(x)  The erf() function can be used to compute traditional statistical functions such as the cumulative standard normal distribution:  from math import * def phi(x):     #'Cumulative distribution function for the standard normal distribution'     return (1.0 + erf(x / sqrt(2.0))) / 2.0   Ref:  https://docs.python.org/2/library/math.html  https://docs.python.org/3/library/math.html  How are the Error Function and Standard Normal distribution function related?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/809362/how-to-calculate-cumulative-normal-distribution-in-python",
        "A_Votes": "27",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a function in Numpy or Scipy (or any rigorous Python library) that will give me the cumulative normal distribution function in Python.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to calculate cumulative normal distribution in Python",
        "A_Content": "  Adapted from here http://mail.python.org/pipermail/python-list/2000-June/039873.html  from math import * def erfcc(x):     \"\"\"Complementary error function.\"\"\"     z = abs(x)     t = 1. / (1. + 0.5*z)     r = t * exp(-z*z-1.26551223+t*(1.00002368+t*(.37409196+         t*(.09678418+t*(-.18628806+t*(.27886807+         t*(-1.13520398+t*(1.48851587+t*(-.82215223+         t*.17087277)))))))))     if (x >= 0.):         return r     else:         return 2. - r  def ncdf(x):     return 1. - 0.5*erfcc(x/(2**0.5))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/809362/how-to-calculate-cumulative-normal-distribution-in-python",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a function in Numpy or Scipy (or any rigorous Python library) that will give me the cumulative normal distribution function in Python.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to calculate cumulative normal distribution in Python",
        "A_Content": "  To build upon Unknown's example, the Python equivalent of the function normdist() implemented in a lot of libraries would be:  def normcdf(x, mu, sigma):     t = x-mu;     y = 0.5*erfcc(-t/(sigma*sqrt(2.0)));     if y>1.0:         y = 1.0;     return y  def normpdf(x, mu, sigma):     u = (x-mu)/abs(sigma)     y = (1/(sqrt(2*pi)*abs(sigma)))*exp(-u*u/2)     return y  def normdist(x, mu, sigma, f):     if f:         y = normcdf(x,mu,sigma)     else:         y = normpdf(x,mu,sigma)     return y      ",
        "Language": "Python",
        "Tags": [
            "python",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/809362/how-to-calculate-cumulative-normal-distribution-in-python",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a function in Numpy or Scipy (or any rigorous Python library) that will give me the cumulative normal distribution function in Python.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to calculate cumulative normal distribution in Python",
        "A_Content": "  Alex's answer shows you a solution for standard normal distribution (mean = 0, standard deviation = 1). If you have normal distribution with mean and std (which is sqr(var)) and you want to calculate:  from scipy.stats import norm  # cdf(x < val) print norm.cdf(val, m, s)  # cdf(x > val) print 1 - norm.cdf(val, m, s)  # cdf(v1 < x < v2) print norm.cdf(v2, m, s) - norm.cdf(v1, m, s)   Read more about cdf here and scipy implementation of normal distribution with many formulas here.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "statistics"
        ],
        "URL": "https://stackoverflow.com/questions/809362/how-to-calculate-cumulative-normal-distribution-in-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am looking for a function in Numpy or Scipy (or any rigorous Python library) that will give me the cumulative normal distribution function in Python.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - How to sort a list of lists by the fourth element in each list? [duplicate]",
        "A_Content": "  unsorted_list.sort(key=lambda x: x[3])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting"
        ],
        "URL": "https://stackoverflow.com/questions/17555218/python-how-to-sort-a-list-of-lists-by-the-fourth-element-in-each-list",
        "A_Votes": "119",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Sort a list of tuples by 2nd item (integer value) [duplicate]                                        9 answers                                          I would like to sort the following list of lists by the fourth element (the integer) in each individual list.  unsorted_list = [['a','b','c','5','d'],['e','f','g','3','h'],['i','j','k','4','m']]   How can I do this? Thank you!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python - How to sort a list of lists by the fourth element in each list? [duplicate]",
        "A_Content": "  Use sorted() with a key as follows -   >>> unsorted_list = [['a','b','c','5','d'],['e','f','g','3','h'],['i','j','k','4','m']] >>> sorted(unsorted_list, key = lambda x: int(x[3])) [['e', 'f', 'g', '3', 'h'], ['i', 'j', 'k', '4', 'm'], ['a', 'b', 'c', '5', 'd']]   The lambda returns the fourth element of each of the inner lists and the sorted function uses that to sort those list. This assumes that int(elem) will not fail for the list.  Or use itemgetter (As Ashwini's comment pointed out, this method would not work if you have string representations of the numbers, since they are bound to fail somewhere for 2+ digit numbers)  >>> from operator import itemgetter >>> sorted(unsorted_list, key = itemgetter(3)) [['e', 'f', 'g', '3', 'h'], ['i', 'j', 'k', '4', 'm'], ['a', 'b', 'c', '5', 'd']]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting"
        ],
        "URL": "https://stackoverflow.com/questions/17555218/python-how-to-sort-a-list-of-lists-by-the-fourth-element-in-each-list",
        "A_Votes": "32",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Sort a list of tuples by 2nd item (integer value) [duplicate]                                        9 answers                                          I would like to sort the following list of lists by the fourth element (the integer) in each individual list.  unsorted_list = [['a','b','c','5','d'],['e','f','g','3','h'],['i','j','k','4','m']]   How can I do this? Thank you!     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Ambiguity in Pandas Dataframe / Numpy Array axis definition",
        "A_Content": "  It's perhaps simplest to remember it as 0=down and 1=across.   This means:   Use axis=0 to apply a method down each column, or to the row labels (the index). Use axis=1 to apply a method across each row, or to the column labels.   Here's a picture to show the parts of a DataFrame that each axis refers to:    It's also useful to remember that Pandas follows NumPy's use of the word axis. The usage is explained in NumPy's glossary of terms:     Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). [my emphasis]    So, concerning the method in the question, df.mean(axis=1), seems to be correctly defined. It takes the mean of entries horizontally across columns, that is, along each individual row. On the other hand, df.mean(axis=0) would be an operation acting vertically downwards across rows.  Similarly, df.drop(name, axis=1) refers to an action on column labels, because they intuitively go across the horizontal axis. Specifying axis=0 would make the method act on rows instead.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/25773245/ambiguity-in-pandas-dataframe-numpy-array-axis-definition",
        "A_Votes": "141",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:  >>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[\"col1\", \"col2\", \"col3\", \"col4\"]) >>> df    col1  col2  col3  col4 0     1     1     1     1 1     2     2     2     2 2     3     3     3     3   So if we call df.mean(axis=1), we'll get a mean across the rows:  >>> df.mean(axis=1) 0    1 1    2 2    3   However, if we call df.drop(name, axis=1), we actually drop a column, not a row:  >>> df.drop(\"col4\", axis=1)    col1  col2  col3 0     1     1     1 1     2     2     2 2     3     3     3   Can someone help me understand what is meant by an \"axis\" in pandas/numpy/scipy?  A side note, DataFrame.mean just might be defined wrong. It says in the documentation for DataFrame.mean that axis=1 is supposed to mean a mean over the columns, not the rows...     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Ambiguity in Pandas Dataframe / Numpy Array axis definition",
        "A_Content": "  Another way to explain:  // Not realistic but ideal for understanding the axis parameter  df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]],                   columns=[\"idx1\", \"idx2\", \"idx3\", \"idx4\"],                   index=[\"idx1\", \"idx2\", \"idx3\"]                  )  ---------------------------------------1 |          idx1  idx2  idx3  idx4 |    idx1     1     1     1     1 |    idx2     2     2     2     2 |    idx3     3     3     3     3 0   About df.drop (axis means the position)  A: I wanna remove idx3. B: **Which one**? // typing while waiting response: df.drop(\"idx3\", A: The one which is on axis 1 B: OK then it is >> df.drop(\"idx3\", axis=1)  // Result ---------------------------------------1 |          idx1  idx2     idx4 |    idx1     1     1     1 |    idx2     2     2     2 |    idx3     3     3     3 0   About df.apply (axis means direction)  A: I wanna apply sum. B: Which direction? // typing while waiting response: df.apply(lambda x: x.sum(), A: The one which is on *parallel to axis 0* B: OK then it is >> df.apply(lambda x: x.sum(), axis=0)  // Result idx1    6 idx2    6 idx3    6 idx4    6      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/25773245/ambiguity-in-pandas-dataframe-numpy-array-axis-definition",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:  >>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[\"col1\", \"col2\", \"col3\", \"col4\"]) >>> df    col1  col2  col3  col4 0     1     1     1     1 1     2     2     2     2 2     3     3     3     3   So if we call df.mean(axis=1), we'll get a mean across the rows:  >>> df.mean(axis=1) 0    1 1    2 2    3   However, if we call df.drop(name, axis=1), we actually drop a column, not a row:  >>> df.drop(\"col4\", axis=1)    col1  col2  col3 0     1     1     1 1     2     2     2 2     3     3     3   Can someone help me understand what is meant by an \"axis\" in pandas/numpy/scipy?  A side note, DataFrame.mean just might be defined wrong. It says in the documentation for DataFrame.mean that axis=1 is supposed to mean a mean over the columns, not the rows...     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Ambiguity in Pandas Dataframe / Numpy Array axis definition",
        "A_Content": "  There are already right answers, but I give you another example with > 2 dimensions.   The parameter axis means axis to be changed. For example, consider that there is a dataframe with dimension a x b x c.     df.mean(axis=1) returns a dataframe with dimenstion a x 1 x c.  df.drop(\"col4\", axis=1) returns a dataframe with dimension a x (b-1) x c.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/25773245/ambiguity-in-pandas-dataframe-numpy-array-axis-definition",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:  >>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[\"col1\", \"col2\", \"col3\", \"col4\"]) >>> df    col1  col2  col3  col4 0     1     1     1     1 1     2     2     2     2 2     3     3     3     3   So if we call df.mean(axis=1), we'll get a mean across the rows:  >>> df.mean(axis=1) 0    1 1    2 2    3   However, if we call df.drop(name, axis=1), we actually drop a column, not a row:  >>> df.drop(\"col4\", axis=1)    col1  col2  col3 0     1     1     1 1     2     2     2 2     3     3     3   Can someone help me understand what is meant by an \"axis\" in pandas/numpy/scipy?  A side note, DataFrame.mean just might be defined wrong. It says in the documentation for DataFrame.mean that axis=1 is supposed to mean a mean over the columns, not the rows...     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Ambiguity in Pandas Dataframe / Numpy Array axis definition",
        "A_Content": "  It should be more widely known that the string aliases 'index' and 'columns' can be used in place of the integers 0/1. The aliases are much more explicit and help me remember how the calculations take place. Another alias for 'index' is 'rows'.  When axis='index' is used, then the calculations happen down the columns, which is confusing. But, I remember it as getting a result that is the same size as another row.  Let's get some data on the screen to see what I am talking about:  df = pd.DataFrame(np.random.rand(10, 4), columns=list('abcd'))           a         b         c         d 0  0.990730  0.567822  0.318174  0.122410 1  0.144962  0.718574  0.580569  0.582278 2  0.477151  0.907692  0.186276  0.342724 3  0.561043  0.122771  0.206819  0.904330 4  0.427413  0.186807  0.870504  0.878632 5  0.795392  0.658958  0.666026  0.262191 6  0.831404  0.011082  0.299811  0.906880 7  0.749729  0.564900  0.181627  0.211961 8  0.528308  0.394107  0.734904  0.961356 9  0.120508  0.656848  0.055749  0.290897   When we want to take the mean of all the columns, we use axis='index' to get the following:  df.mean(axis='index') a    0.562664 b    0.478956 c    0.410046 d    0.546366 dtype: float64   The same result would be gotten by:  df.mean() # default is axis=0 df.mean(axis=0) df.mean(axis='rows')   To get use an operation left to right on the rows, use axis='columns'. I remember it by thinking that an additional column may be added to my DataFrame:  df.mean(axis='columns') 0    0.499784 1    0.506596 2    0.478461 3    0.448741 4    0.590839 5    0.595642 6    0.512294 7    0.427054 8    0.654669 9    0.281000 dtype: float64   The same result would be gotten by:  df.mean(axis=1)     Add a new row with axis=0/index/rows  Let's use these results to add additional rows or columns to complete the explanation. So, whenever using axis = 0/index/rows, its like getting a new row of the DataFrame. Let's add a row:  df.append(df.mean(axis='rows'), ignore_index=True)             a         b         c         d 0   0.990730  0.567822  0.318174  0.122410 1   0.144962  0.718574  0.580569  0.582278 2   0.477151  0.907692  0.186276  0.342724 3   0.561043  0.122771  0.206819  0.904330 4   0.427413  0.186807  0.870504  0.878632 5   0.795392  0.658958  0.666026  0.262191 6   0.831404  0.011082  0.299811  0.906880 7   0.749729  0.564900  0.181627  0.211961 8   0.528308  0.394107  0.734904  0.961356 9   0.120508  0.656848  0.055749  0.290897 10  0.562664  0.478956  0.410046  0.546366     Add a new column with axis=1/columns  Similarly, when axis=1/columns it will create data that can be easily made into its own column:  df.assign(e=df.mean(axis='columns'))            a         b         c         d         e 0  0.990730  0.567822  0.318174  0.122410  0.499784 1  0.144962  0.718574  0.580569  0.582278  0.506596 2  0.477151  0.907692  0.186276  0.342724  0.478461 3  0.561043  0.122771  0.206819  0.904330  0.448741 4  0.427413  0.186807  0.870504  0.878632  0.590839 5  0.795392  0.658958  0.666026  0.262191  0.595642 6  0.831404  0.011082  0.299811  0.906880  0.512294 7  0.749729  0.564900  0.181627  0.211961  0.427054 8  0.528308  0.394107  0.734904  0.961356  0.654669 9  0.120508  0.656848  0.055749  0.290897  0.281000   It appears that you can see all the aliases with the following private variables:  df._AXIS_ALIASES {'rows': 0}  df._AXIS_NUMBERS {'columns': 1, 'index': 0}  df._AXIS_NAMES {0: 'index', 1: 'columns'}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "pandas",
            "numpy",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/25773245/ambiguity-in-pandas-dataframe-numpy-array-axis-definition",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:  >>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[\"col1\", \"col2\", \"col3\", \"col4\"]) >>> df    col1  col2  col3  col4 0     1     1     1     1 1     2     2     2     2 2     3     3     3     3   So if we call df.mean(axis=1), we'll get a mean across the rows:  >>> df.mean(axis=1) 0    1 1    2 2    3   However, if we call df.drop(name, axis=1), we actually drop a column, not a row:  >>> df.drop(\"col4\", axis=1)    col1  col2  col3 0     1     1     1 1     2     2     2 2     3     3     3   Can someone help me understand what is meant by an \"axis\" in pandas/numpy/scipy?  A side note, DataFrame.mean just might be defined wrong. It says in the documentation for DataFrame.mean that axis=1 is supposed to mean a mean over the columns, not the rows...     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Numpy where function multiple conditions",
        "A_Content": "  The best way in your particular case would just be to change your two criteria to one criterion:  dists[abs(dists - r - dr/2.) <= dr/2.]   It only creates one boolean array, and in my opinion is easier to read because it says, is dist within a dr or r? (Though I'd redefine r to be the center of your region of interest instead of the beginning, so r = r + dr/2.)  But that doesn't answer your question.    The answer to your question: You don't actually need where if you're just trying to filter out the elements of dists that don't fit your criteria:  dists[(dists >= r) & (dists <= r+dr)]   Because the & will give you an elementwise and (the parentheses are necessary).  Or, if you do want to use where for some reason, you can do:   dists[(np.where((dists >= r) & (dists <= r + dr)))]     Why: The reason it doesn't work is because np.where returns a list of indices, not a boolean array.  You're trying to get and between two lists of numbers, which of course doesn't have the True/False values that you expect.  If a and b are both True values, then a and b returns b.  So saying something like [0,1,2] and [2,3,4] will just give you [2,3,4].  Here it is in action:  In [230]: dists = np.arange(0,10,.5) In [231]: r = 5 In [232]: dr = 1  In [233]: np.where(dists >= r) Out[233]: (array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),)  In [234]: np.where(dists <= r+dr) Out[234]: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]),)  In [235]: np.where(dists >= r) and np.where(dists <= r+dr) Out[235]: (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]),)   What you were expecting to compare was simply the boolean array, for example  In [236]: dists >= r Out[236]:  array([False, False, False, False, False, False, False, False, False,        False,  True,  True,  True,  True,  True,  True,  True,  True,         True,  True], dtype=bool)  In [237]: dists <= r + dr Out[237]:  array([ True,  True,  True,  True,  True,  True,  True,  True,  True,         True,  True,  True,  True, False, False, False, False, False,        False, False], dtype=bool)  In [238]: (dists >= r) & (dists <= r + dr) Out[238]:  array([False, False, False, False, False, False, False, False, False,        False,  True,  True,  True, False, False, False, False, False,        False, False], dtype=bool)   Now you can call np.where on the combined boolean array:  In [239]: np.where((dists >= r) & (dists <= r + dr)) Out[239]: (array([10, 11, 12]),)  In [240]: dists[np.where((dists >= r) & (dists <= r + dr))] Out[240]: array([ 5. ,  5.5,  6. ])   Or simply index the original array with the boolean array using fancy indexing  In [241]: dists[(dists >= r) & (dists <= r + dr)] Out[241]: array([ 5. ,  5.5,  6. ])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions",
        "A_Votes": "124",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:   dists[(np.where(dists >= r)) and (np.where(dists <= r + dr))]   However this selects only for the condition    (np.where(dists <= r + dr))   If I do the commands sequentially by using a temporary variable it works fine. Why does the above code not work, and how do I get it to work?   Cheers     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Numpy where function multiple conditions",
        "A_Content": "  Since the accepted answer explained the problem very well. you can also use numpy logical functions which is more suitable here for multiple condition :  np.where(np.logical_and(np.greater_equal(dists,r),np.greater_equal(dists,r + dr)))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:   dists[(np.where(dists >= r)) and (np.where(dists <= r + dr))]   However this selects only for the condition    (np.where(dists <= r + dr))   If I do the commands sequentially by using a temporary variable it works fine. Why does the above code not work, and how do I get it to work?   Cheers     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Numpy where function multiple conditions",
        "A_Content": "  Try:  np.intersect1d(np.where(dists >= r)[0],np.where(dists <= r + dr)[0])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:   dists[(np.where(dists >= r)) and (np.where(dists <= r + dr))]   However this selects only for the condition    (np.where(dists <= r + dr))   If I do the commands sequentially by using a temporary variable it works fine. Why does the above code not work, and how do I get it to work?   Cheers     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Numpy where function multiple conditions",
        "A_Content": "  I like to use np.vectorize for such tasks. Consider the following:  >>> # function which returns True when constraints are satisfied. >>> func = lambda d: d >= r and d<= (r+dr)  >>> >>> # Apply constraints element-wise to the dists array. >>> result = np.vectorize(func)(dists)  >>> >>> result = np.where(result) # Get output.   You can also use np.argwhere instead of np.where for clear output. But that is your call :)  Hope it helps.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:   dists[(np.where(dists >= r)) and (np.where(dists <= r + dr))]   However this selects only for the condition    (np.where(dists <= r + dr))   If I do the commands sequentially by using a temporary variable it works fine. Why does the above code not work, and how do I get it to work?   Cheers     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Numpy where function multiple conditions",
        "A_Content": "  I have worked out this simple example   import numpy as np  ar = np.array([3,4,5,14,2,4,3,7])  print [X for X in list(ar) if (X >= 3 and X <= 6)]  >>>  [3, 4, 5, 4, 3]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:   dists[(np.where(dists >= r)) and (np.where(dists <= r + dr))]   However this selects only for the condition    (np.where(dists <= r + dr))   If I do the commands sequentially by using a temporary variable it works fine. Why does the above code not work, and how do I get it to work?   Cheers     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  A regular expression will do the trick with very little code:  import re  ...  if re.match(\"^[A-Za-z0-9_-]*$\", my_little_string):     # do something here      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "103",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  [Edit]  There's another solution not mentioned yet, and it seems to outperform the others given so far in most cases.  Use string.translate to replace all valid characters in the string, and see if we have any invalid ones left over.  This is pretty fast as it uses the underlying C function to do the work, with very little python bytecode involved.  Obviously performance isn't everything - going for the most readable solutions is probably the best approach when not in a performance critical codepath, but just to see how the solutions stack up, here's a performance comparison of all the methods proposed so far.  check_trans is the one using the string.translate method.  Test code:  import string, re, timeit  pat = re.compile('[\\w-]*$') pat_inv = re.compile ('[^\\w-]') allowed_chars=string.ascii_letters + string.digits + '_-' allowed_set = set(allowed_chars) trans_table = string.maketrans('','')  def check_set_diff(s):     return not set(s) - allowed_set  def check_set_all(s):     return all(x in allowed_set for x in s)  def check_set_subset(s):     return set(s).issubset(allowed_set)  def check_re_match(s):     return pat.match(s)  def check_re_inverse(s): # Search for non-matching character.     return not pat_inv.search(s)  def check_trans(s):     return not s.translate(trans_table,allowed_chars)  test_long_almost_valid='a_very_long_string_that_is_mostly_valid_except_for_last_char'*99 + '!' test_long_valid='a_very_long_string_that_is_completely_valid_' * 99 test_short_valid='short_valid_string' test_short_invalid='/$%$%&' test_long_invalid='/$%$%&' * 99 test_empty=''  def main():     funcs = sorted(f for f in globals() if f.startswith('check_'))     tests = sorted(f for f in globals() if f.startswith('test_'))     for test in tests:         print \"Test %-15s (length = %d):\" % (test, len(globals()[test]))         for func in funcs:             print \"  %-20s : %.3f\" % (func,                     timeit.Timer('%s(%s)' % (func, test), 'from __main__ import pat,allowed_set,%s' % ','.join(funcs+tests)).timeit(10000))         print  if __name__=='__main__': main()   The results on my system are:  Test test_empty      (length = 0):   check_re_inverse     : 0.042   check_re_match       : 0.030   check_set_all        : 0.027   check_set_diff       : 0.029   check_set_subset     : 0.029   check_trans          : 0.014  Test test_long_almost_valid (length = 5941):   check_re_inverse     : 2.690   check_re_match       : 3.037   check_set_all        : 18.860   check_set_diff       : 2.905   check_set_subset     : 2.903   check_trans          : 0.182  Test test_long_invalid (length = 594):   check_re_inverse     : 0.017   check_re_match       : 0.015   check_set_all        : 0.044   check_set_diff       : 0.311   check_set_subset     : 0.308   check_trans          : 0.034  Test test_long_valid (length = 4356):   check_re_inverse     : 1.890   check_re_match       : 1.010   check_set_all        : 14.411   check_set_diff       : 2.101   check_set_subset     : 2.333   check_trans          : 0.140  Test test_short_invalid (length = 6):   check_re_inverse     : 0.017   check_re_match       : 0.019   check_set_all        : 0.044   check_set_diff       : 0.032   check_set_subset     : 0.037   check_trans          : 0.015  Test test_short_valid (length = 18):   check_re_inverse     : 0.125   check_re_match       : 0.066   check_set_all        : 0.104   check_set_diff       : 0.051   check_set_subset     : 0.046   check_trans          : 0.017   The translate approach seems best in most cases, dramatically so with long valid strings, but is beaten out by regexes in test_long_invalid (Presumably because the regex can bail out immediately, but translate always has to scan the whole string).  The set approaches are usually worst, beating regexes only for the empty string case.  Using all(x in allowed_set for x in s) performs well if it bails out early, but can be bad if it has to iterate through every character.  isSubSet and set difference are comparable, and are consistently proportional to the length of the string regardless of the data.  There's a similar difference between the regex methods matching all valid characters and searching for invalid characters.  Matching performs a little better when checking for a long, but fully valid string, but worse for invalid characters near the end of the string.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  There are a variety of ways of achieving this goal, some are clearer than others. For each of my examples, 'True' means that the string passed is valid, 'False' means it contains invalid characters.  First of all, there's the naive approach:  import string allowed = string.letters + string.digits + '_' + '-'  def check_naive(mystring):     return all(c in allowed for c in mystring)   Then there's use of a regular expression, you can do this with re.match(). Note that '-' has to be at the end of the [] otherwise it will be used as a 'range' delimiter. Also note the $ which means 'end of string'. Other answers noted in this question use a special character class, '\\w', I always prefer using an explicit character class range using [] because it is easier to understand without having to look up a quick reference guide, and easier to special-case.  import re CHECK_RE = re.compile('[a-zA-Z0-9_-]+$') def check_re(mystring):     return CHECK_RE.match(mystring)   Another solution noted that you can do an inverse match with regular expressions, I've included that here now. Note that [^...] inverts the character class because the ^ is used:  CHECK_INV_RE = re.compile('[^a-zA-Z0-9_-]') def check_inv_re(mystring):    return not CHECK_INV_RE.search(mystring)   You can also do something tricky with the 'set' object. Have a look at this example, which removes from the original string all the characters that are allowed, leaving us with a set containing either a) nothing, or b) the offending characters from the string:  def check_set(mystring):     return not set(mystring) - set(allowed)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  If it were not for the dashes and underscores, the easiest solution would be  my_little_string.isalnum()   (Section 3.6.1 of the Python Library Reference)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  As an alternative to using regex you could do it in Sets:  from sets import Set  allowed_chars = Set('0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_-')  if Set(my_little_sting).issubset(allowed_chars):     # your action     print True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "   pat = re.compile ('[^\\w-]')   def onlyallowed(s):     return not pat.search (s)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  Well you can ask the help of regex, the great in here :)  code:  import re  string = 'adsfg34wrtwe4r2_()' #your string that needs to be matched. regex = r'^[\\w\\d_()]*$' # you can also add a space in regex if u want to allow it in the string   if re.match(regex,string):     print 'yes' else:      print 'false'   Output:  yes     Hope this helps :)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  You could always use a list comprehension and check the results with all, it would be a little less resource intensive than using a regex: all([c in string.letters + string.digits + [\"_\", \"-\"] for c in mystring])     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  Here's something based on Jerub's \"naive approach\" (naive being his words, not mine!):  import string ALLOWED = frozenset(string.ascii_letters + string.digits + '_' + '-')  def check(mystring):     return all(c in ALLOWED for c in mystring)   If ALLOWED was a string then I think c in ALLOWED would involve iterating over each character in the string until it found a match or reached the end.  Which, to quote Joel Spolsky, is something of a Shlemiel the Painter algorithm.  But testing for existence in a set should be more efficient, or at least less dependent on the number of allowed characters.  Certainly this approach is a little bit faster on my machine.  It's clear and I think it performs plenty well enough for most cases (on my slow machine I can validate tens of thousands of short-ish strings in a fraction of a second).  I like it.  ACTUALLY on my machine a regexp works out several times faster, and is just as simple as this (arguably simpler).  So that probably is the best way forward.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How do I verify that a string only contains letters, numbers, underscores and dashes?",
        "A_Content": "  use a regex and see if it matches!  ([a-z][A-Z][0-9]\\_\\-)*      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/89909/how-do-i-verify-that-a-string-only-contains-letters-numbers-underscores-and-da",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know how to do this if I iterate through all of the characters in the string but I am looking for a more elegant method.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "bash: mkvirtualenv: command not found",
        "A_Content": "  Solution 1:  For some reason, virtualenvwrapper.sh installed in /usr/bin/virtualenvwrapper.sh, instead of under /usr/local/bin.  The following in my .bash_profile works...  source \"/usr/bin/virtualenvwrapper.sh\" export WORKON_HOME=\"/opt/virtual_env/\"   My install seems to work fine without sourcing virtualenvwrapper_bashrc  Solution 2:  Alternatively as mentioned below, you could leverage the chance that virtualenvwrapper.sh is already in your shell's PATH and just issue a source `which virtualenvwrapper.sh`      ",
        "Language": "Python",
        "Tags": [
            "python",
            "bash",
            "centos",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/13855463/bash-mkvirtualenv-command-not-found",
        "A_Votes": "78",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.  [mpenning@tsunami ~]$ mkvirtualenv test -bash: mkvirtualenv: command not found [mpenning@tsunami ~]$   It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.  I'm running CentOS 6 and python 2.6.6, if this matters.    # File: ~/.bash_profile # ...  export WORKON_HOME=\"/opt/virtual_env/\" source \"/opt/virtual_env/bin/virtualenvwrapper_bashrc\"      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "bash: mkvirtualenv: command not found",
        "A_Content": "  I had the same issue on OS X 10.9.1 with python 2.7.5.  No issues with WORKON_HOME for me, but I did have to manually add source \"/usr/local/bin/virtualenvwrapper.sh\" to ~/.bash_profile (or ~/.bashrc in unix) after I ran pip install virtualenvwrapper     ",
        "Language": "Python",
        "Tags": [
            "python",
            "bash",
            "centos",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/13855463/bash-mkvirtualenv-command-not-found",
        "A_Votes": "35",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.  [mpenning@tsunami ~]$ mkvirtualenv test -bash: mkvirtualenv: command not found [mpenning@tsunami ~]$   It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.  I'm running CentOS 6 and python 2.6.6, if this matters.    # File: ~/.bash_profile # ...  export WORKON_HOME=\"/opt/virtual_env/\" source \"/opt/virtual_env/bin/virtualenvwrapper_bashrc\"      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "bash: mkvirtualenv: command not found",
        "A_Content": "  Try:  source `which virtualenvwrapper.sh`      ",
        "Language": "Python",
        "Tags": [
            "python",
            "bash",
            "centos",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/13855463/bash-mkvirtualenv-command-not-found",
        "A_Votes": "29",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.  [mpenning@tsunami ~]$ mkvirtualenv test -bash: mkvirtualenv: command not found [mpenning@tsunami ~]$   It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.  I'm running CentOS 6 and python 2.6.6, if this matters.    # File: ~/.bash_profile # ...  export WORKON_HOME=\"/opt/virtual_env/\" source \"/opt/virtual_env/bin/virtualenvwrapper_bashrc\"      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "bash: mkvirtualenv: command not found",
        "A_Content": "  Prerequisites to execute this command -  1) pip (recursive acronym of Pip Install Python)  is a package management system used to install and manage software packages written in Python. Many packages can be found in the Python Package Index (PyPI).  sudo apt-get install python-pip   2) Install Virtual Environment. Used to create virtual environment, to install packages and dependencies of multiple projects isolated from each other.  sudo pip install virtualenv   3) Install virtual environment wrapper About virtual env wrapper  sudo pip install virtualenvwrapper   After Installing prerequisites you need to bring virtual environment wrapper into action to create virtual environment. Following are the steps -  1) set virtual environment directory in path variable- export WORKON_HOME=(directory you need to save envs)  2) source /usr/local/bin/virtualenvwrapper.sh -p $WORKON_HOME  As mentioned by @Mike, source `which virtualenvwrapper.sh` or which virtualenvwrapper.sh can used to locate virtualenvwrapper.sh file.  It's best to put above two lines in ~/.bashrc to avoid executing the above commands every time you open new shell. That's all you need to create environment using mkvirtualenv  Points to keep in mind -   Under Ubuntu, you may need install virtualenv and virtualenvwrapper as root. Simply prefix the command above with sudo. Depending on the process used to install virtualenv, the path to virtualenvwrapper.sh may vary. Find the appropriate path by running $ find /usr -name virtualenvwrapper.sh. Adjust the line in your .bash_profile or .bashrc script accordingly.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "bash",
            "centos",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/13855463/bash-mkvirtualenv-command-not-found",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.  [mpenning@tsunami ~]$ mkvirtualenv test -bash: mkvirtualenv: command not found [mpenning@tsunami ~]$   It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.  I'm running CentOS 6 and python 2.6.6, if this matters.    # File: ~/.bash_profile # ...  export WORKON_HOME=\"/opt/virtual_env/\" source \"/opt/virtual_env/bin/virtualenvwrapper_bashrc\"      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "bash: mkvirtualenv: command not found",
        "A_Content": "  On Windows 7 and Git Bash this helps me:   Create a ~/.bashrc file (under your user home folder) Add line export WORKON_HOME=$HOME/.virtualenvs (you must create this folder if it doesn't exist) Add line source \"C:\\Program Files (x86)\\Python36-32\\Scripts\\virtualenvwrapper.sh\" (change path for your virtualenvwrapper.sh)   Restart your git bash and mkvirtualenv command now will work nicely.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "bash",
            "centos",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/13855463/bash-mkvirtualenv-command-not-found",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.  [mpenning@tsunami ~]$ mkvirtualenv test -bash: mkvirtualenv: command not found [mpenning@tsunami ~]$   It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.  I'm running CentOS 6 and python 2.6.6, if this matters.    # File: ~/.bash_profile # ...  export WORKON_HOME=\"/opt/virtual_env/\" source \"/opt/virtual_env/bin/virtualenvwrapper_bashrc\"      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "bash: mkvirtualenv: command not found",
        "A_Content": "  Using Git Bash on Windows 10 and Python36 for Windows I found the virtualenvwrapper.sh in a slightly different place and running this resolved the issue  source virtualenvwrapper.sh  /c/users/[myUserName]/AppData/Local/Programs/Python36/Scripts      ",
        "Language": "Python",
        "Tags": [
            "python",
            "bash",
            "centos",
            "virtualenv",
            "virtualenvwrapper"
        ],
        "URL": "https://stackoverflow.com/questions/13855463/bash-mkvirtualenv-command-not-found",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    After following the instructions on Doug Hellman's virtualenvwrapper post, I still could not fire up a test environment.  [mpenning@tsunami ~]$ mkvirtualenv test -bash: mkvirtualenv: command not found [mpenning@tsunami ~]$   It should be noted that I'm using WORKON_HOME that is not in my $HOME.  I tried looking for /usr/local/bin/virtualenvwrapper.sh as shown in the virtualenvwrapper installation docs, but it does not exist.  I'm running CentOS 6 and python 2.6.6, if this matters.    # File: ~/.bash_profile # ...  export WORKON_HOME=\"/opt/virtual_env/\" source \"/opt/virtual_env/bin/virtualenvwrapper_bashrc\"      ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python Request Post with param data",
        "A_Content": "  params is for GET-style URL parameters, data is for POST-style body information. It is perfectly legal to provide both types of information in a request, and your request does so too, but you encoded the URL parameters into the URL already.  Your raw post contains JSON data though, you better use the json module to properly encode that:  import json  data = {\"eventType\": \"AAS_PORTAL_START\", \"data\": {\"uid\": \"hfe3hf45huf33545\", \"aid\": \"1\", \"vid\": \"1\"}} data = json.dumps(data)   You could split out the URL parameters too:  params = {'sessionKey': '9ebbd0b25760557393a43064a92bae539d962103', 'format': 'xml', 'platformId': 1}   Then post this with:  import requests import json  headers = {'content-type': 'application/json'} url = 'http://192.168.3.45:8080/api/v2/event/log'  data = {\"eventType\": \"AAS_PORTAL_START\", \"data\": {\"uid\": \"hfe3hf45huf33545\", \"aid\": \"1\", \"vid\": \"1\"}} params = {'sessionKey': '9ebbd0b25760557393a43064a92bae539d962103', 'format': 'xml', 'platformId': 1}  requests.post(url, params=params, data=json.dumps(data), headers=headers)     Edit  If you are using requests version 2.4.2 or newer, you can have the library do the JSON encoding for you; it'll set the correct Content-Header too; all you need to do is pass in the Python object to be encoded as JSON into the json keyword argument:  import requests  url = 'http://192.168.3.45:8080/api/v2/event/log'  data = {\"eventType\": \"AAS_PORTAL_START\", \"data\": {\"uid\": \"hfe3hf45huf33545\", \"aid\": \"1\", \"vid\": \"1\"}} params = {'sessionKey': '9ebbd0b25760557393a43064a92bae539d962103', 'format': 'xml', 'platformId': 1}  requests.post(url, params=params, json=data)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "httprequest",
            "python-requests",
            "http-status-codes"
        ],
        "URL": "https://stackoverflow.com/questions/15900338/python-request-post-with-param-data",
        "A_Votes": "144",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    This is the raw request for an API call:  POST http://192.168.3.45:8080/api/v2/event/log?sessionKey=b299d17b896417a7b18f46544d40adb734240cc2&format=json HTTP/1.1 Accept-Encoding: gzip,deflate Content-Type: application/json Content-Length: 86 Host: 192.168.3.45:8080 Connection: Keep-Alive User-Agent: Apache-HttpClient/4.1.1 (java 1.5)  {\"eventType\":\"AAS_PORTAL_START\",\"data\":{\"uid\":\"hfe3hf45huf33545\",\"aid\":\"1\",\"vid\":\"1\"}}\"\"\"   This request returns a success (2xx) response.  Now I am trying to post this request using requests:  >>> import requests >>> headers = {'content-type' : 'application/json'} >>> data ={\"eventType\":\"AAS_PORTAL_START\",\"data{\"uid\":\"hfe3hf45huf33545\",\"aid\":\"1\",\"vid\":\"1\"}} >>> url = \"http://192.168.3.45:8080/api/v2/event/log?sessionKey=9ebbd0b25760557393a43064a92bae539d962103&format=xml&platformId=1\" >>> requests.post(url,params=data,headers=headers) <Response [400]>   Everything looks fine to me and I am not quite sure what I posting wrong to get a 400 response.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python Request Post with param data",
        "A_Content": "  Set data to this:   data ={\"eventType\":\"AAS_PORTAL_START\",\"data\":{\"uid\":\"hfe3hf45huf33545\",\"aid\":\"1\",\"vid\":\"1\"}}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "httprequest",
            "python-requests",
            "http-status-codes"
        ],
        "URL": "https://stackoverflow.com/questions/15900338/python-request-post-with-param-data",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is the raw request for an API call:  POST http://192.168.3.45:8080/api/v2/event/log?sessionKey=b299d17b896417a7b18f46544d40adb734240cc2&format=json HTTP/1.1 Accept-Encoding: gzip,deflate Content-Type: application/json Content-Length: 86 Host: 192.168.3.45:8080 Connection: Keep-Alive User-Agent: Apache-HttpClient/4.1.1 (java 1.5)  {\"eventType\":\"AAS_PORTAL_START\",\"data\":{\"uid\":\"hfe3hf45huf33545\",\"aid\":\"1\",\"vid\":\"1\"}}\"\"\"   This request returns a success (2xx) response.  Now I am trying to post this request using requests:  >>> import requests >>> headers = {'content-type' : 'application/json'} >>> data ={\"eventType\":\"AAS_PORTAL_START\",\"data{\"uid\":\"hfe3hf45huf33545\",\"aid\":\"1\",\"vid\":\"1\"}} >>> url = \"http://192.168.3.45:8080/api/v2/event/log?sessionKey=9ebbd0b25760557393a43064a92bae539d962103&format=xml&platformId=1\" >>> requests.post(url,params=data,headers=headers) <Response [400]>   Everything looks fine to me and I am not quite sure what I posting wrong to get a 400 response.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python Request Post with param data",
        "A_Content": "  Assign the response to a value and test the attributes of it. These should tell you something useful.  response = requests.post(url,params=data,headers=headers) response.status_code response.text    status_code should just reconfirm the code you were given before, of course      ",
        "Language": "Python",
        "Tags": [
            "python",
            "httprequest",
            "python-requests",
            "http-status-codes"
        ],
        "URL": "https://stackoverflow.com/questions/15900338/python-request-post-with-param-data",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is the raw request for an API call:  POST http://192.168.3.45:8080/api/v2/event/log?sessionKey=b299d17b896417a7b18f46544d40adb734240cc2&format=json HTTP/1.1 Accept-Encoding: gzip,deflate Content-Type: application/json Content-Length: 86 Host: 192.168.3.45:8080 Connection: Keep-Alive User-Agent: Apache-HttpClient/4.1.1 (java 1.5)  {\"eventType\":\"AAS_PORTAL_START\",\"data\":{\"uid\":\"hfe3hf45huf33545\",\"aid\":\"1\",\"vid\":\"1\"}}\"\"\"   This request returns a success (2xx) response.  Now I am trying to post this request using requests:  >>> import requests >>> headers = {'content-type' : 'application/json'} >>> data ={\"eventType\":\"AAS_PORTAL_START\",\"data{\"uid\":\"hfe3hf45huf33545\",\"aid\":\"1\",\"vid\":\"1\"}} >>> url = \"http://192.168.3.45:8080/api/v2/event/log?sessionKey=9ebbd0b25760557393a43064a92bae539d962103&format=xml&platformId=1\" >>> requests.post(url,params=data,headers=headers) <Response [400]>   Everything looks fine to me and I am not quite sure what I posting wrong to get a 400 response.     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python and regular expression with Unicode",
        "A_Content": "  Are you using python 2.x or 3.0?  If you're using 2.x, try making the regex string a unicode-escape string, with 'u'. Since it's regex it's good practice to make your regex string a raw string, with 'r'. Also, putting your entire pattern in parentheses is superfluous.  re.sub(ur'[\\u064B-\\u0652\\u06D4\\u0670\\u0674\\u06D5-\\u06ED]+', '', ...)   http://docs.python.org/tutorial/introduction.html#unicode-strings  Edit:  It's also good practice to use the re.UNICODE/re.U/(?u) flag for unicode regexes, but it only affects character class aliases like \\w or \\b, of which this pattern does not use any and so would not be affected by.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "character-properties"
        ],
        "URL": "https://stackoverflow.com/questions/393843/python-and-regular-expression-with-unicode",
        "A_Votes": "95",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I need to delete some Unicode symbols from the string '   '  I know they exist here for sure. I tried:  re.sub('([\\u064B-\\u0652\\u06D4\\u0670\\u0674\\u06D5-\\u06ED]+)', '', '   ')   but it doesn't work. String stays the same. What am I doing wrong?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "Python and regular expression with Unicode",
        "A_Content": "  Use unicode strings. Use the re.UNICODE flag.  >>> myre = re.compile(ur'[\\u064B-\\u0652\\u06D4\\u0670\\u0674\\u06D5-\\u06ED]+',                        re.UNICODE) >>> myre <_sre.SRE_Pattern object at 0xb20b378> >>> mystr = u'   ' >>> result = myre.sub('', mystr) >>> len(mystr), len(result) (38, 22) >>> print result       Read the article by Joel Spolsky called The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "character-properties"
        ],
        "URL": "https://stackoverflow.com/questions/393843/python-and-regular-expression-with-unicode",
        "A_Votes": "62",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to delete some Unicode symbols from the string '   '  I know they exist here for sure. I tried:  re.sub('([\\u064B-\\u0652\\u06D4\\u0670\\u0674\\u06D5-\\u06ED]+)', '', '   ')   but it doesn't work. String stays the same. What am I doing wrong?     ",
        "Q_Votes": "72"
    },
    {
        "Q_Title": "How to keep index when using pandas merge",
        "A_Content": "  In [5]: a.reset_index().merge(b, how=\"left\").set_index('index') Out[5]:        col1  to_merge_on  col2 index a         1            1     1 b         2            3     2 c         3            4   NaN      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/11976503/how-to-keep-index-when-using-pandas-merge",
        "A_Votes": "94",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I would like to merge two DataFrames, and keep the index from the first frame as the index on the merged dataset.  However, when I do the merge, the resulting DataFrame has integer index.  How can I specify that I want to keep the index from the left data frame?  In [4]: a = pd.DataFrame({'col1': {'a': 1, 'b': 2, 'c': 3},                            'to_merge_on': {'a': 1, 'b': 3, 'c': 4}})  In [5]: b = pd.DataFrame({'col2': {0: 1, 1: 2, 2: 3},                            'to_merge_on': {0: 1, 1: 3, 2: 5}})  In [6]: a Out[6]:    col1  to_merge_on a     1            1 b     2            3 c     3            4  In [7]: b Out[7]:    col2  to_merge_on 0     1            1 1     2            3 2     3            5  In [8]: a.merge(b, how='left') Out[8]:    col1  to_merge_on  col2 0     1            1   1.0 1     2            3   2.0 2     3            4   NaN  In [9]: _.index Out[9]: Int64Index([0, 1, 2], dtype='int64')   EDIT: Switched to example code that can be easily reproduced     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to keep index when using pandas merge",
        "A_Content": "  There is a non-pd.merge solution. Using map and set_index  In [1744]: a.assign(col2=a['to_merge_on'].map(b.set_index('to_merge_on')['col2'])) Out[1744]:    col1  to_merge_on  col2 a     1            1   1.0 b     2            3   2.0 c     3            4   NaN   And, doesn't introduce a dummy index name for the index.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/11976503/how-to-keep-index-when-using-pandas-merge",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I would like to merge two DataFrames, and keep the index from the first frame as the index on the merged dataset.  However, when I do the merge, the resulting DataFrame has integer index.  How can I specify that I want to keep the index from the left data frame?  In [4]: a = pd.DataFrame({'col1': {'a': 1, 'b': 2, 'c': 3},                            'to_merge_on': {'a': 1, 'b': 3, 'c': 4}})  In [5]: b = pd.DataFrame({'col2': {0: 1, 1: 2, 2: 3},                            'to_merge_on': {0: 1, 1: 3, 2: 5}})  In [6]: a Out[6]:    col1  to_merge_on a     1            1 b     2            3 c     3            4  In [7]: b Out[7]:    col2  to_merge_on 0     1            1 1     2            3 2     3            5  In [8]: a.merge(b, how='left') Out[8]:    col1  to_merge_on  col2 0     1            1   1.0 1     2            3   2.0 2     3            4   NaN  In [9]: _.index Out[9]: Int64Index([0, 1, 2], dtype='int64')   EDIT: Switched to example code that can be easily reproduced     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Creating a new corpus with NLTK",
        "A_Content": "  I think the PlaintextCorpusReader already segments the input with a punkt tokenizer, at least if your input language is english.  PlainTextCorpusReader's constructor  def __init__(self, root, fileids,              word_tokenizer=WordPunctTokenizer(),              sent_tokenizer=nltk.data.LazyLoader(                  'tokenizers/punkt/english.pickle'),              para_block_reader=read_blankline_block,              encoding='utf8'):   You can pass the reader a word and sentence tokenizer, but for the latter the default already is nltk.data.LazyLoader('tokenizers/punkt/english.pickle').  For a single string, a tokenizer would be used as follows (explained here, see section 5 for punkt tokenizer).  >>> import nltk.data >>> text = \"\"\" ... Punkt knows that the periods in Mr. Smith and Johann S. Bach ... do not mark sentence boundaries.  And sometimes sentences ... can start with non-capitalized words.  i is a good variable ... name. ... \"\"\" >>> tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') >>> tokenizer.tokenize(text.strip())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "nlp",
            "nltk",
            "corpus"
        ],
        "URL": "https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I reckoned that often the answer to my title is to go and read the documentations, but I ran through the NLTK book but it doesn't give the answer. I'm kind of new to python.  I have a bunch of .txt files and I want to be able to use the corpus functions that NLTK provides for the corpus nltk_data.   I've tried PlaintextCorpusReader but I couldn't get further than:  >>>import nltk >>>from nltk.corpus import PlaintextCorpusReader >>>corpus_root = './' >>>newcorpus = PlaintextCorpusReader(corpus_root, '.*') >>>newcorpus.words()   How do I segment the newcorpus sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read PlaintextCorpusReader class?  Can you also lead me to how I can write the segmented data into text files?  Edit: This question had a bounty once, and it now has a second bounty. See text in bounty box.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Creating a new corpus with NLTK",
        "A_Content": "  After some years of figuring out how it works, here's the updated tutorial of   How to create an NLTK corpus with a directory of textfiles?  The main idea is to make use of the nltk.corpus.reader package. In the case that you have a directory of textfiles in English, it's best to use the PlaintextCorpusReader.   If you have a directory that looks like this:  newcorpus/          file1.txt          file2.txt          ...   Simply use these lines of code and you can get a corpus:  import os from nltk.corpus.reader.plaintext import PlaintextCorpusReader  corpusdir = 'newcorpus/' # Directory of corpus.  newcorpus = PlaintextCorpusReader(corpusdir, '.*')   NOTE: that the PlaintextCorpusReader will use the default nltk.tokenize.sent_tokenize() and nltk.tokenize.word_tokenize() to split your texts into sentences and words and these functions are build for English, it may NOT work for all languages.  Here's the full code with creation of test textfiles and how to create a corpus with NLTK and how to access the corpus at different levels:  import os from nltk.corpus.reader.plaintext import PlaintextCorpusReader  # Let's create a corpus with 2 texts in different textfile. txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\" txt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\" corpus = [txt1,txt2]  # Make new dir for the corpus. corpusdir = 'newcorpus/' if not os.path.isdir(corpusdir):     os.mkdir(corpusdir)  # Output the files into the directory. filename = 0 for text in corpus:     filename+=1     with open(corpusdir+str(filename)+'.txt','w') as fout:         print>>fout, text  # Check that our corpus do exist and the files are correct. assert os.path.isdir(corpusdir) for infile, text in zip(sorted(os.listdir(corpusdir)),corpus):     assert open(corpusdir+infile,'r').read().strip() == text.strip()   # Create a new corpus by specifying the parameters # (1) directory of the new corpus # (2) the fileids of the corpus # NOTE: in this case the fileids are simply the filenames. newcorpus = PlaintextCorpusReader('newcorpus/', '.*')  # Access each file in the corpus. for infile in sorted(newcorpus.fileids()):     print infile # The fileids of each file.     with newcorpus.open(infile) as fin: # Opens the file.         print fin.read().strip() # Prints the content of the file print  # Access the plaintext; outputs pure string/basestring. print newcorpus.raw().strip() print   # Access paragraphs in the corpus. (list of list of list of strings) # NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and  #       nltk.tokenize.word_tokenize. # # Each element in the outermost list is a paragraph, and # Each paragraph contains sentence(s), and # Each sentence contains token(s) print newcorpus.paras() print  # To access pargraphs of a specific fileid. print newcorpus.paras(newcorpus.fileids()[0])  # Access sentences in the corpus. (list of list of strings) # NOTE: That the texts are flattened into sentences that contains tokens. print newcorpus.sents() print  # To access sentences of a specific fileid. print newcorpus.sents(newcorpus.fileids()[0])  # Access just tokens/words in the corpus. (list of strings) print newcorpus.words()  # To access tokens of a specific fileid. print newcorpus.words(newcorpus.fileids()[0])   Finally, to read a directory of texts and create an NLTK corpus in another languages, you must first ensure that you have a python-callable word tokenization and sentence tokenization modules that takes string/basestring input and produces such output:  >>> from nltk.tokenize import sent_tokenize, word_tokenize >>> txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\" >>> sent_tokenize(txt1) ['This is a foo bar sentence.', 'And this is the first txtfile in the corpus.'] >>> word_tokenize(sent_tokenize(txt1)[0]) ['This', 'is', 'a', 'foo', 'bar', 'sentence', '.']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "nlp",
            "nltk",
            "corpus"
        ],
        "URL": "https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk",
        "A_Votes": "52",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I reckoned that often the answer to my title is to go and read the documentations, but I ran through the NLTK book but it doesn't give the answer. I'm kind of new to python.  I have a bunch of .txt files and I want to be able to use the corpus functions that NLTK provides for the corpus nltk_data.   I've tried PlaintextCorpusReader but I couldn't get further than:  >>>import nltk >>>from nltk.corpus import PlaintextCorpusReader >>>corpus_root = './' >>>newcorpus = PlaintextCorpusReader(corpus_root, '.*') >>>newcorpus.words()   How do I segment the newcorpus sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read PlaintextCorpusReader class?  Can you also lead me to how I can write the segmented data into text files?  Edit: This question had a bounty once, and it now has a second bounty. See text in bounty box.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Creating a new corpus with NLTK",
        "A_Content": "   >>> import nltk  >>> from nltk.corpus import PlaintextCorpusReader  >>> corpus_root = './'  >>> newcorpus = PlaintextCorpusReader(corpus_root, '.*')  \"\"\"  if the ./ dir contains the file my_corpus.txt, then you   can view say all the words it by doing this   \"\"\"  >>> newcorpus.words('my_corpus.txt')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "nlp",
            "nltk",
            "corpus"
        ],
        "URL": "https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I reckoned that often the answer to my title is to go and read the documentations, but I ran through the NLTK book but it doesn't give the answer. I'm kind of new to python.  I have a bunch of .txt files and I want to be able to use the corpus functions that NLTK provides for the corpus nltk_data.   I've tried PlaintextCorpusReader but I couldn't get further than:  >>>import nltk >>>from nltk.corpus import PlaintextCorpusReader >>>corpus_root = './' >>>newcorpus = PlaintextCorpusReader(corpus_root, '.*') >>>newcorpus.words()   How do I segment the newcorpus sentences using punkt? I tried using the punkt functions but the punkt functions couldn't read PlaintextCorpusReader class?  Can you also lead me to how I can write the segmented data into text files?  Edit: This question had a bounty once, and it now has a second bounty. See text in bounty box.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Can modules have properties the same way that objects can?",
        "A_Content": "  Only instances of new-style classes can have properties.  You can make Python believe such an instance is a module by stashing it in sys.modules[thename] = theinstance.  So, for example, your m.py module file could be:  import sys class _M(object):   def __init__(self):     self.c = 0   def afunction(self):     self.c += 1     return self.c   y = property(afunction) sys.modules[__name__] = _M()   Edited: removed an implicit dependency on globals (had nothing to do with the point of the example but did confuse things by making the original code fail!).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "properties",
            "python-module"
        ],
        "URL": "https://stackoverflow.com/questions/880530/can-modules-have-properties-the-same-way-that-objects-can",
        "A_Votes": "53",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    With python properties, I can make it such that   obj.y    calls a function rather than just returning a value.  Is there a way to do this with modules? I have a case where I want  module.y    to call a function, rather than just returning the value stored there.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Can modules have properties the same way that objects can?",
        "A_Content": "  I would do this in order to properly inherit all the attributes of a module, and be correctly identified by isinstance()  import types  class MyModule(types.ModuleType):     @property     def y(self):         return 5   >>> a=MyModule(\"test\") >>> a <module 'test' (built-in)> >>> a.y 5   And then you can insert this into sys.modules:  sys.modules[__name__] = MyModule(__name__)  # remember to instantiate the class      ",
        "Language": "Python",
        "Tags": [
            "python",
            "properties",
            "python-module"
        ],
        "URL": "https://stackoverflow.com/questions/880530/can-modules-have-properties-the-same-way-that-objects-can",
        "A_Votes": "45",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    With python properties, I can make it such that   obj.y    calls a function rather than just returning a value.  Is there a way to do this with modules? I have a case where I want  module.y    to call a function, rather than just returning the value stored there.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Can modules have properties the same way that objects can?",
        "A_Content": "  A typical use case is: enriching a (huge) existing module with some (few) dynamic attributes - without turning all module stuff into a class layout. Unfortunately a most simple module class patch like sys.modules[__name__].__class__ = MyPropertyModule fails with TypeError: __class__ assignment: only for heap types. So module creation needs to be rewired.  This approach does it without Python import hooks, just by having some prolog on top of the module code:  # propertymodule.py \"\"\" Module property example \"\"\"  if '__orgmod__' not in globals():      # constant prolog for having module properties / supports reload()      print \"PropertyModule stub execution\", __name__     import sys, types     class PropertyModule(types.ModuleType):         def __str__(self):             return \"<PropertyModule %r from %r>\" % (self.__name__, self.__file__)     modnew = PropertyModule(__name__, __doc__)     modnew.__modclass__ = PropertyModule             modnew.__file__ = __file__     modnew.__orgmod__ = sys.modules[__name__]     sys.modules[__name__] = modnew     exec sys._getframe().f_code in modnew.__dict__  else:      # normal module code (usually vast) ..      print \"regular module execution\"     a = 7      def get_dynval(module):         return \"property function returns %s in module %r\" % (a * 4, module.__name__)         __modclass__.dynval = property(get_dynval)   Usage:  >>> import propertymodule PropertyModule stub execution propertymodule regular module execution >>> propertymodule.dynval \"property function returns 28 in module 'propertymodule'\" >>> reload(propertymodule)   # AFTER EDITS regular module execution <module 'propertymodule' from 'propertymodule.pyc'> >>> propertymodule.dynval \"property function returns 36 in module 'propertymodule'\"   Note: Something like from propertymodule import dynval will produce a frozen copy of course - corresponding to dynval = someobject.dynval     ",
        "Language": "Python",
        "Tags": [
            "python",
            "properties",
            "python-module"
        ],
        "URL": "https://stackoverflow.com/questions/880530/can-modules-have-properties-the-same-way-that-objects-can",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    With python properties, I can make it such that   obj.y    calls a function rather than just returning a value.  Is there a way to do this with modules? I have a case where I want  module.y    to call a function, rather than just returning the value stored there.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is bool a subclass of int?",
        "A_Content": "  From a comment on http://www.peterbe.com/plog/bool-is-int     It is perfectly logical, if you were around when the bool type was   added to python (sometime around 2.2 or 2.3).      Prior to introduction of an actual bool type, 0 and 1 were the   official representation for truth value, similar to C89. To avoid   unnecessarily breaking non-ideal but working code, the new bool type   needed to work just like 0 and 1. This goes beyond merely truth value,   but all integral operations. No one would recommend using a boolean   result in a numeric context, nor would most people recommend testing   equality to determine truth value, no one wanted to find out the hard   way just how much existing code is that way. Thus the decision to make   True and False masquerade as 1 and 0, respectively. This is merely a   historical artifact of the linguistic evolution.   Credit goes to dman13 for this nice explanation.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "boolean"
        ],
        "URL": "https://stackoverflow.com/questions/8169001/why-is-bool-a-subclass-of-int",
        "A_Votes": "80",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    When storing a bool in memcached through python-memcached I noticed that it's returned as an integer. Checking the code of the library showed me that there is a place where isinstance(val, int) is checked to flag the value as an integer.  So I tested it in the python shell and noticed the following:  >>> isinstance(True, int) True >>> issubclass(bool, int) True   But why exactly is bool a subclass of int?  It kind of makes sense because a boolean basically is an int which can just take two values but it needs much less operations/space than an actual integer (no arithmetics, only a single bit of storage space)....     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is bool a subclass of int?",
        "A_Content": "  See PEP 285 -- Adding a bool type.  Relevent passage:     6) Should bool inherit from int?      => Yes.      In an ideal world, bool might be better implemented as a      separate integer type that knows how to perform mixed-mode      arithmetic.  However, inheriting bool from int eases the      implementation enormously (in part since all C code that calls      PyInt_Check() will continue to work -- this returns true for      subclasses of int).        ",
        "Language": "Python",
        "Tags": [
            "python",
            "boolean"
        ],
        "URL": "https://stackoverflow.com/questions/8169001/why-is-bool-a-subclass-of-int",
        "A_Votes": "24",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When storing a bool in memcached through python-memcached I noticed that it's returned as an integer. Checking the code of the library showed me that there is a place where isinstance(val, int) is checked to flag the value as an integer.  So I tested it in the python shell and noticed the following:  >>> isinstance(True, int) True >>> issubclass(bool, int) True   But why exactly is bool a subclass of int?  It kind of makes sense because a boolean basically is an int which can just take two values but it needs much less operations/space than an actual integer (no arithmetics, only a single bit of storage space)....     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is bool a subclass of int?",
        "A_Content": "  Can also use help to check the Bool's value in Console:  help(True)  help(True) Help on bool object: class bool(int)  |  bool(x) -> bool  |    |  Returns True when the argument x is true, False otherwise.  |  The builtins True and False are the only two instances of the class bool.  |  The class bool is a subclass of the class int, and cannot be subclassed.  |    |  Method resolution order:  |      bool  |      int  |      object  |     help(False)  help(False) Help on bool object: class bool(int)  |  bool(x) -> bool  |    |  Returns True when the argument x is true, False otherwise.  |  The builtins True and False are the only two instances of the class bool.  |  The class bool is a subclass of the class int, and cannot be subclassed.  |    |  Method resolution order:  |      bool  |      int  |      object      ",
        "Language": "Python",
        "Tags": [
            "python",
            "boolean"
        ],
        "URL": "https://stackoverflow.com/questions/8169001/why-is-bool-a-subclass-of-int",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When storing a bool in memcached through python-memcached I noticed that it's returned as an integer. Checking the code of the library showed me that there is a place where isinstance(val, int) is checked to flag the value as an integer.  So I tested it in the python shell and noticed the following:  >>> isinstance(True, int) True >>> issubclass(bool, int) True   But why exactly is bool a subclass of int?  It kind of makes sense because a boolean basically is an int which can just take two values but it needs much less operations/space than an actual integer (no arithmetics, only a single bit of storage space)....     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Class constants in python",
        "A_Content": "  Since Horse is a subclass of Animal, you can just change  print(Animal.SIZES[1])   with  print(self.SIZES[1])   Still, you need to remember that SIZES[1] means \"big\", so probably you could improve your code by doing something like:  class Animal:     SIZE_HUGE=\"Huge\"     SIZE_BIG=\"Big\"     SIZE_MEDIUM=\"Medium\"     SIZE_SMALL=\"Small\"  class Horse(Animal):     def printSize(self):         print(self.SIZE_BIG)   Alternatively, you could create intermediate classes: HugeAnimal, BigAnimal, and so on. That would be especially helpful if each animal class will contain different logic.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/10672419/class-constants-in-python",
        "A_Votes": "84",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, I want a class to have some \"constants\" (practically, variables) which will be common in all subclasses. Is there a way to do it with friendly syntax? Right now I use:  class Animal:     SIZES=[\"Huge\",\"Big\",\"Medium\",\"Small\"]  class Horse(Animal):     def printSize(self):         print(Animal.SIZES[1])   and I'm wondering if there is a better way to do it or a way to do it without then having to write \"Animal.\" before the sizes. Thanks! edit: forgot to mention that horse inherits from animal.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Class constants in python",
        "A_Content": "  You can get to SIZES by means of self.SIZES (in an instance method) or cls.SIZES (in a class method).  In any case, you will have to be explicit about where to find SIZES. An alternative is to put SIZES in the module containing the classes, but then you need to define all classes in a single module.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/10672419/class-constants-in-python",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, I want a class to have some \"constants\" (practically, variables) which will be common in all subclasses. Is there a way to do it with friendly syntax? Right now I use:  class Animal:     SIZES=[\"Huge\",\"Big\",\"Medium\",\"Small\"]  class Horse(Animal):     def printSize(self):         print(Animal.SIZES[1])   and I'm wondering if there is a better way to do it or a way to do it without then having to write \"Animal.\" before the sizes. Thanks! edit: forgot to mention that horse inherits from animal.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Class constants in python",
        "A_Content": "  class Animal:     HUGE = \"Huge\"     BIG = \"Big\"  class Horse:     def printSize(self):         print(Animal.HUGE)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/10672419/class-constants-in-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, I want a class to have some \"constants\" (practically, variables) which will be common in all subclasses. Is there a way to do it with friendly syntax? Right now I use:  class Animal:     SIZES=[\"Huge\",\"Big\",\"Medium\",\"Small\"]  class Horse(Animal):     def printSize(self):         print(Animal.SIZES[1])   and I'm wondering if there is a better way to do it or a way to do it without then having to write \"Animal.\" before the sizes. Thanks! edit: forgot to mention that horse inherits from animal.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Class constants in python",
        "A_Content": "  Expanding on betabandido's answer, you could write a function to inject the attributes as constants into the module:  def module_register_class_constants(klass, attr_prefix):     globals().update(         (name, getattr(klass, name)) for name in dir(klass) if name.startswith(attr_prefix)     )  class Animal(object):     SIZE_HUGE = \"Huge\"     SIZE_BIG = \"Big\"  module_register_class_constants(Animal, \"SIZE_\")  class Horse(Animal):     def printSize(self):         print SIZE_BIG      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/10672419/class-constants-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In python, I want a class to have some \"constants\" (practically, variables) which will be common in all subclasses. Is there a way to do it with friendly syntax? Right now I use:  class Animal:     SIZES=[\"Huge\",\"Big\",\"Medium\",\"Small\"]  class Horse(Animal):     def printSize(self):         print(Animal.SIZES[1])   and I'm wondering if there is a better way to do it or a way to do it without then having to write \"Animal.\" before the sizes. Thanks! edit: forgot to mention that horse inherits from animal.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Putting text in top left corner of matplotlib plot",
        "A_Content": "  You can use text.   text(x, y, s, fontsize=12)   text coordinates can be given relative to the axis, so the position of your text will be independent of the size of the plot:     The default transform specifies that text is in data coords,   alternatively, you can specify text in axis coords (0,0 is lower-left   and 1,1 is upper-right).  The example below places text in the center   of the axes::   text(0.5, 0.5,'matplotlib',      horizontalalignment='center',      verticalalignment='center',      transform = ax.transAxes)   To prevent the text to interfere with any point of your scatter is more difficult afaik. The easier method is to set y_axis (ymax in ylim((ymin,ymax))) to a value a bit higher than the max y-coordinate of your points. In this way you will always have this free space for the text.  EDIT: here you have an example:  In [18]: f = figure() In [19]: ax = f.add_subplot(111) In [20]: scatter([3,5,2,6,8],[5,3,2,1,5]) Out[20]: <matplotlib.collections.CircleCollection object at 0x0000000007439A90> In [21]: text(0.1, 0.9,'matplotlib', ha='center', va='center', transform=ax.transAxes) Out[21]: <matplotlib.text.Text object at 0x0000000007415B38> In [22]:     The ha and va parameters set the alignment of your text relative to the insertion point. ie. ha='left' is a good set to prevent a long text to go out of the left axis when the frame is reduced (made narrower) manually.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "plot",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/8482588/putting-text-in-top-left-corner-of-matplotlib-plot",
        "A_Votes": "108",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How can I put text in the top left (or top right) corner of a matplotlib figure, e.g. where a top left legend would be, or on top of the plot but in the top left corner?  E.g. if it's a plt.scatter(), then something that would be within the square of the scatter, put in the top left most corner.  I'd like to do this without ideally knowing the scale of the scatterplot being plotted for example, since it will change from dataset to data set.  I just want it the text to be roughly in the upper left, or roughly in the upper right. With legend type positioning it should not overlap with any scatter plot points anyway.  thanks!     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Putting text in top left corner of matplotlib plot",
        "A_Content": "  One solution would be to use the plt.legend function, even if you don't want an actual legend.  You can specify the placement of the legend box by using the loc keyterm.  More information can be found at this website but I've also included an example showing how to place a legend:  ax.scatter(xa,ya, marker='o', s=20, c=\"lightgreen\", alpha=0.9) ax.scatter(xb,yb, marker='o', s=20, c=\"dodgerblue\", alpha=0.9) ax.scatter(xc,yc marker='o', s=20, c=\"firebrick\", alpha=1.0) ax.scatter(xd,xd,xd, marker='o', s=20, c=\"goldenrod\", alpha=0.9) line1 = Line2D(range(10), range(10), marker='o', color=\"goldenrod\") line2 = Line2D(range(10), range(10), marker='o',color=\"firebrick\") line3 = Line2D(range(10), range(10), marker='o',color=\"lightgreen\") line4 = Line2D(range(10), range(10), marker='o',color=\"dodgerblue\") plt.legend((line1,line2,line3, line4),('line1','line2', 'line3', 'line4'),numpoints=1, loc=2)    Note that because loc=2, the legend is in the upper-left corner of the plot.  And if the text overlaps with the plot, you can make it smaller by using legend.fontsize, which will then make the legend smaller.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "plot",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/8482588/putting-text-in-top-left-corner-of-matplotlib-plot",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I put text in the top left (or top right) corner of a matplotlib figure, e.g. where a top left legend would be, or on top of the plot but in the top left corner?  E.g. if it's a plt.scatter(), then something that would be within the square of the scatter, put in the top left most corner.  I'd like to do this without ideally knowing the scale of the scatterplot being plotted for example, since it will change from dataset to data set.  I just want it the text to be roughly in the upper left, or roughly in the upper right. With legend type positioning it should not overlap with any scatter plot points anyway.  thanks!     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I force Django to ignore any caches and reload data?",
        "A_Content": "  Having had this problem and found two definitive solutions for it I thought it worth posting another answer.  This is a problem with MySQL's default transaction mode.  Django opens a transaction at the start, which means that by default you won't see changes made in the database.  Demonstrate like this  Run a django shell in terminal 1  >>> MyModel.objects.get(id=1).my_field u'old'   And another in terminal 2  >>> MyModel.objects.get(id=1).my_field u'old' >>> a = MyModel.objects.get(id=1) >>> a.my_field = \"NEW\" >>> a.save() >>> MyModel.objects.get(id=1).my_field u'NEW' >>>    Back to terminal 1 to demonstrate the problem  - we still read the old value from the database.  >>> MyModel.objects.get(id=1).my_field u'old'   Now in terminal 1 demonstrate the solution  >>> from django.db import transaction >>>  >>> @transaction.commit_manually ... def flush_transaction(): ...     transaction.commit() ...  >>> MyModel.objects.get(id=1).my_field u'old' >>> flush_transaction() >>> MyModel.objects.get(id=1).my_field u'NEW' >>>    The new data is now read  Here is that code in an easy to paste block with docstring  from django.db import transaction  @transaction.commit_manually def flush_transaction():     \"\"\"     Flush the current transaction so we don't read stale data      Use in long running processes to make sure fresh data is read from     the database.  This is a problem with MySQL and the default     transaction mode.  You can fix it by setting     \"transaction-isolation = READ-COMMITTED\" in my.cnf or by calling     this function at the appropriate moment     \"\"\"     transaction.commit()   The alternative solution is to change my.cnf for MySQL to change the default transaction mode  transaction-isolation = READ-COMMITTED   Note that that is a relatively new feature for Mysql and has some consequences for binary logging / slaving.  You could also put this in the django connection preamble if you wanted.  Update 3 years later  Now that Django 1.6 has turned on autocommit in MySQL this is no longer a problem.  The example above now works fine without the flush_transaction() code whether your MySQL is in REPEATABLE-READ (the default) or READ-COMMITTED transaction isolation mode.  What was happening in previous versions of Django which ran in non autocommit mode was that the first select statement opened a transaction.  Since MySQL's default mode is REPEATABLE-READ this means that no updates to the database will be read  by subsequent select statements - hence the need for the flush_transaction() code above which stops the transaction and starts a new one.  There are still reasons why you might want to use READ-COMMITTED transaction isolation though. If you were to put terminal 1 in a transaction and you wanted to see the writes from the terminal 2 you would need READ-COMMITTED.  The flush_transaction() code now produces a deprecation warning in Django 1.6 so I recommend you remove it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "caching"
        ],
        "URL": "https://stackoverflow.com/questions/3346124/how-do-i-force-django-to-ignore-any-caches-and-reload-data",
        "A_Votes": "92",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.  What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:  >>> MyModel.objects.count() 885 # (Here I added some more data from another process.) >>> MyModel.objects.count() 885 >>> MyModel.objects.update() 0 >>> MyModel.objects.count() 1025   As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.  I can't find any documentation on that update() method and have no idea what other bad things it might do.  My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I force Django to ignore any caches and reload data?",
        "A_Content": "  We've struggled a fair bit with forcing django to refresh the \"cache\" - which it turns out wasn't really a cache at all but an artifact due to transactions. This might not apply to your example, but certainly in django views, by default, there's an implicit call to a transaction, which mysql then isolates from any changes that happen from other processes ater you start.  we used the @transaction.commit_manually decorator and calls to transaction.commit() just before every occasion where you need up-to-date info.  As I say, this definitely applies to views, not sure whether it would apply to django code not being run inside a view.  detailed info here:   http://devblog.resolversystems.com/?p=439     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "caching"
        ],
        "URL": "https://stackoverflow.com/questions/3346124/how-do-i-force-django-to-ignore-any-caches-and-reload-data",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.  What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:  >>> MyModel.objects.count() 885 # (Here I added some more data from another process.) >>> MyModel.objects.count() 885 >>> MyModel.objects.update() 0 >>> MyModel.objects.count() 1025   As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.  I can't find any documentation on that update() method and have no idea what other bad things it might do.  My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I force Django to ignore any caches and reload data?",
        "A_Content": "  Seems like the count() goes to cache after the first time. This is the django source for QuerySet.count:  def count(self):     \"\"\"     Performs a SELECT COUNT() and returns the number of records as an     integer.      If the QuerySet is already fully cached this simply returns the length     of the cached results set to avoid multiple SELECT COUNT(*) calls.     \"\"\"     if self._result_cache is not None and not self._iter:         return len(self._result_cache)      return self.query.get_count(using=self.db)   update does seem to be doing quite a bit of extra work, besides what you need. But I can't think of any better way to do this, short of writing your own SQL for the count. If performance is not super important, I would just do what you're doing, calling update before count.  QuerySet.update:  def update(self, **kwargs):     \"\"\"     Updates all elements in the current QuerySet, setting all the given     fields to the appropriate values.     \"\"\"     assert self.query.can_filter(), \\             \"Cannot update a query once a slice has been taken.\"     self._for_write = True     query = self.query.clone(sql.UpdateQuery)     query.add_update_values(kwargs)     if not transaction.is_managed(using=self.db):         transaction.enter_transaction_management(using=self.db)         forced_managed = True     else:         forced_managed = False     try:         rows = query.get_compiler(self.db).execute_sql(None)         if forced_managed:             transaction.commit(using=self.db)         else:             transaction.commit_unless_managed(using=self.db)     finally:         if forced_managed:             transaction.leave_transaction_management(using=self.db)     self._result_cache = None     return rows update.alters_data = True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "caching"
        ],
        "URL": "https://stackoverflow.com/questions/3346124/how-do-i-force-django-to-ignore-any-caches-and-reload-data",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.  What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:  >>> MyModel.objects.count() 885 # (Here I added some more data from another process.) >>> MyModel.objects.count() 885 >>> MyModel.objects.update() 0 >>> MyModel.objects.count() 1025   As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.  I can't find any documentation on that update() method and have no idea what other bad things it might do.  My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I force Django to ignore any caches and reload data?",
        "A_Content": "  I'm not sure I'd recommend it...but you can just kill the cache yourself:  >>> qs = MyModel.objects.all() >>> qs.count() 1 >>> MyModel().save() >>> qs.count()  # cached! 1 >>> qs._result_cache = None >>> qs.count() 2   And here's a better technique that doesn't rely on fiddling with the innards of the QuerySet: Remember that the caching is happening within a QuerySet, but refreshing the data simply requires the underlying Query to be re-executed. The QuerySet is really just a high-level API wrapping a Query object, plus a container (with caching!) for Query results. Thus, given a queryset, here is a general-purpose way of forcing a refresh:  >>> MyModel().save() >>> qs = MyModel.objects.all() >>> qs.count() 1 >>> MyModel().save() >>> qs.count()  # cached! 1 >>> from django.db.models import QuerySet >>> qs = QuerySet(model=MyModel, query=qs.query) >>> qs.count()  # refreshed! 2 >>> party_time()   Pretty easy! You can of course implement this as a helper function and use as needed.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "caching"
        ],
        "URL": "https://stackoverflow.com/questions/3346124/how-do-i-force-django-to-ignore-any-caches-and-reload-data",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.  What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:  >>> MyModel.objects.count() 885 # (Here I added some more data from another process.) >>> MyModel.objects.count() 885 >>> MyModel.objects.update() 0 >>> MyModel.objects.count() 1025   As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.  I can't find any documentation on that update() method and have no idea what other bad things it might do.  My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I force Django to ignore any caches and reload data?",
        "A_Content": "  If you append .all() to a queryset, it'll force a reread from the DB. Try MyModel.objects.all().count() instead of MyModel.objects.count().     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "caching"
        ],
        "URL": "https://stackoverflow.com/questions/3346124/how-do-i-force-django-to-ignore-any-caches-and-reload-data",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.  What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:  >>> MyModel.objects.count() 885 # (Here I added some more data from another process.) >>> MyModel.objects.count() 885 >>> MyModel.objects.update() 0 >>> MyModel.objects.count() 1025   As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.  I can't find any documentation on that update() method and have no idea what other bad things it might do.  My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I force Django to ignore any caches and reload data?",
        "A_Content": "  You can also use MyModel.objects._clone().count().  All of the methods in the the QuerySet call _clone() prior to doing any work - that ensures that any internal caches are invalidated.  The root cause is that MyModel.objects is the same instance each time.  By cloning it you're creating a new instance without the cached value.  Of course, you can always reach in and invalidate the cache if you'd prefer to use the same instance.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "caching"
        ],
        "URL": "https://stackoverflow.com/questions/3346124/how-do-i-force-django-to-ignore-any-caches-and-reload-data",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm using the Django database models from a process that's not called from an HTTP request.  The process is supposed to poll for new data every few seconds and do some processing on it.   I have a loop that sleeps for a few seconds and then gets all unhandled data from the database.  What I'm seeing is that after the first fetch, the process never sees any new data.  I ran a few tests and it looks like Django is caching results, even though I'm building new QuerySets every time.  To verify this, I did this from a Python shell:  >>> MyModel.objects.count() 885 # (Here I added some more data from another process.) >>> MyModel.objects.count() 885 >>> MyModel.objects.update() 0 >>> MyModel.objects.count() 1025   As you can see, adding new data doesn't change the result count.  However, calling the manager's update() method seems to fix the problem.  I can't find any documentation on that update() method and have no idea what other bad things it might do.  My question is, why am I seeing this caching behavior, which contradicts what Django docs say?  And how do I prevent it from happening?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Imshow: extent and aspect",
        "A_Content": "  You can do it by setting the aspect of the image manually (or by letting it auto-scale to fill up the extent of the figure).  By default, imshow sets the aspect of the plot to 1, as this is often what people want for image data.  In your case, you can do something like:  import matplotlib.pyplot as plt import numpy as np  grid = np.random.random((10,10))  fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, figsize=(6,10))  ax1.imshow(grid, extent=[0,100,0,1]) ax1.set_title('Default')  ax2.imshow(grid, extent=[0,100,0,1], aspect='auto') ax2.set_title('Auto-scaled Aspect')  ax3.imshow(grid, extent=[0,100,0,1], aspect=100) ax3.set_title('Manually Set Aspect')  plt.tight_layout() plt.show()        ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib",
            "imshow"
        ],
        "URL": "https://stackoverflow.com/questions/13384653/imshow-extent-and-aspect",
        "A_Votes": "116",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm writing a software system that visualizes slices and projections through a 3D dataset.  I'm using matplotlib and specifically imshow to visualize the image buffers I get back from my analysis code.  Since I'd like to annotate the images with plot axes, I use the extent keyword that imshow supplies to map the image buffer pixel coordinates to a data space coordinate system.  Unfortuantely, matplotlib doesn't know about units.  Say (taking an artificial example) that I want to plot an image with dimensions of 1000 m X 1 km.  In that case the extent would be something like [0, 1000, 0, 1].  Even though the image array is square, since the aspect ratio implied by the extent keyword is 1000, the resulting plot axes also have an aspect ratio of 1000.   Is it possible to force the aspect ratio of the plot while still keeping the automatically generated major tick marks and labels I get by using the extent keyword?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Is there a built-in product() in Python? [duplicate]",
        "A_Content": "  Pronouncement  Yes, that's right.  Guido rejected the idea for a built-in prod() function because he thought it was rarely needed.  Alternative with reduce()  As you suggested, it is not hard to make your own using reduce() and operator.mul():  def prod(iterable):     return reduce(operator.mul, iterable, 1)  >>> prod(range(1, 5)) 24   In Python 3, the reduce() function was moved to the functools module, so you would need to add:  from functools import reduce   Specific case: Factorials  As a side note, the primary motivating use case for prod() is to compute factorials.  We already have support for that in the math module:  >>> import math  >>> math.factorial(10) 3628800   Alternative with logarithms  If your data consists of floats, you can compute a product using sum() with exponents and logarithms:  >>> from math import log, exp  >>> data = [1.2, 1.5, 2.5, 0.9, 14.2, 3.8] >>> exp(sum(map(log, data))) 218.53799999999993  >>> 1.2 * 1.5 * 2.5 * 0.9 * 14.2 * 3.8 218.53799999999998      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "product"
        ],
        "URL": "https://stackoverflow.com/questions/7948291/is-there-a-built-in-product-in-python",
        "A_Votes": "81",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              What's the function like sum() but for multiplication? product()?                                        7 answers                                          I've been looking through a tutorial and book but I can find no mention of a built in product function i.e. of the same type as sum(), but I could not find anything such as prod().  Is the only way I could find the product of items in a list by importing the mul() operator?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Is there a built-in product() in Python? [duplicate]",
        "A_Content": "  There is no product in Python, but you can define it as  def product(iterable):     return reduce(operator.mul, iterable, 1)   Or, if you have NumPy, use numpy.product.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "product"
        ],
        "URL": "https://stackoverflow.com/questions/7948291/is-there-a-built-in-product-in-python",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              What's the function like sum() but for multiplication? product()?                                        7 answers                                          I've been looking through a tutorial and book but I can find no mention of a built in product function i.e. of the same type as sum(), but I could not find anything such as prod().  Is the only way I could find the product of items in a list by importing the mul() operator?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Is there a built-in product() in Python? [duplicate]",
        "A_Content": "   Since the reduce() function has been removed in python 3.0, you have to take a different approach.  You could use functools.reduce() to make the change unhappen:  product = functools.reduce(operator.mul, iterable, 1)   Or, if you want to follow the spirit of the python-team (which removed reduce() because they think for would be more readable), do it with a loop:  product = 1 for x in iterable:     product *= x      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "product"
        ],
        "URL": "https://stackoverflow.com/questions/7948291/is-there-a-built-in-product-in-python",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              What's the function like sum() but for multiplication? product()?                                        7 answers                                          I've been looking through a tutorial and book but I can find no mention of a built in product function i.e. of the same type as sum(), but I could not find anything such as prod().  Is the only way I could find the product of items in a list by importing the mul() operator?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Is there a built-in product() in Python? [duplicate]",
        "A_Content": "  from numpy import multiply, product list1 = [2,2,2] list2 = [2,2,2] mult = 3 prod_of_lists = multiply(list1,list2) >>>[4,4,4] prod_of_list_by_mult = multiply(list1,mult) >>>[6,6,6] prod_of_single_array = product(list1) >>>8   numpy has many really cool functions for lists!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "product"
        ],
        "URL": "https://stackoverflow.com/questions/7948291/is-there-a-built-in-product-in-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              What's the function like sum() but for multiplication? product()?                                        7 answers                                          I've been looking through a tutorial and book but I can find no mention of a built in product function i.e. of the same type as sum(), but I could not find anything such as prod().  Is the only way I could find the product of items in a list by importing the mul() operator?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  If you want to mainly base your code on the .NET framework, I'd highly recommend IronPython vs Python.NET.  IronPython is pretty much native .NET - so it just works great when integrating with other .NET langauges.    Python.NET is good if you want to just integrate one or two components from .NET into a standard python application.  There are notable differences when using IronPython - but most of them are fairly subtle.  Python.NET uses the standard CPython runtime, so this Wiki page is a relevant discussion of the differences between the two implementations.  The largest differences occur in the cost of exceptions - so some of the standard python libraries don't perform as well in IronPython due to their implementation.     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "63",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  While agreeing with the answers given by Reed Copsey and Alex Martelli, I'd like to point out one further difference - the Global Interpreter Lock (GIL). While IronPython doesn't have the limitations of the GIL, CPython does - so it would appear that for those applications where the GIL is a bottleneck, say in certain multicore scenarios, IronPython has an advantage over Python.NET.  From the Python.NET documentation:     Important Note for embedders: Python   is not free-threaded and uses a global   interpreter lock to allow   multi-threaded applications to   interact safely with the Python   interpreter. Much more information   about this is available in the Python   C API documentation on the   www.python.org Website.      When embedding Python in a managed   application, you have to manage the   GIL in just the same way you would   when embedding Python in a C or C++   application.      Before interacting with any of the   objects or APIs provided by the   Python.Runtime namespace, calling code   must have acquired the Python global   interpreter lock by calling the   PythonEngine.AcquireLock method. The   only exception to this rule is the   PythonEngine.Initialize method, which   may be called at startup without   having acquired the GIL.      When finished using Python APIs,   managed code must call a corresponding   PythonEngine.ReleaseLock to release   the GIL and allow other threads to use   Python.      The AcquireLock and ReleaseLock   methods are thin wrappers over the   unmanaged PyGILState_Ensure and   PyGILState_Release functions from the   Python API, and the documentation for   those APIs applies to the managed   versions.   Another issue is IDE support. CPython probably has better IDE support at present than IronPython - so this may be a factor in the choosing of one over the other.     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  Most of scientific and numerical Python libraries that rely on CPython C-API (numpy, scipy, matplotlib, pandas, cython, etc.) are working mostly under CPython, so in that case your best bet is pythonnet (other names - Python.NET and Python for .NET). The same is true for CPython GUI bindings such as WxWidgets, PyQt/PySide, GTK, Kivy, etc., although both pythonnet and IronPython can use WPF and WinForms.  And finally IronPython does not fully support Python 3 yet.     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  IronPython is \".NET-native\" -- so it will be preferable if you want to fully integrate your Python code with .NET all the way; Python.NET works with Classic Python, so it lets you keep your Python code's \"arm's length\" away from .NET proper.  (Note that with this code you can actually use extensions written for CPython from your IronPython code, so that's not a discriminating condition any more).     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  IronPython comes from Microsoft, so I would go with my gut and use that one first since you have to assume it will play nicer with other MSFT technologies.       ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  As for 2016.   In my company we used IronPython, but we were not satisfied with performances (mostly memory use - garbage collector was too slow) so we decided to switch to standard Python and integrate it with .Net using Zeroce-s ICE.      ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  IronPython, currently,  doesn't support Python 3.6 (only 2.7)  from IronPython 3 \"Builds of IronPython 3 are not yet provided.\"     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "   Ironpython is like C# in turn it relies on static prebuilt libraries while unlike C# is a dynamic language. Cpython is like C++ like Ironpython is a dynamic language and has access to dynamic libraries which in turn translates to being forced to write everything. Ironpython is faster than C# in certain areas but not faster than Cpython, however you can link Ironpython to any language thus over coming problems but then again you can do the same with Cpython.   A funny, simple and powerful language regardless what you choose!     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  Iron Python is basically Python 2.7 with integrated .net support it probably will never support Python 3. It loses out on C and Python libraries, however on the twist side has access to .net and can be extended with C#. So if you use C# already then Iron Python is a bonus.     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "IronPython vs. Python .NET",
        "A_Content": "  I mainly prefer Python for .NET, because IronPython is compiled as managed code, which can be easily decompiled (what I most hate), but with py2exe or pyinstaller you can compile Python with NET module as an unmanaged application.     ",
        "Language": "Python",
        "Tags": [
            "python",
            ".net",
            "ironpython",
            "cpython",
            "python.net"
        ],
        "URL": "https://stackoverflow.com/questions/1168914/ironpython-vs-python-net",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to access some .NET assemblies written in C# from Python code.   A little research showed I have two choices:   IronPython with .NET interface capability/support built-in Python with the Python .NET package   What are the trade-offs between both solutions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Moving x-axis to the top of a plot in matplotlib",
        "A_Content": "  Use  ax.xaxis.tick_top()   to place the tick marks at the top of the image. The command  ax.set_xlabel('X LABEL')     ax.xaxis.set_label_position('top')    affects the label, not the tick marks.  import matplotlib.pyplot as plt import numpy as np column_labels = list('ABCD') row_labels = list('WXYZ') data = np.random.rand(4, 4) fig, ax = plt.subplots() heatmap = ax.pcolor(data, cmap=plt.cm.Blues)  # put the major ticks at the middle of each cell ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False) ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)  # want a more natural, table-like display ax.invert_yaxis() ax.xaxis.tick_top()  ax.set_xticklabels(column_labels, minor=False) ax.set_yticklabels(row_labels, minor=False) plt.show()          ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib",
            "plot",
            "data-visualization"
        ],
        "URL": "https://stackoverflow.com/questions/14406214/moving-x-axis-to-the-top-of-a-plot-in-matplotlib",
        "A_Votes": "84",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Based on this question about heatmaps in matplotlib, I wanted to move the x-axis titles to the top of the plot.  import matplotlib.pyplot as plt import numpy as np column_labels = list('ABCD') row_labels = list('WXYZ') data = np.random.rand(4,4) fig, ax = plt.subplots() heatmap = ax.pcolor(data, cmap=plt.cm.Blues)  # put the major ticks at the middle of each cell ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False) ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)  # want a more natural, table-like display ax.invert_yaxis() ax.xaxis.set_label_position('top') # <-- This doesn't work!  ax.set_xticklabels(row_labels, minor=False) ax.set_yticklabels(column_labels, minor=False) plt.show()   However, calling matplotlib's set_label_position (as notated above) doesn't seem to have the desired effect. Here's my output:    What am I doing wrong?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Moving x-axis to the top of a plot in matplotlib",
        "A_Content": "  You want set_ticks_position rather than set_label_position:  ax.xaxis.set_ticks_position('top') # the rest is the same   This gives me:       ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib",
            "plot",
            "data-visualization"
        ],
        "URL": "https://stackoverflow.com/questions/14406214/moving-x-axis-to-the-top-of-a-plot-in-matplotlib",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Based on this question about heatmaps in matplotlib, I wanted to move the x-axis titles to the top of the plot.  import matplotlib.pyplot as plt import numpy as np column_labels = list('ABCD') row_labels = list('WXYZ') data = np.random.rand(4,4) fig, ax = plt.subplots() heatmap = ax.pcolor(data, cmap=plt.cm.Blues)  # put the major ticks at the middle of each cell ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False) ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)  # want a more natural, table-like display ax.invert_yaxis() ax.xaxis.set_label_position('top') # <-- This doesn't work!  ax.set_xticklabels(row_labels, minor=False) ax.set_yticklabels(column_labels, minor=False) plt.show()   However, calling matplotlib's set_label_position (as notated above) doesn't seem to have the desired effect. Here's my output:    What am I doing wrong?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Moving x-axis to the top of a plot in matplotlib",
        "A_Content": "  tick_params is very useful for setting tick properties. Labels can be moved to the top with:      ax.tick_params(labelbottom='off',labeltop='on')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib",
            "plot",
            "data-visualization"
        ],
        "URL": "https://stackoverflow.com/questions/14406214/moving-x-axis-to-the-top-of-a-plot-in-matplotlib",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Based on this question about heatmaps in matplotlib, I wanted to move the x-axis titles to the top of the plot.  import matplotlib.pyplot as plt import numpy as np column_labels = list('ABCD') row_labels = list('WXYZ') data = np.random.rand(4,4) fig, ax = plt.subplots() heatmap = ax.pcolor(data, cmap=plt.cm.Blues)  # put the major ticks at the middle of each cell ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False) ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)  # want a more natural, table-like display ax.invert_yaxis() ax.xaxis.set_label_position('top') # <-- This doesn't work!  ax.set_xticklabels(row_labels, minor=False) ax.set_yticklabels(column_labels, minor=False) plt.show()   However, calling matplotlib's set_label_position (as notated above) doesn't seem to have the desired effect. Here's my output:    What am I doing wrong?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Moving x-axis to the top of a plot in matplotlib",
        "A_Content": "  You've got to do some extra massaging if you want the ticks (not labels) to show up on the top and bottom (not just the top). The only way I could do this is with a minor change to unutbu's code:  import matplotlib.pyplot as plt import numpy as np column_labels = list('ABCD') row_labels = list('WXYZ') data = np.random.rand(4, 4) fig, ax = plt.subplots() heatmap = ax.pcolor(data, cmap=plt.cm.Blues)  # put the major ticks at the middle of each cell ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False) ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)  # want a more natural, table-like display ax.invert_yaxis() ax.xaxis.tick_top() ax.xaxis.set_ticks_position('both') # THIS IS THE ONLY CHANGE  ax.set_xticklabels(column_labels, minor=False) ax.set_yticklabels(row_labels, minor=False) plt.show()   Output:       ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib",
            "plot",
            "data-visualization"
        ],
        "URL": "https://stackoverflow.com/questions/14406214/moving-x-axis-to-the-top-of-a-plot-in-matplotlib",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Based on this question about heatmaps in matplotlib, I wanted to move the x-axis titles to the top of the plot.  import matplotlib.pyplot as plt import numpy as np column_labels = list('ABCD') row_labels = list('WXYZ') data = np.random.rand(4,4) fig, ax = plt.subplots() heatmap = ax.pcolor(data, cmap=plt.cm.Blues)  # put the major ticks at the middle of each cell ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False) ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)  # want a more natural, table-like display ax.invert_yaxis() ax.xaxis.set_label_position('top') # <-- This doesn't work!  ax.set_xticklabels(row_labels, minor=False) ax.set_yticklabels(column_labels, minor=False) plt.show()   However, calling matplotlib's set_label_position (as notated above) doesn't seem to have the desired effect. Here's my output:    What am I doing wrong?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "why is plotting with Matplotlib so slow?",
        "A_Content": "  First off, (though this won't change the performance at all) consider cleaning up your code, similar to this:  import matplotlib.pyplot as plt import numpy as np import time  x = np.arange(0, 2*np.pi, 0.01) y = np.sin(x)  fig, axes = plt.subplots(nrows=6) styles = ['r-', 'g-', 'y-', 'm-', 'k-', 'c-'] lines = [ax.plot(x, y, style)[0] for ax, style in zip(axes, styles)]  fig.show()  tstart = time.time() for i in xrange(1, 20):     for j, line in enumerate(lines, start=1):         line.set_ydata(np.sin(j*x + i/10.0))     fig.canvas.draw()  print 'FPS:' , 20/(time.time()-tstart)   With the above example, I get around 10fps.    Just a quick note, depending on your exact use case, matplotlib may not be a great choice.  It's oriented towards publication-quality figures, not real-time display.  However, there are a lot of things you can do to speed this example up.  There are two main reasons why this is as slow as it is.    1) Calling fig.canvas.draw() redraws everything.  It's your bottleneck.  In your case, you don't need to re-draw things like the axes boundaries, tick labels, etc.   2) In your case, there are a lot of subplots with a lot of tick labels.  These take a long time to draw.    Both these can be fixed by using blitting.  To do blitting efficiently, you'll have to use backend-specific code.  In practice, if you're really worried about smooth animations, you're usually embedding matplotlib plots in some sort of gui toolkit, anyway, so this isn't much of an issue.  However, without knowing a bit more about what you're doing, I can't help you there.  Nonetheless, there is a gui-neutral way of doing it that is still reasonably fast.  import matplotlib.pyplot as plt import numpy as np import time  x = np.arange(0, 2*np.pi, 0.1) y = np.sin(x)  fig, axes = plt.subplots(nrows=6)  fig.show()  # We need to draw the canvas before we start animating... fig.canvas.draw()  styles = ['r-', 'g-', 'y-', 'm-', 'k-', 'c-'] def plot(ax, style):     return ax.plot(x, y, style, animated=True)[0] lines = [plot(ax, style) for ax, style in zip(axes, styles)]  # Let's capture the background of the figure backgrounds = [fig.canvas.copy_from_bbox(ax.bbox) for ax in axes]  tstart = time.time() for i in xrange(1, 2000):     items = enumerate(zip(lines, axes, backgrounds), start=1)     for j, (line, ax, background) in items:         fig.canvas.restore_region(background)         line.set_ydata(np.sin(j*x + i/10.0))         ax.draw_artist(line)         fig.canvas.blit(ax.bbox)  print 'FPS:' , 2000/(time.time()-tstart)   This gives me ~200fps.  To make this a bit more convenient, there's an animations module in recent versions of matplotlib.  As an example:  import matplotlib.pyplot as plt import matplotlib.animation as animation import numpy as np  x = np.arange(0, 2*np.pi, 0.1) y = np.sin(x)  fig, axes = plt.subplots(nrows=6)  styles = ['r-', 'g-', 'y-', 'm-', 'k-', 'c-'] def plot(ax, style):     return ax.plot(x, y, style, animated=True)[0] lines = [plot(ax, style) for ax, style in zip(axes, styles)]  def animate(i):     for j, line in enumerate(lines, start=1):         line.set_ydata(np.sin(j*x + i/10.0))     return lines  # We'd normally specify a reasonable \"interval\" here... ani = animation.FuncAnimation(fig, animate, xrange(1, 200),                                interval=0, blit=True) plt.show()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/8955869/why-is-plotting-with-matplotlib-so-slow",
        "A_Votes": "94",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm currently evaluating different python plotting libraries. Right now I'm trying matplotlib and I'm quite disappointed with the performance. The following example is modified from SciPy examples and gives me only ~ 8 frames per second!  Any ways of speeding this up or should I pick a different plotting library?   from pylab import * import time  ion() fig = figure() ax1 = fig.add_subplot(611) ax2 = fig.add_subplot(612) ax3 = fig.add_subplot(613) ax4 = fig.add_subplot(614) ax5 = fig.add_subplot(615) ax6 = fig.add_subplot(616)  x = arange(0,2*pi,0.01) y = sin(x) line1, = ax1.plot(x, y, 'r-') line2, = ax2.plot(x, y, 'g-') line3, = ax3.plot(x, y, 'y-') line4, = ax4.plot(x, y, 'm-') line5, = ax5.plot(x, y, 'k-') line6, = ax6.plot(x, y, 'p-')  # turn off interactive plotting - speeds things up by 1 Frame / second plt.ioff()   tstart = time.time()               # for profiling for i in arange(1, 200):     line1.set_ydata(sin(x+i/10.0))  # update the data     line2.set_ydata(sin(2*x+i/10.0))     line3.set_ydata(sin(3*x+i/10.0))     line4.set_ydata(sin(4*x+i/10.0))     line5.set_ydata(sin(5*x+i/10.0))     line6.set_ydata(sin(6*x+i/10.0))     draw()                         # redraw the canvas  print 'FPS:' , 200/(time.time()-tstart)      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "why is plotting with Matplotlib so slow?",
        "A_Content": "  Matplotlib makes great publication-quality graphics, but is not very well optimized for speed. There are a variety of python plotting packages that are designed with speed in mind:   http://pyqwt.sourceforge.net/ [ edit: pyqwt is no longer maintained; the previous maintainer is recommending pyqtgraph ] http://code.google.com/p/guiqwt/ http://code.enthought.com/projects/chaco/ http://www.pyqtgraph.org/      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/8955869/why-is-plotting-with-matplotlib-so-slow",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently evaluating different python plotting libraries. Right now I'm trying matplotlib and I'm quite disappointed with the performance. The following example is modified from SciPy examples and gives me only ~ 8 frames per second!  Any ways of speeding this up or should I pick a different plotting library?   from pylab import * import time  ion() fig = figure() ax1 = fig.add_subplot(611) ax2 = fig.add_subplot(612) ax3 = fig.add_subplot(613) ax4 = fig.add_subplot(614) ax5 = fig.add_subplot(615) ax6 = fig.add_subplot(616)  x = arange(0,2*pi,0.01) y = sin(x) line1, = ax1.plot(x, y, 'r-') line2, = ax2.plot(x, y, 'g-') line3, = ax3.plot(x, y, 'y-') line4, = ax4.plot(x, y, 'm-') line5, = ax5.plot(x, y, 'k-') line6, = ax6.plot(x, y, 'p-')  # turn off interactive plotting - speeds things up by 1 Frame / second plt.ioff()   tstart = time.time()               # for profiling for i in arange(1, 200):     line1.set_ydata(sin(x+i/10.0))  # update the data     line2.set_ydata(sin(2*x+i/10.0))     line3.set_ydata(sin(3*x+i/10.0))     line4.set_ydata(sin(4*x+i/10.0))     line5.set_ydata(sin(5*x+i/10.0))     line6.set_ydata(sin(6*x+i/10.0))     draw()                         # redraw the canvas  print 'FPS:' , 200/(time.time()-tstart)      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "why is plotting with Matplotlib so slow?",
        "A_Content": "  To start, Joe Kington's answer provides very good advice using a gui-neutral approach, and you should definitely take his advice (especially about Blitting) and put it into practice. More info on this approach, read the Matplotlib Cookbook  However, the non-GUI-neutral (GUI-biased?) approach is key to speeding up the plotting. In other words, the backend is extremely important to plot speed.  Put these two lines before you import anything else from matplotlib:  import matplotlib matplotlib.use('GTKAgg')    Of course, there are various options to use instead of GTKAgg, but according to the cookbook mentioned before, this was the fastest. See the link about backends for more options.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/8955869/why-is-plotting-with-matplotlib-so-slow",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently evaluating different python plotting libraries. Right now I'm trying matplotlib and I'm quite disappointed with the performance. The following example is modified from SciPy examples and gives me only ~ 8 frames per second!  Any ways of speeding this up or should I pick a different plotting library?   from pylab import * import time  ion() fig = figure() ax1 = fig.add_subplot(611) ax2 = fig.add_subplot(612) ax3 = fig.add_subplot(613) ax4 = fig.add_subplot(614) ax5 = fig.add_subplot(615) ax6 = fig.add_subplot(616)  x = arange(0,2*pi,0.01) y = sin(x) line1, = ax1.plot(x, y, 'r-') line2, = ax2.plot(x, y, 'g-') line3, = ax3.plot(x, y, 'y-') line4, = ax4.plot(x, y, 'm-') line5, = ax5.plot(x, y, 'k-') line6, = ax6.plot(x, y, 'p-')  # turn off interactive plotting - speeds things up by 1 Frame / second plt.ioff()   tstart = time.time()               # for profiling for i in arange(1, 200):     line1.set_ydata(sin(x+i/10.0))  # update the data     line2.set_ydata(sin(2*x+i/10.0))     line3.set_ydata(sin(3*x+i/10.0))     line4.set_ydata(sin(4*x+i/10.0))     line5.set_ydata(sin(5*x+i/10.0))     line6.set_ydata(sin(6*x+i/10.0))     draw()                         # redraw the canvas  print 'FPS:' , 200/(time.time()-tstart)      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "why is plotting with Matplotlib so slow?",
        "A_Content": "  For the first solution proposed by Joe Kington ( .copy_from_bbox & .draw_artist & canvas.blit), I had to capture the backgrounds after the fig.canvas.draw() line, otherwise the background had no effect and I got the same result as you mentioned. If you put it after the fig.show() it still does not work as proposed by Michael Browne.   So just put the background line after the canvas.draw():  [...] fig.show()  # We need to draw the canvas before we start animating... fig.canvas.draw()  # Let's capture the background of the figure backgrounds = [fig.canvas.copy_from_bbox(ax.bbox) for ax in axes]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/8955869/why-is-plotting-with-matplotlib-so-slow",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently evaluating different python plotting libraries. Right now I'm trying matplotlib and I'm quite disappointed with the performance. The following example is modified from SciPy examples and gives me only ~ 8 frames per second!  Any ways of speeding this up or should I pick a different plotting library?   from pylab import * import time  ion() fig = figure() ax1 = fig.add_subplot(611) ax2 = fig.add_subplot(612) ax3 = fig.add_subplot(613) ax4 = fig.add_subplot(614) ax5 = fig.add_subplot(615) ax6 = fig.add_subplot(616)  x = arange(0,2*pi,0.01) y = sin(x) line1, = ax1.plot(x, y, 'r-') line2, = ax2.plot(x, y, 'g-') line3, = ax3.plot(x, y, 'y-') line4, = ax4.plot(x, y, 'm-') line5, = ax5.plot(x, y, 'k-') line6, = ax6.plot(x, y, 'p-')  # turn off interactive plotting - speeds things up by 1 Frame / second plt.ioff()   tstart = time.time()               # for profiling for i in arange(1, 200):     line1.set_ydata(sin(x+i/10.0))  # update the data     line2.set_ydata(sin(2*x+i/10.0))     line3.set_ydata(sin(3*x+i/10.0))     line4.set_ydata(sin(4*x+i/10.0))     line5.set_ydata(sin(5*x+i/10.0))     line6.set_ydata(sin(6*x+i/10.0))     draw()                         # redraw the canvas  print 'FPS:' , 200/(time.time()-tstart)      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "why is plotting with Matplotlib so slow?",
        "A_Content": "  This may not apply to many of you, but I'm usually operating my computers under Linux, so by default I save my matplotlib plots as PNG and SVG. This works fine under Linux but is unbearably slow on my Windows 7 installations [MiKTeX under Python(x,y) or Anaconda], so I've taken to adding this code, and things work fine over there again:  import platform     # Don't save as SVG if running under Windows. # # Plot code goes here. # fig.savefig('figure_name.png', dpi = 200) if platform.system() != 'Windows':     # In my installations of Windows 7, it takes an inordinate amount of time to save     # graphs as .svg files, so on that platform I've disabled the call that does so.     # The first run of a script is still a little slow while everything is loaded in,     # but execution times of subsequent runs are improved immensely.     fig.savefig('figure_name.svg')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/8955869/why-is-plotting-with-matplotlib-so-slow",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm currently evaluating different python plotting libraries. Right now I'm trying matplotlib and I'm quite disappointed with the performance. The following example is modified from SciPy examples and gives me only ~ 8 frames per second!  Any ways of speeding this up or should I pick a different plotting library?   from pylab import * import time  ion() fig = figure() ax1 = fig.add_subplot(611) ax2 = fig.add_subplot(612) ax3 = fig.add_subplot(613) ax4 = fig.add_subplot(614) ax5 = fig.add_subplot(615) ax6 = fig.add_subplot(616)  x = arange(0,2*pi,0.01) y = sin(x) line1, = ax1.plot(x, y, 'r-') line2, = ax2.plot(x, y, 'g-') line3, = ax3.plot(x, y, 'y-') line4, = ax4.plot(x, y, 'm-') line5, = ax5.plot(x, y, 'k-') line6, = ax6.plot(x, y, 'p-')  # turn off interactive plotting - speeds things up by 1 Frame / second plt.ioff()   tstart = time.time()               # for profiling for i in arange(1, 200):     line1.set_ydata(sin(x+i/10.0))  # update the data     line2.set_ydata(sin(2*x+i/10.0))     line3.set_ydata(sin(3*x+i/10.0))     line4.set_ydata(sin(4*x+i/10.0))     line5.set_ydata(sin(5*x+i/10.0))     line6.set_ydata(sin(6*x+i/10.0))     draw()                         # redraw the canvas  print 'FPS:' , 200/(time.time()-tstart)      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print to console in Py Test?",
        "A_Content": "  By default, py.test captures the result of standard out so that it can control how it prints it out. If it didn't do this, it would spew out a lot of text without the context of what test printed that text.  However, if a test fails, it will include a section in the resulting report that shows what was printed to standard out in that particular test.  For example,  def test_good():     for i in range(1000):         print(i)  def test_bad():     print('this should fail!')     assert False   Results in the following output:  >>> py.test tmp.py ============================= test session starts ============================== platform darwin -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2 plugins: cache, cov, pep8, xdist collected 2 items  tmp.py .F  =================================== FAILURES =================================== ___________________________________ test_bad ___________________________________      def test_bad():         print('this should fail!') >       assert False E       assert False  tmp.py:7: AssertionError ------------------------------- Captured stdout -------------------------------- this should fail! ====================== 1 failed, 1 passed in 0.04 seconds ======================   Note the Captured stdout section.  If you would like to see print statements as they are executed, you can pass the -s flag to py.test. However, note that this can sometimes be difficult to parse.  >>> py.test tmp.py -s ============================= test session starts ============================== platform darwin -- Python 2.7.6 -- py-1.4.20 -- pytest-2.5.2 plugins: cache, cov, pep8, xdist collected 2 items  tmp.py 0 1 2 3 ... and so on ... 997 998 999 .this should fail! F  =================================== FAILURES =================================== ___________________________________ test_bad ___________________________________      def test_bad():         print('this should fail!') >       assert False E       assert False  tmp.py:7: AssertionError ====================== 1 failed, 1 passed in 0.02 seconds ======================      ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "python-2.7",
            "py.test"
        ],
        "URL": "https://stackoverflow.com/questions/24617397/how-to-print-to-console-in-py-test",
        "A_Votes": "90",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm trying to use Test-Driven Development with the pytest module. pytest will not print to the console when I write print.  I use py.test my_tests.py to run it...  The documentation seems to say that it should work by default: http://pytest.org/latest/capture.html  But:  import myapplication as tum  class TestBlogger:      @classmethod     def setup_class(self):         self.user = \"alice\"         self.b = tum.Blogger(self.user)         print \"This should be printed, but it won't be!\"      def test_inherit(self):         assert issubclass(tum.Blogger, tum.Site)         links = self.b.get_links(posts)         print len(links)   # This won't print either.   Nothing gets printed to my standard output console (just the normal progress and how many many tests passed/failed).  And the script that I'm testing contains print:  class Blogger(Site):     get_links(self, posts):         print len(posts)   # It won't get printed in the test.   In unittest module, everything gets printed by default, which is exactly what I need. However, I wish to use pytest for other reasons. It seems like such basic functionality that perhaps I'm missing it!?  Does anyone know how to make the print statements get shown?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print to console in Py Test?",
        "A_Content": "    Using -s option will print output of all functions, which may be too much.   If you need particular output, the doc page you mentioned offers few suggestions:   Insert assert False, \"dumb assert to make PyTest print my stuff\" at the end of your function, and you will see your output due to failed test. You have special object passed to you by PyTest, and you can write the output into a file to inspect it later, like  def test_good1(capsys):     for i in range(5):         print i     out, err = capsys.readouterr()     open(\"err.txt\", \"w\").write(err)     open(\"out.txt\", \"w\").write(out)   You can open the out and err files in a separate tab and let editor automatically refresh it for you, or do a simple py.test; cat out.txt shell command to run your test.   That is rather hackish way to do stuff, but may be it is the stuff you need: after all, TDD means you mess with stuff and leave it clean and silent when it's ready :-).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "python-2.7",
            "py.test"
        ],
        "URL": "https://stackoverflow.com/questions/24617397/how-to-print-to-console-in-py-test",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to use Test-Driven Development with the pytest module. pytest will not print to the console when I write print.  I use py.test my_tests.py to run it...  The documentation seems to say that it should work by default: http://pytest.org/latest/capture.html  But:  import myapplication as tum  class TestBlogger:      @classmethod     def setup_class(self):         self.user = \"alice\"         self.b = tum.Blogger(self.user)         print \"This should be printed, but it won't be!\"      def test_inherit(self):         assert issubclass(tum.Blogger, tum.Site)         links = self.b.get_links(posts)         print len(links)   # This won't print either.   Nothing gets printed to my standard output console (just the normal progress and how many many tests passed/failed).  And the script that I'm testing contains print:  class Blogger(Site):     get_links(self, posts):         print len(posts)   # It won't get printed in the test.   In unittest module, everything gets printed by default, which is exactly what I need. However, I wish to use pytest for other reasons. It seems like such basic functionality that perhaps I'm missing it!?  Does anyone know how to make the print statements get shown?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print to console in Py Test?",
        "A_Content": "  I needed to print important warning about skipped tests exactly when PyTest muted literally everything.   I didn't want to fail a test to send a signal, so I did a hack as follow:  def test_2_YellAboutBrokenAndMutedTests():     import atexit     def report():         print C_patch.tidy_text(\"\"\" In silent mode PyTest breaks low level stream structure I work with, so I cannot test if my functionality work fine. I skipped corresponding tests. Run `py.test -s` to make sure everything is tested.\"\"\")     if sys.stdout != sys.__stdout__:         atexit.register(report)   The atexit module allows me to print stuff after PyTest released the output streams. The output looks as follow:  ============================= test session starts ============================== platform linux2 -- Python 2.7.3, pytest-2.9.2, py-1.4.31, pluggy-0.3.1 rootdir: /media/Storage/henaro/smyth/Alchemist2-git/sources/C_patch, inifile:  collected 15 items   test_C_patch.py .....ssss....s.  ===================== 10 passed, 5 skipped in 0.15 seconds ===================== In silent mode PyTest breaks low level stream structure I work with, so I cannot test if my functionality work fine. I skipped corresponding tests. Run `py.test -s` to make sure everything is tested. ~/.../sources/C_patch$   Message is printed even when PyTest is in silent mode, and is not printed if you run stuff with py.test -s, so everything is tested nicely already.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "python-2.7",
            "py.test"
        ],
        "URL": "https://stackoverflow.com/questions/24617397/how-to-print-to-console-in-py-test",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to use Test-Driven Development with the pytest module. pytest will not print to the console when I write print.  I use py.test my_tests.py to run it...  The documentation seems to say that it should work by default: http://pytest.org/latest/capture.html  But:  import myapplication as tum  class TestBlogger:      @classmethod     def setup_class(self):         self.user = \"alice\"         self.b = tum.Blogger(self.user)         print \"This should be printed, but it won't be!\"      def test_inherit(self):         assert issubclass(tum.Blogger, tum.Site)         links = self.b.get_links(posts)         print len(links)   # This won't print either.   Nothing gets printed to my standard output console (just the normal progress and how many many tests passed/failed).  And the script that I'm testing contains print:  class Blogger(Site):     get_links(self, posts):         print len(posts)   # It won't get printed in the test.   In unittest module, everything gets printed by default, which is exactly what I need. However, I wish to use pytest for other reasons. It seems like such basic functionality that perhaps I'm missing it!?  Does anyone know how to make the print statements get shown?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print to console in Py Test?",
        "A_Content": "  According to the pytest docs, pytest --capture=sys should work. If you want to capture standard out inside a test, refer to the capsys fixture.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "unit-testing",
            "python-2.7",
            "py.test"
        ],
        "URL": "https://stackoverflow.com/questions/24617397/how-to-print-to-console-in-py-test",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to use Test-Driven Development with the pytest module. pytest will not print to the console when I write print.  I use py.test my_tests.py to run it...  The documentation seems to say that it should work by default: http://pytest.org/latest/capture.html  But:  import myapplication as tum  class TestBlogger:      @classmethod     def setup_class(self):         self.user = \"alice\"         self.b = tum.Blogger(self.user)         print \"This should be printed, but it won't be!\"      def test_inherit(self):         assert issubclass(tum.Blogger, tum.Site)         links = self.b.get_links(posts)         print len(links)   # This won't print either.   Nothing gets printed to my standard output console (just the normal progress and how many many tests passed/failed).  And the script that I'm testing contains print:  class Blogger(Site):     get_links(self, posts):         print len(posts)   # It won't get printed in the test.   In unittest module, everything gets printed by default, which is exactly what I need. However, I wish to use pytest for other reasons. It seems like such basic functionality that perhaps I'm missing it!?  Does anyone know how to make the print statements get shown?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "   urllib2 is found in every Python install everywhere, so is a good base upon which to start. PycURL is useful for people already used to using libcurl, exposes more of the low-level details of HTTP, plus it gains any fixes or improvements applied to libcurl. mechanize is used to persistently drive a connection much like a browser would.   It's not a matter of one being better than the other, it's a matter of choosing the appropriate tool for the job.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "34",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "  I think this talk (at pycon 2009), has the answers for what you're looking for (Asheesh Laroia has lots of experience on the matter). And he points out the good and the bad from most of your listing   Scrape the Web: Strategies for  programming websites that don't  expect it (Part 1 of 3)     Scrape the Web: Strategies for programming websites that don't  expect it (Part 2 of 3)  Scrape the Web: Strategies for programming websites that don't expect it (Part 3 of 3)   From the PYCON 2009 schedule:     Do you find yourself faced with   websites that have data you need to   extract?    Would your life be simpler if   you could programmatically input data   into web applications, even those   tuned to resist interaction by bots?      We'll discuss the basics of web   scraping, and then dive into the   details of different methods and where   they are most applicable.       You'll leave   with an understanding of when to apply   different tools, and learn about a   \"heavy hammer\" for screen scraping   that I picked up at a project for the   Electronic Frontier Foundation.      Atendees should bring a laptop, if   possible, to try the examples we   discuss and optionally take notes.   Update: Asheesh Laroia has updated his presentation for pycon 2010   PyCon 2010: Scrape the Web: Strategies for programming websites that don't expected it   * My motto: \"The website is the API.\" * Choosing a parser: BeautifulSoup, lxml, HTMLParse, and html5lib. * Extracting information, even in the face of bad HTML: Regular expressions, BeautifulSoup, SAX, and XPath. * Automatic template reverse-engineering tools. * Submitting to forms. * Playing with XML-RPC * DO NOT BECOME AN EVIL COMMENT SPAMMER. * Countermeasures, and circumventing them:       o IP address limits       o Hidden form fields       o User-agent detection       o JavaScript       o CAPTCHAs  * Plenty of full source code to working examples:       o Submitting to forms for text-to-speech.       o Downloading music from web stores.       o Automating Firefox with Selenium RC to navigate a pure-JavaScript service.  * Q&A; and workshopping * Use your power for good, not evil.     Update 2:  PyCon US 2012 - Web scraping: Reliably and efficiently pull data from pages that don't expect it     Exciting information is trapped in web pages and behind HTML forms. In this tutorial, >you'll learn how to parse those pages and when to apply advanced techniques that make >scraping faster and more stable. We'll cover parallel downloading with Twisted, gevent, >and others; analyzing sites behind SSL; driving JavaScript-y sites with Selenium; and >evading common anti-scraping techniques.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "45",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "  Python requests is also a good candidate for HTTP stuff. It has a nicer api IMHO, an example http request from their offcial documentation:  >>> r = requests.get('https://api.github.com', auth=('user', 'pass')) >>> r.status_code 204 >>> r.headers['content-type'] 'application/json' >>> r.content ...      ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "38",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "  To \"get some webpages\", use requests!  From http://docs.python-requests.org/en/latest/ :     Pythons standard urllib2 module provides most of the HTTP   capabilities you need, but the API is thoroughly broken. It was built   for a different time  and a different web. It requires an enormous   amount of work (even method overrides) to perform the simplest of   tasks.      Things shouldnt be this way. Not in Python.   >>> r = requests.get('https://api.github.com/user', auth=('user', 'pass')) >>> r.status_code 200 >>> r.headers['content-type'] 'application/json; charset=utf8' >>> r.encoding 'utf-8' >>> r.text u'{\"type\":\"User\"...' >>> r.json() {u'private_gists': 419, u'total_private_repos': 77, ...}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "  Don't worry about \"last updated\". HTTP hasn't changed much in the last few years ;)  urllib2 is best (as it's inbuilt), then switch to mechanize if you need cookies from Firefox. mechanize can be used as a drop-in replacement for urllib2 - they have similar methods etc. Using Firefox cookies means you can get things from sites (like say StackOverflow) using your personal login credentials. Just be responsible with your number of requests (or you'll get blocked).  PycURL is for people who need all the low level stuff in libcurl. I would try the other libraries first.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "  Urllib2 only supports HTTP GET and POST, there might be workarounds, but If your app depends on other HTTP verbs, you will probably prefer a different module.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "  Every python library that speaks HTTP has its own advantages.  Use the one that has the minimum amount of features necessary for a particular task.  Your list is missing at least urllib3 - a cool third party HTTP library which can reuse a HTTP connection, thus speeding up greatly the process of retrieving multiple URLs from the same site.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Which is best in Python: urllib2, PycURL or mechanize?",
        "A_Content": "  Take a look on Grab (http://grablib.org). It is a network library which provides two main interfaces: 1) Grab for creating network requests and parsing retrieved data  2) Spider for creating bulk site scrapers  Under the hood Grab uses pycurl and lxml but it is possible to use other network transports (for example, requests library). Requests transport is not well tested yet.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "urllib2",
            "mechanize",
            "pycurl"
        ],
        "URL": "https://stackoverflow.com/questions/2385855/which-is-best-in-python-urllib2-pycurl-or-mechanize",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Ok so I need to download some web pages using Python and did a quick investigation of my options.  Included with Python:  urllib - seems to me that I should use urllib2 instead. urllib has no cookie support, HTTP/FTP/local files only (no SSL)  urllib2 - complete HTTP/FTP client, supports most needed things like cookies, does not support all HTTP verbs (only GET and POST, no TRACE, etc.)  Full featured:  mechanize - can use/save Firefox/IE cookies, take actions like follow second link, actively maintained (0.2.5 released in March 2011)  PycURL - supports everything curl does (FTP, FTPS, HTTP, HTTPS, GOPHER, TELNET, DICT, FILE and LDAP), bad news: not updated since Sep 9, 2008 (7.19.0)  New possibilities:  urllib3 - supports connection re-using/pooling and file posting  Deprecated (a.k.a. use urllib/urllib2 instead):  httplib - HTTP/HTTPS only (no FTP)  httplib2 - HTTP/HTTPS only (no FTP)  The first thing that strikes me is that urllib/urllib2/PycURL/mechanize are all pretty mature solutions that work well. mechanize and PycURL ship with a number of Linux distributions (e.g. Fedora 13) and BSDs so installation is a non issue typically (so that's good).   urllib2 looks good but I'm wondering why PycURL and mechanize both seem very popular, is there something I am missing (i.e. if I use urllib2 will I paint myself in to a corner at some point?). I'd really like some feedback on the pros/cons of these things so I can make the best choice for myself.   Edit: added note on verb support in urllib2     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python [Errno 98] Address already in use",
        "A_Content": "  Yes, it is intended. Here you can read detailed explanation. It is possible to override this behavior by setting SO_REUSEADDR option on a socket. For example:  sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sockets",
            "connection",
            "errno"
        ],
        "URL": "https://stackoverflow.com/questions/4465959/python-errno-98-address-already-in-use",
        "A_Votes": "95",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In my Python socket program, I sometimes need to interrupt it with Ctrl-C. When I do this, it does close the connection using socket.close().  However, when I try to reopen it I have to wait what seems like a minute before I can connect again. How does one correctly close a socket? Or is this intended?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python [Errno 98] Address already in use",
        "A_Content": "  $ ps -fA | grep python 501 81211 12368   0  10:11PM ttys000    0:03.12   python -m SimpleHTTPServer  $ kill 81211      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sockets",
            "connection",
            "errno"
        ],
        "URL": "https://stackoverflow.com/questions/4465959/python-errno-98-address-already-in-use",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my Python socket program, I sometimes need to interrupt it with Ctrl-C. When I do this, it does close the connection using socket.close().  However, when I try to reopen it I have to wait what seems like a minute before I can connect again. How does one correctly close a socket? Or is this intended?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python [Errno 98] Address already in use",
        "A_Content": "  If you use a TCPServer, UDPServer or their subclasses in the SocketServer module, you can set this class variable (before instanciating a server):  SocketServer.TCPServer.allow_reuse_address = True   (via SocketServer.ThreadingTCPServer - Cannot bind to address after program restart )  This causes the init (constructor) to:   if self.allow_reuse_address:      self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sockets",
            "connection",
            "errno"
        ],
        "URL": "https://stackoverflow.com/questions/4465959/python-errno-98-address-already-in-use",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my Python socket program, I sometimes need to interrupt it with Ctrl-C. When I do this, it does close the connection using socket.close().  However, when I try to reopen it I have to wait what seems like a minute before I can connect again. How does one correctly close a socket? Or is this intended?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python [Errno 98] Address already in use",
        "A_Content": "  Nothing worked for me except running a subprocess with this command, before calling HTTPServer(('', 443), myHandler):  kill -9 $(lsof -ti tcp:443)  Of course this is only for linux-like OS!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sockets",
            "connection",
            "errno"
        ],
        "URL": "https://stackoverflow.com/questions/4465959/python-errno-98-address-already-in-use",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my Python socket program, I sometimes need to interrupt it with Ctrl-C. When I do this, it does close the connection using socket.close().  However, when I try to reopen it I have to wait what seems like a minute before I can connect again. How does one correctly close a socket? Or is this intended?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python [Errno 98] Address already in use",
        "A_Content": "  A simple solution that worked for me is to close the Terminal and restart it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sockets",
            "connection",
            "errno"
        ],
        "URL": "https://stackoverflow.com/questions/4465959/python-errno-98-address-already-in-use",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my Python socket program, I sometimes need to interrupt it with Ctrl-C. When I do this, it does close the connection using socket.close().  However, when I try to reopen it I have to wait what seems like a minute before I can connect again. How does one correctly close a socket? Or is this intended?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to get name of exception that was caught in Python?",
        "A_Content": "  Here are two different ways to get the name of the exception:   type(exception).__name__ exception.__class__.__name__   e.g.,  try:     foo = bar except Exception as exception:     assert type(exception).__name__ == 'NameError'     assert exception.__class__.__name__ == 'NameError'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling"
        ],
        "URL": "https://stackoverflow.com/questions/18176602/how-to-get-name-of-exception-that-was-caught-in-python",
        "A_Votes": "129",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How can I get the name of an exception that was raised in Python?  e.g.,  try:     foo = bar except Exception as exception:     name_of_exception = ???     assert name_of_exception == 'NameError'     print \"Failed with exception [%s]\" % name_of_exception   For example, I am catching multiple (or all) exceptions, and want to print the name of the exception in an error message.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to get name of exception that was caught in Python?",
        "A_Content": "  This works, but it seems like there must be an easier, more direct way?  try:     foo = bar except Exception as exception:     assert repr(exception) == '''NameError(\"name 'bar' is not defined\",)'''     name = repr(exception).split('(')[0]     assert name == 'NameError'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "exception",
            "exception-handling"
        ],
        "URL": "https://stackoverflow.com/questions/18176602/how-to-get-name-of-exception-that-was-caught-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get the name of an exception that was raised in Python?  e.g.,  try:     foo = bar except Exception as exception:     name_of_exception = ???     assert name_of_exception == 'NameError'     print \"Failed with exception [%s]\" % name_of_exception   For example, I am catching multiple (or all) exceptions, and want to print the name of the exception in an error message.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "gnuplot vs Matplotlib",
        "A_Content": "   You can check matplotlib's documentation yourself.  I find it quite comprehensive. I have very little experience with gnuplot-py, so I can not say whether it can do all gnuplot can. Matplotlib is written in and designed specifically for Python, so it fits very nicely with Python idioms and such. Matplotlib is a mature project. NASA uses it for some stuff. I've plotted tens of millions of points in Matplotlib, and it still looked beautiful and responded quickly. Beyond the object-oriented way of using Matplotlib is the pylab interface, which makes plotting as easy as it is in MATLAB -- that is, very easy. As for porting from gnuplot-py to matplotlib, I have no idea.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging",
            "matplotlib",
            "gnuplot",
            "graphing"
        ],
        "URL": "https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib",
        "A_Votes": "48",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the  collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?  My general considerations are:   While gnuplot has large amounts of documentation, gnuplot-py doesn't. How good is documentation community for Matplotlib? Are there things which gnuplot can do, but gnuplot-py can't?  Does Matplotlib have better Python support? Are there are big show stopping bugs in either? Annoyances? Currently gnuplot is graphing 100,000's of points, I'm planning on scaling this up to millions. Should I expect problems? How well does Matplotlib handle this? Ease of use, turnaround time for gnuplot vs Matplotlib? How easy would it be to port existing gnuplot-py code to Matplotlib?   How would you approach this task?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "gnuplot vs Matplotlib",
        "A_Content": "  Matplotlib = ease of use, Gnuplot = (slightly better) performance    I know this post is old and answered but I was passing by and wanted to put my two cents. Here is my conclusion: if you have a not-so-big data set, you should use Matplotlib. It's easier and looks better. However, if you really need performance, you could use Gnuplot.  I've added some code to test it out on your machine and see for yourself if it makes a real difference (this is not a real performance benchmark but should give a first idea).  The following graph represents the required time (in seconds) to:   Plot a random scatter graph Save the graph to a png file     Configuration:   gnuplot: 5.2.2 gnuplot-py: 1.8 matplotlib: 2.1.2   I remember the performance gap being much wider when running on an older computer with older versions of the libraries (~30 seconds difference for a large scatter plot).   Moreover, as mentionned in the comments, you can get equivalent quality of plots. But you will have to put more sweat into that to do it with Gnuplot.    Here's the code to generate the graph if you want to give it a try on your machine:  # -*- coding: utf-8 -*-  from timeit import default_timer as timer import matplotlib.pyplot as plt import Gnuplot, Gnuplot.funcutils import numpy as np import sys import os  def mPlotAndSave(x, y):     plt.scatter(x, y)     plt.savefig('mtmp.png')     plt.clf()  def gPlotAndSave(data, g):     g(\"set output 'gtmp.png'\")     g.plot(data)     g(\"clear\")  def cleanup():     try:         os.remove('gtmp.png')     except OSError:         pass     try:         os.remove('mtmp.png')     except OSError:         pass  begin = 2 end = 500000 step = 10000 numberOfPoints = range(begin, end, step) n = len(numberOfPoints) gnuplotTime = [] matplotlibTime = [] progressBarWidth = 30  # Init Gnuplot g = Gnuplot.Gnuplot() g(\"set terminal png size 640,480\")  # Init matplotlib to avoid a peak in the beginning plt.clf()  for idx, val in enumerate(numberOfPoints):     # Print a nice progress bar (crucial)     sys.stdout.write('\\r')     progress = (idx+1)*progressBarWidth/n     bar = \"\" + \"\"*progress + \"\"*(progressBarWidth-progress) + \"\" + str(idx) + \"/\" + str(n-1)     sys.stdout.write(bar)     sys.stdout.flush()      # Generate random data     x = np.random.randint(sys.maxint, size=val)       y = np.random.randint(sys.maxint, size=val)     gdata = zip(x,y)      # Generate string call to a matplotlib plot and save, call it and save execution time     start = timer()     mPlotAndSave(x, y)     end = timer()     matplotlibTime.append(end - start)      # Generate string call to a gnuplot plot and save, call it and save execution time     start = timer()     gPlotAndSave(gdata, g)     end = timer()     gnuplotTime.append(end - start)      # Clean up the files     cleanup()  del g sys.stdout.write('\\n') plt.plot(numberOfPoints, gnuplotTime, label=\"gnuplot\") plt.plot(numberOfPoints, matplotlibTime, label=\"matplotlib\") plt.legend(loc='upper right') plt.xlabel('Number of points in the scatter graph') plt.ylabel('Execution time (s)') plt.savefig('execution.png') plt.show()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging",
            "matplotlib",
            "gnuplot",
            "graphing"
        ],
        "URL": "https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib",
        "A_Votes": "36",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the  collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?  My general considerations are:   While gnuplot has large amounts of documentation, gnuplot-py doesn't. How good is documentation community for Matplotlib? Are there things which gnuplot can do, but gnuplot-py can't?  Does Matplotlib have better Python support? Are there are big show stopping bugs in either? Annoyances? Currently gnuplot is graphing 100,000's of points, I'm planning on scaling this up to millions. Should I expect problems? How well does Matplotlib handle this? Ease of use, turnaround time for gnuplot vs Matplotlib? How easy would it be to port existing gnuplot-py code to Matplotlib?   How would you approach this task?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "gnuplot vs Matplotlib",
        "A_Content": "  matplotlib has pretty good documentation, and seems to be quite stable. The plots it produces are beautiful - \"publication quality\" for sure. Due to the good documentation and the amount of example code available online, it's easy to learn and use, and I don't think you'll have much trouble translating gnuplot code to it. After all, matplotlib is being used by scientists to plot data and prepare reports - so it includes everything one needs.  One marked advantage of matplotlib is that you can integrate it with Python GUIs (wxPython and PyQt, at least) and create GUI application with nice plots.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging",
            "matplotlib",
            "gnuplot",
            "graphing"
        ],
        "URL": "https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the  collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?  My general considerations are:   While gnuplot has large amounts of documentation, gnuplot-py doesn't. How good is documentation community for Matplotlib? Are there things which gnuplot can do, but gnuplot-py can't?  Does Matplotlib have better Python support? Are there are big show stopping bugs in either? Annoyances? Currently gnuplot is graphing 100,000's of points, I'm planning on scaling this up to millions. Should I expect problems? How well does Matplotlib handle this? Ease of use, turnaround time for gnuplot vs Matplotlib? How easy would it be to port existing gnuplot-py code to Matplotlib?   How would you approach this task?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "gnuplot vs Matplotlib",
        "A_Content": "  After using GNUplot (with my own Python wrapper) for a long time (and really not liking the 80s-looking output), I just started having a look at matplotlib. I must say I like it very much, the output looks really nice and the docs are high quality and extensive (although that also goes for GNUplot). The one thing I spent ages looking for in the matplotlib docs is how to write to an image file rather than to the screen! Luckily this page explains it pretty well: http://www.dalkescientific.com/writings/diary/archive/2005/04/23/matplotlib_without_gui.html     ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging",
            "matplotlib",
            "gnuplot",
            "graphing"
        ],
        "URL": "https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the  collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?  My general considerations are:   While gnuplot has large amounts of documentation, gnuplot-py doesn't. How good is documentation community for Matplotlib? Are there things which gnuplot can do, but gnuplot-py can't?  Does Matplotlib have better Python support? Are there are big show stopping bugs in either? Annoyances? Currently gnuplot is graphing 100,000's of points, I'm planning on scaling this up to millions. Should I expect problems? How well does Matplotlib handle this? Ease of use, turnaround time for gnuplot vs Matplotlib? How easy would it be to port existing gnuplot-py code to Matplotlib?   How would you approach this task?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "gnuplot vs Matplotlib",
        "A_Content": "  I have played with both, and I like Matplotlib much better in terms of Python integration, options, and quality of graphs/plots.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging",
            "matplotlib",
            "gnuplot",
            "graphing"
        ],
        "URL": "https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the  collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?  My general considerations are:   While gnuplot has large amounts of documentation, gnuplot-py doesn't. How good is documentation community for Matplotlib? Are there things which gnuplot can do, but gnuplot-py can't?  Does Matplotlib have better Python support? Are there are big show stopping bugs in either? Annoyances? Currently gnuplot is graphing 100,000's of points, I'm planning on scaling this up to millions. Should I expect problems? How well does Matplotlib handle this? Ease of use, turnaround time for gnuplot vs Matplotlib? How easy would it be to port existing gnuplot-py code to Matplotlib?   How would you approach this task?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "gnuplot vs Matplotlib",
        "A_Content": "  What Gnuplot can do Gnuplot-Py can do too. Because Gnuplot can be driven by pipe(pgnuplot). Gnuplot-Py is just a thin layer for it. So you don't need worry about it.  Why I prefer gnuplot maybe the many output format(PDF, PS and LaTex), which is very useful in papers, and the default output looks more scientific-style :)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging",
            "matplotlib",
            "gnuplot",
            "graphing"
        ],
        "URL": "https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the  collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?  My general considerations are:   While gnuplot has large amounts of documentation, gnuplot-py doesn't. How good is documentation community for Matplotlib? Are there things which gnuplot can do, but gnuplot-py can't?  Does Matplotlib have better Python support? Are there are big show stopping bugs in either? Annoyances? Currently gnuplot is graphing 100,000's of points, I'm planning on scaling this up to millions. Should I expect problems? How well does Matplotlib handle this? Ease of use, turnaround time for gnuplot vs Matplotlib? How easy would it be to port existing gnuplot-py code to Matplotlib?   How would you approach this task?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "gnuplot vs Matplotlib",
        "A_Content": "  About performance and plotting a great number of points: I compared this for a scatterplot of 500.000 points loaded from a text file and saved to a png, using gnuplot* and matplotlib.  500.000 points scatterplot gnuplot:      5.171 s matplotlib: 230.693 s   I ran it only once and the results don't look identical, but I think the idea is clear: gnuplot wins at performance.  *I used gnuplot directly since the gnuplotpy demo doesn't work out-of-the-box for me. Matplotlib wins at Python integration.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "logging",
            "matplotlib",
            "gnuplot",
            "graphing"
        ],
        "URL": "https://stackoverflow.com/questions/911655/gnuplot-vs-matplotlib",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've started on a project graphing Tomcat logs using gnuplot-py, specifically correlating particular requests with memory allocation and garbage collection. What is the  collective wisdom on gnuplot-py vs Matplotlib for Python graphing. Are there better graphing libraries out there I haven't heard of?  My general considerations are:   While gnuplot has large amounts of documentation, gnuplot-py doesn't. How good is documentation community for Matplotlib? Are there things which gnuplot can do, but gnuplot-py can't?  Does Matplotlib have better Python support? Are there are big show stopping bugs in either? Annoyances? Currently gnuplot is graphing 100,000's of points, I'm planning on scaling this up to millions. Should I expect problems? How well does Matplotlib handle this? Ease of use, turnaround time for gnuplot vs Matplotlib? How easy would it be to port existing gnuplot-py code to Matplotlib?   How would you approach this task?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Any tutorials for developing chatbots? [closed]",
        "A_Content": "  You can read a nice introduction to various techniques used to design chatbots here: http://www.gamasutra.com/view/feature/6305/beyond_fa%C3%A7ade_pattern_matching_.php  Also, here are a few useful links:   http://web.archive.org/web/20120320060043/ http://ai-programming.com/bot_tutorial.htm http://www.alicebot.org/be.html http://en.wikipedia.org/wiki/List_of_chatterbots http://www.codeproject.com/Articles/36106/Chatbot-Tutorial http://www.slideshare.net/amyiris/ai-and-python-developing-a-conversational-interface-using-python   The Natural Language Toolkit (python) implements a few chatbots: http://nltk.github.com/api/nltk.chat.html  Simple pipeline architecture for a spoken dialogue system from the book Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit BySteven Bird, Ewan Klein, Edward Loper:       ",
        "Language": "Python",
        "Tags": [
            "python",
            "artificial-intelligence",
            "nlp",
            "chatbot"
        ],
        "URL": "https://stackoverflow.com/questions/9706769/any-tutorials-for-developing-chatbots",
        "A_Votes": "121",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    As a engineering student, I would like to make a chat bot using python. So, I searched a lot but couldn't really find stuff that would teach me or give me some concrete information to build a intelligent chat bot.  I would like to make a chatbot that gives human-like responses (Simply like a friend chatting with you). I am currently expecting it to be as just a software on my laptop (would like to implement in IM, IRC or websites later).  So, I am looking for a tutorial/ any other information which would certainly help me to get my project done.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Any tutorials for developing chatbots? [closed]",
        "A_Content": "  The two places I would start with are how cleverbot works [part of a podcast] and then go through the Natural Language Toolkit Book to learn about the algorithms to use. (NLTK uses python, but the book is also a python tutorial)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "artificial-intelligence",
            "nlp",
            "chatbot"
        ],
        "URL": "https://stackoverflow.com/questions/9706769/any-tutorials-for-developing-chatbots",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    As a engineering student, I would like to make a chat bot using python. So, I searched a lot but couldn't really find stuff that would teach me or give me some concrete information to build a intelligent chat bot.  I would like to make a chatbot that gives human-like responses (Simply like a friend chatting with you). I am currently expecting it to be as just a software on my laptop (would like to implement in IM, IRC or websites later).  So, I am looking for a tutorial/ any other information which would certainly help me to get my project done.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I copy a directory to a remote machine using Fabric?",
        "A_Content": "  You can use put for that as well (at least in 1.0.0):     local_path may be a relative or absolute local file or directory path, and may contain shell-style wildcards, as understood by the Python glob module. Tilde expansion (as implemented by os.path.expanduser) is also performed.   See: http://docs.fabfile.org/en/1.0.0/api/core/operations.html#fabric.operations.put    Update: This example works fine (for me) on 1.0.0.:  from fabric.api import env from fabric.operations import run, put  env.hosts = ['frodo@middleearth.com']  def copy():     # make sure the directory is there!     run('mkdir -p /home/frodo/tmp')      # our local 'testdirectory' - it may contain files or subdirectories ...     put('testdirectory', '/home/frodo/tmp')  # [frodo@middleearth.com] Executing task 'copy' # [frodo@middleearth.com] run: mkdir -p /home/frodo/tmp # [frodo@middleearth.com] put: testdirectory/HELLO -> \\ #     /home/frodo/tmp/testdirectory/HELLO # [frodo@middleearth.com] put: testdirectory/WORLD -> \\ #     /home/frodo/tmp/testdirectory/WORLD # ...      ",
        "Language": "Python",
        "Tags": [
            "python",
            "fabric"
        ],
        "URL": "https://stackoverflow.com/questions/5314711/how-do-i-copy-a-directory-to-a-remote-machine-using-fabric",
        "A_Votes": "106",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a directory on my local machine that I would like to copy to a remote machine (and rename it) using Fabric. I know I can copy file using put(), but what about a directory. I know it's easy enough using scp, but I would prefer to do it from within my fabfile.py if possible.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I copy a directory to a remote machine using Fabric?",
        "A_Content": "  I would also look at the Project Tools module: fabric.contrib.project Documentation  This has an upload_project function which takes a source and target directory.  Even better, there is an rsync_project function that uses rsync.  This is nice because it only updates the files that have changed and it accepts extra args like \"exclude\" which is nice for doing things like excluding your .git directory.  For example:  from fabric.contrib.project import rsync_project  def _deploy_ec2(loc):      rsync_project(local_dir=loc, remote_dir='/var/www', exclude='.git')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "fabric"
        ],
        "URL": "https://stackoverflow.com/questions/5314711/how-do-i-copy-a-directory-to-a-remote-machine-using-fabric",
        "A_Votes": "29",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a directory on my local machine that I would like to copy to a remote machine (and rename it) using Fabric. I know I can copy file using put(), but what about a directory. I know it's easy enough using scp, but I would prefer to do it from within my fabfile.py if possible.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Crop the image using PIL in python",
        "A_Content": "  There is a crop() method:  w, h = yourImage.size yourImage.crop((0, 30, w, h-30)).save(...)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/9983263/crop-the-image-using-pil-in-python",
        "A_Votes": "124",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to crop image in the way by removing first 30 rows and last 30 rows from the given image. I have searched but did not get the exact solution. Does somebody have some suggestions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Crop the image using PIL in python",
        "A_Content": "  You need to import PIL (Pillow) for this. Suppose you have an image of size 1200, 1600. We will crop image from 400, 400 to 800, 800  from PIL import Image img = Image.open(\"ImageName.jpg\") area = (400, 400, 800, 800) cropped_img = img.crop(area) cropped_img.show()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/9983263/crop-the-image-using-pil-in-python",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to crop image in the way by removing first 30 rows and last 30 rows from the given image. I have searched but did not get the exact solution. Does somebody have some suggestions?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to call a Python function from Node.js",
        "A_Content": "  Easiest way I know of is to use \"child_process\" package which comes packaged with node.  Then you can do something like:  const spawn = require(\"child_process\").spawn; const pythonProcess = spawn('python',[\"path/to/script.py\", arg1, arg2, ...]);   Then all you have to do is make sure that you import sys in your python script, and then you can access arg1 using  sys.argv[1], arg2 using  sys.argv[2], and so on.    To send data back to node just do the following in the python script:  print(dataToSendBack) sys.stdout.flush()   And then node can listen for data using:  pythonProcess.stdout.on('data', (data) => {     // Do something with the data returned from python script });   Since this allows multiple arguments to be passed to a script using spawn, you can restructure a python script so that one of the arguments decides which function to call, and the other argument gets passed to that function, etc.   Hope this was clear.  Let me know if something needs clarification.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "node.js",
            "express"
        ],
        "URL": "https://stackoverflow.com/questions/23450534/how-to-call-a-python-function-from-node-js",
        "A_Votes": "103",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to call a Python function from Node.js",
        "A_Content": "  Example for people who are from Python background and want to integrate their machine learning model in the Node.js application:  It uses the child_process core module:  const express = require('express') const app = express()  app.get('/', (req, res) => {      const { spawn } = require('child_process');     const pyProg = spawn('python', ['./../pypy.py']);      pyProg.stdout.on('data', function(data) {          console.log(data.toString());         res.write(data);         res.end('end');     }); })  app.listen(4000, () => console.log('Application listening on port 4000!'))   It doesn't require sys module in your Python script.  Below is a more modular way of performing the task using Promise:  const express = require('express') const app = express()  let runPy = new Promise(function(success, nosuccess) {      const { spawn } = require('child_process');     const pyprog = spawn('python', ['./../pypy.py']);      pyprog.stdout.on('data', function(data) {          success(data);     });      pyprog.stderr.on('data', (data) => {          nosuccess(data);     }); });  app.get('/', (req, res) => {      res.write('welcome\\n');      runPy.then(function(fromRunpy) {         console.log(fromRunpy.toString());         res.end(fromRunpy);     }); })  app.listen(4000, () => console.log('Application listening on port 4000!'))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "node.js",
            "express"
        ],
        "URL": "https://stackoverflow.com/questions/23450534/how-to-call-a-python-function-from-node-js",
        "A_Votes": "27",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to call a Python function from Node.js",
        "A_Content": "  The python-shell module by extrabacon is a simple way to run Python scripts from Node.js with basic, but efficient inter-process communication and better error handling.  Installation: npm install python-shell.  Running a simple Python script:  var PythonShell = require('python-shell');  PythonShell.run('my_script.py', function (err) {   if (err) throw err;   console.log('finished'); });   Running a Python script with arguments and options:  var PythonShell = require('python-shell');  var options = {   mode: 'text',   pythonPath: 'path/to/python',   pythonOptions: ['-u'],   scriptPath: 'path/to/my/scripts',   args: ['value1', 'value2', 'value3'] };  PythonShell.run('my_script.py', options, function (err, results) {   if (err)      throw err;   // Results is an array consisting of messages collected during execution   console.log('results: %j', results); });   For the full documentation and source code, check out https://github.com/extrabacon/python-shell     ",
        "Language": "Python",
        "Tags": [
            "python",
            "node.js",
            "express"
        ],
        "URL": "https://stackoverflow.com/questions/23450534/how-to-call-a-python-function-from-node-js",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to call a Python function from Node.js",
        "A_Content": "  The child process module for Node.js provides functionality to run scripts or commands in languages other than JavaScript too (like Python). We can implement machine learning algorithms, deep learning algorithms and many features provided via a Python library into a Node.js application. The child process module allows us to run Python script in a Node.js application and stream in/out data into/from a Python script.  child_process.spawn(): This method helps us to spawn a child process asynchronously.  Lets create a simple Python script that will take two command line arguments as a first name and last name and then display them. Later well run that script from a Node.js application and display output in the browser window.  Python script:  import sys # Takes first name and last name via command # line arguments and then display them print(\"Output from Python\") print(\"First name: \" + sys.argv[1]) print(\"Last name: \" + sys.argv[2])  # Save the script as hello.py   Node.js server code:  // Import Express.js JavaScript module into the application // and creates its variable. var express = require('express'); var app = express();  // Creates a server which runs on port 3000 and // can be accessed through localhost:3000 app.listen(3000, function() {     console.log('server running on port 3000'); } )  // Function callName() is executed whenever // the URL is of the form localhost:3000/name app.get('/name', callName);  function callName(req, res) {      // Use child_process.spawn method from     // child_process module and assign it     // to variable spawn     var spawn = require(\"child_process\").spawn;      // Parameters passed in spawn -     // 1. type_of_script     // 2. List containing Path of the script     //    and arguments for the script      // E.g.: http://localhost:3000/name?firstname=Mike&lastname=Will     // So, first name = Mike and last name = Will     var process = spawn('python',[\"./hello.py\",                             req.query.firstname,                             req.query.lastname] );      // Takes stdout data from script which executed     // with arguments and send this data to res object     process.stdout.on('data', function(data) {         res.send(data.toString());     } ) }  // Save code as start.js   After saving the Python script and server script code, run the code from its source folder by the following command:  node start.js      ",
        "Language": "Python",
        "Tags": [
            "python",
            "node.js",
            "express"
        ],
        "URL": "https://stackoverflow.com/questions/23450534/how-to-call-a-python-function-from-node-js",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to call a Python function from Node.js",
        "A_Content": "  I'm on node 10 and child process 1.0.2. The data from python is a byte array and has to be converted. Just another quick example of making a http request in python.  node  const process = spawn(\"python\", [\"services/request.py\", \"https://www.google.com\"])  return new Promise((resolve, reject) =>{     process.stdout.on(\"data\", data =>{         resolve(data.toString()); // <------------ by default converts to utf-8     })     process.stderr.on(\"data\", reject) })   request.py  import urllib.request import sys  response = urllib.request.urlopen(sys.argv[1]) html = response.read() print(html) sys.stdout.flush()   p.s. not a contrived example since node's http module doesn't load a few requests I need to make     ",
        "Language": "Python",
        "Tags": [
            "python",
            "node.js",
            "express"
        ],
        "URL": "https://stackoverflow.com/questions/23450534/how-to-call-a-python-function-from-node-js",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have an Express Node.js application, but I also have a machine learning algorithm to use in Python. Is there a way I can call Python functions from my Node.js application to make use of the power of machine learning libraries?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to write string literals in python without having to escape them?",
        "A_Content": "  Raw string literals:  >>> r'abc\\dev\\t' 'abc\\\\dev\\\\t'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "escaping"
        ],
        "URL": "https://stackoverflow.com/questions/4703516/how-to-write-string-literals-in-python-without-having-to-escape-them",
        "A_Votes": "87",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value?   I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks!     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to write string literals in python without having to escape them?",
        "A_Content": "  If you're dealing with very large strings, specifically multiline strings, be aware of the triple-quote syntax:  a = r\"\"\"This is a multiline string with more than one line in the source code.\"\"\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "escaping"
        ],
        "URL": "https://stackoverflow.com/questions/4703516/how-to-write-string-literals-in-python-without-having-to-escape-them",
        "A_Votes": "48",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value?   I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks!     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to write string literals in python without having to escape them?",
        "A_Content": "  There is no such thing. It looks like you want something like \"here documents\" in Perl and the shells, but Python doesn't have that.  Using raw strings or multiline strings only means that there are fewer things to worry about. If you use a raw string then you still have to work around a terminal \"\\\" and with any string solution you'll have to worry about the closing \", ', ''' or \"\"\" if it is included in your data.  That is, there's no way to have the string   '   ''' \"\"\"  \" \\   properly stored in any Python string literal without internal escaping of some sort.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "escaping"
        ],
        "URL": "https://stackoverflow.com/questions/4703516/how-to-write-string-literals-in-python-without-having-to-escape-them",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value?   I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks!     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to write string literals in python without having to escape them?",
        "A_Content": "  (Assuming you are not required to input the string from directly within Python code)  to get around the Issue Andrew Dalke pointed out, simply type the literal string into a text file and then use this;  input_ = '/directory_of_text_file/your_text_file.txt'  input_open   = open(input_,'r+') input_string = input_open.read()  print input_string   This will print the literal text of whatever is in the text file, even if it is;   '   ''' \"\"\"   \\   Not fun or optimal, but can be useful, especially if you have 3 pages of code that wouldve needed character escaping.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "escaping"
        ],
        "URL": "https://stackoverflow.com/questions/4703516/how-to-write-string-literals-in-python-without-having-to-escape-them",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value?   I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks!     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to write string literals in python without having to escape them?",
        "A_Content": "  You will find Python's string literal documentation here:  http://docs.python.org/tutorial/introduction.html#strings  and here:  http://docs.python.org/reference/lexical_analysis.html#literals  The simplest example would be using the 'r' prefix:  ss = r'Hello\\nWorld' print(ss) Hello\\nWorld      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "escaping"
        ],
        "URL": "https://stackoverflow.com/questions/4703516/how-to-write-string-literals-in-python-without-having-to-escape-them",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to declare a string variable in python such that everything inside of it is automatically escaped, or has its literal character value?   I'm not asking how to escape the quotes with slashes, that's obvious. What I'm asking for is a general purpose way for making everything in a string literal so that I don't have to manually go through and escape everything for very large strings. Anyone know of a solution? Thanks!     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python: Maximum recursion depth exceeded",
        "A_Content": "  You can increment the stack depth allowed - with this, deeper recursive calls will be possible, like this:  import sys sys.setrecursionlimit(10000) # 10000 is an example, try with different values   ... But I'd advise you to first try to optimize your code, for instance, using iteration instead of recursion.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "recursion",
            "max",
            "depth",
            "tree-traversal"
        ],
        "URL": "https://stackoverflow.com/questions/8177073/python-maximum-recursion-depth-exceeded",
        "A_Votes": "147",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have the following recursion code, at each node I call sql query to get the nodes belong to the parent node.   here is the error:   Exception RuntimeError: 'maximum recursion depth exceeded' in <bound method DictCursor.__del__ of <MySQLdb.cursors.DictCursor object at 0x879768c>> ignored  RuntimeError: maximum recursion depth exceeded while calling a Python object Exception AttributeError: \"'DictCursor' object has no attribute 'connection'\" in <bound method DictCursor.__del__ of <MySQLdb.cursors.DictCursor object at 0x879776c>> ignored   Method that I call to get sql results:  def returnCategoryQuery(query, variables={}):     cursor = db.cursor(cursors.DictCursor);     catResults = [];     try:         cursor.execute(query, variables);         for categoryRow in cursor.fetchall():             catResults.append(categoryRow['cl_to']);         return catResults;     except Exception, e:         traceback.print_exc();   I actually don't have any issue with the above method but I put it anyways to give proper overview of the question.  Recursion Code:  def leaves(first, path=[]):     if first:         for elem in first:             if elem.lower() != 'someString'.lower():                 if elem not in path:                     queryVariable = {'title': elem}                     for sublist in leaves(returnCategoryQuery(categoryQuery, variables=queryVariable)):                         path.append(sublist)                         yield sublist                     yield elem   Calling the recursive function   for key, value in idTitleDictionary.iteritems():     for startCategory in value[0]:         print startCategory + \" ==== Start Category\";         categoryResults = [];         try:             categoryRow = \"\";             baseCategoryTree[startCategory] = [];             #print categoryQuery % {'title': startCategory};             cursor.execute(categoryQuery, {'title': startCategory});             done = False;             while not done:                 categoryRow = cursor.fetchone();                 if not categoryRow:                     done = True;                     continue;                 rowValue = categoryRow['cl_to'];                 categoryResults.append(rowValue);         except Exception, e:             traceback.print_exc();         try:             print \"Printing depth \" + str(depth);             baseCategoryTree[startCategory].append(leaves(categoryResults))         except Exception, e:             traceback.print_exc();   Code to print the dictionary,   print \"---Printing-------\" for key, value in baseCategoryTree.iteritems():     print key,     for elem in value[0]:         print elem + ',';     raw_input(\"Press Enter to continue...\")     print   If the recursion is too deep I should be getting the error when I call my recursion function, but when I get this error when I print the dictionary.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  You can create your own dictionary type by subclassing dict and adding the logic that you want. Here's a basic example:  class TwoWayDict(dict):     def __setitem__(self, key, value):         # Remove any previous connections with these values         if key in self:             del self[key]         if value in self:             del self[value]         dict.__setitem__(self, key, value)         dict.__setitem__(self, value, key)      def __delitem__(self, key):         dict.__delitem__(self, self[key])         dict.__delitem__(self, key)      def __len__(self):         \"\"\"Returns the number of connections\"\"\"         return dict.__len__(self) // 2   And it works like so:  >>> d = TwoWayDict() >>> d['foo'] = 'bar' >>> d['foo'] 'bar' >>> d['bar'] 'foo' >>> len(d) 1 >>> del d['foo'] >>> d['bar'] Traceback (most recent call last):   File \"<stdin>\", line 7, in <module> KeyError: 'bar'   I'm sure I didn't cover all the cases, but that should get you started.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "72",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  In your special case you can store both in one dictionary:  relation = {} relation['Alice'] = 'Bob' relation['Bob'] = 'Alice'   Since what you are describing is a symmetric relationship. A -> B => B -> A     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "35",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  I would just populate a second hash, with  reverse_map = dict((reversed(item) for item in forward_map.items()))      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  Two hash maps is actually probably the fastest-performing solution assuming you can spare the memory.  I would wrap those in a single class - the burden on the programmer is in ensuring that two the hash maps sync up correctly.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  I know it's an older question, but I wanted to mention another great solution to this problem, namely the python package bidict. It's extremely straight forward to use:  from bidict import bidict map = bidict(Bob = \"Alice\") print(map[\"Bob\"]) print(map.inv[\"Alice\"])      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  No, there is really no way to do this without creating two dictionaries. How would it be possible to implement this with just one dictionary while continuing to offer comparable performance?  You are better off creating a custom type that encapsulates two dictionaries and exposes the functionality you want.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  You have two separate issues.   You have a \"Conversation\" object.  It refers to two Persons.  Since a Person can have multiple conversations, you have a many-to-many relationship. You have a Map from Person to a list of Conversations.  A Conversion will have a pair of Persons.   Do something like this  from collections import defaultdict switchboard= defaultdict( list )  x = Conversation( \"Alice\", \"Bob\" ) y = Conversation( \"Alice\", \"Charlie\" )  for c in ( x, y ):     switchboard[c.p1].append( c )     switchboard[c.p2].append( c )      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  You may be able to use a DoubleDict as shown in recipe 578224 on the Python Cookbook.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  Another possible solution is to implement a subclass of dict, that holds the original dictionary and keeps track of a reversed version of it. Keeping two seperate dicts can be useful if keys and values are overlapping.   class TwoWayDict(dict):     def __init__(self, my_dict):         dict.__init__(self, my_dict)         self.rev_dict = {v : k for k,v in my_dict.iteritems()}      def __setitem__(self, key, value):         dict.__setitem__(self, key, value)         self.rev_dict.__setitem__(value, key)      def pop(self, key):         self.rev_dict.pop(self[key])         dict.pop(self, key)      # The above is just an idea other methods     # should also be overridden.    Example:  >>> d = {'a' : 1, 'b' : 2} # suppose we need to use d and its reversed version >>> twd = TwoWayDict(d)    # create a two-way dict >>> twd {'a': 1, 'b': 2} >>> twd.rev_dict {1: 'a', 2: 'b'} >>> twd['a'] 1 >>> twd.rev_dict[2] 'b' >>> twd['c'] = 3    # we add to twd and reversed version also changes >>> twd {'a': 1, 'c': 3, 'b': 2} >>> twd.rev_dict {1: 'a', 2: 'b', 3: 'c'} >>> twd.pop('a')   # we pop elements from twd and reversed  version changes >>> twd {'c': 3, 'b': 2} >>> twd.rev_dict {2: 'b', 3: 'c'}      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  The kjbuckets C extension module provides a \"graph\" data structure which I believe gives you what you want.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  There's the collections-extended library on pypi: https://pypi.python.org/pypi/collections-extended/0.6.0  Using the bijection class is as easy as:  RESPONSE_TYPES = bijection({     0x03 : 'module_info',     0x09 : 'network_status_response',     0x10 : 'trust_center_device_update' }) >>> RESPONSE_TYPES[0x03] 'module_info' >>> RESPONSE_TYPES.inverse['network_status_response'] 0x09      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Two way/reverse map",
        "A_Content": "  Here's one more two-way dictionary implementation by extending pythons dict class in case you didn't like any of those other ones:  class DoubleD(dict):     \"\"\" Access and delete dictionary elements by key or value. \"\"\"       def __getitem__(self, key):         if key not in self:             inv_dict = {v:k for k,v in self.items()}             return inv_dict[key]         return dict.__getitem__(self, key)      def __delitem__(self, key):         if key not in self:             inv_dict = {v:k for k,v in self.items()}             dict.__delitem__(self, inv_dict[key])         else:             dict.__delitem__(self, key)   Use it as a normal python dictionary except in construction:  dd = DoubleD() dd['foo'] = 'bar'      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1456373/two-way-reverse-map",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm doing this switchboard thing in python where I need to keep track of who's talking to whom, so if Alice --> Bob, then that implies that Bob --> Alice.   Yes, I could populate two hash maps, but I'm wondering if anyone has an idea to do it with one.   Or suggest another data structure.  There are no multiple conversations. Let's say this is for a customer service call center, so when Alice dials into the switchboard, she's only going to talk to Bob. His replies also go only to her.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check whether a number is divisible by another number (Python)?",
        "A_Content": "  You do this using the modulus operator, %  n % k == 0   evaluates true if and only if n is an exact multiple of k. In elementary maths this is known as the remainder from a division.  In your current approach you perform a division and the result will be either   always an integer if you use integer division, or always a float if you use floating point division.   It's just the wrong way to go about testing divisibility.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "modulus"
        ],
        "URL": "https://stackoverflow.com/questions/8002217/how-do-you-check-whether-a-number-is-divisible-by-another-number-python",
        "A_Votes": "161",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I need to test whether each number from 1 to 1000 is a multiple of 3 or a multiple of 5. The way I thought I'd do this would be to divide the number by 3, and if the result is an integer then it would be a multiple of 3. Same with 5.   How do I test whether the number is an integer?  here is my current code:  n = 0 s = 0  while (n < 1001):     x = n/3     if isinstance(x, (int, long)):         print 'Multiple of 3!'         s = s + n     if False:         y = n/5         if isinstance(y, (int, long)):             s = s + n      print 'Number: '     print n     print 'Sum:'     print s     n = n + 1      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check whether a number is divisible by another number (Python)?",
        "A_Content": "  You can simply use % Modulus operator to check divisibility. For example: n % 2 == 0 means n is exactly divisible by 2 and n % 2 != 0 means n is not exactly divisible by 2.       ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "modulus"
        ],
        "URL": "https://stackoverflow.com/questions/8002217/how-do-you-check-whether-a-number-is-divisible-by-another-number-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to test whether each number from 1 to 1000 is a multiple of 3 or a multiple of 5. The way I thought I'd do this would be to divide the number by 3, and if the result is an integer then it would be a multiple of 3. Same with 5.   How do I test whether the number is an integer?  here is my current code:  n = 0 s = 0  while (n < 1001):     x = n/3     if isinstance(x, (int, long)):         print 'Multiple of 3!'         s = s + n     if False:         y = n/5         if isinstance(y, (int, long)):             s = s + n      print 'Number: '     print n     print 'Sum:'     print s     n = n + 1      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check whether a number is divisible by another number (Python)?",
        "A_Content": "  You can use % operator to check divisiblity of a given number  The code to check whether given no. is divisible by 3 or 5 when no. less than 1000 is given below:  n=0 while n<1000:     if n%3==0 or n%5==0:         print n,'is multiple of 3 or 5'     n=n+1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "modulus"
        ],
        "URL": "https://stackoverflow.com/questions/8002217/how-do-you-check-whether-a-number-is-divisible-by-another-number-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to test whether each number from 1 to 1000 is a multiple of 3 or a multiple of 5. The way I thought I'd do this would be to divide the number by 3, and if the result is an integer then it would be a multiple of 3. Same with 5.   How do I test whether the number is an integer?  here is my current code:  n = 0 s = 0  while (n < 1001):     x = n/3     if isinstance(x, (int, long)):         print 'Multiple of 3!'         s = s + n     if False:         y = n/5         if isinstance(y, (int, long)):             s = s + n      print 'Number: '     print n     print 'Sum:'     print s     n = n + 1      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check whether a number is divisible by another number (Python)?",
        "A_Content": "  This code appears to do what you are asking for.  for value in range(1,1000):     if value % 3 == 0 or value % 5 == 0:         print(value)   Or something like  for value in range(1,1000):     if value % 3 == 0 or value % 5 == 0:         some_list.append(value)   Or any number of things.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "modulus"
        ],
        "URL": "https://stackoverflow.com/questions/8002217/how-do-you-check-whether-a-number-is-divisible-by-another-number-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to test whether each number from 1 to 1000 is a multiple of 3 or a multiple of 5. The way I thought I'd do this would be to divide the number by 3, and if the result is an integer then it would be a multiple of 3. Same with 5.   How do I test whether the number is an integer?  here is my current code:  n = 0 s = 0  while (n < 1001):     x = n/3     if isinstance(x, (int, long)):         print 'Multiple of 3!'         s = s + n     if False:         y = n/5         if isinstance(y, (int, long)):             s = s + n      print 'Number: '     print n     print 'Sum:'     print s     n = n + 1      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check whether a number is divisible by another number (Python)?",
        "A_Content": "  I had the same approach. Because I didn't understand how to use the module(%) operator.   6 % 3 = 0 *This means if you divide 6 by 3 you will not have a remainder, 3 is a factor of 6.   Now you have to relate it to your given problem.  if n % 3 == 0 *This is saying, if my number(n) is divisible by 3 leaving a 0 remainder.   Add your then(print, return) statement and continue your      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "modulus"
        ],
        "URL": "https://stackoverflow.com/questions/8002217/how-do-you-check-whether-a-number-is-divisible-by-another-number-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to test whether each number from 1 to 1000 is a multiple of 3 or a multiple of 5. The way I thought I'd do this would be to divide the number by 3, and if the result is an integer then it would be a multiple of 3. Same with 5.   How do I test whether the number is an integer?  here is my current code:  n = 0 s = 0  while (n < 1001):     x = n/3     if isinstance(x, (int, long)):         print 'Multiple of 3!'         s = s + n     if False:         y = n/5         if isinstance(y, (int, long)):             s = s + n      print 'Number: '     print n     print 'Sum:'     print s     n = n + 1      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check whether a number is divisible by another number (Python)?",
        "A_Content": "  For small numbers n%3 == 0 will be fine. For very large numbers I propose to calculate the cross sum first and then check if the cross sum is a multiple of 3:  def is_divisible_by_3(number):     if sum(map(int, str(number))) % 3 != 0:         my_bool = False     return my_bool      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "modulus"
        ],
        "URL": "https://stackoverflow.com/questions/8002217/how-do-you-check-whether-a-number-is-divisible-by-another-number-python",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I need to test whether each number from 1 to 1000 is a multiple of 3 or a multiple of 5. The way I thought I'd do this would be to divide the number by 3, and if the result is an integer then it would be a multiple of 3. Same with 5.   How do I test whether the number is an integer?  here is my current code:  n = 0 s = 0  while (n < 1001):     x = n/3     if isinstance(x, (int, long)):         print 'Multiple of 3!'         s = s + n     if False:         y = n/5         if isinstance(y, (int, long)):             s = s + n      print 'Number: '     print n     print 'Sum:'     print s     n = n + 1      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Convert Python dictionary to JSON array",
        "A_Content": "  If you are fine with non-printable symbols in your json, then add ensure_ascii=False to dumps call.  >>> json.dumps(your_data, ensure_ascii=False)      If ensure_ascii is false, then the return value will be a   unicode instance subject to normal Python str to unicode   coercion rules instead of being escaped to an ASCII str.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/14661051/convert-python-dictionary-to-json-array",
        "A_Votes": "131",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Currently I have this dictionary, printed using pprint:    {'AlarmExTempHum': '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00',   'AlarmIn': 0,   'AlarmOut': '\\x00\\x00',   'AlarmRain': 0,   'AlarmSoilLeaf': '\\x00\\x00\\x00\\x00',   'BarTrend': 60,   'BatteryStatus': 0,   'BatteryVolts': 4.751953125,   'CRC': 55003, 'EOL': '\\n\\r', 'ETDay': 0, 'ETMonth': 0, 'ETYear': 0, 'ExtraHum1': None, 'ExtraHum2': None, 'ExtraHum3': None, 'ExtraHum4': None, 'ExtraHum5': None, 'ExtraHum6': None, 'ExtraHum7': None, 'ExtraTemp1': None, 'ExtraTemp2': None, 'ExtraTemp3': None, 'ExtraTemp4': None, 'ExtraTemp5': None, 'ExtraTemp6': None, 'ExtraTemp7': None, 'ForecastIcon': 2, 'ForecastRuleNo': 122, 'HumIn': 31, 'HumOut': 94, 'LOO': 'LOO', 'LeafTemps': '\\xff\\xff\\xff\\xff', 'LeafWetness': '\\xff\\xff\\xff\\x00', 'NextRec': 37, 'PacketType': 0, 'Pressure': 995.9363359295631, 'RainDay': 0.0, 'RainMonth': 0.0, 'RainRate': 0.0, 'RainStorm': 0.0, 'RainYear': 2.8, 'SoilMoist': '\\xff\\xff\\xff\\xff', 'SoilTemps': '\\xff\\xff\\xff\\xff', 'SolarRad': None, 'StormStartDate': '2127-15-31', 'SunRise': 849, 'SunSet': 1611, 'TempIn': 21.38888888888889, 'TempOut': 0.8888888888888897, 'UV': None, 'WindDir': 219, 'WindSpeed': 3.6, 'WindSpeed10Min': 3.6}   When I do this:    import json d = (my dictionary above) jsonarray = json.dumps(d)   I get this error: 'utf8' codec can't decode byte 0xff in position 0: invalid start byte     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Convert Python dictionary to JSON array",
        "A_Content": "  ensure_ascii=False really only defers the issue to the decoding stage:  >>> dict2 = {'LeafTemps': '\\xff\\xff\\xff\\xff',} >>> json1 = json.dumps(dict2, ensure_ascii=False) >>> print(json1) {\"LeafTemps\": \"\"} >>> json.loads(json1) Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"/usr/lib/python2.7/json/__init__.py\", line 328, in loads     return _default_decoder.decode(s)   File \"/usr/lib/python2.7/json/decoder.py\", line 365, in decode     obj, end = self.raw_decode(s, idx=_w(s, 0).end())   File \"/usr/lib/python2.7/json/decoder.py\", line 381, in raw_decode     obj, end = self.scan_once(s, idx) UnicodeDecodeError: 'utf8' codec can't decode byte 0xff in position 0: invalid start byte   Ultimately you can't store raw bytes in a JSON document, so you'll want to use some means of unambiguously encoding a sequence of arbitrary bytes as an ASCII string - such as base64.  >>> import json >>> from base64 import b64encode, b64decode >>> my_dict = {'LeafTemps': '\\xff\\xff\\xff\\xff',}  >>> my_dict['LeafTemps'] = b64encode(my_dict['LeafTemps']) >>> json.dumps(my_dict) '{\"LeafTemps\": \"/////w==\"}' >>> json.loads(json.dumps(my_dict)) {u'LeafTemps': u'/////w=='} >>> new_dict = json.loads(json.dumps(my_dict)) >>> new_dict['LeafTemps'] = b64decode(new_dict['LeafTemps']) >>> print new_dict {u'LeafTemps': '\\xff\\xff\\xff\\xff'}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/14661051/convert-python-dictionary-to-json-array",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I have this dictionary, printed using pprint:    {'AlarmExTempHum': '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00',   'AlarmIn': 0,   'AlarmOut': '\\x00\\x00',   'AlarmRain': 0,   'AlarmSoilLeaf': '\\x00\\x00\\x00\\x00',   'BarTrend': 60,   'BatteryStatus': 0,   'BatteryVolts': 4.751953125,   'CRC': 55003, 'EOL': '\\n\\r', 'ETDay': 0, 'ETMonth': 0, 'ETYear': 0, 'ExtraHum1': None, 'ExtraHum2': None, 'ExtraHum3': None, 'ExtraHum4': None, 'ExtraHum5': None, 'ExtraHum6': None, 'ExtraHum7': None, 'ExtraTemp1': None, 'ExtraTemp2': None, 'ExtraTemp3': None, 'ExtraTemp4': None, 'ExtraTemp5': None, 'ExtraTemp6': None, 'ExtraTemp7': None, 'ForecastIcon': 2, 'ForecastRuleNo': 122, 'HumIn': 31, 'HumOut': 94, 'LOO': 'LOO', 'LeafTemps': '\\xff\\xff\\xff\\xff', 'LeafWetness': '\\xff\\xff\\xff\\x00', 'NextRec': 37, 'PacketType': 0, 'Pressure': 995.9363359295631, 'RainDay': 0.0, 'RainMonth': 0.0, 'RainRate': 0.0, 'RainStorm': 0.0, 'RainYear': 2.8, 'SoilMoist': '\\xff\\xff\\xff\\xff', 'SoilTemps': '\\xff\\xff\\xff\\xff', 'SolarRad': None, 'StormStartDate': '2127-15-31', 'SunRise': 849, 'SunSet': 1611, 'TempIn': 21.38888888888889, 'TempOut': 0.8888888888888897, 'UV': None, 'WindDir': 219, 'WindSpeed': 3.6, 'WindSpeed10Min': 3.6}   When I do this:    import json d = (my dictionary above) jsonarray = json.dumps(d)   I get this error: 'utf8' codec can't decode byte 0xff in position 0: invalid start byte     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Convert Python dictionary to JSON array",
        "A_Content": "  If you use Python 2, don't forget to add the UTF-8 file encoding comment on the first line of your script.  # -*- coding: UTF-8 -*-   This will fix some Unicode problems and make your life easier.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/14661051/convert-python-dictionary-to-json-array",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I have this dictionary, printed using pprint:    {'AlarmExTempHum': '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00',   'AlarmIn': 0,   'AlarmOut': '\\x00\\x00',   'AlarmRain': 0,   'AlarmSoilLeaf': '\\x00\\x00\\x00\\x00',   'BarTrend': 60,   'BatteryStatus': 0,   'BatteryVolts': 4.751953125,   'CRC': 55003, 'EOL': '\\n\\r', 'ETDay': 0, 'ETMonth': 0, 'ETYear': 0, 'ExtraHum1': None, 'ExtraHum2': None, 'ExtraHum3': None, 'ExtraHum4': None, 'ExtraHum5': None, 'ExtraHum6': None, 'ExtraHum7': None, 'ExtraTemp1': None, 'ExtraTemp2': None, 'ExtraTemp3': None, 'ExtraTemp4': None, 'ExtraTemp5': None, 'ExtraTemp6': None, 'ExtraTemp7': None, 'ForecastIcon': 2, 'ForecastRuleNo': 122, 'HumIn': 31, 'HumOut': 94, 'LOO': 'LOO', 'LeafTemps': '\\xff\\xff\\xff\\xff', 'LeafWetness': '\\xff\\xff\\xff\\x00', 'NextRec': 37, 'PacketType': 0, 'Pressure': 995.9363359295631, 'RainDay': 0.0, 'RainMonth': 0.0, 'RainRate': 0.0, 'RainStorm': 0.0, 'RainYear': 2.8, 'SoilMoist': '\\xff\\xff\\xff\\xff', 'SoilTemps': '\\xff\\xff\\xff\\xff', 'SolarRad': None, 'StormStartDate': '2127-15-31', 'SunRise': 849, 'SunSet': 1611, 'TempIn': 21.38888888888889, 'TempOut': 0.8888888888888897, 'UV': None, 'WindDir': 219, 'WindSpeed': 3.6, 'WindSpeed10Min': 3.6}   When I do this:    import json d = (my dictionary above) jsonarray = json.dumps(d)   I get this error: 'utf8' codec can't decode byte 0xff in position 0: invalid start byte     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Convert Python dictionary to JSON array",
        "A_Content": "  One possible solution that I use is to use python3.  It seems to solve many utf issues.  Sorry for the late answer, but it may help people in the future.  For example,  #!/usr/bin/env python3 import json # your code follows      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/14661051/convert-python-dictionary-to-json-array",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I have this dictionary, printed using pprint:    {'AlarmExTempHum': '\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00',   'AlarmIn': 0,   'AlarmOut': '\\x00\\x00',   'AlarmRain': 0,   'AlarmSoilLeaf': '\\x00\\x00\\x00\\x00',   'BarTrend': 60,   'BatteryStatus': 0,   'BatteryVolts': 4.751953125,   'CRC': 55003, 'EOL': '\\n\\r', 'ETDay': 0, 'ETMonth': 0, 'ETYear': 0, 'ExtraHum1': None, 'ExtraHum2': None, 'ExtraHum3': None, 'ExtraHum4': None, 'ExtraHum5': None, 'ExtraHum6': None, 'ExtraHum7': None, 'ExtraTemp1': None, 'ExtraTemp2': None, 'ExtraTemp3': None, 'ExtraTemp4': None, 'ExtraTemp5': None, 'ExtraTemp6': None, 'ExtraTemp7': None, 'ForecastIcon': 2, 'ForecastRuleNo': 122, 'HumIn': 31, 'HumOut': 94, 'LOO': 'LOO', 'LeafTemps': '\\xff\\xff\\xff\\xff', 'LeafWetness': '\\xff\\xff\\xff\\x00', 'NextRec': 37, 'PacketType': 0, 'Pressure': 995.9363359295631, 'RainDay': 0.0, 'RainMonth': 0.0, 'RainRate': 0.0, 'RainStorm': 0.0, 'RainYear': 2.8, 'SoilMoist': '\\xff\\xff\\xff\\xff', 'SoilTemps': '\\xff\\xff\\xff\\xff', 'SolarRad': None, 'StormStartDate': '2127-15-31', 'SunRise': 849, 'SunSet': 1611, 'TempIn': 21.38888888888889, 'TempOut': 0.8888888888888897, 'UV': None, 'WindDir': 219, 'WindSpeed': 3.6, 'WindSpeed10Min': 3.6}   When I do this:    import json d = (my dictionary above) jsonarray = json.dumps(d)   I get this error: 'utf8' codec can't decode byte 0xff in position 0: invalid start byte     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "python pandas: apply a function with arguments to a series",
        "A_Content": "  The documentation explains this clearly. The apply method accepts a python function which should have a single parameter. If you want to pass more parameters you should use functools.partial as suggested by Joel Cornett in his comment.  An example:  >>> import functools >>> import operator >>> add_3 = functools.partial(operator.add,3) >>> add_3(2) 5 >>> add_3(7) 10   You can also pass keyword arguments using partial.  Another way would be to create a lambda:  my_series.apply((lambda x: your_func(a,b,c,d,...,x)))   But I think using partial is better.    Note that newer versions of pandas do allow you to pass extra arguments (see the new documentation). So now you can do:  my_series.apply(your_function, args=(2,3,4), extra_kw=1)   The positional arguments are added after the element of the series.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "apply"
        ],
        "URL": "https://stackoverflow.com/questions/12182744/python-pandas-apply-a-function-with-arguments-to-a-series",
        "A_Votes": "92",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to apply a function with arguments to a series in python pandas:  x = my_series.apply(my_function, more_arguments_1) y = my_series.apply(my_function, more_arguments_2) ...   The documentation describes support for an apply method, but it doesn't accept any arguments.  Is there a different method that accepts arguments?  Alternatively, am I missing a simple workaround?  Update (October 2017):  Note that since this question was originally asked that pandas apply() has been updated to handle positional and keyword arguments and the documentation link above now reflects that and shows how to include either type of argument.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "python pandas: apply a function with arguments to a series",
        "A_Content": "  Series.apply(func, convert_dtype=True, args=(), **kwds)  args : tuple  x = my_series.apply(my_function, args = (arg1,))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "apply"
        ],
        "URL": "https://stackoverflow.com/questions/12182744/python-pandas-apply-a-function-with-arguments-to-a-series",
        "A_Votes": "33",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to apply a function with arguments to a series in python pandas:  x = my_series.apply(my_function, more_arguments_1) y = my_series.apply(my_function, more_arguments_2) ...   The documentation describes support for an apply method, but it doesn't accept any arguments.  Is there a different method that accepts arguments?  Alternatively, am I missing a simple workaround?  Update (October 2017):  Note that since this question was originally asked that pandas apply() has been updated to handle positional and keyword arguments and the documentation link above now reflects that and shows how to include either type of argument.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "python pandas: apply a function with arguments to a series",
        "A_Content": "  Steps:   Create a dataframe Create a function Use the named arguments of the function in the apply statement.   Example  x=pd.DataFrame([1,2,3,4])    def add(i1, i2):       return i1+i2  x.apply(add,i2=9)   The outcome of this example is that each number in the dataframe will be added to the number 9.        0 0  10 1  11 2  12 3  13   Explanation:  The \"add\" function has two parameters: i1, i2.  The first parameter is going to be the value in data frame and the second is whatever we pass to the \"apply\" function.  In this case, we are passing \"9\" to the apply function using the keyword argument \"i2\".       ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "apply"
        ],
        "URL": "https://stackoverflow.com/questions/12182744/python-pandas-apply-a-function-with-arguments-to-a-series",
        "A_Votes": "23",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to apply a function with arguments to a series in python pandas:  x = my_series.apply(my_function, more_arguments_1) y = my_series.apply(my_function, more_arguments_2) ...   The documentation describes support for an apply method, but it doesn't accept any arguments.  Is there a different method that accepts arguments?  Alternatively, am I missing a simple workaround?  Update (October 2017):  Note that since this question was originally asked that pandas apply() has been updated to handle positional and keyword arguments and the documentation link above now reflects that and shows how to include either type of argument.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "python pandas: apply a function with arguments to a series",
        "A_Content": "  You can pass any number of arguments to the function that apply is calling through either unnamed arguments, passed as a tuple to the args parameter, or through other keyword arguments internally captured as a dictionary by the kwds parameter.  For instance, let's build a function that returns True for values between 3 and 6, and False otherwise.  s = pd.Series(np.random.randint(0,10, 10)) s  0    5 1    3 2    1 3    1 4    6 5    0 6    3 7    4 8    9 9    6 dtype: int64  s.apply(lambda x: x >= 3 and x <= 6)  0     True 1     True 2    False 3    False 4     True 5    False 6     True 7     True 8    False 9     True dtype: bool   This anonymous function isn't very flexible. Let's created a normal function with two arguments to control the min and max values we want in our Series.  def between(x, low, high):     return x >= low and x =< high   We can replicate the output of the first function by passing unnamed arguments to args:  s.apply(between, args=(3,6))   Or we can use the named arguments  s.apply(between, low=3, high=6)   Or even a combination of both  s.apply(between, args=(3,), high=6)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "apply"
        ],
        "URL": "https://stackoverflow.com/questions/12182744/python-pandas-apply-a-function-with-arguments-to-a-series",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to apply a function with arguments to a series in python pandas:  x = my_series.apply(my_function, more_arguments_1) y = my_series.apply(my_function, more_arguments_2) ...   The documentation describes support for an apply method, but it doesn't accept any arguments.  Is there a different method that accepts arguments?  Alternatively, am I missing a simple workaround?  Update (October 2017):  Note that since this question was originally asked that pandas apply() has been updated to handle positional and keyword arguments and the documentation link above now reflects that and shows how to include either type of argument.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Get path from open file in Python",
        "A_Content": "  The key here is the name attribute of the f object representing the opened file. You get it like that:  >>> f = open('/Users/Desktop/febROSTER2012.xls') >>> f.name '/Users/Desktop/febROSTER2012.xls'   Does it help?     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/9542435/get-path-from-open-file-in-python",
        "A_Votes": "96",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    If I have an opened file, is there an os call to get the complete path as a string?  f = open('/Users/Desktop/febROSTER2012.xls')   From f, how would I get \"/Users/Desktop/febROSTER2012.xls\" ?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Get path from open file in Python",
        "A_Content": "  I had the exact same issue. If you are using a relative path os.path.dirname(path) will only return the relative path. os.path.realpath does the trick:  >>> import os >>> f = open('file.txt') >>> os.path.realpath(f.name)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/9542435/get-path-from-open-file-in-python",
        "A_Votes": "52",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    If I have an opened file, is there an os call to get the complete path as a string?  f = open('/Users/Desktop/febROSTER2012.xls')   From f, how would I get \"/Users/Desktop/febROSTER2012.xls\" ?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Get path from open file in Python",
        "A_Content": "  And if you just want to get the directory name and no need for the filename coming with it, then you can do that in the following conventional way using os Python module.  >>> import os >>> f = open('/Users/Desktop/febROSTER2012.xls') >>> os.path.dirname(f.name) >>> '/Users/Desktop/'   This way you can get hold of the directory structure.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/9542435/get-path-from-open-file-in-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    If I have an opened file, is there an os call to get the complete path as a string?  f = open('/Users/Desktop/febROSTER2012.xls')   From f, how would I get \"/Users/Desktop/febROSTER2012.xls\" ?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Get path from open file in Python",
        "A_Content": "  You can get it like this also.  filepath = os.path.abspath(f.name)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/9542435/get-path-from-open-file-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    If I have an opened file, is there an os call to get the complete path as a string?  f = open('/Users/Desktop/febROSTER2012.xls')   From f, how would I get \"/Users/Desktop/febROSTER2012.xls\" ?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Windows can't find the file on subprocess.call()",
        "A_Content": "  I am not sure why but, on my windows machine I had to add a 'shell=True' to the call.  E.g. for dir you would type:  import subprocess subprocess.call('dir', shell=True)   Hope this helps,  Douglas  To quote from the documentation: The only time you need to specify shell=True on Windows is when the command you wish to execute is built into the shell (e.g. dir or copy). You do not need shell=True to run a batch file or console-based executable.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "path",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/3022013/windows-cant-find-the-file-on-subprocess-call",
        "A_Votes": "132",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am getting the following error:  WindowsError: [Error 2] The system cannot find the file specified   My code is:  subprocess.call([\"<<executable file found in PATH>>\"])   Windows 7, 64 bit. Python 3.x latest, stable.  Any ideas?  Thanks,     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Windows can't find the file on subprocess.call()",
        "A_Content": "  On Windows, I believe the subprocess module doesn't look in the PATH unless you pass shell=True. However, shell=True can be a security risk if you're passing arguments that may come from outside your program. To make subprocess nonetheless able to find the correct executable, you can use shutil.which. Suppose the executable in your PATH is named frob:  subprocess.call([shutil.which('frob'), arg1, arg2])   (This works on Python 3.3 and above.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "path",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/3022013/windows-cant-find-the-file-on-subprocess-call",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am getting the following error:  WindowsError: [Error 2] The system cannot find the file specified   My code is:  subprocess.call([\"<<executable file found in PATH>>\"])   Windows 7, 64 bit. Python 3.x latest, stable.  Any ideas?  Thanks,     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Windows can't find the file on subprocess.call()",
        "A_Content": "  On Windows you have to call through cmd.exe. As Apalala mentioned, Windows commands are implemented in cmd.exe not as separate executables.  e.g.   subprocess.call(['cmd', '/c', 'dir'])   /c tells cmd to run the follow command  This is safer than using shell=True, which allows shell injections.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "path",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/3022013/windows-cant-find-the-file-on-subprocess-call",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am getting the following error:  WindowsError: [Error 2] The system cannot find the file specified   My code is:  subprocess.call([\"<<executable file found in PATH>>\"])   Windows 7, 64 bit. Python 3.x latest, stable.  Any ideas?  Thanks,     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Windows can't find the file on subprocess.call()",
        "A_Content": "  If the path has spaces, is it quoted?  And of course, you escaped backslashes properly, or used slashes, right?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "path",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/3022013/windows-cant-find-the-file-on-subprocess-call",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am getting the following error:  WindowsError: [Error 2] The system cannot find the file specified   My code is:  subprocess.call([\"<<executable file found in PATH>>\"])   Windows 7, 64 bit. Python 3.x latest, stable.  Any ideas?  Thanks,     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Windows can't find the file on subprocess.call()",
        "A_Content": "  If you are using powershell, then in it will be subprocess.call(['powershell','-command','dir']). Powershell supports a large portion of POSIX commands     ",
        "Language": "Python",
        "Tags": [
            "python",
            "path",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/3022013/windows-cant-find-the-file-on-subprocess-call",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am getting the following error:  WindowsError: [Error 2] The system cannot find the file specified   My code is:  subprocess.call([\"<<executable file found in PATH>>\"])   Windows 7, 64 bit. Python 3.x latest, stable.  Any ideas?  Thanks,     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to send requests with JSONs in unit tests",
        "A_Content": "  Changing the post to  response=self.app.post('/test_function',                         data=json.dumps(dict(foo='bar')),                        content_type='application/json')   fixed it.  Thanks to user3012759.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "json",
            "flask",
            "python-unittest"
        ],
        "URL": "https://stackoverflow.com/questions/28836893/how-to-send-requests-with-jsons-in-unit-tests",
        "A_Votes": "135",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have code within a Flask application that uses JSONs in the request, and I can get the JSON object like so:  Request = request.get_json()   This has been working fine, however I am trying to create unit tests using Python's unittest module and I'm having difficulty finding a way to send a JSON with the request.  response=self.app.post('/test_function',                         data=json.dumps(dict(foo = 'bar')))   This gives me:  >>> request.get_data() '{\"foo\": \"bar\"}' >>> request.get_json() None   Flask seems to have a JSON argument where you can set json=dict(foo='bar') within the post request, but I don't know how to do that with the unittest module.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to send requests with JSONs in unit tests",
        "A_Content": "  UPDATE: Since Flask 1.0 released flask.testing.FlaskClient methods accepts json argument and Response.get_json method added, see example.  for Flask 0.x you may use receipt below:  from flask import Flask, Response as BaseResponse, json from flask.testing import FlaskClient from werkzeug.utils import cached_property   class Response(BaseResponse):     @cached_property     def json(self):         return json.loads(self.data)   class TestClient(FlaskClient):     def open(self, *args, **kwargs):         if 'json' in kwargs:             kwargs['data'] = json.dumps(kwargs.pop('json'))             kwargs['content_type'] = 'application/json'         return super(TestClient, self).open(*args, **kwargs)   app = Flask(__name__) app.response_class = Response app.test_client_class = TestClient app.testing = True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "json",
            "flask",
            "python-unittest"
        ],
        "URL": "https://stackoverflow.com/questions/28836893/how-to-send-requests-with-jsons-in-unit-tests",
        "A_Votes": "23",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have code within a Flask application that uses JSONs in the request, and I can get the JSON object like so:  Request = request.get_json()   This has been working fine, however I am trying to create unit tests using Python's unittest module and I'm having difficulty finding a way to send a JSON with the request.  response=self.app.post('/test_function',                         data=json.dumps(dict(foo = 'bar')))   This gives me:  >>> request.get_data() '{\"foo\": \"bar\"}' >>> request.get_json() None   Flask seems to have a JSON argument where you can set json=dict(foo='bar') within the post request, but I don't know how to do that with the unittest module.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  As a C++ developer you can think of Python variables as pointers.  Thus when you write spam = 100, this means that you \"assign the pointer\", which was previously pointing to the object 42, to point to the object 100.  Earlier on, cheese was assigned to point to the same object as spam pointed to, which happened to be 42 at that time. Since you have not modified cheese, it still points to 42.  Immutability has nothing to do with it in this case, since pointer assignment does not change anything about the object being pointed to.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "78",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  The way I see it there are different views of a language.   The \"language lawyer\" perspective.  The \"practical programmer\" perspective. the \"implementor\" perspective.   From the language lawyer perspective python variables always \"point at\" an object. However unlike Java and C++ the behvaiour of == <= >= etc depends on the runtime type of the objects that the variables point at. Furthermore in python memory management is handled by the language.  From a practical programmer perspective we can treat the fact that integers, strings, tuples etc are immutable* objects rather than straight values as an irrelevent detail. The exception is when storing large ammounts of numeric data we may want to use types that can store the values directly (e.g. numpy arrays) rather than types that will end up with an array full of references to tiny objects.  From an implementers perspective most languages have some sort of as-if rule such that if the specified behaviours are correct the implementation is correct regardless of how things are actually done under the hood.  So yes your explanation is correct from a language lawyer perspective. Your book is correct from a practical programmer perspective. What an implementation actually does depends on the implementation. In cpython integers are real objects though small value integers are taken from a cache pool rather than created anew. I'm not sure what the other implementations (e.g. pypy and jython) do.  * note the distinction between mutable and immutable objects here. With a mutable object we have to be careful about treating it \"like a value\" because some other code might mutate it. With an immutable object we have no such concerns.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  It is correct you can more or less thing of variables as pointers. However example code would help greatly with explaining how this actually is working.  First, we will heavily utilize the id function:     Return the identity of an object. This is an integer which is guaranteed to be unique and constant for this object during its lifetime. Two objects with non-overlapping lifetimes may have the same id() value.   It's likely this will return different absolute values on your machine.  Consider this example:  >>> foo = 'a string' >>> id(foo)  4565302640 >>> bar = 'a different string' >>> id(bar) 4565321816 >>> bar = foo >>> id(bar) == id(foo) True >>> id(bar) 4565302640   You can see that:   The original foo/bar have different ids, because they point to different objects When bar is assigned to foo, their ids are now the same. This is similar to them both pointing to the same location in memory that you see in making a C++ pointer   when we change the value of foo, it is assigned to a different id:  >>> foo = 42 >>> id(foo) 4561661488 >>> foo = 'oh no' >>> id(foo) 4565257832   An interesting observation too is that integers implicitly have this functionality up to 256:  >>> a = 100 >>> b = 100 >>> c = 100 >>> id(a) == id(b) == id(c) True   However beyond 256 this is no longer true:  >>> a = 256 >>> b = 256 >>> id(a) == id(b) True >>> a = 257 >>> b = 257 >>> id(a) == id(b) False   however assigning a to b will indeed keep the id the same as shown before:  >>> a = b >>> id(a) == id(b) True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  Python is neither pass-by-reference or pass-by-value. Python variables are not pointers, they are not references, they are not values. Python variables are names.  Think of it as \"pass-by-alias\" if you need the same phrase type, or possibly \"pass-by-object\", because you can mutate the same object from any variable that indicates it, if it's mutable, but reassignment of a variable (alias) only changes that one variable.     If it helps: C variables are boxes that you write values into. Python names are tags that you put on values.   A Python variable's name is a key in the global (or local) namespace, which is effectively a dictionary. The underlying value is some object in memory. Assignment gives a name to that object. Assignment of one variable to another variable means both variables are names for the same object. Re-assignment of one variable changes what object is named by that variable without changing the other variable. You've moved the tag but not changed the previous object or any other tags on it.  In the underlying C code of the CPython implementation, every Python object is a PyObject*, so you can think of it as working like C if you only ever had pointers to data (no pointers-to-pointers, no directly-passed values).     you could say that Python is pass-by-value, where the values are pointers or you could say Python is pass-by-reference, where the references are copies.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  When you run spam = 100 python create one more object in the memory but not change existing. so you still have pointer cheese to 42 and spam to 100     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  What is happening in spam = 100 line is replacement of previous value (pointer to object of type int with value 42) with another pointer to another object (type int, value 100)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  As @DeepSpace mentioned in the comments, Ned Batchelder does a great job demystifying variables (names) and assignments to values in a blog, from which he delivered a talk at PyCon 2015, Facts and Myths about Python names and values.  It can be insightful for Pythonistas at any level of mastery.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "What happens when you assign the value of one variable to another variable in Python?",
        "A_Content": "  When you store spam = 42 , it creates an object in the memory. Then you assign cheese = spam , It assigns the object referenced by spam to cheese. And finally, when you change spam = 100, it changes only spam object. So cheese = 42.       ",
        "Language": "Python",
        "Tags": [
            "python",
            "variables",
            "object"
        ],
        "URL": "https://stackoverflow.com/questions/45053461/what-happens-when-you-assign-the-value-of-one-variable-to-another-variable-in-py",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This is my second day of learning python (I know the basics of C++ and some OOP.), and I have some slight confusion regarding variables in python.  Here is how I understand them currently:  Python variables are references (or pointers?) to objects (which are either mutable or immutable). When we have something like num = 5, the immutable object 5 is created somewhere in memory, and the name-object reference pair num is created in a certain namespace. When we have a = num, nothing is being copied, but now both variables refer to the same object and a is added to the same namespace.  This is where my book, Automate the boring stuff with Python, confuses me. As it's a newbie book, it doesn't mention objects, namespaces, etc., and it attempts to explain the following code:  >>> spam = 42 >>> cheese = spam >>> spam = 100 >>> spam 100 >>> cheese 42   The explanation it offers is exactly the same as that of a C++ book, which I am not happy about as we are dealing with references/pointers to objects. So in this case, I guess that in the 3rd line, as integers are immutable, spam is being assigned an entirely new pointer/reference to a different location in memory, i.e. the memory that it was initially pointing to wasn't modified. Hence we have cheese referring to the initial object referred to by spam. Is this the correct explanation?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "pandas DataFrame: replace nan values with average of columns",
        "A_Content": "  You can simply use DataFrame.fillna to fill the nan's directly:  In [27]: df  Out[27]:            A         B         C 0 -0.166919  0.979728 -0.632955 1 -0.297953 -0.912674 -1.365463 2 -0.120211 -0.540679 -0.680481 3       NaN -2.027325  1.533582 4       NaN       NaN  0.461821 5 -0.788073       NaN       NaN 6 -0.916080 -0.612343       NaN 7 -0.887858  1.033826       NaN 8  1.948430  1.025011 -2.982224 9  0.019698 -0.795876 -0.046431  In [28]: df.mean() Out[28]:  A   -0.151121 B   -0.231291 C   -0.530307 dtype: float64  In [29]: df.fillna(df.mean()) Out[29]:            A         B         C 0 -0.166919  0.979728 -0.632955 1 -0.297953 -0.912674 -1.365463 2 -0.120211 -0.540679 -0.680481 3 -0.151121 -2.027325  1.533582 4 -0.151121 -0.231291  0.461821 5 -0.788073 -0.231291 -0.530307 6 -0.916080 -0.612343 -0.530307 7 -0.887858  1.033826 -0.530307 8  1.948430  1.025011 -2.982224 9  0.019698 -0.795876 -0.046431   The docstring of fillna says that value should be a scalar or a dict, however, it seems to work with a Series as well. If you want to pass a dict, you could use df.mean().to_dict().     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "nan"
        ],
        "URL": "https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns",
        "A_Votes": "126",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.  How can I replace the nans with averages of columns where they are?  This question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "pandas DataFrame: replace nan values with average of columns",
        "A_Content": "  Try:  sub2['income'].fillna((sub2['income'].mean()), inplace=True)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "nan"
        ],
        "URL": "https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.  How can I replace the nans with averages of columns where they are?  This question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "pandas DataFrame: replace nan values with average of columns",
        "A_Content": "  In [16]: df = DataFrame(np.random.randn(10,3))  In [17]: df.iloc[3:5,0] = np.nan  In [18]: df.iloc[4:6,1] = np.nan  In [19]: df.iloc[5:8,2] = np.nan  In [20]: df Out[20]:            0         1         2 0  1.148272  0.227366 -2.368136 1 -0.820823  1.071471 -0.784713 2  0.157913  0.602857  0.665034 3       NaN -0.985188 -0.324136 4       NaN       NaN  0.238512 5  0.769657       NaN       NaN 6  0.141951  0.326064       NaN 7 -1.694475 -0.523440       NaN 8  0.352556 -0.551487 -1.639298 9 -2.067324 -0.492617 -1.675794  In [22]: df.mean() Out[22]:  0   -0.251534 1   -0.040622 2   -0.841219 dtype: float64   Apply per-column the mean of that columns and fill  In [23]: df.apply(lambda x: x.fillna(x.mean()),axis=0) Out[23]:            0         1         2 0  1.148272  0.227366 -2.368136 1 -0.820823  1.071471 -0.784713 2  0.157913  0.602857  0.665034 3 -0.251534 -0.985188 -0.324136 4 -0.251534 -0.040622  0.238512 5  0.769657 -0.040622 -0.841219 6  0.141951  0.326064 -0.841219 7 -1.694475 -0.523440 -0.841219 8  0.352556 -0.551487 -1.639298 9 -2.067324 -0.492617 -1.675794      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "nan"
        ],
        "URL": "https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.  How can I replace the nans with averages of columns where they are?  This question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "pandas DataFrame: replace nan values with average of columns",
        "A_Content": "  # To read data from csv file Dataset = pd.read_csv('Data.csv')  # To divide input in X and y axis X = Dataset.iloc[:, :-1].values Y = Dataset.iloc[:, 3].values  # To calculate mean use imputer class  from sklearn.preprocessing import Imputer imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)    imputer = imputer.fit(X[:, 1:3])     X[:, 1:3] = imputer.transform(X[:, 1:3])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "nan"
        ],
        "URL": "https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.  How can I replace the nans with averages of columns where they are?  This question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "pandas DataFrame: replace nan values with average of columns",
        "A_Content": "  Another option besides those above is:  df = df.groupby(df.columns, axis = 1).transform(lambda x: x.fillna(x.mean()))   It's less elegant than previous responses for mean, but it could be shorter if you desire to replace nulls by some other column function.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "nan"
        ],
        "URL": "https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.  How can I replace the nans with averages of columns where they are?  This question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "pandas DataFrame: replace nan values with average of columns",
        "A_Content": "  If you want to impute missing values with mean and you want to go column by column, then this will only impute with the mean of that column. This might be a little more readable.  sub2['income'] = sub2['income'].fillna((sub2['income'].mean()))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "nan"
        ],
        "URL": "https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.  How can I replace the nans with averages of columns where they are?  This question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "pandas DataFrame: replace nan values with average of columns",
        "A_Content": "  Directly use df.fillna(df.mean()) to fill all the null value with mean  If you want to fill null value with mean of that column then you can use this  suppose x=df['Item_Weight'] here Item_Weight is column name  here we are assigning (fill null values of x with mean of x into x)  df['Item_Weight'] = df['Item_Weight'].fillna((df['Item_Weight'].mean()))   If you want to fill null value with some string then use   here Outlet_size is column name   df.Outlet_Size = df.Outlet_Size.fillna('Missing')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "nan"
        ],
        "URL": "https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.  How can I replace the nans with averages of columns where they are?  This question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Print new output on same line [duplicate]",
        "A_Content": "  From help(print):  Help on built-in function print in module builtins:  print(...)     print(value, ..., sep=' ', end='\\n', file=sys.stdout)      Prints the values to a stream, or to sys.stdout by default.     Optional keyword arguments:     file: a file-like object (stream); defaults to the current sys.stdout.     sep:  string inserted between values, default a space.     end:  string appended after the last value, default a newline.   You can use the end keyword:  >>> for i in range(1, 11): ...     print(i, end='') ...  12345678910>>>    Note that you'll have to print() the final newline yourself.  BTW, you won't get \"12345678910\" in Python 2 with the trailing comma, you'll get 1 2 3 4 5 6 7 8 9 10 instead.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "printing"
        ],
        "URL": "https://stackoverflow.com/questions/12032214/print-new-output-on-same-line",
        "A_Votes": "130",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              Print in one line dynamically                                        17 answers                                          I want to print the looped output to the screen on the same line.  How do I this in the simplest way for Python 3.x  I know this question has been asked for Python 2.7 by using a comma at the end of the line i.e. print I, but I can't find a solution for Python 3.x.  i = 0  while i <10:      i += 1       ## print (i) # python 2.7 would be print i,      print (i) # python 2.7 would be 'print i,'   Screen output.  1 2 3 4 5 6 7 8 9 10     What I want to print is:   12345678910   New readers visit this link aswell http://docs.python.org/release/3.0.1/whatsnew/3.0.html     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Print new output on same line [duplicate]",
        "A_Content": "  * for python 2.x *  Use a trailing comma to avoid a newline.  print \"Hey Guys!\", print \"This is how we print on the same line.\"   The output for the above code snippet would be,  Hey Guys! This is how we print on the same line.   * for python 3.x *  for i in range(10):     print(i, end=\"<separator>\") # <separator> = \\n, <space> etc.   The output for the above code snippet would be (when <separator> = \" \"),  0 1 2 3 4 5 6 7 8 9      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "printing"
        ],
        "URL": "https://stackoverflow.com/questions/12032214/print-new-output-on-same-line",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Print in one line dynamically                                        17 answers                                          I want to print the looped output to the screen on the same line.  How do I this in the simplest way for Python 3.x  I know this question has been asked for Python 2.7 by using a comma at the end of the line i.e. print I, but I can't find a solution for Python 3.x.  i = 0  while i <10:      i += 1       ## print (i) # python 2.7 would be print i,      print (i) # python 2.7 would be 'print i,'   Screen output.  1 2 3 4 5 6 7 8 9 10     What I want to print is:   12345678910   New readers visit this link aswell http://docs.python.org/release/3.0.1/whatsnew/3.0.html     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Print new output on same line [duplicate]",
        "A_Content": "  Similar like what have been suggested, you can do:  print(i,end=',')      Output: 0, 1, 2, 3,      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "printing"
        ],
        "URL": "https://stackoverflow.com/questions/12032214/print-new-output-on-same-line",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Print in one line dynamically                                        17 answers                                          I want to print the looped output to the screen on the same line.  How do I this in the simplest way for Python 3.x  I know this question has been asked for Python 2.7 by using a comma at the end of the line i.e. print I, but I can't find a solution for Python 3.x.  i = 0  while i <10:      i += 1       ## print (i) # python 2.7 would be print i,      print (i) # python 2.7 would be 'print i,'   Screen output.  1 2 3 4 5 6 7 8 9 10     What I want to print is:   12345678910   New readers visit this link aswell http://docs.python.org/release/3.0.1/whatsnew/3.0.html     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Print new output on same line [duplicate]",
        "A_Content": "  You can do something such as:  >>> print(''.join(map(str,range(1,11)))) 12345678910      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "printing"
        ],
        "URL": "https://stackoverflow.com/questions/12032214/print-new-output-on-same-line",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Print in one line dynamically                                        17 answers                                          I want to print the looped output to the screen on the same line.  How do I this in the simplest way for Python 3.x  I know this question has been asked for Python 2.7 by using a comma at the end of the line i.e. print I, but I can't find a solution for Python 3.x.  i = 0  while i <10:      i += 1       ## print (i) # python 2.7 would be print i,      print (i) # python 2.7 would be 'print i,'   Screen output.  1 2 3 4 5 6 7 8 9 10     What I want to print is:   12345678910   New readers visit this link aswell http://docs.python.org/release/3.0.1/whatsnew/3.0.html     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Print new output on same line [duplicate]",
        "A_Content": "  print(\"single\",end=\" \") print(\"line\")   this will give output  single line   for the question asked use  i = 0  while i <10:      i += 1       print (i,end=\"\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "printing"
        ],
        "URL": "https://stackoverflow.com/questions/12032214/print-new-output-on-same-line",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Print in one line dynamically                                        17 answers                                          I want to print the looped output to the screen on the same line.  How do I this in the simplest way for Python 3.x  I know this question has been asked for Python 2.7 by using a comma at the end of the line i.e. print I, but I can't find a solution for Python 3.x.  i = 0  while i <10:      i += 1       ## print (i) # python 2.7 would be print i,      print (i) # python 2.7 would be 'print i,'   Screen output.  1 2 3 4 5 6 7 8 9 10     What I want to print is:   12345678910   New readers visit this link aswell http://docs.python.org/release/3.0.1/whatsnew/3.0.html     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Print new output on same line [duplicate]",
        "A_Content": "  >>> for i in range(1, 11): ...     print(i, end=' ') ...     if i==len(range(1, 11)): print() ...  1 2 3 4 5 6 7 8 9 10  >>>    This is how to do it so that the printing does not run behind the prompt on the next line.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "printing"
        ],
        "URL": "https://stackoverflow.com/questions/12032214/print-new-output-on-same-line",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Print in one line dynamically                                        17 answers                                          I want to print the looped output to the screen on the same line.  How do I this in the simplest way for Python 3.x  I know this question has been asked for Python 2.7 by using a comma at the end of the line i.e. print I, but I can't find a solution for Python 3.x.  i = 0  while i <10:      i += 1       ## print (i) # python 2.7 would be print i,      print (i) # python 2.7 would be 'print i,'   Screen output.  1 2 3 4 5 6 7 8 9 10     What I want to print is:   12345678910   New readers visit this link aswell http://docs.python.org/release/3.0.1/whatsnew/3.0.html     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Print new output on same line [duplicate]",
        "A_Content": "  Lets take an example where you want to print numbers from 0 to n in the same line. You can do this with the help of following code.  n=int(raw_input()) i=0 while(i<n):     print i,     i = i+1      At input, n = 5   Output : 0 1 2 3 4       ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "printing"
        ],
        "URL": "https://stackoverflow.com/questions/12032214/print-new-output-on-same-line",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Print in one line dynamically                                        17 answers                                          I want to print the looped output to the screen on the same line.  How do I this in the simplest way for Python 3.x  I know this question has been asked for Python 2.7 by using a comma at the end of the line i.e. print I, but I can't find a solution for Python 3.x.  i = 0  while i <10:      i += 1       ## print (i) # python 2.7 would be print i,      print (i) # python 2.7 would be 'print i,'   Screen output.  1 2 3 4 5 6 7 8 9 10     What I want to print is:   12345678910   New readers visit this link aswell http://docs.python.org/release/3.0.1/whatsnew/3.0.html     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Remove leading and trailing spaces?",
        "A_Content": "  You can use the strip() to remove trailing and leading spaces.  >>> s = '   abd cde   ' >>> s.strip() 'abd cde'   Note: the internal spaces are preserved     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/10443400/remove-leading-and-trailing-spaces",
        "A_Votes": "168",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm having a hard time trying to use .strip with the following line of code.   Thanks for the help.  f.write(re.split(\"Tech ID:|Name:|Account #:\",line)[-1])      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Remove leading and trailing spaces?",
        "A_Content": "  Expand your one liner into multiple lines. Then it becomes easy:  f.write(re.split(\"Tech ID:|Name:|Account #:\",line)[-1])  parts = re.split(\"Tech ID:|Name:|Account #:\",line) wanted_part = parts[-1] wanted_part_stripped = wanted_part.strip() f.write(wanted_part_stripped)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/10443400/remove-leading-and-trailing-spaces",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm having a hard time trying to use .strip with the following line of code.   Thanks for the help.  f.write(re.split(\"Tech ID:|Name:|Account #:\",line)[-1])      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Replace first occurrence of string in Python",
        "A_Content": "  string replace() function if perfectly solves this problem:     string.replace(s, old, new[, maxreplace])      Return a copy of string s with all occurrences of substring old replaced by new. If the optional argument maxreplace is given, the first maxreplace occurrences are replaced.   >>> u'longlongTESTstringTEST'.replace('TEST', '?', 1) u'longlong?stringTEST'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex"
        ],
        "URL": "https://stackoverflow.com/questions/4628618/replace-first-occurrence-of-string-in-python",
        "A_Votes": "162",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have some sample string. How can I replace first occurrence of this string in a longer string with empty string?  regex = re.compile('text') match = regex.match(url) if match:     url = url.replace(regex, '')      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Replace first occurrence of string in Python",
        "A_Content": "  Use re.sub directly, this allows you to specify a count:  regex.sub('', url, 1)   (Note that the order of arguments is replacement, original not the opposite, as might be suspected.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex"
        ],
        "URL": "https://stackoverflow.com/questions/4628618/replace-first-occurrence-of-string-in-python",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have some sample string. How can I replace first occurrence of this string in a longer string with empty string?  regex = re.compile('text') match = regex.match(url) if match:     url = url.replace(regex, '')      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  A naive algorithm won't give good results when applied to real-world data. Here is a 20-line algorithm that exploits relative word frequency to give accurate results for real-word text.  (If you want an answer to your original question which does not use word frequency, you need to refine what exactly is meant by \"longest word\": is it better to have a 20-letter word and ten 3-letter words, or is it better to have five 10-letter words? Once you settle on a precise definition, you just have to change the line defining wordcost to reflect the intended meaning.)  The idea  The best way to proceed is to model the distribution of the output. A good first approximation is to assume all words are independently distributed. Then you only need to know the relative frequency of all words. It is reasonable to assume that they follow Zipf's law, that is the word with rank n in the list of words has probability roughly 1/(n log N) where N is the number of words in the dictionary.  Once you have fixed the model, you can use dynamic programming to infer the position of the spaces. The most likely sentence is the one that maximizes the product of the probability of each individual word, and it's easy to compute it with dynamic programming. Instead of directly using the probability we use a cost defined as the logarithm of the inverse of the probability to avoid overflows.  The code  from math import log  # Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability). words = open(\"words-by-frequency.txt\").read().split() wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words)) maxword = max(len(x) for x in words)  def infer_spaces(s):     \"\"\"Uses dynamic programming to infer the location of spaces in a string     without spaces.\"\"\"      # Find the best match for the i first characters, assuming cost has     # been built for the i-1 first characters.     # Returns a pair (match_cost, match_length).     def best_match(i):         candidates = enumerate(reversed(cost[max(0, i-maxword):i]))         return min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)      # Build the cost array.     cost = [0]     for i in range(1,len(s)+1):         c,k = best_match(i)         cost.append(c)      # Backtrack to recover the minimal-cost string.     out = []     i = len(s)     while i>0:         c,k = best_match(i)         assert c == cost[i]         out.append(s[i-k:i])         i -= k      return \" \".join(reversed(out))   which you can use with  s = 'thumbgreenappleactiveassignmentweeklymetaphor' print(infer_spaces(s))     The results  I am using this quick-and-dirty 125k-word dictionary I put together from a small subset of Wikipedia.     Before: thumbgreenappleactiveassignmentweeklymetaphor.   After: thumb green apple active assignment weekly metaphor.        Before: thereismassesoftextinformationofpeoplescommentswhichisparsedfromhtmlbuttherearen   odelimitedcharactersinthemforexamplethumbgreenappleactiveassignmentweeklymetapho   rapparentlytherearethumbgreenappleetcinthestringialsohavealargedictionarytoquery   whetherthewordisreasonablesowhatsthefastestwayofextractionthxalot.      After: there is masses of text information of peoples comments which is parsed from html but there are no delimited characters in them for example thumb green apple active assignment weekly metaphor apparently there are thumb green apple etc in the string i also have a large dictionary to query whether the word is reasonable so what s the fastest way of extraction thx a lot.        Before: itwasadarkandstormynighttherainfellintorrentsexceptatoccasionalintervalswhenitwascheckedbyaviolentgustofwindwhichsweptupthestreetsforitisinlondonthatoursceneliesrattlingalongthehousetopsandfiercelyagitatingthescantyflameofthelampsthatstruggledagainstthedarkness.      After: it was a dark and stormy night the rain fell in torrents except at occasional intervals when it was checked by a violent gust of wind which swept up the streets for it is in london that our scene lies rattling along the housetops and fiercely agitating the scanty flame of the lamps that struggled against the darkness.   As you can see it is essentially flawless. The most important part is to make sure your word list was trained to a corpus similar to what you will actually encounter, otherwise the results will be very bad.    Optimization  The implementation consumes a linear amount of time and memory, so it is reasonably efficient. If you need further speedups, you can build a suffix tree from the word list to reduce the size of the set of candidates.  If you need to process a very large consecutive string it would be reasonable to split the string to avoid excessive memory usage. For example you could process the text in blocks of 10000 characters plus a margin of 1000 characters on either side to avoid boundary effects. This will keep memory usage to a minimum and will have almost certainly no effect on the quality.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "122",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  Based on the excellent work in the top answer, I've created a pip package for easy use.  >>> import wordninja >>> wordninja.split('derekanderson') ['derek', 'anderson']   To install, run pip install wordninja.  The only differences are minor.  This returns a list rather than a str, it works in python3, it includes the word list and properly splits even if there are non-alpha chars (like underscores, dashes, etc).  Thanks again to Generic Human!  https://github.com/keredson/wordninja     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  Here is solution using recursive search:  def find_words(instring, prefix = '', words = None):     if not instring:         return []     if words is None:         words = set()         with open('/usr/share/dict/words') as f:             for line in f:                 words.add(line.strip())     if (not prefix) and (instring in words):         return [instring]     prefix, suffix = prefix + instring[0], instring[1:]     solutions = []     # Case 1: prefix in solution     if prefix in words:         try:             solutions.append([prefix] + find_words(suffix, '', words))         except ValueError:             pass     # Case 2: prefix not in solution     try:         solutions.append(find_words(suffix, prefix, words))     except ValueError:         pass     if solutions:         return sorted(solutions,                       key = lambda solution: [len(word) for word in solution],                       reverse = True)[0]     else:         raise ValueError('no solution')  print(find_words('tableapplechairtablecupboard')) print(find_words('tableprechaun', words = set(['tab', 'table', 'leprechaun'])))   yields  ['table', 'apple', 'chair', 'table', 'cupboard'] ['tab', 'leprechaun']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  Using a trie data structure, which holds the list of possible words, it would not be too complicated to do the following:   Advance pointer (in the concatenated string) Lookup and store the corresponding node in the trie If the trie node has children (e.g. there are longer words), go to 1. If the node reached has no children, a longest word match happened; add the word (stored in the node or just concatenated during trie traversal) to the result list, reset the pointer in the trie (or reset the reference), and start over      ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  Unutbu's solution was quite close but I find the code difficult to read, and it didn't yield the expected result. Generic Human's solution has the drawback that it needs word frequencies. Not appropriate for all use case.  Here's a simple solution using a Divide and Conquer algorithm.   It tries to minimize the number of words E.g. find_words('cupboard') will return ['cupboard'] rather than ['cup', 'board'] (assuming that cupboard, cup and board are in the dictionnary) The optimal solution is not unique, the implementation below returns a solution. find_words('charactersin') could return ['characters', 'in'] or maybe it will return ['character', 'sin'] (as seen below). You could quite easily modify the algorithm to return all optimal solutions. In this implementation solutions are memoized so that it runs in a reasonable time.   The code:  words = set() with open('/usr/share/dict/words') as f:     for line in f:         words.add(line.strip())  solutions = {} def find_words(instring):     # First check if instring is in the dictionnary     if instring in words:         return [instring]     # No... But maybe it's a result we already computed     if instring in solutions:         return solutions[instring]     # Nope. Try to split the string at all position to recursively search for results     best_solution = None     for i in range(1, len(instring) - 1):         part1 = find_words(instring[:i])         part2 = find_words(instring[i:])         # Both parts MUST have a solution         if part1 is None or part2 is None:             continue         solution = part1 + part2         # Is the solution found \"better\" than the previous one?         if best_solution is None or len(solution) < len(best_solution):             best_solution = solution     # Remember (memoize) this solution to avoid having to recompute it     solutions[instring] = best_solution     return best_solution   This will take about about 5sec on my 3GHz machine:  result = find_words(\"thereismassesoftextinformationofpeoplescommentswhichisparsedfromhtmlbuttherearenodelimitedcharactersinthemforexamplethumbgreenappleactiveassignmentweeklymetaphorapparentlytherearethumbgreenappleetcinthestringialsohavealargedictionarytoquerywhetherthewordisreasonablesowhatsthefastestwayofextractionthxalot\") assert(result is not None) print ' '.join(result)      the reis masses of text information of peoples comments which is parsed from h t m l but there are no delimited character sin them for example thumb green apple active assignment weekly metaphor apparently there are thumb green apple e t c in the string i also have a large dictionary to query whether the word is reasonable so whats the fastest way of extraction t h x a lot      ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  The answer by https://stackoverflow.com/users/1515832/generic-human is great. But the best implementation of this I've ever seen was written Peter Norvig himself in his book 'Beautiful Data'.  Before I paste his code, let me expand on why Norvig's method is more accurate (although a little slower and longer in terms of code).   1) The data is a bit better - both in terms of size and in terms of precision (he uses a word count rather than a simple ranking) 2) More importantly, it's the logic behind n-grams that really makes the approach so accurate.  The example he provides in his book is the problem of splitting a string 'sitdown'. Now a non-bigram method of string split would consider p('sit') * p ('down'), and if this less than the p('sitdown') - which will be the case quite often - it will NOT split it, but we'd want it to (most of the time).  However when you have the bigram model you could value p('sit down') as a bigram vs p('sitdown') and the former wins. Basically, if you don't use bigrams, it treats the probability of the words you're splitting as independent, which is not the case, some words are more likely to appear one after the other. Unfortunately those are also the words that are often stuck together in a lot of instances and confuses the splitter.  Here's the link to the data (it's data for 3 separate problems and segmentation is only one. Please read the chapter for details): http://norvig.com/ngrams/  and here's the link to the code: http://norvig.com/ngrams/ngrams.py  These links have been up a while, but I'll copy paste the segmentation part of the code here anyway  import re, string, random, glob, operator, heapq from collections import defaultdict from math import log10  def memo(f):     \"Memoize function f.\"     table = {}     def fmemo(*args):         if args not in table:             table[args] = f(*args)         return table[args]     fmemo.memo = table     return fmemo  def test(verbose=None):     \"\"\"Run some tests, taken from the chapter.     Since the hillclimbing algorithm is randomized, some tests may fail.\"\"\"     import doctest     print 'Running tests...'     doctest.testfile('ngrams-test.txt', verbose=verbose)  ################ Word Segmentation (p. 223)  @memo def segment(text):     \"Return a list of words that is the best segmentation of text.\"     if not text: return []     candidates = ([first]+segment(rem) for first,rem in splits(text))     return max(candidates, key=Pwords)  def splits(text, L=20):     \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"     return [(text[:i+1], text[i+1:])              for i in range(min(len(text), L))]  def Pwords(words):      \"The Naive Bayes probability of a sequence of words.\"     return product(Pw(w) for w in words)  #### Support functions (p. 224)  def product(nums):     \"Return the product of a sequence of numbers.\"     return reduce(operator.mul, nums, 1)  class Pdist(dict):     \"A probability distribution estimated from counts in datafile.\"     def __init__(self, data=[], N=None, missingfn=None):         for key,count in data:             self[key] = self.get(key, 0) + int(count)         self.N = float(N or sum(self.itervalues()))         self.missingfn = missingfn or (lambda k, N: 1./N)     def __call__(self, key):          if key in self: return self[key]/self.N           else: return self.missingfn(key, self.N)  def datafile(name, sep='\\t'):     \"Read key,value pairs from file.\"     for line in file(name):         yield line.split(sep)  def avoid_long_words(key, N):     \"Estimate the probability of an unknown word.\"     return 10./(N * 10**len(key))  N = 1024908267229 ## Number of tokens  Pw  = Pdist(datafile('count_1w.txt'), N, avoid_long_words)  #### segment2: second version, with bigram counts, (p. 226-227)  def cPw(word, prev):     \"Conditional probability of word, given previous word.\"     try:         return P2w[prev + ' ' + word]/float(Pw[prev])     except KeyError:         return Pw(word)  P2w = Pdist(datafile('count_2w.txt'), N)  @memo  def segment2(text, prev='<S>'):      \"Return (log P(words), words), where words is the best segmentation.\"      if not text: return 0.0, []      candidates = [combine(log10(cPw(first, prev)), first, segment2(rem, first))                    for first,rem in splits(text)]      return max(candidates)   def combine(Pfirst, first, (Prem, rem)):      \"Combine first and rem results into one (probability, words) pair.\"      return Pfirst+Prem, [first]+rem       ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  If you precompile the wordlist into a DFA (which will be very slow), then the time it takes to match an input will be proportional to the length of the string (in fact, only a little slower than just iterating over the string).  This is effectively a more general version of the trie algorithm which was mentioned earlier. I only mention it for completeless -- as of yet, there's no DFA implementation you can just use. RE2 would work, but I don't know if the Python bindings let you tune how large you allow a DFA to be before it just throws away the compiled DFA data and does NFA search.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  It seems like fairly mundane backtracking will do. Start at the beggining of the string. Scan right until you have a word. Then, call the function on the rest of the string. Function returns \"false\" if it scans all the way to the right without recognizing a word. Otherwise, returns the word it found and the list of words returned by the recursive call.  Example: \"tableapple\". Finds \"tab\", then \"leap\", but no word in \"ple\". No other word in \"leapple\". Finds \"table\", then \"app\". \"le\" not a word, so tries apple, recognizes, returns.  To get longest possible, keep going, only emitting (rather than returning) correct solutions; then, choose the optimal one by any criterion you choose (maxmax, minmax, average, etc.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  Based on unutbu's solution I've implemented a Java version:  private static List<String> splitWordWithoutSpaces(String instring, String suffix) {     if(isAWord(instring)) {         if(suffix.length() > 0) {             List<String> rest = splitWordWithoutSpaces(suffix, \"\");             if(rest.size() > 0) {                 List<String> solutions = new LinkedList<>();                 solutions.add(instring);                 solutions.addAll(rest);                 return solutions;             }         } else {             List<String> solutions = new LinkedList<>();             solutions.add(instring);             return solutions;         }      }     if(instring.length() > 1) {         String newString = instring.substring(0, instring.length()-1);         suffix = instring.charAt(instring.length()-1) + suffix;         List<String> rest = splitWordWithoutSpaces(newString, suffix);         return rest;     }     return Collections.EMPTY_LIST; }   Input: \"tableapplechairtablecupboard\"  Output: [table, apple, chair, table, cupboard]  Input: \"tableprechaun\"  Output: [tab, leprechaun]     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  For German language there is CharSplit which uses machine learning and works pretty good for strings of a few words.  https://github.com/dtuggener/CharSplit     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  Expanding on @miku's suggestion to use a Trie, an append-only Trie is relatively straight-forward to implement in python:  class Node:     def __init__(self, is_word=False):         self.children = {}         self.is_word = is_word  class TrieDictionary:     def __init__(self, words=tuple()):         self.root = Node()         for word in words:             self.add(word)      def add(self, word):         node = self.root         for c in word:             node = node.children.setdefault(c, Node())         node.is_word = True      def lookup(self, word, from_node=None):         node = self.root if from_node is None else from_node         for c in word:             try:                 node = node.children[c]             except KeyError:                 return None          return node   We can then build a Trie-based dictionary from a set of words:  dictionary = {\"a\", \"pea\", \"nut\", \"peanut\", \"but\", \"butt\", \"butte\", \"butter\"} trie_dictionary = TrieDictionary(words=dictionary)   Which will produce a tree that looks like this (* indicates beginning or end of a word):  * -> a*  \\\\\\    \\\\\\-> p -> e -> a*    \\\\              \\-> n -> u -> t*     \\\\      \\\\-> b -> u -> t*       \\\\             \\-> t*        \\\\                 \\-> e*         \\\\                     \\-> r*          \\           \\-> n -> u -> t*   We can incorporate this into a solution by combining it with a heuristic about how to choose words. For example we can prefer longer words over shorter words:  def using_trie_longest_word_heuristic(s):     node = None     possible_indexes = []      # O(1) short-circuit if whole string is a word, doesn't go against longest-word wins     if s in dictionary:         return [ s ]      for i in range(len(s)):         # traverse the trie, char-wise to determine intermediate words         node = trie_dictionary.lookup(s[i], from_node=node)          # no more words start this way         if node is None:             # iterate words we have encountered from biggest to smallest             for possible in possible_indexes[::-1]:                 # recurse to attempt to solve the remaining sub-string                 end_of_phrase = using_trie_longest_word_heuristic(s[possible+1:])                  # if we have a solution, return this word + our solution                 if end_of_phrase:                     return [ s[:possible+1] ] + end_of_phrase              # unsolvable             break          # if this is a leaf, append the index to the possible words list         elif node.is_word:             possible_indexes.append(i)      # empty string OR unsolvable case      return []   We can use this function like this:  >>> using_trie_longest_word_heuristic(\"peanutbutter\") [ \"peanut\", \"butter\" ]   Because we maintain our position in the Trie as we search for longer and longer words, we traverse the trie at most once per possible solution (rather than 2 times for peanut: pea, peanut). The final short-circuit saves us from walking char-wise through the string in the worst-case.  The final result is only a handful of inspections:  'peanutbutter' - not a word, go charwise 'p' - in trie, use this node 'e' - in trie, use this node 'a' - in trie and edge, store potential word and use this node 'n' - in trie, use this node 'u' - in trie, use this node 't' - in trie and edge, store potential word and use this node 'b' - not in trie from `peanut` vector 'butter' - remainder of longest is a word   A benefit of this solution is in the fact that you know very quickly if longer words exist with a given prefix, which spares the need to exhaustively test sequence combinations against a dictionary. It also makes getting to an unsolvable answer comparatively cheap to other implementations.   The downsides of this solution are a large memory footprint for the trie and the cost of building the trie up-front.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to split text without spaces into list of words?",
        "A_Content": "  You need to identify your vocabulary - perhaps any free word list will do.  Once done, use that vocabulary to build a suffix tree, and match your stream of input against that: http://en.wikipedia.org/wiki/Suffix_tree     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "text",
            "split"
        ],
        "URL": "https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Input: \"tableapplechairtablecupboard...\" many words    What would be an efficient algorithm to split such text to the list of words and get:    Output: [\"table\", \"apple\", \"chair\", \"table\", [\"cupboard\", [\"cup\", \"board\"]], ...]  First thing that cames to mind is to go through all possible words (starting with first letter) and find the longest word possible, continue from position=word_position+len(word)  P.S. We have a list of all possible words. Word \"cupboard\" can be \"cup\" and \"board\", select longest. Language: python, but main thing is the algorithm itself.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django-DB-Migrations: cannot ALTER TABLE because it has pending trigger events",
        "A_Content": "  Every migration is inside a transaction. In PostgreSQL you must not update the table and then alter the table schema in one transaction.  You need to split the data migration and the schema migration. First create the data migration with this code:   for sender in orm['fooapp.EmailSender'].objects.filter(footer=None):     sender.footer=''     sender.save()   Then create the schema migration:  manage.py schemamigration fooapp --auto   Now you have two transactions and the migration in two steps should work.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "postgresql",
            "django-migrations"
        ],
        "URL": "https://stackoverflow.com/questions/12838111/django-db-migrations-cannot-alter-table-because-it-has-pending-trigger-events",
        "A_Votes": "84",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to remove null=True from a TextField:  -    footer=models.TextField(null=True, blank=True) +    footer=models.TextField(blank=True, default='')   I created a schema migration:  manage.py schemamigration fooapp --auto   Since some footer columns contain NULL I get this error if I run the migration:     django.db.utils.IntegrityError: column \"footer\" contains null values   I added this to the schema migration:      for sender in orm['fooapp.EmailSender'].objects.filter(footer=None):         sender.footer=''         sender.save()   Now I get:  django.db.utils.DatabaseError: cannot ALTER TABLE \"fooapp_emailsender\" because it has pending trigger events   What is wrong?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django-DB-Migrations: cannot ALTER TABLE because it has pending trigger events",
        "A_Content": "  Another reason for this maybe because you try to set a column to NOT NULL when it actually already has NULL values.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "postgresql",
            "django-migrations"
        ],
        "URL": "https://stackoverflow.com/questions/12838111/django-db-migrations-cannot-alter-table-because-it-has-pending-trigger-events",
        "A_Votes": "83",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to remove null=True from a TextField:  -    footer=models.TextField(null=True, blank=True) +    footer=models.TextField(blank=True, default='')   I created a schema migration:  manage.py schemamigration fooapp --auto   Since some footer columns contain NULL I get this error if I run the migration:     django.db.utils.IntegrityError: column \"footer\" contains null values   I added this to the schema migration:      for sender in orm['fooapp.EmailSender'].objects.filter(footer=None):         sender.footer=''         sender.save()   Now I get:  django.db.utils.DatabaseError: cannot ALTER TABLE \"fooapp_emailsender\" because it has pending trigger events   What is wrong?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Django-DB-Migrations: cannot ALTER TABLE because it has pending trigger events",
        "A_Content": "  Have just hit this problem. You can also use db.start_transaction() and db.commit_transaction() in the schema migration to separate data changes from schema changes. Probably not so clean as to have a separate data migration but in my case I would need schema, data, and then another schema migration so I decided to do it all at once.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "postgresql",
            "django-migrations"
        ],
        "URL": "https://stackoverflow.com/questions/12838111/django-db-migrations-cannot-alter-table-because-it-has-pending-trigger-events",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to remove null=True from a TextField:  -    footer=models.TextField(null=True, blank=True) +    footer=models.TextField(blank=True, default='')   I created a schema migration:  manage.py schemamigration fooapp --auto   Since some footer columns contain NULL I get this error if I run the migration:     django.db.utils.IntegrityError: column \"footer\" contains null values   I added this to the schema migration:      for sender in orm['fooapp.EmailSender'].objects.filter(footer=None):         sender.footer=''         sender.save()   Now I get:  django.db.utils.DatabaseError: cannot ALTER TABLE \"fooapp_emailsender\" because it has pending trigger events   What is wrong?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  You misunderstood what the is operator tests. It tests if two variables point the same object, not if two variables have the same value.  From the documentation for the is operator:     The operators is and is not test for object identity: x is y is true if and only if x and y are the same object.   Use the == operator instead:  print x == y   This prints True. x and y are two separate lists:  x[0] = 4 print y  # prints [1, 2, 3] print x == y  # prints False   If you use the id() function you'll see that x and y have different identifiers:  >>> id(x) 4401064560 >>> id(y) 4401098192   but if you were to assign y to x then both point to the same object:  >>> x = y >>> id(x) 4401064560 >>> id(y) 4401064560 >>> x is y True   and is shows both are the same object, it returns True.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  Another duplicate was asking why two equal strings are generally not identical, which isn't really answered here:  >>> x = 'a'  >>> x += 'bc' >>> y = 'abc' >>> x == y True >>> x is y False   So, why aren't they the same string? Especially given this:  >>> z = 'abc' >>> w = 'abc' >>> z is w True     Let's put off the second part for a bit. How could the first one be true?  The interpreter would have to have an \"interning table\", a table mapping string values to string objects, so every time you try to create a new string with the contents 'abc', you get back the same object. Wikipedia has a more detailed discussion on how interning works.  And Python has a string interning table; you can manually intern strings with the sys.intern method.  In fact, Python is allowed to automatically intern any immutable types, but not required to do so. Different implementations will intern different values.  CPython (the implementation you're using if you don't know which implementation you're using) auto-interns small integers and some special singletons like False, but not strings (or large integers, or small tuples, or anything else). You can see this pretty easily:  >>> a = 0 >>> a += 1 >>> b = 1 >>> a is b True >>> a = False >>> a = not a >>> b = True a is b True >>> a = 1000 >>> a += 1 >>> b = 1001 >>> a is b False     OK, but why were z and w identical?  That's not the interpreter automatically interning, that's the compiler folding values.  If the same compile-time string appears twice in the same module (what exactly this means is hard to defineit's not the same thing as a string literal, because r'abc', 'abc', and 'a' 'b' 'c' are all different literals but the same stringbut easy to understand intuitively), the compiler will only create one instance of the string, with two references.  In fact, the compiler can go even farther: 'ab' + 'c' can be converted to 'abc' by the optimizer, in which case it can be folded together with an 'abc' constant in the same module.  Again, this is something Python is allowed but not required to do. But in this case, CPython always folds small strings (and also, e.g., small tuples). (Although the interactive interpreter's statement-by-statement compiler doesn't run the same optimization as the module-at-a-time compiler, so you won't see exactly the same results interactively.)    So, what should you do about this as a programmer?  Wellnothing. You almost never have any reason to care if two immutable values are identical. If you want to know when you can use a is b instead of a == b, you're asking the wrong question. Just always use a == b except in two cases:   For more readable comparisons to the singleton values like x is None. For mutable values, when you need to know whether mutating x will affect the y.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "33",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  is only returns true if they're actually the same object. If they were the same, a change to one would also show up in the other. Here's an example of the difference.  >>> x = [1, 2, 3] >>> y = [1, 2, 3] >>> print x is y False >>> z = y >>> print y is z True >>> print x is z False >>> y[0] = 5 >>> print z [5, 2, 3]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  Prompted by a duplicate question, this analogy might work:  # - Darling, I want some pudding! # - There is some in the fridge.  pudding_to_eat = fridge_pudding pudding_to_eat is fridge_pudding # => True  # - Honey, what's with all the dirty dishes? # - I wanted to eat pudding so I made some. Sorry about the mess, Darling. # - But there was already some in the fridge.  pudding_to_eat = make_pudding(ingredients) pudding_to_eat is fridge_pudding # => False      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  is and is not are the two identity operators in Python. is operator does not compare the values of the variables, but compares the identities of the variables. Consider this:  >>> a = [1,2,3] >>> b = [1,2,3] >>> hex(id(a)) '0x1079b1440' >>> hex(id(b)) '0x107960878' >>> a is b False >>> a == b True >>>   The above example shows you that the identity (can also be the memory address in Cpython) is different for both a and b (even though their values are the same). That is why when you say a is b it returns false due to the mismatch in the identities of both the operands. However when you say a == b, it returns true because the == operation only verifies if both the operands have the same value assigned to them.  Interesting example (for the extra grade):  >>> del a >>> del b >>> a = 132 >>> b = 132 >>> hex(id(a)) '0x7faa2b609738' >>> hex(id(b)) '0x7faa2b609738' >>> a is b True >>> a == b True >>>   In the above example, even though a and b are two different variables, a is b returned True. This is because the type of a is int which is an immutable object. So python (I guess to save memory) allocated the same object to b when it was created with the same value. So in this case, the identities of the variables matched and a is b turned out to be True.  This will apply for all immutable objects:  >>> del a >>> del b >>> a = \"asd\" >>> b = \"asd\" >>> hex(id(a)) '0x1079b05a8' >>> hex(id(b)) '0x1079b05a8' >>> a is b True >>> a == b True >>>   Hope that helps.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  As you can check here to a small integers. Numbers above 257 are not an small ints, so it is calculated as a different object.  It is better to use == instead in this case.  Further information is here: http://docs.python.org/2/c-api/int.html     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  x is y is same as id(x) == id(y), comparing identity of objects.    As @tomasz-kurgan pointed out in the comment below is operator behaves unusually with certain objects.    E.g.  >>> class A(object): ...   def foo(self): ...     pass ...  >>> a = A() >>> a.foo is a.foo False >>> id(a.foo) == id(a.foo) True   Ref; https://docs.python.org/2/reference/expressions.html#is-not https://docs.python.org/2/reference/expressions.html#id24     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  X points to an array, Y points to a different array. Those arrays are identical, but the is operator will look at those pointers, which are not identical.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Understanding Python's is operator",
        "A_Content": "  It compares object identity, that is, whether the variables refer to the same object in memory. It's like the == in Java or C (when comparing pointers).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "operators"
        ],
        "URL": "https://stackoverflow.com/questions/13650293/understanding-pythons-is-operator",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       The is operator does not match the values of the variables, but the   instances themselves.   What does it really mean?  I declared two variables named x and y assigning the same values in both variables, but it returns false when I use the is operator.  I need a clarification. Here is my code.  x = [1, 2, 3] y = [1, 2, 3]  print x is y #It prints false!      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Nested Function in Python",
        "A_Content": "  Normally you do it to make closures:  def make_adder(x):     def add(y):         return x + y     return add  plus5 = make_adder(5) print(plus5(12))  # prints 17   Inner functions can access variables from the enclosing scope (in this case, the local variable x).  If you're not accessing any variables from the enclosing scope, they're really just ordinary functions with a different scope.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "nested-function"
        ],
        "URL": "https://stackoverflow.com/questions/1589058/nested-function-in-python",
        "A_Votes": "94",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    What benefit or implications could we get with Python code like this:  class some_class(parent_class):     def doOp(self, x, y):         def add(x, y):             return x + y         return add(x, y)   I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Nested Function in Python",
        "A_Content": "  Aside from function generators, where internal function creation is almost the definition of a function generator, the reason I create nested functions is to improve readability.  If I have a tiny function that will only be invoked by the outer function, then I inline the definition so you don't have to skip around to determine what that function is doing.  I can always move the inner method outside of the encapsulating method if I find a need to reuse the function at a later date.  Toy example:  import sys  def Foo():     def e(s):         sys.stderr.write('ERROR: ')         sys.stderr.write(s)         sys.stderr.write('\\n')     e('I regret to inform you')     e('that a shameful thing has happened.')     e('Thus, I must issue this desultory message')     e('across numerous lines.') Foo()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "nested-function"
        ],
        "URL": "https://stackoverflow.com/questions/1589058/nested-function-in-python",
        "A_Votes": "49",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What benefit or implications could we get with Python code like this:  class some_class(parent_class):     def doOp(self, x, y):         def add(x, y):             return x + y         return add(x, y)   I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Nested Function in Python",
        "A_Content": "  One potential benefit of using inner methods is that it allows you to use outer method local variables without passing them as arguments.  def helper(feature, resultBuffer):   resultBuffer.print(feature)   resultBuffer.printLine()   resultBuffer.flush()  def save(item, resultBuffer):    helper(item.description, resultBuffer)   helper(item.size, resultBuffer)   helper(item.type, resultBuffer)   can be written as follows, which arguably reads better  def save(item, resultBuffer):    def helper(feature):     resultBuffer.print(feature)     resultBuffer.printLine()     resultBuffer.flush()    helper(item.description)   helper(item.size)   helper(item.type)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "nested-function"
        ],
        "URL": "https://stackoverflow.com/questions/1589058/nested-function-in-python",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What benefit or implications could we get with Python code like this:  class some_class(parent_class):     def doOp(self, x, y):         def add(x, y):             return x + y         return add(x, y)   I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Nested Function in Python",
        "A_Content": "  I can't image any good reason for code like that.  Maybe there was a reason for the inner function in older revisions, like other Ops.   For example, this makes slightly more sense:  class some_class(parent_class):     def doOp(self, op, x, y):         def add(x, y):             return x + y         def sub(x,y):             return x - y         return locals()[op](x,y)  some_class().doOp('add', 1,2)   but then the inner function should be (\"private\") class methods instead:  class some_class(object):     def _add(self, x, y):         return x + y     def doOp(self, x, y):         return self._add(x,y)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "nested-function"
        ],
        "URL": "https://stackoverflow.com/questions/1589058/nested-function-in-python",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What benefit or implications could we get with Python code like this:  class some_class(parent_class):     def doOp(self, x, y):         def add(x, y):             return x + y         return add(x, y)   I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Nested Function in Python",
        "A_Content": "  The idea behind local methods is similar to local variables: don't pollute the larger name space.  Obviously the benefits are limited since most languages don't also provide such functionality directly.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "nested-function"
        ],
        "URL": "https://stackoverflow.com/questions/1589058/nested-function-in-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What benefit or implications could we get with Python code like this:  class some_class(parent_class):     def doOp(self, x, y):         def add(x, y):             return x + y         return add(x, y)   I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Nested Function in Python",
        "A_Content": "  Are you sure the code was exactly like this? The normal reason for doing something like this is for creating a partial - a function with baked-in parameters. Calling the outer function returns a callable that needs no parameters, and so therefore can be stored and used somewhere it is impossible to pass parameters. However, the code you've posted won't do that - it calls the function immediately and returns the result, rather than the callable. It might be useful to post the actual code you saw.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "nested-function"
        ],
        "URL": "https://stackoverflow.com/questions/1589058/nested-function-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What benefit or implications could we get with Python code like this:  class some_class(parent_class):     def doOp(self, x, y):         def add(x, y):             return x + y         return add(x, y)   I found this in an open-source project, doing something useful inside the nested function, but doing absolutely nothing outside it except calling it. (The actual code can be found here.) Why might someone code it like this? Is there some benefit or side effect for writing the code inside the nested function rather than in the outer, normal function?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Random row selection in Pandas dataframe",
        "A_Content": "  Something like this?  import random  def some(x, n):     return x.ix[random.sample(x.index, n)]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe",
        "A_Votes": "35",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is there a way to select random rows from a DataFrame in Pandas.  In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.  I have also looked at the slicing documentation and there seems to be nothing equivalent.  Update  Now using version 20. There is a sample method.  df.sample(n)     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Random row selection in Pandas dataframe",
        "A_Content": "  With pandas version 0.16.1 and up, there is now a DataFrame.sample method built-in:  import pandas  df = pandas.DataFrame(pandas.np.random.random(100))  # Randomly sample 70% of your dataframe df_percent = df.sample(frac=0.7)  # Randomly sample 7 elements from your dataframe df_elements = df.sample(n=7)   For either approach above, you can get the rest of the rows by doing:  df_rest = df.loc[~df.index.isin(df_percent.index)]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe",
        "A_Votes": "142",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to select random rows from a DataFrame in Pandas.  In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.  I have also looked at the slicing documentation and there seems to be nothing equivalent.  Update  Now using version 20. There is a sample method.  df.sample(n)     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Random row selection in Pandas dataframe",
        "A_Content": "  The best way to do this is with the sample function from the random module,  import numpy as np import pandas as pd from random import sample  # given data frame df  # create random index rindex =  np.array(sample(xrange(len(df)), 10))  # get 10 random rows from df dfr = df.ix[rindex]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to select random rows from a DataFrame in Pandas.  In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.  I have also looked at the slicing documentation and there seems to be nothing equivalent.  Update  Now using version 20. There is a sample method.  df.sample(n)     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Random row selection in Pandas dataframe",
        "A_Content": "  Actually this will give you repeated indices np.random.random_integers(0, len(df), N) where N is a large number.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to select random rows from a DataFrame in Pandas.  In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.  I have also looked at the slicing documentation and there seems to be nothing equivalent.  Update  Now using version 20. There is a sample method.  df.sample(n)     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Random row selection in Pandas dataframe",
        "A_Content": "  Below line will randomly select n number of rows out of the total existing row numbers from the dataframe df without replacement.  df=df.take(np.random.permutation(len(df))[:n])     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a way to select random rows from a DataFrame in Pandas.  In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.  I have also looked at the slicing documentation and there seems to be nothing equivalent.  Update  Now using version 20. There is a sample method.  df.sample(n)     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is 'a' in ('abc') True while 'a' in ['abc'] is False?",
        "A_Content": "  ('abc') is the same as 'abc'. 'abc' contains the substring 'a', hence 'a' in 'abc' == True.  If you want the tuple instead, you need to write ('abc', ).  ['abc'] is a list (containing a single element, the string 'abc'). 'a' is not a member of this list, so 'a' in ['abc'] == False     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/31900892/why-is-a-in-abc-true-while-a-in-abc-is-false",
        "A_Votes": "135",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When using the interpreter, the expression 'a' in ('abc') returns True, while 'a' in ['abc'] returns False. Can somebody explain this behaviour?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is 'a' in ('abc') True while 'a' in ['abc'] is False?",
        "A_Content": "  ('abc') is not a tuple. I think you confused that with  tuple ('abc',).   Actually, ('abc') is same as 'abc', an array of characters where a as character is in it, hence, the first lookup returns True:  >>> 'a' in 'abc' True   On the other hand, ['abc'] is a list of string or a list of list of characters (you can think of it as a 2-d matrix of characters [['a', 'b', 'c']]) where a, as a single character, is not the member of the outer list. In fact, it is the first character of the inner list:  >>> 'a' in ['abc'] False  >>> 'a' in ['abc'][0] True  >>> 'a' == ['abc'][0][0] True      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/31900892/why-is-a-in-abc-true-while-a-in-abc-is-false",
        "A_Votes": "50",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When using the interpreter, the expression 'a' in ('abc') returns True, while 'a' in ['abc'] returns False. Can somebody explain this behaviour?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is 'a' in ('abc') True while 'a' in ['abc'] is False?",
        "A_Content": "  For ('abc') you get 'a' in ('abc') return true.  But for ['abc'] as it is a array list you get 'a' in ['abc'] return false.  Example:  Input: ('abc') == 'abc'  Output: True  So if we call 'a' in ('abc') it is same as 'a' in 'abc' because ('abc') is not a tuple but 'abc' is a array of character where character 'a' is in index 0 of string 'abc'.   On the other hand ['abc'] is array list where 'abc' is a single string which is at index 0 of array ['abc'].  Tupple Example: x = ('abc', 'def', 'mnop')  Array Example: x = ['abc', 'def', 'mnop']  or   x = ('abc'), y = 'abc'   Here always x == y.  But in case of 'a' in ['abc'] =>  x = ['abc'], y = 'abc'   Here x != y but x[0] == y      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/31900892/why-is-a-in-abc-true-while-a-in-abc-is-false",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When using the interpreter, the expression 'a' in ('abc') returns True, while 'a' in ['abc'] returns False. Can somebody explain this behaviour?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is 'a' in ('abc') True while 'a' in ['abc'] is False?",
        "A_Content": "  As others has mentioned, ('abc') is not a tuple.  'a' is not a element of ['abc']. The only element in that list is 'abc'  x='abc' in ['abc']  print (x)  True #This will evaluate to true   This will also evaluate to true:  y = 'a' in ['a','b','c']  print (y)  True      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/31900892/why-is-a-in-abc-true-while-a-in-abc-is-false",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When using the interpreter, the expression 'a' in ('abc') returns True, while 'a' in ['abc'] returns False. Can somebody explain this behaviour?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Why is 'a' in ('abc') True while 'a' in ['abc'] is False?",
        "A_Content": "  ('abc') is equivalent to 'abc'.  'a' in ('abc') is equivalent to 'a' in 'abc'.  'a' in ('abc', ) returns False as 'a' in ['abc'].  'a' in ['a', 'b', 'c'] returns True as 'a' in 'abc'.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/31900892/why-is-a-in-abc-true-while-a-in-abc-is-false",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When using the interpreter, the expression 'a' in ('abc') returns True, while 'a' in ['abc'] returns False. Can somebody explain this behaviour?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check in python whether a string contains only numbers?",
        "A_Content": "  You'll want to use the isdigit method on your str object:  if len(isbn) == 10 and isbn.isdigit():   From the isdigit documentation:   str.isdigit()       Return true if all characters in the string are digits   and there is at least one character, false otherwise.      For 8-bit strings, this method is locale-dependent.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "numbers"
        ],
        "URL": "https://stackoverflow.com/questions/21388541/how-do-you-check-in-python-whether-a-string-contains-only-numbers",
        "A_Votes": "154",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How do you check whether a string contains only numbers?  I've given it a go here. I'd like to see the simplest way to accomplish this.  import string  def main():     isbn = input(\"Enter your 10 digit ISBN number: \")     if len(isbn) == 10 and string.digits == True:         print (\"Works\")     else:         print(\"Error, 10 digit number was not inputted and/or letters were inputted.\")         main()  if __name__ == \"__main__\":     main()     input(\"Press enter to exit: \")      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check in python whether a string contains only numbers?",
        "A_Content": "  Use str.isdigit:  >>> \"12345\".isdigit() True >>> \"12345a\".isdigit() False >>>      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "numbers"
        ],
        "URL": "https://stackoverflow.com/questions/21388541/how-do-you-check-in-python-whether-a-string-contains-only-numbers",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you check whether a string contains only numbers?  I've given it a go here. I'd like to see the simplest way to accomplish this.  import string  def main():     isbn = input(\"Enter your 10 digit ISBN number: \")     if len(isbn) == 10 and string.digits == True:         print (\"Works\")     else:         print(\"Error, 10 digit number was not inputted and/or letters were inputted.\")         main()  if __name__ == \"__main__\":     main()     input(\"Press enter to exit: \")      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check in python whether a string contains only numbers?",
        "A_Content": "  Use string isdigit function:  >>> s = '12345' >>> s.isdigit() True >>> s = '1abc' >>> s.isdigit() False      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "numbers"
        ],
        "URL": "https://stackoverflow.com/questions/21388541/how-do-you-check-in-python-whether-a-string-contains-only-numbers",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you check whether a string contains only numbers?  I've given it a go here. I'd like to see the simplest way to accomplish this.  import string  def main():     isbn = input(\"Enter your 10 digit ISBN number: \")     if len(isbn) == 10 and string.digits == True:         print (\"Works\")     else:         print(\"Error, 10 digit number was not inputted and/or letters were inputted.\")         main()  if __name__ == \"__main__\":     main()     input(\"Press enter to exit: \")      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check in python whether a string contains only numbers?",
        "A_Content": "  You can use try catch block here:  s=\"1234\" try:     num=int(s)     print \"S contains only digits\" except:     print \"S doesn't contain digits ONLY\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "numbers"
        ],
        "URL": "https://stackoverflow.com/questions/21388541/how-do-you-check-in-python-whether-a-string-contains-only-numbers",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you check whether a string contains only numbers?  I've given it a go here. I'd like to see the simplest way to accomplish this.  import string  def main():     isbn = input(\"Enter your 10 digit ISBN number: \")     if len(isbn) == 10 and string.digits == True:         print (\"Works\")     else:         print(\"Error, 10 digit number was not inputted and/or letters were inputted.\")         main()  if __name__ == \"__main__\":     main()     input(\"Press enter to exit: \")      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check in python whether a string contains only numbers?",
        "A_Content": "  As every time I encounter an issue with the check is because the str can be None sometimes, and if the str can be None, only use str.isdigit() is not enough as you will get an error      AttributeError: 'NoneType' object has no attribute 'isdigit'   and then you need to first validate the str is None or not. To avoid a multi-if branch, a clear way to do this is:  if str and str.isdigit():   Hope this helps for people have the same issue like me.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "numbers"
        ],
        "URL": "https://stackoverflow.com/questions/21388541/how-do-you-check-in-python-whether-a-string-contains-only-numbers",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you check whether a string contains only numbers?  I've given it a go here. I'd like to see the simplest way to accomplish this.  import string  def main():     isbn = input(\"Enter your 10 digit ISBN number: \")     if len(isbn) == 10 and string.digits == True:         print (\"Works\")     else:         print(\"Error, 10 digit number was not inputted and/or letters were inputted.\")         main()  if __name__ == \"__main__\":     main()     input(\"Press enter to exit: \")      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check in python whether a string contains only numbers?",
        "A_Content": "  What about of float numbers, negatives numbers, etc.. All the examples before will be wrong.   Until now I got something like this, but I think it could be a lot better:  '95.95'.replace('.','',1).isdigit()   will return true only if there is one or no '.' in the string of digits.  '9.5.9.5'.replace('.','',1).isdigit()   will return false     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "numbers"
        ],
        "URL": "https://stackoverflow.com/questions/21388541/how-do-you-check-in-python-whether-a-string-contains-only-numbers",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you check whether a string contains only numbers?  I've given it a go here. I'd like to see the simplest way to accomplish this.  import string  def main():     isbn = input(\"Enter your 10 digit ISBN number: \")     if len(isbn) == 10 and string.digits == True:         print (\"Works\")     else:         print(\"Error, 10 digit number was not inputted and/or letters were inputted.\")         main()  if __name__ == \"__main__\":     main()     input(\"Press enter to exit: \")      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do you check in python whether a string contains only numbers?",
        "A_Content": "  You can also use the regex,   import re   eg:-1) word = \"3487954\"  re.match('^[0-9]*$',word)   eg:-2) word = \"3487.954\"  re.match('^[0-9\\.]*$',word)   eg:-3) word = \"3487.954 328\"  re.match('^[0-9\\.\\ ]*$',word)   As you can see all 3 eg means that there is only no in your string. So you can follow the respective solutions given with them.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "numbers"
        ],
        "URL": "https://stackoverflow.com/questions/21388541/how-do-you-check-in-python-whether-a-string-contains-only-numbers",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How do you check whether a string contains only numbers?  I've given it a go here. I'd like to see the simplest way to accomplish this.  import string  def main():     isbn = input(\"Enter your 10 digit ISBN number: \")     if len(isbn) == 10 and string.digits == True:         print (\"Works\")     else:         print(\"Error, 10 digit number was not inputted and/or letters were inputted.\")         main()  if __name__ == \"__main__\":     main()     input(\"Press enter to exit: \")      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "type object 'datetime.datetime' has no attribute 'datetime'",
        "A_Content": "  Datetime is a module that allows for handling of dates, times and datetimes (all of which are datatypes). This means that datetime is both a top-level module as well as being a type within that module. This is confusing.  Your error is probably based on the confusing naming of the module, and what either you or a module you're using has already imported.  >>> import datetime >>> datetime <module 'datetime' from '/usr/lib/python2.6/lib-dynload/datetime.so'> >>> datetime.datetime(2001,5,1) datetime.datetime(2001, 5, 1, 0, 0)   But, if you import datetime.datetime:  >>> from datetime import datetime >>> datetime <type 'datetime.datetime'> >>> datetime.datetime(2001,5,1) # You shouldn't expect this to work                                  # as you imported the type, not the module Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> AttributeError: type object 'datetime.datetime' has no attribute 'datetime' >>> datetime(2001,5,1) datetime.datetime(2001, 5, 1, 0, 0)   I suspect you or one of the modules you're using has imported like this:  from datetime import datetime.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/12906402/type-object-datetime-datetime-has-no-attribute-datetime",
        "A_Votes": "130",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm following a django tutorial, and I have gotten the following error:     type object 'datetime.datetime' has no attribute 'datetime'   On the following line:  date = datetime.datetime(int(year), int(month), 1)   Does anybody know the reason for the error?  I imported datetime with from datetime import datetime if that helps  Thanks     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "type object 'datetime.datetime' has no attribute 'datetime'",
        "A_Content": "  For python 3.3  from datetime import datetime, timedelta futuredate = datetime.now() + timedelta(days=10)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/12906402/type-object-datetime-datetime-has-no-attribute-datetime",
        "A_Votes": "54",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm following a django tutorial, and I have gotten the following error:     type object 'datetime.datetime' has no attribute 'datetime'   On the following line:  date = datetime.datetime(int(year), int(month), 1)   Does anybody know the reason for the error?  I imported datetime with from datetime import datetime if that helps  Thanks     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "type object 'datetime.datetime' has no attribute 'datetime'",
        "A_Content": "  You should use  date = datetime(int(year), int(month), 1)   Or change  from datetime import datetime   to  import datetime      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/12906402/type-object-datetime-datetime-has-no-attribute-datetime",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm following a django tutorial, and I have gotten the following error:     type object 'datetime.datetime' has no attribute 'datetime'   On the following line:  date = datetime.datetime(int(year), int(month), 1)   Does anybody know the reason for the error?  I imported datetime with from datetime import datetime if that helps  Thanks     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "type object 'datetime.datetime' has no attribute 'datetime'",
        "A_Content": "  You should really import the module into its own alias.  import datetime as dt my_datetime = dt.datetime(year, month, day)   The above has the following benefits over the other solutions:   Calling the variable my_datetime instead of date reduces confusion since there is already a date in the datetime module (datetime.date). The module and the class (both called datetime) do not shadow each other.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/12906402/type-object-datetime-datetime-has-no-attribute-datetime",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm following a django tutorial, and I have gotten the following error:     type object 'datetime.datetime' has no attribute 'datetime'   On the following line:  date = datetime.datetime(int(year), int(month), 1)   Does anybody know the reason for the error?  I imported datetime with from datetime import datetime if that helps  Thanks     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "type object 'datetime.datetime' has no attribute 'datetime'",
        "A_Content": "  If you have used:  from datetime import datetime   Then simply write the code as:  date = datetime(int(year), int(month), 1)   But if you have used:  import datetime   then only you can write:  date = datetime.datetime(int(2005), int(5), 1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/12906402/type-object-datetime-datetime-has-no-attribute-datetime",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm following a django tutorial, and I have gotten the following error:     type object 'datetime.datetime' has no attribute 'datetime'   On the following line:  date = datetime.datetime(int(year), int(month), 1)   Does anybody know the reason for the error?  I imported datetime with from datetime import datetime if that helps  Thanks     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "type object 'datetime.datetime' has no attribute 'datetime'",
        "A_Content": "  I found this to be a lot easier   from dateutil import relativedelta relativedelta.relativedelta(end_time,start_time).seconds      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django"
        ],
        "URL": "https://stackoverflow.com/questions/12906402/type-object-datetime-datetime-has-no-attribute-datetime",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm following a django tutorial, and I have gotten the following error:     type object 'datetime.datetime' has no attribute 'datetime'   On the following line:  date = datetime.datetime(int(year), int(month), 1)   Does anybody know the reason for the error?  I imported datetime with from datetime import datetime if that helps  Thanks     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Output first 100 characters in a string",
        "A_Content": "  print my_string[0:100]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/3486384/output-first-100-characters-in-a-string",
        "A_Votes": "110",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Can seem to find a substring function in python.  Say I want to output the first 100 characters in a string, how can I do this?   I want to do it safely also, meaing if the string is 50 characters it shouldn't fail.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Output first 100 characters in a string",
        "A_Content": "  From python tutorial:     Degenerate slice indices are handled   gracefully: an index that is too large   is replaced by the string size, an   upper bound smaller than the lower   bound returns an empty string.   So it is safe to use x[:100].     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/3486384/output-first-100-characters-in-a-string",
        "A_Votes": "52",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can seem to find a substring function in python.  Say I want to output the first 100 characters in a string, how can I do this?   I want to do it safely also, meaing if the string is 50 characters it shouldn't fail.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Output first 100 characters in a string",
        "A_Content": "  Easy:  print mystring[:100]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/3486384/output-first-100-characters-in-a-string",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can seem to find a substring function in python.  Say I want to output the first 100 characters in a string, how can I do this?   I want to do it safely also, meaing if the string is 50 characters it shouldn't fail.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Output first 100 characters in a string",
        "A_Content": "  To answer Philipp's concern ( in the comments ), slicing works ok for unicode strings too  >>> greek=u\"\" >>> print len(greek) 25 >>> print greek[:10]    If you want to run the above code as a script, put this line at the top  # -*- coding: utf-8 -*-   If your editor doesn't save in utf-8, substitute the correct encoding     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/3486384/output-first-100-characters-in-a-string",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can seem to find a substring function in python.  Say I want to output the first 100 characters in a string, how can I do this?   I want to do it safely also, meaing if the string is 50 characters it shouldn't fail.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Output first 100 characters in a string",
        "A_Content": "  Slicing of arrays is done with [first:last+1].  One trick I tend to use a lot of is to indicate extra information with ellipses. So, if your field is one hundred characters, I would use:  if len(s) <= 100:     print s else:     print \"%s...\"%(s[:97])   And yes, I know () is superfluous in this case for the % formatting operator, it's just my style.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/3486384/output-first-100-characters-in-a-string",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can seem to find a substring function in python.  Say I want to output the first 100 characters in a string, how can I do this?   I want to do it safely also, meaing if the string is 50 characters it shouldn't fail.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Output first 100 characters in a string",
        "A_Content": "  Most of previous examples will raise an exception in case your string is not long enough.  Another approach is to use  'yourstring'.ljust(100)[:100].strip().  This will give you first 100 chars. You might get a shorter string in case your string last chars are spaces.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/3486384/output-first-100-characters-in-a-string",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can seem to find a substring function in python.  Say I want to output the first 100 characters in a string, how can I do this?   I want to do it safely also, meaing if the string is 50 characters it shouldn't fail.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  In Python 2:  mylist = ['x', 3, 'b'] print '[%s]' % ', '.join(map(str, mylist))   In Python 3 (where print is a builtin function and not a syntax feature anymore):  mylist = ['x', 3, 'b'] print('[%s]' % ', '.join(map(str, mylist)))   Both return:  [x, 3, b]   This is using the map() function to call str for each element of mylist, creating a new list of strings that is then joined into one string with str.join(). Then, the % string formatting operator substitutes the string in instead of %s in \"[%s]\".     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "148",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  If you are using Python3:  print('[',end='');print(*L, sep=', ', end='');print(']')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  This is simple code, so if you are new you should understand it easily enough.      mylist = [\"x\", 3, \"b\"]     for items in mylist:         print(items)   It prints all of them without quotes, like you wanted.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  Instead of using map, I'd recommend using a generator expression with the capability of join to accept an iterator:  def get_nice_string(list_or_iterator):     return \"[\" + \", \".join( str(x) for x in list_or_iterator) + \"]\"   Here, join is a member function of the string class str. It takes one argument: a list (or iterator) of strings, then returns a new string with all of the elements concatenated by, in this case, ,.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  Using only print:  >>> l = ['x', 3, 'b'] >>> print(*l, sep='\\n') x 3 b >>> print(*l, sep=', ') x, 3, b      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  You can delete all unwanted characters from a string using its translate() method with None for the table argument followed by a string containing the character(s) you want removed for its deletechars argument.  lst = ['x', 3, 'b']  print str(lst).translate(None, \"'\")  # [x, 3, b]   If you're using a version of Python before 2.6, you'll need to use the string module's translate() function instead because the ability to pass None as the table argument wasn't added until Python 2.6. Using it looks like this:  import string  print string.translate(str(lst), None, \"'\")   Using the string.translate() function will also work in 2.6+, so using it might be preferable.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  Here's an interactive session showing some of the steps in @TokenMacGuy's one-liner. First he uses the map function to convert each item in the list to a string (actually, he's making a new list, not converting the items in the old list). Then he's using the string method join to combine those strings with ', ' between them. The rest is just string formatting, which is pretty straightforward. (Edit: this instance is straightforward; string formatting in general can be somewhat complex.)  Note that using join is a simple and efficient way to build up a string from several substrings, much more efficient than doing it by successively adding strings to strings, which involves a lot of copying behind the scenes.  >>> mylist = ['x', 3, 'b'] >>> m = map(str, mylist) >>> m ['x', '3', 'b'] >>> j = ', '.join(m) >>> j 'x, 3, b'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  Using .format for string formatting,  mylist = ['x', 3, 'b'] print(\"[{0}]\".format(', '.join(map(str, mylist))))   Output:  [x, 3, b]     Explanation:   map is used to map each element of the list to string type. The elements are joined together into a string with , as separator. We use [ and ] in the print statement to show the list braces.   Reference:  .format for string formatting PEP-3101     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to properly print a list?",
        "A_Content": "  This is another way... maybe a little bit more labourious...  mylist = ['x', 3, 'b'] print('[', end='') print(*myList, sep=', ', end='') print(']')   You will remove \\nand this way you can concatenate the strings.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5445970/how-to-properly-print-a-list",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So I have a list:  ['x', 3, 'b']  And I want the output to be:  [x, 3, b]  How can I do this in python?  If I do str(['x', 3, 'b']), I get one with quotes, but I don't want quotes.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove duplicates from Python list and keep order? [duplicate]",
        "A_Content": "  A list can be sorted and deduplicated using built-in functions:  myList = sorted(set(myList))    set is a built-in function for Python >= 2.3 sorted is a built-in function for Python >= 2.4      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting",
            "unique"
        ],
        "URL": "https://stackoverflow.com/questions/479897/how-to-remove-duplicates-from-python-list-and-keep-order",
        "A_Votes": "183",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              How do you remove duplicates from a list whilst preserving order?                                        28 answers                                          Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:  from sets import Set [...] myHash = Set(myList)   but I don't know how to retrieve the list members from the hash in alphabetical order.  I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove duplicates from Python list and keep order? [duplicate]",
        "A_Content": "  If your input is already sorted, then there may be a simpler way to do it:  from operator import itemgetter from itertools import groupby unique_list = list(map(itemgetter(0), groupby(yourList)))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting",
            "unique"
        ],
        "URL": "https://stackoverflow.com/questions/479897/how-to-remove-duplicates-from-python-list-and-keep-order",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              How do you remove duplicates from a list whilst preserving order?                                        28 answers                                          Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:  from sets import Set [...] myHash = Set(myList)   but I don't know how to retrieve the list members from the hash in alphabetical order.  I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove duplicates from Python list and keep order? [duplicate]",
        "A_Content": "  If you want to keep order of the original list, just use OrderedDict with None as values.  In Python2:      from collections import OrderedDict     from itertools import izip, repeat      unique_list = list(OrderedDict(izip(my_list, repeat(None))))   In Python3 it's even simpler:      from collections import OrderedDict     from itertools import repeat      unique_list = list(OrderedDict(zip(my_list, repeat(None))))   If you don't like iterators (zip and repeat) you can use a generator (works both in 2 & 3):      from collections import OrderedDict     unique_list = list(OrderedDict((element, None) for element in my_list))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting",
            "unique"
        ],
        "URL": "https://stackoverflow.com/questions/479897/how-to-remove-duplicates-from-python-list-and-keep-order",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              How do you remove duplicates from a list whilst preserving order?                                        28 answers                                          Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:  from sets import Set [...] myHash = Set(myList)   but I don't know how to retrieve the list members from the hash in alphabetical order.  I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove duplicates from Python list and keep order? [duplicate]",
        "A_Content": "  If it's clarity you're after, rather than speed, I think this is very clear:  def sortAndUniq(input):   output = []   for x in input:     if x not in output:       output.append(x)   output.sort()   return output   It's O(n^2) though, with the repeated use of not in for each element of the input list.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting",
            "unique"
        ],
        "URL": "https://stackoverflow.com/questions/479897/how-to-remove-duplicates-from-python-list-and-keep-order",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              How do you remove duplicates from a list whilst preserving order?                                        28 answers                                          Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:  from sets import Set [...] myHash = Set(myList)   but I don't know how to retrieve the list members from the hash in alphabetical order.  I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove duplicates from Python list and keep order? [duplicate]",
        "A_Content": "  > but I don't know how to retrieve the list members from the hash in alphabetical order.  Not really your main question, but for future reference Rod's answer using sorted can be used for traversing a dict's keys in sorted order:  for key in sorted(my_dict.keys()):    print key, my_dict[key]    ...   and also because tuple's are ordered by the first member of the tuple, you can do the same with items:  for key, val in sorted(my_dict.items()):     print key, val     ...      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting",
            "unique"
        ],
        "URL": "https://stackoverflow.com/questions/479897/how-to-remove-duplicates-from-python-list-and-keep-order",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              How do you remove duplicates from a list whilst preserving order?                                        28 answers                                          Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:  from sets import Set [...] myHash = Set(myList)   but I don't know how to retrieve the list members from the hash in alphabetical order.  I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to remove duplicates from Python list and keep order? [duplicate]",
        "A_Content": "  For the string data   output = []       def uniq(input):          if input not in output:             output.append(input)  print output           ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "sorting",
            "unique"
        ],
        "URL": "https://stackoverflow.com/questions/479897/how-to-remove-duplicates-from-python-list-and-keep-order",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              How do you remove duplicates from a list whilst preserving order?                                        28 answers                                          Given a list of strings, I want to sort it alphabetically and remove duplicates. I know I can do this:  from sets import Set [...] myHash = Set(myList)   but I don't know how to retrieve the list members from the hash in alphabetical order.  I'm not married to the hash, so any way to accomplish this will work. Also, performance is not an issue, so I'd prefer a solution that is expressed in code clearly to a fast but more opaque one.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Removing python module installed in develop mode",
        "A_Content": "  Use the --uninstall or -u option to develop, i.e:  python setup.py develop --uninstall   This will remove it from easy-install.pth and delete the .egg-link.  The only thing it doesn't do is delete scripts (yet).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "setuptools"
        ],
        "URL": "https://stackoverflow.com/questions/3606457/removing-python-module-installed-in-develop-mode",
        "A_Votes": "199",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Hi I was trying the python packaging using setuptools and to test I installed the module in develop mode. i.e  python setup.py develop   This has added my modules directory to sys.path. Now I want to remove the module is there any way to do this?  Thanks in advance     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Removing python module installed in develop mode",
        "A_Content": "  Edit easy-install.pth in your site-packages directory and remove the line that points to your development version of that package.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "setuptools"
        ],
        "URL": "https://stackoverflow.com/questions/3606457/removing-python-module-installed-in-develop-mode",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Hi I was trying the python packaging using setuptools and to test I installed the module in develop mode. i.e  python setup.py develop   This has added my modules directory to sys.path. Now I want to remove the module is there any way to do this?  Thanks in advance     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Removing python module installed in develop mode",
        "A_Content": "  I have had a similar problem to this before. What I did was I loaded up the Python shell, imported the module and then printed its __file__ attribute. From there I would just remove the folder or file that was being associated.  What you may want to look into is using virtualenv this system allows you to create a instance of python separate from your system. Any modules you install or use in this instance are self contained including the version of the module.   I keep all my projects now inside of there own contained virtualenv, which allows me to install and use whatever modules I want without worrying about screwing up modules from other projects.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "setuptools"
        ],
        "URL": "https://stackoverflow.com/questions/3606457/removing-python-module-installed-in-develop-mode",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Hi I was trying the python packaging using setuptools and to test I installed the module in develop mode. i.e  python setup.py develop   This has added my modules directory to sys.path. Now I want to remove the module is there any way to do this?  Thanks in advance     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  Use the builtin function max.  Example:  max(2, 4) returns 4.  Just for giggles, there's a min as well...should you need it. :P     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "165",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  max()     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  max(number_one, number_two)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  You can use max(value, run)  The function max takes any number of arguments, or (alternatively) an iterable, and returns the maximum value.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  max(value,run)   should do it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  Just for the fun of it, after the party has finished and the horse bolted.  The answer is: max() !     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  You could also achieve the same result by using a Conditional Expression:  maxnum = run if run > value else value   a bit more flexible than max but admittedly longer to type.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  I noticed that if you have divisions it rounds off to integer, it would be better to use:  c=float(max(a1,...,an))/b  Sorry for the late post!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  numberList=[16,19,42,43,74,66]  largest = numberList[0]  for num2 in numberList:      if num2 > largest:          largest=num2  print(largest)   gives largest number out of the numberslist without using a Max statement     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "maximum of 2 numbers",
        "A_Content": "  (num1>=num2)*num1+(num2>num1)*num2 will return the maximum of two values.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "max"
        ],
        "URL": "https://stackoverflow.com/questions/3357369/maximum-of-2-numbers",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How to find the maximum of 2 numbers?  value = -9999 run = problem.getscore()   I need to compare the 2 values i.e value and run and find the maximum of 2. I need some python function to operate it?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print a linebreak in a python function?",
        "A_Content": "  You have your slash backwards, it should be \"\\n\"     ",
        "Language": "Python",
        "Tags": [
            "python",
            "line-breaks"
        ],
        "URL": "https://stackoverflow.com/questions/5982206/how-to-print-a-linebreak-in-a-python-function",
        "A_Votes": "183",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a list of strings in my code;  A = ['a1', 'a2', 'a3' ...] B = ['b1', 'b2', 'b3' ...]   and I want to print them separated by a linebreak, like this:  >a1 b1 >a2 b2 >a3 b3   I've tried:  print '>' + A + '/n' + B   But /n isn't recognized like a line break.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print a linebreak in a python function?",
        "A_Content": "  The newline character is actually '\\n'.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "line-breaks"
        ],
        "URL": "https://stackoverflow.com/questions/5982206/how-to-print-a-linebreak-in-a-python-function",
        "A_Votes": "27",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of strings in my code;  A = ['a1', 'a2', 'a3' ...] B = ['b1', 'b2', 'b3' ...]   and I want to print them separated by a linebreak, like this:  >a1 b1 >a2 b2 >a3 b3   I've tried:  print '>' + A + '/n' + B   But /n isn't recognized like a line break.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print a linebreak in a python function?",
        "A_Content": "  for pair in zip(A, B):     print \">\"+'\\n'.join(pair)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "line-breaks"
        ],
        "URL": "https://stackoverflow.com/questions/5982206/how-to-print-a-linebreak-in-a-python-function",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of strings in my code;  A = ['a1', 'a2', 'a3' ...] B = ['b1', 'b2', 'b3' ...]   and I want to print them separated by a linebreak, like this:  >a1 b1 >a2 b2 >a3 b3   I've tried:  print '>' + A + '/n' + B   But /n isn't recognized like a line break.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print a linebreak in a python function?",
        "A_Content": "  >>> A = ['a1', 'a2', 'a3'] >>> B = ['b1', 'b2', 'b3']  >>> for x in A:         for i in B:             print \">\" + x + \"\\n\" + i   Outputs:  >a1 b1 >a1 b2 >a1 b3 >a2 b1 >a2 b2 >a2 b3 >a3 b1 >a3 b2 >a3 b3   Notice that you are using /n which is not correct!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "line-breaks"
        ],
        "URL": "https://stackoverflow.com/questions/5982206/how-to-print-a-linebreak-in-a-python-function",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of strings in my code;  A = ['a1', 'a2', 'a3' ...] B = ['b1', 'b2', 'b3' ...]   and I want to print them separated by a linebreak, like this:  >a1 b1 >a2 b2 >a3 b3   I've tried:  print '>' + A + '/n' + B   But /n isn't recognized like a line break.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print a linebreak in a python function?",
        "A_Content": "  \\n is an escape sequence, denoted by the backslash. A normal forward slash, such as /n will not do the job. In your code you are using /n instead of \\n.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "line-breaks"
        ],
        "URL": "https://stackoverflow.com/questions/5982206/how-to-print-a-linebreak-in-a-python-function",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of strings in my code;  A = ['a1', 'a2', 'a3' ...] B = ['b1', 'b2', 'b3' ...]   and I want to print them separated by a linebreak, like this:  >a1 b1 >a2 b2 >a3 b3   I've tried:  print '>' + A + '/n' + B   But /n isn't recognized like a line break.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How to print a linebreak in a python function?",
        "A_Content": "  All three way you can use for newline character :  '\\n'  \"\\n\"  \"\"\"\\n\"\"\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "line-breaks"
        ],
        "URL": "https://stackoverflow.com/questions/5982206/how-to-print-a-linebreak-in-a-python-function",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of strings in my code;  A = ['a1', 'a2', 'a3' ...] B = ['b1', 'b2', 'b3' ...]   and I want to print them separated by a linebreak, like this:  >a1 b1 >a2 b2 >a3 b3   I've tried:  print '>' + A + '/n' + B   But /n isn't recognized like a line break.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  Hashing is the process of converting some large amount of data into a much smaller amount (typically a single integer) in a repeatable way so that it can be looked up in a table in constant-time (O(1)), which is important for high-performance algorithms and data structures.  Immutability is the idea that an object will not change in some important way after it has been created, especially in any way that might change the hash value of that object.  The two ideas are related because objects which are used as hash keys must typically be immutable so their hash value doesn't change.  If it was allowed to change then the location of that object in a data structure such as a hashtable would change and then the whole purpose of hashing for efficiency is defeated.  To really grasp the idea you should try to implement your own hashtable in a language like C/C++, or read the Java implementation of the HashMap class.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "78",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  Technically, hashable means that the class defines __hash__(). According to the docs:     __hash__() should return an integer. The only required property is that objects which compare equal have the same hash value; it is advised to somehow mix together (e.g. using exclusive or) the hash values for the components of the object that also play a part in comparison of objects.   I think that for the python builtin types, all hashable types are also immutable. It would be difficult but perhaps not impossible to have a mutable object that nonetheless defined __hash__().     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  From the Python Glossary:     An object is hashable if it has a hash value which never changes during its lifetime (it needs a __hash__() method), and can be compared to other objects (it needs an __eq__() or __cmp__() method). Hashable objects which compare equal must have the same hash value.      Hashability makes an object usable as a dictionary key and a set member, because these data structures use the hash value internally.      All of Pythons immutable built-in objects are hashable, while no mutable containers (such as lists or dictionaries) are. Objects which are instances of user-defined classes are hashable by default; they all compare unequal, and their hash value is their id().   Dicts and sets must use a hash for efficient lookup in a hash table; the hash values must be immutable, because changing the hash will mess up the data structures and cause the dict or set to fail. The easiest way to make the hash value immutable is to make the whole object immutable, which is why the two are often mentioned together.  While none of the built-in mutable objects are hashable, it is possible to make a mutable object with a hash value that's not mutable. It's common for only a portion of the object to represent its identity, while the rest of the object contains properties that are free to change. As long as the hash value and comparison functions are based on the identity but not the mutable properties, and the identity never changes, you've met the requirements.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "   Are there mutable objects that are hashable or immutable objects that are not hashable?   In Python, tuple is immutable, but it is hashable only if all its elements are hashable.  >>> tt = (1, 2, (30, 40)) >>> hash(tt) 8027212646858338501 >>> tl = (1, 2, [30, 40]) >>> hash(tl) TypeError: unhashable type: 'list'   Hashable Types   The atomic immutable types are all hashable, such as str, bytes, numeric types A frozen set is always hashable(its elements must be hashable by definition) A tuple is hashable only if all its elements are hashable User-defined types are hashable by default because their hash value is their id()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  There is an implicit even if there is no explicit relationship forced between immutable and hashable due the interplay between    Hashable objects which compare equal must have the same hash value An object is hashable if it has a hash value which never changes during its lifetime.   There is no problem here unless you redefine __eq__ so the the objects class defines equivalence on value.  Once you've done that you need to find a stable hash function which always returns the same value for objects which represent the same value (eg, where __eq__) returns True, and never changes during the lifetime of an object.  It hard to see an application where this is possible, consider a possible class A which meets these requirements. Although there is the obvious degenerate case where __hash__ returns a constant.  Now:-  >>> a = A(1) >>> b = A(1) >>> c = A(2) >>> a == b True >>> a == c False >>> hash(a) == hash(b) True >>> a.set_value(c) >>> a == c True >>> assert(hash(a) == hash(c)) # Because a == c => hash(a) == hash(c) >>> assert(hash(a) == hash(b)) # Because hash(a) and hash(b) have compared equal                                   before and the result must stay static over the objects lifetime.   In fact this means at creation hash(b) == hash(c), despite the fact there are never compared equal. I struggle to see anyway to usefully define __hash__() for a mutable object which defines compare by value.   Note: __lt__, __le__ , __gt__ and __ge__ comparsions aren't affected so you can still define an ordering of hashable objects, mutable or otherwise based on their value.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  just because this is the top Google hit, here's a simple way to make a mutable object hashable:  >>> class HashableList(list): ...  instancenumber = 0  # class variable ...  def __init__(self, initial = []): ...   super(HashableList, self).__init__(initial) ...   self.hashvalue = HashableList.instancenumber ...   HashableList.instancenumber += 1 ...  def __hash__(self): ...   return self.hashvalue ...  >>> l = [1,2,3] >>> m = HashableList(l) >>> n = HashableList([1,2,3]) >>> m == n True >>> a={m:1, n:2} >>> a[l] = 3 Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> TypeError: unhashable type: 'list' >>> m.hashvalue, n.hashvalue (0, 1)   I actually found a use for something like this when creating a class to cast SQLAlchemy records into something mutable and more useful to me, while maintaining their hashability for use as dict keys.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  Immutable means that object will not change in any significant manner during its lifetime. It's a vague but common idea in programming languages.  Hashability is slightly different, and refers to comparison.      hashable An object is hashable if it has a hash value which never   changes during its lifetime (it needs a __hash__() method), and can be   compared to other objects (it needs an __eq__() or __cmp__() method).   Hashable objects which compare equal must have the same hash value.   All user-defined classes have __hash__ method, which by default just returns the object ID. So an object that meets the criteria for hashability is not necessarily immutable.  Objects of any new class you declare can be used as a dictionary key, unless you prevent it by, for example, throwing from __hash__  We could say that all immutable objects are hashable, because if the hash changes during the object's lifetime, then it means that the object mutated.   But not quite. Consider a tuple which has a list (mutable). Some say tuple is immutable, but at the same time it is somewhat not hashable (throws).  d = dict() d[ (0,0) ] = 1    #perfectly fine d[ (0,[0]) ] = 1  #throws   Hashability and immutability refer to object instancess, not type. For example, an object of type tuple can be hashable or not.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  In Python they're mostly interchangeable; since the hash is supposed to represent the content, so it's just as mutable as the object, and having an object change the hash value would make it unusable as a dict key.  In other languages, the hash value is more related to the objects 'identity', and not (necessarily) to the value.  Thus, for a mutable object, the pointer could be used to start the hashing.  Assuming, of course, that an object doesn't move in memory (as some GC do).  This is the approach used in Lua, for example.  This makes a mutable object usable as a table key; but creates several (unpleasant) surprises for newbies.  In the end, having an immutable sequence type (tuples) makes it nicer for 'multi-value keys'.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Hashable, immutable",
        "A_Content": "  Hashable means that an variable's value can be represented (or, rather, encoded) by a constant -- string, number, etc. Now, something that is subject to change (mutable) cannot be represented by something that is not. Therefore, any variable that is mutable cannot be hashable and, by the same token, only immutable variables can be hashable.  Hope this helps ...     ",
        "Language": "Python",
        "Tags": [
            "python",
            "data-structures",
            "hash",
            "immutability"
        ],
        "URL": "https://stackoverflow.com/questions/2671376/hashable-immutable",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    From a recent SO question (see Create a dictionary in python which is indexed by lists) I realized I probably had a wrong conception of the meaning of hashable and immutable objects in python.   What does hashable mean in practice? What is the relation between hashable and immmutable? Are there mutable objects that are hashable or immutable objects that are not hashable?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Regular expression matching a multiline block of text",
        "A_Content": "  Try this:  re.compile(r\"^(.+)\\n((?:\\n.+)+)\", re.MULTILINE)   I think your biggest problem is that you're expecting the ^ and $ anchors to match linefeeds, but they don't.  In multiline mode, ^ matches the position immediately following a newline and $ matches the position immediately preceding a newline.  Be aware, too, that a newline can consist of a linefeed (\\n), a carriage-return (\\r), or a carriage-return+linefeed (\\r\\n).  If you aren't certain that your target text uses only linefeeds, you should use this more inclusive version of the regex:  re.compile(r\"^(.+)(?:\\n|\\r\\n?)((?:(?:\\n|\\r\\n?).+)+)\", re.MULTILINE)   BTW, you don't want to use the DOTALL modifier here; you're relying on the fact that the dot matches everything except newlines.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "multiline"
        ],
        "URL": "https://stackoverflow.com/questions/587345/regular-expression-matching-a-multiline-block-of-text",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm having a bit of trouble getting a Python regex to work when matching against text that spans multiple lines. The example text is ('\\n' is a newline)  some Varying TEXT\\n \\n DSJFKDAFJKDAFJDSAKFJADSFLKDLAFKDSAF\\n [more of the above, ending with a newline]\\n [yep, there is a variable number of lines here]\\n \\n (repeat the above a few hundred times).   I'd like to capture two things: the 'some_Varying_TEXT' part, and all of the lines of uppercase text that comes two lines below it in one capture (i can strip out the newline characters later).  I've tried with a few approaches:  re.compile(r\"^>(\\w+)$$([.$]+)^$\", re.MULTILINE) # try to capture both parts re.compile(r\"(^[^>][\\w\\s]+)$\", re.MULTILINE|re.DOTALL) # just textlines   and a lot of variations hereof with no luck. The last one seems to match the lines of text one by one, which is not what I really want. I can catch the first part, no problem, but I can't seem to catch the 4-5 lines of uppercase text. I'd like match.group(1) to be some_Varying_Text and group(2) to be line1+line2+line3+etc until the empty line is encountered.  If anyone's curious, its supposed to be a sequence of aminoacids that make up a protein.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Regular expression matching a multiline block of text",
        "A_Content": "  This will work:  >>> import re >>> rx_sequence=re.compile(r\"^(.+?)\\n\\n((?:[A-Z]+\\n)+)\",re.MULTILINE) >>> rx_blanks=re.compile(r\"\\W+\") # to remove blanks and newlines >>> text=\"\"\"Some varying text1 ... ... AAABBBBBBCCCCCCDDDDDDD ... EEEEEEEFFFFFFFFGGGGGGG ... HHHHHHIIIIIJJJJJJJKKKK ... ... Some varying text 2 ... ... LLLLLMMMMMMNNNNNNNOOOO ... PPPPPPPQQQQQQRRRRRRSSS ... TTTTTUUUUUVVVVVVWWWWWW ... \"\"\" >>> for match in rx_sequence.finditer(text): ...   title, sequence = match.groups() ...   title = title.strip() ...   sequence = rx_blanks.sub(\"\",sequence) ...   print \"Title:\",title ...   print \"Sequence:\",sequence ...   print ... Title: Some varying text1 Sequence: AAABBBBBBCCCCCCDDDDDDDEEEEEEEFFFFFFFFGGGGGGGHHHHHHIIIIIJJJJJJJKKKK  Title: Some varying text 2 Sequence: LLLLLMMMMMMNNNNNNNOOOOPPPPPPPQQQQQQRRRRRRSSSTTTTTUUUUUVVVVVVWWWWWW     Some explanation about this regular expression might be useful: ^(.+?)\\n\\n((?:[A-Z]+\\n)+)   The first character (^) means \"starting at the beginning of a line\".  Be aware that it does not match the newline itself (same for $: it means \"just before a newline\", but it does not match the newline itself). Then (.+?)\\n\\n means \"match as few characters as possible (all characters are allowed) until you reach two newlines\".  The result (without the newlines) is put in the first group. [A-Z]+\\n means \"match as many upper case letters as possible until you reach a newline.  This defines what I will call a textline. ((?:textline)+) means match one or more textlines but do not put each line in a group. Instead, put all the textlines in one group. You could add a final \\n in the regular expression if you want to enforce a double newline at the end. Also, if you are not sure about what type of newline you will get (\\n or \\r or \\r\\n) then just fix the regular expression by replacing every occurrence of \\n by (?:\\n|\\r\\n?).      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "multiline"
        ],
        "URL": "https://stackoverflow.com/questions/587345/regular-expression-matching-a-multiline-block-of-text",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm having a bit of trouble getting a Python regex to work when matching against text that spans multiple lines. The example text is ('\\n' is a newline)  some Varying TEXT\\n \\n DSJFKDAFJKDAFJDSAKFJADSFLKDLAFKDSAF\\n [more of the above, ending with a newline]\\n [yep, there is a variable number of lines here]\\n \\n (repeat the above a few hundred times).   I'd like to capture two things: the 'some_Varying_TEXT' part, and all of the lines of uppercase text that comes two lines below it in one capture (i can strip out the newline characters later).  I've tried with a few approaches:  re.compile(r\"^>(\\w+)$$([.$]+)^$\", re.MULTILINE) # try to capture both parts re.compile(r\"(^[^>][\\w\\s]+)$\", re.MULTILINE|re.DOTALL) # just textlines   and a lot of variations hereof with no luck. The last one seems to match the lines of text one by one, which is not what I really want. I can catch the first part, no problem, but I can't seem to catch the 4-5 lines of uppercase text. I'd like match.group(1) to be some_Varying_Text and group(2) to be line1+line2+line3+etc until the empty line is encountered.  If anyone's curious, its supposed to be a sequence of aminoacids that make up a protein.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Regular expression matching a multiline block of text",
        "A_Content": "  If each file only has one sequence of aminoacids, I wouldn't use regular expressions at all.  Just something like this:  def read_amino_acid_sequence(path):     with open(path) as sequence_file:         title = sequence_file.readline() # read 1st line         aminoacid_sequence = sequence_file.read() # read the rest      # some cleanup, if necessary     title = title.strip() # remove trailing white spaces and newline     aminoacid_sequence = aminoacid_sequence.replace(\" \",\"\").replace(\"\\n\",\"\")     return title, aminoacid_sequence      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "multiline"
        ],
        "URL": "https://stackoverflow.com/questions/587345/regular-expression-matching-a-multiline-block-of-text",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm having a bit of trouble getting a Python regex to work when matching against text that spans multiple lines. The example text is ('\\n' is a newline)  some Varying TEXT\\n \\n DSJFKDAFJKDAFJDSAKFJADSFLKDLAFKDSAF\\n [more of the above, ending with a newline]\\n [yep, there is a variable number of lines here]\\n \\n (repeat the above a few hundred times).   I'd like to capture two things: the 'some_Varying_TEXT' part, and all of the lines of uppercase text that comes two lines below it in one capture (i can strip out the newline characters later).  I've tried with a few approaches:  re.compile(r\"^>(\\w+)$$([.$]+)^$\", re.MULTILINE) # try to capture both parts re.compile(r\"(^[^>][\\w\\s]+)$\", re.MULTILINE|re.DOTALL) # just textlines   and a lot of variations hereof with no luck. The last one seems to match the lines of text one by one, which is not what I really want. I can catch the first part, no problem, but I can't seem to catch the 4-5 lines of uppercase text. I'd like match.group(1) to be some_Varying_Text and group(2) to be line1+line2+line3+etc until the empty line is encountered.  If anyone's curious, its supposed to be a sequence of aminoacids that make up a protein.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Regular expression matching a multiline block of text",
        "A_Content": "  find:  ^>([^\\n\\r]+)[\\n\\r]([A-Z\\n\\r]+)   \\1 = some_varying_text   \\2 = lines of all CAPS  Edit (proof that this works):  text = \"\"\"> some_Varying_TEXT  DSJFKDAFJKDAFJDSAKFJADSFLKDLAFKDSAF GATACAACATAGGATACA GGGGGAAAAAAAATTTTTTTTT CCCCAAAA  > some_Varying_TEXT2  DJASDFHKJFHKSDHF HHASGDFTERYTERE GAGAGAGAGAG PPPPPAAAAAAAAAAAAAAAP \"\"\"  import re  regex = re.compile(r'^>([^\\n\\r]+)[\\n\\r]([A-Z\\n\\r]+)', re.MULTILINE) matches = [m.groups() for m in regex.finditer(text)]  for m in matches:     print 'Name: %s\\nSequence:%s' % (m[0], m[1])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "multiline"
        ],
        "URL": "https://stackoverflow.com/questions/587345/regular-expression-matching-a-multiline-block-of-text",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm having a bit of trouble getting a Python regex to work when matching against text that spans multiple lines. The example text is ('\\n' is a newline)  some Varying TEXT\\n \\n DSJFKDAFJKDAFJDSAKFJADSFLKDLAFKDSAF\\n [more of the above, ending with a newline]\\n [yep, there is a variable number of lines here]\\n \\n (repeat the above a few hundred times).   I'd like to capture two things: the 'some_Varying_TEXT' part, and all of the lines of uppercase text that comes two lines below it in one capture (i can strip out the newline characters later).  I've tried with a few approaches:  re.compile(r\"^>(\\w+)$$([.$]+)^$\", re.MULTILINE) # try to capture both parts re.compile(r\"(^[^>][\\w\\s]+)$\", re.MULTILINE|re.DOTALL) # just textlines   and a lot of variations hereof with no luck. The last one seems to match the lines of text one by one, which is not what I really want. I can catch the first part, no problem, but I can't seem to catch the 4-5 lines of uppercase text. I'd like match.group(1) to be some_Varying_Text and group(2) to be line1+line2+line3+etc until the empty line is encountered.  If anyone's curious, its supposed to be a sequence of aminoacids that make up a protein.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Regular expression matching a multiline block of text",
        "A_Content": "  My preference.  lineIter= iter(aFile) for line in lineIter:     if line.startswith( \">\" ):          someVaryingText= line          break assert len( lineIter.next().strip() ) == 0 acids= [] for line in lineIter:     if len(line.strip()) == 0:         break     acids.append( line )   At this point you have someVaryingText as a string, and the acids as a list of strings. You can do \"\".join( acids ) to make a single string.  I find this less frustrating (and more flexible) than multiline regexes.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "multiline"
        ],
        "URL": "https://stackoverflow.com/questions/587345/regular-expression-matching-a-multiline-block-of-text",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm having a bit of trouble getting a Python regex to work when matching against text that spans multiple lines. The example text is ('\\n' is a newline)  some Varying TEXT\\n \\n DSJFKDAFJKDAFJDSAKFJADSFLKDLAFKDSAF\\n [more of the above, ending with a newline]\\n [yep, there is a variable number of lines here]\\n \\n (repeat the above a few hundred times).   I'd like to capture two things: the 'some_Varying_TEXT' part, and all of the lines of uppercase text that comes two lines below it in one capture (i can strip out the newline characters later).  I've tried with a few approaches:  re.compile(r\"^>(\\w+)$$([.$]+)^$\", re.MULTILINE) # try to capture both parts re.compile(r\"(^[^>][\\w\\s]+)$\", re.MULTILINE|re.DOTALL) # just textlines   and a lot of variations hereof with no luck. The last one seems to match the lines of text one by one, which is not what I really want. I can catch the first part, no problem, but I can't seem to catch the 4-5 lines of uppercase text. I'd like match.group(1) to be some_Varying_Text and group(2) to be line1+line2+line3+etc until the empty line is encountered.  If anyone's curious, its supposed to be a sequence of aminoacids that make up a protein.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the Python egg cache (PYTHON_EGG_CACHE)?",
        "A_Content": "  From my investigations it turns out that some eggs are packaged as zip files, and are saved as such in Python's site-packages directory.  These zipped eggs need to be unzipped before they can be executed, so are expanded into the PYTHON_EGG_CACHE directory which by default is ~/.python-eggs (located in the user's home directory). If this doesn't exist it causes problems when trying to run applications.  There are a number of fixes:   Create a .python-eggs directory in the user's home directory and make it writable for the user. Create a global directory for unzipping (eg. /tmp/python-eggs) and set the environment variable PYTHON_EGG_CACHE to this directory. Use the -Z switch when using easy_install to unzip the package when installing.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-egg-cache"
        ],
        "URL": "https://stackoverflow.com/questions/2192323/what-is-the-python-egg-cache-python-egg-cache",
        "A_Votes": "64",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've just upgraded from Python 2.6.1 to 2.6.4 on my development machine and upon starting a python script was presented with the following message:     Can't extract file(s) to egg cache      The following error occurred while   trying to extract file(s) to  the   Python egg cache:      [Errno 13] Permission denied:   '/var/www/.python-eggs'      The Python egg cache directory is   currently set to:      /var/www/.python-eggs      Perhaps your account does not have   write access to this directory?   You   can change the cache directory by   setting the PYTHON_EGG_CACHE    environment variable to point to an   accessible directory.   There isn't anything in the python docs so I'm at a bit of a loss regarding best-practices on where to put this directory and what it's used for.  Can someone explain what the Python egg cache is?  Also, can you explain why/how it is different to the site-packages directory Python uses to store eggs (as I understand it)?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the Python egg cache (PYTHON_EGG_CACHE)?",
        "A_Content": "  The python egg cache is simply a directory used by setuptools to store packages installed that conform to the egg specification.  You can read more about setuptools here.  Additionally, as the error message states, you can specify a different egg cache directory in your environment by setting PYTHON_EGG_CACHE=/some/other/dir.  The easiest way to do this is to set it in your ~/.bash_profile (assuming you're using bash), like this:  export PYTHON_EGG_CACHE=/some/other/dir   You may need to set it in your Apache environment if you're using a Web application.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-egg-cache"
        ],
        "URL": "https://stackoverflow.com/questions/2192323/what-is-the-python-egg-cache-python-egg-cache",
        "A_Votes": "30",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've just upgraded from Python 2.6.1 to 2.6.4 on my development machine and upon starting a python script was presented with the following message:     Can't extract file(s) to egg cache      The following error occurred while   trying to extract file(s) to  the   Python egg cache:      [Errno 13] Permission denied:   '/var/www/.python-eggs'      The Python egg cache directory is   currently set to:      /var/www/.python-eggs      Perhaps your account does not have   write access to this directory?   You   can change the cache directory by   setting the PYTHON_EGG_CACHE    environment variable to point to an   accessible directory.   There isn't anything in the python docs so I'm at a bit of a loss regarding best-practices on where to put this directory and what it's used for.  Can someone explain what the Python egg cache is?  Also, can you explain why/how it is different to the site-packages directory Python uses to store eggs (as I understand it)?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the Python egg cache (PYTHON_EGG_CACHE)?",
        "A_Content": "  This is a dark side-effect of using otherwise nice eggs mechanism.  Eggs are packages (a directory full of files) packed into one .egg file to simplify depolyment.   They are stored in /site-packages/ dir.  As long as the files stored in the egg are .py files it works great. Python import can import things from any file-like object just like it was an ordinary file.  But when something like .so happens to drop in there, python cannot explain to the underlying OS that it wants to load an library which doesn't have a physical name. And the only workaround distutils authors have thought of is unzipping it into a temp dir. Naturally it is not /site-packages/ since /site-packages/ is not writable for ordinary users.  So you can either    set PYTHON_EGG_DIR to /tmp, or  give user www write permission to /var/www/.python-eggs (so that the files don't get unzipped every time /tmp is cleaned up) or better then unzip the egg as suggested by @shalley303 (and avoid unzipping of the egg in the run-time altogether).      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-egg-cache"
        ],
        "URL": "https://stackoverflow.com/questions/2192323/what-is-the-python-egg-cache-python-egg-cache",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've just upgraded from Python 2.6.1 to 2.6.4 on my development machine and upon starting a python script was presented with the following message:     Can't extract file(s) to egg cache      The following error occurred while   trying to extract file(s) to  the   Python egg cache:      [Errno 13] Permission denied:   '/var/www/.python-eggs'      The Python egg cache directory is   currently set to:      /var/www/.python-eggs      Perhaps your account does not have   write access to this directory?   You   can change the cache directory by   setting the PYTHON_EGG_CACHE    environment variable to point to an   accessible directory.   There isn't anything in the python docs so I'm at a bit of a loss regarding best-practices on where to put this directory and what it's used for.  Can someone explain what the Python egg cache is?  Also, can you explain why/how it is different to the site-packages directory Python uses to store eggs (as I understand it)?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the Python egg cache (PYTHON_EGG_CACHE)?",
        "A_Content": "  Python eggs are zip-compressed packages containing both Python modules and metadata. The egg cache is where the the extracted contents of the egg are stored so that the Python modules contained within are usable.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-egg-cache"
        ],
        "URL": "https://stackoverflow.com/questions/2192323/what-is-the-python-egg-cache-python-egg-cache",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've just upgraded from Python 2.6.1 to 2.6.4 on my development machine and upon starting a python script was presented with the following message:     Can't extract file(s) to egg cache      The following error occurred while   trying to extract file(s) to  the   Python egg cache:      [Errno 13] Permission denied:   '/var/www/.python-eggs'      The Python egg cache directory is   currently set to:      /var/www/.python-eggs      Perhaps your account does not have   write access to this directory?   You   can change the cache directory by   setting the PYTHON_EGG_CACHE    environment variable to point to an   accessible directory.   There isn't anything in the python docs so I'm at a bit of a loss regarding best-practices on where to put this directory and what it's used for.  Can someone explain what the Python egg cache is?  Also, can you explain why/how it is different to the site-packages directory Python uses to store eggs (as I understand it)?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the Python egg cache (PYTHON_EGG_CACHE)?",
        "A_Content": "  You can also disable the use of the .egg after it has been installed. You need to go into the site-packages directory, extract the .egg, and then move it to a hidden file (or delete it, or whatever).   Here is an example of what I did to disable the MySQLdb module .egg file which was causing this error when the python script was being run from Zabbix.   cd /usr/local/lib/python2.7/site-packages unzip MySQL_python-1.2.3-py2.7-linux-x86_64.egg mv MySQL_python-1.2.3-py2.7-linux-x86_64.egg .MySQL_python-1.2.3-py2.7-linux-x86_64.egg      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-egg-cache"
        ],
        "URL": "https://stackoverflow.com/questions/2192323/what-is-the-python-egg-cache-python-egg-cache",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've just upgraded from Python 2.6.1 to 2.6.4 on my development machine and upon starting a python script was presented with the following message:     Can't extract file(s) to egg cache      The following error occurred while   trying to extract file(s) to  the   Python egg cache:      [Errno 13] Permission denied:   '/var/www/.python-eggs'      The Python egg cache directory is   currently set to:      /var/www/.python-eggs      Perhaps your account does not have   write access to this directory?   You   can change the cache directory by   setting the PYTHON_EGG_CACHE    environment variable to point to an   accessible directory.   There isn't anything in the python docs so I'm at a bit of a loss regarding best-practices on where to put this directory and what it's used for.  Can someone explain what the Python egg cache is?  Also, can you explain why/how it is different to the site-packages directory Python uses to store eggs (as I understand it)?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the Python egg cache (PYTHON_EGG_CACHE)?",
        "A_Content": "  Phillip B Oldham's right. You can add these lines in your code:  import os   os.environ['PYTHON_EGG_CACHE'] = '/tmp' # a writable directory       ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-egg-cache"
        ],
        "URL": "https://stackoverflow.com/questions/2192323/what-is-the-python-egg-cache-python-egg-cache",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've just upgraded from Python 2.6.1 to 2.6.4 on my development machine and upon starting a python script was presented with the following message:     Can't extract file(s) to egg cache      The following error occurred while   trying to extract file(s) to  the   Python egg cache:      [Errno 13] Permission denied:   '/var/www/.python-eggs'      The Python egg cache directory is   currently set to:      /var/www/.python-eggs      Perhaps your account does not have   write access to this directory?   You   can change the cache directory by   setting the PYTHON_EGG_CACHE    environment variable to point to an   accessible directory.   There isn't anything in the python docs so I'm at a bit of a loss regarding best-practices on where to put this directory and what it's used for.  Can someone explain what the Python egg cache is?  Also, can you explain why/how it is different to the site-packages directory Python uses to store eggs (as I understand it)?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the Python egg cache (PYTHON_EGG_CACHE)?",
        "A_Content": "  A simple fix would be to create the directory and provide www-data access to it.   $ mkdir /var/www/.python-eggs $ chown www-data:www-data /var/www/.python-eggs      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-egg-cache"
        ],
        "URL": "https://stackoverflow.com/questions/2192323/what-is-the-python-egg-cache-python-egg-cache",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've just upgraded from Python 2.6.1 to 2.6.4 on my development machine and upon starting a python script was presented with the following message:     Can't extract file(s) to egg cache      The following error occurred while   trying to extract file(s) to  the   Python egg cache:      [Errno 13] Permission denied:   '/var/www/.python-eggs'      The Python egg cache directory is   currently set to:      /var/www/.python-eggs      Perhaps your account does not have   write access to this directory?   You   can change the cache directory by   setting the PYTHON_EGG_CACHE    environment variable to point to an   accessible directory.   There isn't anything in the python docs so I'm at a bit of a loss regarding best-practices on where to put this directory and what it's used for.  Can someone explain what the Python egg cache is?  Also, can you explain why/how it is different to the site-packages directory Python uses to store eggs (as I understand it)?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Calling a class function inside of __init__",
        "A_Content": "  Call the function in this way:  self.parse_file()   You also need to define your parse_file() function like this:  def parse_file(self):   The parse_file method has to be bound to an object upon calling it (because it's not a static method). This is done by calling the function on an instance of the object, in your case the instance is self.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/12646326/calling-a-class-function-inside-of-init",
        "A_Votes": "100",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm writing some code that takes a filename, opens the file, and parses out some data. I'd like to do this in a class. The following code works:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None          def parse_file():             #do some parsing             self.stat1 = result_from_parse1             self.stat2 = result_from_parse2             self.stat3 = result_from_parse3             self.stat4 = result_from_parse4             self.stat5 = result_from_parse5          parse_file()   But it involves me putting all of the parsing machinery in the scope of the __init__ function for my class. That looks fine now for this simplified code, but the function parse_file has quite a few levels of indention as well. I'd prefer to define the function parse_file() as a class function like below:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None         parse_file()      def parse_file():         #do some parsing         self.stat1 = result_from_parse1         self.stat2 = result_from_parse2         self.stat3 = result_from_parse3         self.stat4 = result_from_parse4         self.stat5 = result_from_parse5   Of course this code doesn't work because the function parse_file() is not within the scope of the __init__ function. Is there a way to call a class function from within __init__ of that class? Or am I thinking about this the wrong way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Calling a class function inside of __init__",
        "A_Content": "  If I'm not wrong, both functions are part of your class, you should use it like this:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None         self.parse_file()      def parse_file(self):         #do some parsing         self.stat1 = result_from_parse1         self.stat2 = result_from_parse2         self.stat3 = result_from_parse3         self.stat4 = result_from_parse4         self.stat5 = result_from_parse5   replace your line:  parse_file()    with:  self.parse_file()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/12646326/calling-a-class-function-inside-of-init",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm writing some code that takes a filename, opens the file, and parses out some data. I'd like to do this in a class. The following code works:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None          def parse_file():             #do some parsing             self.stat1 = result_from_parse1             self.stat2 = result_from_parse2             self.stat3 = result_from_parse3             self.stat4 = result_from_parse4             self.stat5 = result_from_parse5          parse_file()   But it involves me putting all of the parsing machinery in the scope of the __init__ function for my class. That looks fine now for this simplified code, but the function parse_file has quite a few levels of indention as well. I'd prefer to define the function parse_file() as a class function like below:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None         parse_file()      def parse_file():         #do some parsing         self.stat1 = result_from_parse1         self.stat2 = result_from_parse2         self.stat3 = result_from_parse3         self.stat4 = result_from_parse4         self.stat5 = result_from_parse5   Of course this code doesn't work because the function parse_file() is not within the scope of the __init__ function. Is there a way to call a class function from within __init__ of that class? Or am I thinking about this the wrong way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Calling a class function inside of __init__",
        "A_Content": "  How about:  class MyClass(object):     def __init__(self, filename):         self.filename = filename          self.stats = parse_file(filename)  def parse_file(filename):     #do some parsing     return results_from_parse   By the way, if you have variables named stat1, stat2, etc., the situation is begging for a tuple: stats = (...).  So let parse_file return a tuple, and store the tuple in self.stats.  Then, for example, you can access what used to be called stat3 with self.stats[2].     ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/12646326/calling-a-class-function-inside-of-init",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm writing some code that takes a filename, opens the file, and parses out some data. I'd like to do this in a class. The following code works:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None          def parse_file():             #do some parsing             self.stat1 = result_from_parse1             self.stat2 = result_from_parse2             self.stat3 = result_from_parse3             self.stat4 = result_from_parse4             self.stat5 = result_from_parse5          parse_file()   But it involves me putting all of the parsing machinery in the scope of the __init__ function for my class. That looks fine now for this simplified code, but the function parse_file has quite a few levels of indention as well. I'd prefer to define the function parse_file() as a class function like below:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None         parse_file()      def parse_file():         #do some parsing         self.stat1 = result_from_parse1         self.stat2 = result_from_parse2         self.stat3 = result_from_parse3         self.stat4 = result_from_parse4         self.stat5 = result_from_parse5   Of course this code doesn't work because the function parse_file() is not within the scope of the __init__ function. Is there a way to call a class function from within __init__ of that class? Or am I thinking about this the wrong way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Calling a class function inside of __init__",
        "A_Content": "  In parse_file, take the self argument (just like in __init__). If there's any other context you need then just pass it as additional arguments as usual.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/12646326/calling-a-class-function-inside-of-init",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm writing some code that takes a filename, opens the file, and parses out some data. I'd like to do this in a class. The following code works:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None          def parse_file():             #do some parsing             self.stat1 = result_from_parse1             self.stat2 = result_from_parse2             self.stat3 = result_from_parse3             self.stat4 = result_from_parse4             self.stat5 = result_from_parse5          parse_file()   But it involves me putting all of the parsing machinery in the scope of the __init__ function for my class. That looks fine now for this simplified code, but the function parse_file has quite a few levels of indention as well. I'd prefer to define the function parse_file() as a class function like below:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None         parse_file()      def parse_file():         #do some parsing         self.stat1 = result_from_parse1         self.stat2 = result_from_parse2         self.stat3 = result_from_parse3         self.stat4 = result_from_parse4         self.stat5 = result_from_parse5   Of course this code doesn't work because the function parse_file() is not within the scope of the __init__ function. Is there a way to call a class function from within __init__ of that class? Or am I thinking about this the wrong way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Calling a class function inside of __init__",
        "A_Content": "  You must declare parse_file like this; def parse_file(self). The \"self\" parameter is a hidden parameter in most languages, but not in python. You must add it to the definition of all that methods that belong to a class. Then you can call the function from any method inside the class using self.parse_file  your final program is going to look like this:  class MyClass():   def __init__(self, filename):       self.filename = filename         self.stat1 = None       self.stat2 = None       self.stat3 = None       self.stat4 = None       self.stat5 = None       self.parse_file()    def parse_file(self):       #do some parsing       self.stat1 = result_from_parse1       self.stat2 = result_from_parse2       self.stat3 = result_from_parse3       self.stat4 = result_from_parse4       self.stat5 = result_from_parse5      ",
        "Language": "Python",
        "Tags": [
            "python",
            "class"
        ],
        "URL": "https://stackoverflow.com/questions/12646326/calling-a-class-function-inside-of-init",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm writing some code that takes a filename, opens the file, and parses out some data. I'd like to do this in a class. The following code works:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None          def parse_file():             #do some parsing             self.stat1 = result_from_parse1             self.stat2 = result_from_parse2             self.stat3 = result_from_parse3             self.stat4 = result_from_parse4             self.stat5 = result_from_parse5          parse_file()   But it involves me putting all of the parsing machinery in the scope of the __init__ function for my class. That looks fine now for this simplified code, but the function parse_file has quite a few levels of indention as well. I'd prefer to define the function parse_file() as a class function like below:  class MyClass():     def __init__(self, filename):         self.filename = filename           self.stat1 = None         self.stat2 = None         self.stat3 = None         self.stat4 = None         self.stat5 = None         parse_file()      def parse_file():         #do some parsing         self.stat1 = result_from_parse1         self.stat2 = result_from_parse2         self.stat3 = result_from_parse3         self.stat4 = result_from_parse4         self.stat5 = result_from_parse5   Of course this code doesn't work because the function parse_file() is not within the scope of the __init__ function. Is there a way to call a class function from within __init__ of that class? Or am I thinking about this the wrong way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "surface plots in matplotlib",
        "A_Content": "  For surfaces it's a bit different than a list of 3-tuples, you should pass in a grid for the domain in 2d arrays.   If all you have is a list of 3d points, rather than some function f(x, y) -> z, then you will have a problem because there are multiple ways to triangulate that 3d point cloud into a surface.    Here's a smooth surface example:  import numpy as np from mpl_toolkits.mplot3d import Axes3D # This import has side effects required for the kwarg projection='3d' in the call to fig.add_subplot import matplotlib.pyplot as plt import random  def fun(x, y):   return x**2 + y  fig = plt.figure() ax = fig.add_subplot(111, projection='3d') x = y = np.arange(-3.0, 3.0, 0.05) X, Y = np.meshgrid(x, y) zs = np.array([fun(x,y) for x,y in zip(np.ravel(X), np.ravel(Y))]) Z = zs.reshape(X.shape)  ax.plot_surface(X, Y, Z)  ax.set_xlabel('X Label') ax.set_ylabel('Y Label') ax.set_zlabel('Z Label')  plt.show()        ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib",
        "A_Votes": "80",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points. The plot_surface function in the mplot3d package requires as arguments X,Y and Z which are 2d arrays. Is plot_surface the right function to plot surface and how do I transform my data in to the required format ?  data = [(x1,y1,z1),(x2,y2,z2),.....,(xn,yn,zn)]     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "surface plots in matplotlib",
        "A_Content": "  I just came across this same problem.  I have evenly spaced data that is in 3 1-D arrays instead of the 2-D arrays that matplotlib's plot_surface wants.  My data happened to be in a pandas.DataFrame so here is the matplotlib.plot_surface example with the modifications to plot 3 1-D arrays.    from mpl_toolkits.mplot3d import Axes3D from matplotlib import cm from matplotlib.ticker import LinearLocator, FormatStrFormatter import matplotlib.pyplot as plt import numpy as np  X = np.arange(-5, 5, 0.25) Y = np.arange(-5, 5, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R)  fig = plt.figure() ax = fig.gca(projection='3d') surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,     linewidth=0, antialiased=False) ax.set_zlim(-1.01, 1.01)  ax.zaxis.set_major_locator(LinearLocator(10)) ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))  fig.colorbar(surf, shrink=0.5, aspect=5) plt.title('Original Code')   That is the original example.  Adding this next bit on creates the same plot from 3 1-D arrays.  # ~~~~ MODIFICATION TO EXAMPLE BEGINS HERE ~~~~ # import pandas as pd from scipy.interpolate import griddata # create 1D-arrays from the 2D-arrays x = X.reshape(1600) y = Y.reshape(1600) z = Z.reshape(1600) xyz = {'x': x, 'y': y, 'z': z}  # put the data into a pandas DataFrame (this is what my data looks like) df = pd.DataFrame(xyz, index=range(len(xyz['x'])))   # re-create the 2D-arrays x1 = np.linspace(df['x'].min(), df['x'].max(), len(df['x'].unique())) y1 = np.linspace(df['y'].min(), df['y'].max(), len(df['y'].unique())) x2, y2 = np.meshgrid(x1, y1) z2 = griddata((df['x'], df['y']), df['z'], (x2, y2), method='cubic')  fig = plt.figure() ax = fig.gca(projection='3d') surf = ax.plot_surface(x2, y2, z2, rstride=1, cstride=1, cmap=cm.coolwarm,     linewidth=0, antialiased=False) ax.set_zlim(-1.01, 1.01)  ax.zaxis.set_major_locator(LinearLocator(10)) ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))  fig.colorbar(surf, shrink=0.5, aspect=5) plt.title('Meshgrid Created from 3 1D Arrays') # ~~~~ MODIFICATION TO EXAMPLE ENDS HERE ~~~~ #  plt.show()   Here are the resulting figures:        ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points. The plot_surface function in the mplot3d package requires as arguments X,Y and Z which are 2d arrays. Is plot_surface the right function to plot surface and how do I transform my data in to the required format ?  data = [(x1,y1,z1),(x2,y2,z2),.....,(xn,yn,zn)]     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "surface plots in matplotlib",
        "A_Content": "  I do this with some lines in python using PANDAS, the plot is beatiful!  from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from matplotlib import cm import numpy as np import pandas as pd from sys import argv  file = argv[1]  x,y,z = np.loadtxt(file, unpack=True) df = pd.DataFrame({'x': x, 'y': y, 'z': z})  fig = plt.figure() ax = Axes3D(fig) surf = ax.plot_trisurf(df.x, df.y, df.z, cmap=cm.jet, linewidth=0.1) fig.colorbar(surf, shrink=0.5, aspect=5) plt.savefig('teste.pdf') plt.show()   If necessary you can pass vmin and vmax to define the colorbar range, e.g.  surf = ax.plot_trisurf(df.x, df.y, df.z, cmap=cm.jet, linewidth=0.1, vmin=0, vmax=2000)        ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points. The plot_surface function in the mplot3d package requires as arguments X,Y and Z which are 2d arrays. Is plot_surface the right function to plot surface and how do I transform my data in to the required format ?  data = [(x1,y1,z1),(x2,y2,z2),.....,(xn,yn,zn)]     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "surface plots in matplotlib",
        "A_Content": "  check the official example. X,Y and Z are indeed 2d arrays, numpy.meshgrid() is a simple way to get 2d x,y mesh out of 1d x and y values.  http://matplotlib.sourceforge.net/mpl_examples/mplot3d/surface3d_demo.py  here's pythonic way to convert your 3-tuples to 3 1d arrays.  data = [(1,2,3), (10,20,30), (11, 22, 33), (110, 220, 330)] X,Y,Z = zip(*data) In [7]: X Out[7]: (1, 10, 11, 110) In [8]: Y Out[8]: (2, 20, 22, 220) In [9]: Z Out[9]: (3, 30, 33, 330)   Here's mtaplotlib delaunay triangulation (interpolation), it converts 1d x,y,z into something compliant (?):  http://matplotlib.sourceforge.net/api/mlab_api.html#matplotlib.mlab.griddata     ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points. The plot_surface function in the mplot3d package requires as arguments X,Y and Z which are 2d arrays. Is plot_surface the right function to plot surface and how do I transform my data in to the required format ?  data = [(x1,y1,z1),(x2,y2,z2),.....,(xn,yn,zn)]     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "surface plots in matplotlib",
        "A_Content": "  In Matlab I did something similar using the delaunay function on the x, y coords only (not the z), then plotting with trimesh or trisurf, using z as the height.  SciPy has the Delaunay class, which is based on the same underlying QHull library that the Matlab's delaunay function is, so you should get identical results.  From there, it should be a few lines of code to convert this Plotting 3D Polygons in python-matplotlib example into what you wish to achieve, as Delaunay gives you the specification of each triangular polygon.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points. The plot_surface function in the mplot3d package requires as arguments X,Y and Z which are 2d arrays. Is plot_surface the right function to plot surface and how do I transform my data in to the required format ?  data = [(x1,y1,z1),(x2,y2,z2),.....,(xn,yn,zn)]     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "surface plots in matplotlib",
        "A_Content": "  Just to chime in, Emanuel had the answer that I (and probably many others) are looking for. If you have 3d scattered data in 3 separate arrays, pandas is an incredible help and works much better than the other options. To elaborate, suppose your x,y,z are some arbitrary variables. In my case these were c,gamma, and errors because I was testing a support vector machine. There are many potential choices to plot the data:   scatter3D(cParams, gammas, avg_errors_array) - this works but is overly simplistic plot_wireframe(cParams, gammas, avg_errors_array) - this works, but will look ugly if your data isn't sorted nicely, as is potentially the case with massive chunks of real scientific data ax.plot3D(cParams, gammas, avg_errors_array) - similar to wireframe   Wireframe plot of the data    3d scatter of the data    The code looks like this:       fig = plt.figure()     ax = fig.gca(projection='3d')     ax.set_xlabel('c parameter')     ax.set_ylabel('gamma parameter')     ax.set_zlabel('Error rate')     #ax.plot_wireframe(cParams, gammas, avg_errors_array)     #ax.plot3D(cParams, gammas, avg_errors_array)     #ax.scatter3D(cParams, gammas, avg_errors_array, zdir='z',cmap='viridis')      df = pd.DataFrame({'x': cParams, 'y': gammas, 'z': avg_errors_array})     surf = ax.plot_trisurf(df.x, df.y, df.z, cmap=cm.jet, linewidth=0.1)     fig.colorbar(surf, shrink=0.5, aspect=5)         plt.savefig('./plots/avgErrs_vs_C_andgamma_type_%s.png'%(k))     plt.show()   Here is the final output:       ",
        "Language": "Python",
        "Tags": [
            "python",
            "matplotlib"
        ],
        "URL": "https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points. The plot_surface function in the mplot3d package requires as arguments X,Y and Z which are 2d arrays. Is plot_surface the right function to plot surface and how do I transform my data in to the required format ?  data = [(x1,y1,z1),(x2,y2,z2),.....,(xn,yn,zn)]     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get pixel's RGB using PIL",
        "A_Content": "  Yes, this way:  im = Image.open('image.gif') rgb_im = im.convert('RGB') r, g, b = rgb_im.getpixel((1, 1))  print(r, g, b) (65, 100, 137)   The reason you were getting a single value before with pix[1, 1] is because GIF pixels refer to one of the 256 values in the GIF color palette.  See also this SO post: Python and PIL pixel values different for GIF and JPEG and this PIL Reference page  contains more information on the convert() function.  By the way, your code would work just fine for .jpg images.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library",
            "rgb",
            "pixel"
        ],
        "URL": "https://stackoverflow.com/questions/11064786/get-pixels-rgb-using-pil",
        "A_Votes": "107",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is it possible to get the RGB color of a pixel using PIL? I'm using this code:  im = Image.open(\"image.gif\") pix = im.load() print(pix[1,1])   However, it only outputs a number (e.g. 0 or 1) and not three numbers (e.g. 60,60,60 for R,G,B). I guess I'm not understanding something about the function. I'd love some explanation.  Thanks a lot.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get pixel's RGB using PIL",
        "A_Content": "  Not PIL, but scipy.misc.imread might still be interesting:  import scipy.misc im = scipy.misc.imread('um_000000.png', flatten=False, mode='RGB') print(im.shape)   gives  (480, 640, 3)   so it is (height, width, channels). So the pixel at position (x, y) is  color = tuple(im[y][x]) r, g, b = color      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library",
            "rgb",
            "pixel"
        ],
        "URL": "https://stackoverflow.com/questions/11064786/get-pixels-rgb-using-pil",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to get the RGB color of a pixel using PIL? I'm using this code:  im = Image.open(\"image.gif\") pix = im.load() print(pix[1,1])   However, it only outputs a number (e.g. 0 or 1) and not three numbers (e.g. 60,60,60 for R,G,B). I guess I'm not understanding something about the function. I'd love some explanation.  Thanks a lot.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get pixel's RGB using PIL",
        "A_Content": "  GIFs store colors as one of x number of possible colors in a palette. Read about the gif limited color palette. So PIL is giving you the palette index, rather than the color information of that palette color.  Edit: Removed link to a blog post solution that had a typo. Other answers do the same thing without the typo.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library",
            "rgb",
            "pixel"
        ],
        "URL": "https://stackoverflow.com/questions/11064786/get-pixels-rgb-using-pil",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to get the RGB color of a pixel using PIL? I'm using this code:  im = Image.open(\"image.gif\") pix = im.load() print(pix[1,1])   However, it only outputs a number (e.g. 0 or 1) and not three numbers (e.g. 60,60,60 for R,G,B). I guess I'm not understanding something about the function. I'd love some explanation.  Thanks a lot.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get pixel's RGB using PIL",
        "A_Content": "  An alternative to converting the image is to create an RGB index from the palette.  from PIL import Image  def chunk(seq, size, groupByList=True):     \"\"\"Returns list of lists/tuples broken up by size input\"\"\"     func = tuple     if groupByList:         func = list     return [func(seq[i:i + size]) for i in range(0, len(seq), size)]   def getPaletteInRgb(img):     \"\"\"     Returns list of RGB tuples found in the image palette     :type img: Image.Image     :rtype: list[tuple]     \"\"\"     assert img.mode == 'P', \"image should be palette mode\"     pal = img.getpalette()     colors = chunk(pal, 3, False)     return colors  # Usage im = Image.open(\"image.gif\") pal = getPalletteInRgb(im)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library",
            "rgb",
            "pixel"
        ],
        "URL": "https://stackoverflow.com/questions/11064786/get-pixels-rgb-using-pil",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is it possible to get the RGB color of a pixel using PIL? I'm using this code:  im = Image.open(\"image.gif\") pix = im.load() print(pix[1,1])   However, it only outputs a number (e.g. 0 or 1) and not three numbers (e.g. 60,60,60 for R,G,B). I guess I'm not understanding something about the function. I'd love some explanation.  Thanks a lot.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the correct way to set Python's locale on Windows?",
        "A_Content": "  It seems you're using Windows. The locale strings are different there. Take a more precise look at the doc:  locale.setlocale(locale.LC_ALL, 'de_DE') # use German locale; name might vary with platform   On Windows, I think it would be something like:  locale.setlocale(locale.LC_ALL, 'deu_deu')   MSDN has a list of language strings and of country/region strings     ",
        "Language": "Python",
        "Tags": [
            "python",
            "windows",
            "localization",
            "internationalization"
        ],
        "URL": "https://stackoverflow.com/questions/955986/what-is-the-correct-way-to-set-pythons-locale-on-windows",
        "A_Votes": "91",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm attempting to sort a list of strings in a locale-aware manner.  I've used the Babel library for other i18n-related tasks, but it doesn't support sorting.  Python's locale module provides a strcoll function, but requires the locale of the process to be set to the one I want to work with.  Kind of a pain, but I can live with it.  The problem is that I can't seem to actually set the locale.  The documentation for the locale module gives this example:  import locale locale.setlocale(locale.LC_ALL, 'de_DE')   When I run that, I get this:  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Python26\\Lib\\locale.py\", line 494, in setlocale locale.Error: unsupported locale setting   What am I doing wrong?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the correct way to set Python's locale on Windows?",
        "A_Content": "  You should not pass an explicit locale to setlocale, it is wrong. Let it find out from the environment. You have to pass it an empty string  import locale locale.setlocale(locale.LC_ALL, '')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "windows",
            "localization",
            "internationalization"
        ],
        "URL": "https://stackoverflow.com/questions/955986/what-is-the-correct-way-to-set-pythons-locale-on-windows",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm attempting to sort a list of strings in a locale-aware manner.  I've used the Babel library for other i18n-related tasks, but it doesn't support sorting.  Python's locale module provides a strcoll function, but requires the locale of the process to be set to the one I want to work with.  Kind of a pain, but I can live with it.  The problem is that I can't seem to actually set the locale.  The documentation for the locale module gives this example:  import locale locale.setlocale(locale.LC_ALL, 'de_DE')   When I run that, I get this:  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Python26\\Lib\\locale.py\", line 494, in setlocale locale.Error: unsupported locale setting   What am I doing wrong?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the correct way to set Python's locale on Windows?",
        "A_Content": "  This is the only way to do it on Windows (example for the German locale):  import locale  locale.setlocale(category=locale.LC_ALL,                  locale=\"German\")  # Not locale=\"de_DE\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "windows",
            "localization",
            "internationalization"
        ],
        "URL": "https://stackoverflow.com/questions/955986/what-is-the-correct-way-to-set-pythons-locale-on-windows",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm attempting to sort a list of strings in a locale-aware manner.  I've used the Babel library for other i18n-related tasks, but it doesn't support sorting.  Python's locale module provides a strcoll function, but requires the locale of the process to be set to the one I want to work with.  Kind of a pain, but I can live with it.  The problem is that I can't seem to actually set the locale.  The documentation for the locale module gives this example:  import locale locale.setlocale(locale.LC_ALL, 'de_DE')   When I run that, I get this:  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Python26\\Lib\\locale.py\", line 494, in setlocale locale.Error: unsupported locale setting   What am I doing wrong?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the correct way to set Python's locale on Windows?",
        "A_Content": "  Ubuntu  On Ubuntu you may have this problem because you don't have that local installed on your system.  From shell try a:  $> locale -a   and check if you find the locale you are interested in. Otherwise you have to install it:  $> sudo apt-get install language-pack-XXX   where XXX is your language (in my case \"xxx = it\" , italian locale) Then run a dpkg-reconfigure:  $> sudo dpkg-reconfigure locales   After that try again in your python shell:  >>> import locale >>> locale.setlocale(locale.LC_ALL,'it_IT.UTF-8')   (this is for italian locale, which was what I needed)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "windows",
            "localization",
            "internationalization"
        ],
        "URL": "https://stackoverflow.com/questions/955986/what-is-the-correct-way-to-set-pythons-locale-on-windows",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm attempting to sort a list of strings in a locale-aware manner.  I've used the Babel library for other i18n-related tasks, but it doesn't support sorting.  Python's locale module provides a strcoll function, but requires the locale of the process to be set to the one I want to work with.  Kind of a pain, but I can live with it.  The problem is that I can't seem to actually set the locale.  The documentation for the locale module gives this example:  import locale locale.setlocale(locale.LC_ALL, 'de_DE')   When I run that, I get this:  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Python26\\Lib\\locale.py\", line 494, in setlocale locale.Error: unsupported locale setting   What am I doing wrong?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the correct way to set Python's locale on Windows?",
        "A_Content": "  From locale.setlocale docs:  locale.setlocale(category, locale=None):     \"\"\"     Set the locale for the given category.  The locale can be     a string, an iterable of two strings (language code and encoding),     or None.     \"\"\"\"   Under Linux (especially Ubuntu) you can either use   locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')   or  locale.setlocale(locale.LC_ALL, ('de', 'utf-8'))   You will get the same error if the locale is not installed on the system. So, make sure you have the locale installed on your system:  $ locale -a # to list the currently installed locales $ (sudo) locale-gen de_DE.UTF-8 # to install new locale      ",
        "Language": "Python",
        "Tags": [
            "python",
            "windows",
            "localization",
            "internationalization"
        ],
        "URL": "https://stackoverflow.com/questions/955986/what-is-the-correct-way-to-set-pythons-locale-on-windows",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm attempting to sort a list of strings in a locale-aware manner.  I've used the Babel library for other i18n-related tasks, but it doesn't support sorting.  Python's locale module provides a strcoll function, but requires the locale of the process to be set to the one I want to work with.  Kind of a pain, but I can live with it.  The problem is that I can't seem to actually set the locale.  The documentation for the locale module gives this example:  import locale locale.setlocale(locale.LC_ALL, 'de_DE')   When I run that, I get this:  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Python26\\Lib\\locale.py\", line 494, in setlocale locale.Error: unsupported locale setting   What am I doing wrong?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What is the correct way to set Python's locale on Windows?",
        "A_Content": "  I know this has been asked years ago, but I thought I'd try adding what I found out, using Python 3.6 on Windows:  import locale for x in locale.windows_locale.values():     print(x.replace('_','-'))   I tried some and that also seems to be a way of finding out, what's available on Windows.  And then you simply set the locale:  locale.setlocale(locale.LC_ALL, any_item_of_the_printed_strings)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "windows",
            "localization",
            "internationalization"
        ],
        "URL": "https://stackoverflow.com/questions/955986/what-is-the-correct-way-to-set-pythons-locale-on-windows",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm attempting to sort a list of strings in a locale-aware manner.  I've used the Babel library for other i18n-related tasks, but it doesn't support sorting.  Python's locale module provides a strcoll function, but requires the locale of the process to be set to the one I want to work with.  Kind of a pain, but I can live with it.  The problem is that I can't seem to actually set the locale.  The documentation for the locale module gives this example:  import locale locale.setlocale(locale.LC_ALL, 'de_DE')   When I run that, I get this:  Traceback (most recent call last):   File \"<stdin>\", line 1, in <module>   File \"C:\\Python26\\Lib\\locale.py\", line 494, in setlocale locale.Error: unsupported locale setting   What am I doing wrong?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  Similar question here. You can't mix iteration and readline so you need to use one or the other.  while True:     line1 = f.readline()     line2 = f.readline()     if not line2: break  # EOF     ...      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "41",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  import itertools with open('a') as f:     for line1,line2 in itertools.izip_longest(*[f]*2):         print(line1,line2)   izip_longest returns an iterator, so it should work well even if the file is very large.  If there are an odd number of lines, then line2 gets the value None on the last iteration.  izip_longest is in itertools if you have python 2.6 or later. If you use a prior version, you can pick up a python implementation of izip_longest here.  In Python3, itertools.izip_longest is renamed itertools.zip_longest.    In the comments, it has been asked if this solution reads the whole file first, and then iterates over the file a second time. I believe that it does not. The with open('a') as f line opens a file handle, but does not read the file. f is an iterator, so its contents are not read until requested. izip_longest takes iterators as arguments, and returns an iterator.   izip_longest is indeed fed the same iterator, f, twice. But what ends up happening is that f.next() (or next(f) in Python3) is called on the first argument and then on the second argument. Since next() is being called on the same underlying iterator, successive lines are yielded. This is very different than reading in the whole file. Indeed the purpose of using iterators is precisely to avoid reading in the whole file.  I therefore believe the solution works as desired -- the file is only read once by the for-loop.  To corroborate this, I ran the izip_longest solution versus a solution using f.readlines(). I put a raw_input() at the end to pause the scripts, and ran ps axuw on each:  % ps axuw | grep izip_longest_method.py   unutbu   11119  2.2  0.2   4520  2712 pts/0    S+   21:14   0:00 python /home/unutbu/pybin/izip_longest_method.py bigfile  % ps axuw | grep readlines_method.py   unutbu   11317  6.5  8.8  93908 91680 pts/0    S+   21:16   0:00 python /home/unutbu/pybin/readlines_method.py bigfile  The readlines clearly reads in the whole file at once. Since the izip_longest_method uses much less memory, I think it is safe to conclude it is not reading in the whole file at once.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  use line.next(), eg  f=open(\"file\") for line in f:     print line     nextline=f.next()     print \"next line\", nextline     .... f.close()      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  I would proceed in a similar way as ghostdog74, only with the try outside and a few modifications:  try:     with open(filename) as f:         for line1 in f:             line2 = f.next()             # process line1 and line2 here except StopIteration:     print \"(End)\" # do whatever you need to do with line1 alone   This keeps the code simple and yet robust. Using the with closes the file if something else happens, or just closes the resources once you have exhausted it and exit the loop.  Note that with needs 2.6, or 2.5 with the with_statement feature enabled.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  Works for even and odd-length files. It just ignores the unmatched last line.  f=file(\"file\")  lines = f.readlines() for even, odd in zip(lines[0::2], lines[1::2]):     print \"even : \", even     print \"odd : \", odd     print \"end cycle\" f.close()   If you have large files, this is not the correct approach. You are loading all the file in memory with readlines(). I once wrote a class that read the file saving the fseek position of each start of line. This allows you to get specific lines without having all the file in memory, and you can also go forward and backward.  I paste it here. License is Public domain, meaning, do what you want with it. Please note that this class has been written 6 years ago and I haven't touched or checked it since. I think it's not even file compliant. Caveat emptor. Also, note that this is overkill for your problem. I'm not claiming you should definitely go this way, but I had this code and I enjoy sharing it if you need more complex access.  import string import re  class FileReader:     \"\"\"      Similar to file class, but allows to access smoothly the lines      as when using readlines(), with no memory payload, going back and forth,     finding regexps and so on.     \"\"\"     def __init__(self,filename): # fold>>         self.__file=file(filename,\"r\")         self.__currentPos=-1         # get file length         self.__file.seek(0,0)         counter=0         line=self.__file.readline()         while line != '':             counter = counter + 1             line=self.__file.readline()         self.__length = counter         # collect an index of filedescriptor positions against         # the line number, to enhance search         self.__file.seek(0,0)         self.__lineToFseek = []          while True:             cur=self.__file.tell()             line=self.__file.readline()             # if it's not null the cur is valid for             # identifying a line, so store             self.__lineToFseek.append(cur)             if line == '':                 break     # <<fold     def __len__(self): # fold>>         \"\"\"         member function for the operator len()         returns the file length         FIXME: better get it once when opening file         \"\"\"         return self.__length         # <<fold     def __getitem__(self,key): # fold>>         \"\"\"          gives the \"key\" line. The syntax is          import FileReader         f=FileReader.FileReader(\"a_file\")         line=f[2]          to get the second line from the file. The internal         pointer is set to the key line         \"\"\"          mylen = self.__len__()         if key < 0:             self.__currentPos = -1             return ''         elif key > mylen:             self.__currentPos = mylen             return ''          self.__file.seek(self.__lineToFseek[key],0)         counter=0         line = self.__file.readline()         self.__currentPos = key         return line         # <<fold     def next(self): # fold>>         if self.isAtEOF():             raise StopIteration         return self.readline()     # <<fold     def __iter__(self): # fold>>         return self     # <<fold     def readline(self): # fold>>         \"\"\"         read a line forward from the current cursor position.         returns the line or an empty string when at EOF         \"\"\"         return self.__getitem__(self.__currentPos+1)         # <<fold     def readbackline(self): # fold>>         \"\"\"         read a line backward from the current cursor position.         returns the line or an empty string when at Beginning of         file.         \"\"\"         return self.__getitem__(self.__currentPos-1)         # <<fold     def currentLine(self): # fold>>         \"\"\"         gives the line at the current cursor position         \"\"\"         return self.__getitem__(self.__currentPos)         # <<fold     def currentPos(self): # fold>>         \"\"\"          return the current position (line) in the file         or -1 if the cursor is at the beginning of the file         or len(self) if it's at the end of file         \"\"\"         return self.__currentPos         # <<fold     def toBOF(self): # fold>>         \"\"\"         go to beginning of file         \"\"\"         self.__getitem__(-1)         # <<fold     def toEOF(self): # fold>>         \"\"\"         go to end of file         \"\"\"         self.__getitem__(self.__len__())         # <<fold     def toPos(self,key): # fold>>         \"\"\"         go to the specified line         \"\"\"         self.__getitem__(key)         # <<fold     def isAtEOF(self): # fold>>         return self.__currentPos == self.__len__()         # <<fold     def isAtBOF(self): # fold>>         return self.__currentPos == -1         # <<fold     def isAtPos(self,key): # fold>>         return self.__currentPos == key         # <<fold      def findString(self, thestring, count=1, backward=0): # fold>>         \"\"\"         find the count occurrence of the string str in the file         and return the line catched. The internal cursor is placed         at the same line.         backward is the searching flow.         For example, to search for the first occurrence of \"hello         starting from the beginning of the file do:          import FileReader         f=FileReader.FileReader(\"a_file\")         f.toBOF()         f.findString(\"hello\",1,0)          To search the second occurrence string from the end of the         file in backward movement do:          f.toEOF()         f.findString(\"hello\",2,1)          to search the first occurrence from a given (or current) position         say line 150, going forward in the file           f.toPos(150)         f.findString(\"hello\",1,0)          return the string where the occurrence is found, or an empty string         if nothing is found. The internal counter is placed at the corresponding         line number, if the string was found. In other case, it's set at BOF         if the search was backward, and at EOF if the search was forward.          NB: the current line is never evaluated. This is a feature, since         we can so traverse occurrences with a          line=f.findString(\"hello\")         while line == '':             line.findString(\"hello\")          instead of playing with a readline every time to skip the current         line.         \"\"\"         internalcounter=1         if count < 1:             count = 1         while 1:             if backward == 0:                 line=self.readline()             else:                 line=self.readbackline()              if line == '':                 return ''             if string.find(line,thestring) != -1 :                 if count == internalcounter:                     return line                 else:                     internalcounter = internalcounter + 1                     # <<fold     def findRegexp(self, theregexp, count=1, backward=0): # fold>>         \"\"\"         find the count occurrence of the regexp in the file         and return the line catched. The internal cursor is placed         at the same line.         backward is the searching flow.         You need to pass a regexp string as theregexp.         returns a tuple. The fist element is the matched line. The subsequent elements         contains the matched groups, if any.         If no match returns None         \"\"\"         rx=re.compile(theregexp)         internalcounter=1         if count < 1:             count = 1         while 1:             if backward == 0:                 line=self.readline()             else:                 line=self.readbackline()              if line == '':                 return None             m=rx.search(line)             if m != None :                 if count == internalcounter:                     return (line,)+m.groups()                 else:                     internalcounter = internalcounter + 1     # <<fold     def skipLines(self,key): # fold>>         \"\"\"         skip a given number of lines. Key can be negative to skip         backward. Return the last line read.         Please note that skipLines(1) is equivalent to readline()         skipLines(-1) is equivalent to readbackline() and skipLines(0)         is equivalent to currentLine()         \"\"\"         return self.__getitem__(self.__currentPos+key)     # <<fold     def occurrences(self,thestring,backward=0): # fold>>         \"\"\"         count how many occurrences of str are found from the current         position (current line excluded... see skipLines()) to the         begin (or end) of file.         returns a list of positions where each occurrence is found,         in the same order found reading the file.         Leaves unaltered the cursor position.         \"\"\"         curpos=self.currentPos()         list = []         line = self.findString(thestring,1,backward)         while line != '':             list.append(self.currentPos())             line = self.findString(thestring,1,backward)         self.toPos(curpos)         return list         # <<fold     def close(self): # fold>>         self.__file.close()     # <<fold      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  how about this one, anybody seeing a problem with it  f=open('file_name')  for line,line2 in zip(f,f):   print line,line2      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "   file_name = 'your_file_name' file_open = open(file_name, 'r')  def handler(line_one, line_two):     print(line_one, line_two)  while file_open:     try:         one = file_open.next()         two = file_open.next()          handler(one, two)     except(StopIteration):         file_open.close()         break      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  def readnumlines(file, num=2):     f = iter(file)     while True:         lines = [None] * num         for i in range(num):             try:                 lines[i] = f.next()             except StopIteration: # EOF or not enough lines available                 return         yield lines  # use like this f = open(\"thefile.txt\", \"r\") for line1, line2 in readnumlines(f):     # do something with line1 and line2  # or for line1, line2, line3, ..., lineN in readnumlines(f, N):     # do something with N lines      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  f = open(filename, \"r\") for line in f:     line1 = line     f.next()  f.close   Right now, you can read file every two line. If you like you can also check the f status before f.next()     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  My idea is to create a generator that reads two lines from the file at a time, and returns this as a 2-tuple, This means you can then iterate over the results.  from cStringIO import StringIO  def read_2_lines(src):        while True:         line1 = src.readline()         if not line1: break         line2 = src.readline()         if not line2: break         yield (line1, line2)   data = StringIO(\"line1\\nline2\\nline3\\nline4\\n\") for read in read_2_lines(data):     print read   If you have an odd number of lines, it won't work perfectly, but this should give you a good outline.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  I have worked on a similar problem last month. I tried a while loop with f.readline() as well as f.readlines(). My data file is not huge, so I finally chose f.readlines(), which gives me more control of the index, otherwise I have to use f.seek() to move back and forth the file pointer.  My case is more complicated than OP. Because my data file is more flexible on how many lines to be parsed each time, so I have to check a few conditions before I can parse the data.  Another problem I found out about f.seek() is that it doesn't handle utf-8 very well when I use codecs.open('', 'r', 'utf-8'), (not exactly sure about the culprit, eventually I gave up this approach.)     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  Simple little reader. It will pull lines in pairs of two and return them as a tuple as you iterate over the object. You can close it manually or it will close itself when it falls out of scope.  class doublereader:     def __init__(self,filename):         self.f = open(filename, 'r')     def __iter__(self):         return self     def next(self):         return self.f.next(), self.f.next()     def close(self):         if not self.f.closed:             self.f.close()     def __del__(self):         self.close()  #example usage one r = doublereader(r\"C:\\file.txt\") for a, h in r:     print \"x:%s\\ny:%s\" % (a,h) r.close()  #example usage two for x,y in doublereader(r\"C:\\file.txt\"):     print \"x:%s\\ny:%s\" % (x,y) #closes itself as soon as the loop goes out of scope      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  If the file is of reasonable size, another approach that uses list-comprehension to read the entire file into a list of 2-tuples, is this:  filaname = '/path/to/file/name'  with open(filename, 'r') as f:     list_of_2tuples = [ (line,f.readline()) for line in f ]  for (line1,line2) in list_of_2tuples: # Work with them in pairs.     print('%s :: %s', (line1,line2))      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I read two lines from a file at a time using python",
        "A_Content": "  This Python code will print the first two lines:  import linecache   filename = \"ooxx.txt\"   print(linecache.getline(filename,2))      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1657299/how-do-i-read-two-lines-from-a-file-at-a-time-using-python",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am coding a python script that parses a text file. The format of this text file is such that each element in the file uses two lines and for convenience I would like to read both lines before parsing. Can this be done in Python?  I would like to some something like:  f = open(filename, \"r\") for line in f:     line1 = line     line2 = f.readline()  f.close   But this breaks saying that:     ValueError: Mixing iteration and read methods would lose data   Related:   What is the most pythonic way to iterate over a list in chunks?      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Numpy: Divide each row by a vector element",
        "A_Content": "  Here you go. You just need to use None (or alternatively np.newaxis) combined with broadcasting:  In [6]: data - vector[:,None] Out[6]: array([[0, 0, 0],        [0, 0, 0],        [0, 0, 0]])  In [7]: data / vector[:,None] Out[7]: array([[1, 1, 1],        [1, 1, 1],        [1, 1, 1]])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "numpy",
            "scipy"
        ],
        "URL": "https://stackoverflow.com/questions/19602187/numpy-divide-each-row-by-a-vector-element",
        "A_Votes": "114",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Suppose I have a numpy array:  data = np.array([[1,1,1],[2,2,2],[3,3,3]])   and I have a corresponding \"vector:\"  vector = np.array([1,2,3])   How do I operate on data along each row to either subtract or divide so the result is:  sub_result = [[0,0,0], [0,0,0], [0,0,0]] div_result = [[1,1,1], [1,1,1], [1,1,1]]   Long story short: How do I perform an operation on each row of a 2D array with a 1D array of scalars that correspond to each row?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Numpy: Divide each row by a vector element",
        "A_Content": "  As has been mentioned, slicing with None or with np.newaxes is a great way to do this. Another alternative is to use transposes and broadcasting, as in  (data.T - vector).T   and  (data.T / vector).T   For higher dimensional arrays you may want to use the swapaxes method of NumPy arrays or the NumPy rollaxis function. There really are a lot of ways to do this.  For a fuller explanation of broadcasting, see http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html     ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "numpy",
            "scipy"
        ],
        "URL": "https://stackoverflow.com/questions/19602187/numpy-divide-each-row-by-a-vector-element",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Suppose I have a numpy array:  data = np.array([[1,1,1],[2,2,2],[3,3,3]])   and I have a corresponding \"vector:\"  vector = np.array([1,2,3])   How do I operate on data along each row to either subtract or divide so the result is:  sub_result = [[0,0,0], [0,0,0], [0,0,0]] div_result = [[1,1,1], [1,1,1], [1,1,1]]   Long story short: How do I perform an operation on each row of a 2D array with a 1D array of scalars that correspond to each row?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Numpy: Divide each row by a vector element",
        "A_Content": "  JoshAdel's solution uses np.newaxis to add a dimension. An alternative is to use reshape() to align the dimensions in preparation for broadcasting.  data = np.array([[1,1,1],[2,2,2],[3,3,3]]) vector = np.array([1,2,3])  data # array([[1, 1, 1], #        [2, 2, 2], #        [3, 3, 3]]) vector # array([1, 2, 3])  data.shape # (3, 3) vector.shape # (3,)  data / vector.reshape((3,1)) # array([[1, 1, 1], #        [1, 1, 1], #        [1, 1, 1]])   Performing the reshape() allows the dimensions to line up for broadcasting:  data:            3 x 3 vector:              3 vector reshaped: 3 x 1   Note that data/vector is ok, but it doesn't get you the answer that you want. It divides each column of array (instead of each row) by each corresponding element of vector. It's what you would get if you explicitly reshaped vector to be 1x3 instead of 3x1.  data / vector # array([[1, 0, 0], #        [2, 1, 0], #        [3, 1, 1]]) data / vector.reshape((1,3)) # array([[1, 0, 0], #        [2, 1, 0], #        [3, 1, 1]])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "numpy",
            "scipy"
        ],
        "URL": "https://stackoverflow.com/questions/19602187/numpy-divide-each-row-by-a-vector-element",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Suppose I have a numpy array:  data = np.array([[1,1,1],[2,2,2],[3,3,3]])   and I have a corresponding \"vector:\"  vector = np.array([1,2,3])   How do I operate on data along each row to either subtract or divide so the result is:  sub_result = [[0,0,0], [0,0,0], [0,0,0]] div_result = [[1,1,1], [1,1,1], [1,1,1]]   Long story short: How do I perform an operation on each row of a 2D array with a 1D array of scalars that correspond to each row?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Django - how to create a file and save it to a model's FileField?",
        "A_Content": "  You want to have a look at FileField and FieldFile in the Django docs, and especially FieldFile.save().  Basically, a field declared as a FileField, when accessed, gives you an instance of class FieldFile, which gives you several methods to interact with the underlying file. So, what you need to do is:  self.license_file.save(new_name, new_contents)   where new_name is the filename you wish assigned and new_contents is the content of the file. Note that new_contents must be an instance of either django.core.files.File or django.core.files.base.ContentFile (see given links to manual for the details). The two choices boil down to:  # Using File f = open('/path/to/file') self.license_file.save(new_name, File(f)) # Using ContentFile self.license_file.save(new_name, ContentFile('A string with the file content'))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models"
        ],
        "URL": "https://stackoverflow.com/questions/7514964/django-how-to-create-a-file-and-save-it-to-a-models-filefield",
        "A_Votes": "104",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Here's my model.  What I want to do is generate a new file and overwrite the existing one whenever a model instance is saved:  class Kitten(models.Model):     claw_size = ...     license_file = models.FileField(blank=True, upload_to='license')      def save(self, *args, **kwargs):         #Generate a new license file overwriting any previous version         #and update file path         self.license_file = ???         super(Request,self).save(*args, **kwargs)   I see lots of documentation about how to upload a file.  But how do I generate a file, assign it to a model field and have Django store it in the right place?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Django - how to create a file and save it to a model's FileField?",
        "A_Content": "  Accepted answer is certainly a good solution, but here is the way I went about generating a CSV and serving it from a view.  #Model class MonthEnd(models.Model):     report = models.FileField(db_index=True, upload_to='not_used')  import csv from os.path import join  #build and store the file def write_csv():     path = join(settings.MEDIA_ROOT, 'files', 'month_end', 'report.csv')     f = open(path, \"w+b\")      #wipe the existing content     f.truncate()      csv_writer = csv.writer(f)     csv_writer.writerow(('col1'))      for num in range(3):         csv_writer.writerow((num, ))      month_end_file = MonthEnd()     month_end_file.report.name = path     month_end_file.save()  from my_app.models import MonthEnd  #serve it up as a download def get_report(request):     month_end = MonthEnd.objects.get(file_criteria=criteria)      response = HttpResponse(month_end.report, content_type='text/plain')     response['Content-Disposition'] = 'attachment; filename=report.csv'      return response   Thought it was worth while putting this here as it took me a little bit of fiddling to get all the desirable behaviour (overwrite existing file, storing to the right spot, not creating duplicate files etc).  Django 1.4.1  Python 2.7.3     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models"
        ],
        "URL": "https://stackoverflow.com/questions/7514964/django-how-to-create-a-file-and-save-it-to-a-models-filefield",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Here's my model.  What I want to do is generate a new file and overwrite the existing one whenever a model instance is saved:  class Kitten(models.Model):     claw_size = ...     license_file = models.FileField(blank=True, upload_to='license')      def save(self, *args, **kwargs):         #Generate a new license file overwriting any previous version         #and update file path         self.license_file = ???         super(Request,self).save(*args, **kwargs)   I see lots of documentation about how to upload a file.  But how do I generate a file, assign it to a model field and have Django store it in the right place?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Django - how to create a file and save it to a model's FileField?",
        "A_Content": "  Thanks @tawmas. In addition to that,  I got an error if I don't specify the file mode while opening the file. So,  f = open('/path/to/file', 'r')   For ZIP kind of file,  f = open('/path/to/file.zip', 'rb')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models"
        ],
        "URL": "https://stackoverflow.com/questions/7514964/django-how-to-create-a-file-and-save-it-to-a-models-filefield",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Here's my model.  What I want to do is generate a new file and overwrite the existing one whenever a model instance is saved:  class Kitten(models.Model):     claw_size = ...     license_file = models.FileField(blank=True, upload_to='license')      def save(self, *args, **kwargs):         #Generate a new license file overwriting any previous version         #and update file path         self.license_file = ???         super(Request,self).save(*args, **kwargs)   I see lots of documentation about how to upload a file.  But how do I generate a file, assign it to a model field and have Django store it in the right place?       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Why does Python's __import__ require fromlist?",
        "A_Content": "  In fact, the behaviour of __import__() is entirely because of the implementation of the import statement, which calls __import__(). There's basically five slightly different ways __import__() can be called by import (with two main categories):  import pkg import pkg.mod from pkg import mod, mod2 from pkg.mod import func, func2 from pkg.mod import submod   In the first and the second case, the import statement should assign the \"left-most\" module object to the \"left-most\" name: pkg. After import pkg.mod you can do pkg.mod.func() because the import statement introduced the local name pkg, which is a module object that has a mod attribute. So, the __import__() function has to return the \"left-most\" module object so it can be assigned to pkg. Those two import statements thus translate into:  pkg = __import__('pkg') pkg = __import__('pkg.mod')   In the third, fourth and fifth case, the import statement has to do more work: it has to assign to (potentially) multiple names, which it has to get from the module object. The __import__() function can only return one object, and there's no real reason to make it retrieve each of those names from the module object (and it would make the implementation a lot more complicated.) So the simple approach would be something like (for the third case):  tmp = __import__('pkg') mod = tmp.mod mod2 = tmp.mod2   However, that won't work if pkg is a package and mod or mod2 are modules in that package that are not already imported, as they are in the third and fifth case. The __import__() function needs to know that mod and mod2 are names that the import statement will want to have accessible, so that it can see if they are modules and try to import them too. So the call is closer to:  tmp = __import__('pkg', fromlist=['mod', 'mod2']) mod = tmp.mod mod2 = tmp.mod2   which causes __import__() to try and load pkg.mod and pkg.mod2 as well as pkg (but if mod or mod2 don't exist, it's not an error in the __import__() call; producing an error is left to the import statement.) But that still isn't the right thing for the fourth and fifth example, because if the call were so:  tmp = __import__('pkg.mod', fromlist=['submod']) submod = tmp.submod   then tmp would end up being pkg, as before, and not the pkg.mod module you want to get the submod attribute from. The implementation could have decided to make it so the import statement does extra work, splitting the package name on . like the __import__() function already does and traversing the names, but this would have meant duplicating some of the effort. So, instead, the implementation made __import__() return the right-most module instead of the left-most one if and only if fromlist is passed and not empty.  (The import pkg as p and from pkg import mod as m syntax doesn't change anything about this story except which local names get assigned to -- the __import__() function sees nothing different when as is used, it all remains in the import statement implementation.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/2724260/why-does-pythons-import-require-fromlist",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In Python, if you want to programmatically import a module, you can do:  module = __import__('module_name')   If you want to import a submodule, you would think it would be a simple matter of:  module = __import__('module_name.submodule')   Of course, this doesn't work; you just get module_name again. You have to do:  module = __import__('module_name.submodule', fromlist=['blah'])   Why? The actual value of fromlist don't seem to matter at all, as long as it's non-empty. What is the point of requiring an argument, then ignoring its values?  Most stuff in Python seems to be done for good reason, but for the life of me, I can't come up with any reasonable explanation for this behavior to exist.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Why does Python's __import__ require fromlist?",
        "A_Content": "  I still feel weird when I read the answer, so I tried the below code samples.  First, try to build below file structure:  tmpdir   |A      |__init__.py      | B.py      | C.py   Now A is a package, and B or C is a module.  So when we try some code like these in ipython:  Second, run the sample code in ipython:    In [2]: kk = __import__('A',fromlist=['B'])    In [3]: dir(kk)   Out[3]:    ['B',    '__builtins__',    '__doc__',    '__file__',    '__name__',    '__package__',    '__path__']   It seems like the fromlist works as we expected. But things become wired when we try to do the same things on a module. Suppose we have a module called C.py and code in it:    handlers = {}    def hello():       print \"hello\"    test_list = []   So now we try to do the same thing on it.    In [1]: ls   C.py    In [2]: kk = __import__('C')    In [3]: dir(kk)   Out[3]:    ['__builtins__',    '__doc__',    '__file__',    '__name__',    '__package__',    'handlers',    'hello',    'test_list']   So when we just want to import the test_list, does it work?    In [1]: kk = __import__('C',fromlist=['test_list'])    In [2]: dir(kk)   Out[2]:    ['__builtins__',    '__doc__',    '__file__',    '__name__',    '__package__',    'handlers',    'hello',    'test_list']   As the result shows, when we try to use fromlist on a module rather than a package, the fromlist param doesn't help at all because module has been compiled. Once it is imported, there is no way to ignore the other ones.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/2724260/why-does-pythons-import-require-fromlist",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, if you want to programmatically import a module, you can do:  module = __import__('module_name')   If you want to import a submodule, you would think it would be a simple matter of:  module = __import__('module_name.submodule')   Of course, this doesn't work; you just get module_name again. You have to do:  module = __import__('module_name.submodule', fromlist=['blah'])   Why? The actual value of fromlist don't seem to matter at all, as long as it's non-empty. What is the point of requiring an argument, then ignoring its values?  Most stuff in Python seems to be done for good reason, but for the life of me, I can't come up with any reasonable explanation for this behavior to exist.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Why does Python's __import__ require fromlist?",
        "A_Content": "  The answer can found found in the documentation for __import__:     The fromlist should be a list of names to emulate from name import ..., or an empty list to emulate import name.      When importing a module from a package, note that __import__('A.B', ...) returns package A when fromlist is empty, but its submodule B when fromlist is not empty.   So basically, that's just how the implementation of __import__ works: if you want the submodule, you pass a fromlist containing something you want to import from the submodule, and the implementation if __import__ is such that the submodule is returned.  Further explanation  I think the semantics exist so that the most relevant module is returned. In other words, say I have a package foo containing module bar with function baz. If I:  import foo.bar   Then I refer to baz as  foo.bar.baz()   This is like __import__(\"foo.bar\", fromlist=[]).  If instead I import with:  from foo import bar   Then I refer to baz as     bar.baz()  Which would be similar to __imoort__(\"foo.bar\", fromlist=[\"something\"]).  If I do:  from foo.bar import baz   Then I refer to baz as  baz()   Which is like __import__(\"foo.bar\", fromlist=[\"baz\"]).  So in the first case, I'd have to use the fully-qualified name, hence __import__ returns the first module name you'd use to refer to the imported elements, that being foo. In the last case, bar is the most specific module containing the imported elements, so it makes sense that __import__ would return the foo.bar module.  The second case is a little weird, but I am guessing it was written that way to support importing a module using the from <package> import <module> syntax, and in that case bar is still the most specific module to return.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/2724260/why-does-pythons-import-require-fromlist",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, if you want to programmatically import a module, you can do:  module = __import__('module_name')   If you want to import a submodule, you would think it would be a simple matter of:  module = __import__('module_name.submodule')   Of course, this doesn't work; you just get module_name again. You have to do:  module = __import__('module_name.submodule', fromlist=['blah'])   Why? The actual value of fromlist don't seem to matter at all, as long as it's non-empty. What is the point of requiring an argument, then ignoring its values?  Most stuff in Python seems to be done for good reason, but for the life of me, I can't come up with any reasonable explanation for this behavior to exist.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  Django <= 1.7  This gives you the property names for all related objects:  links = [rel.get_accessor_name() for rel in a._meta.get_all_related_objects()]   You can then use something like this to get all related objects:  for link in links:     objects = getattr(a, link).all()     for object in objects:         # do something with related object instance   I spent a while trying to figure this out so I could implement a kind of \"Observer Pattern\" on one of my models. Hope it's helpful.  Django 1.8+  Use _meta.get_fields(): https://docs.djangoproject.com/en/1.10/ref/models/meta/#django.db.models.options.Options.get_fields (see reverse in the _get_fields() source also)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "78",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  @digitalPBK was close... here is probably what you are looking for using Django's built in stuff  from django.db.models.deletion import Collector from django.contrib.admin.util import NestedObjects collector = NestedObjects(using=\"default\") #database name collector.collect([objective]) #list of objects. single one won't do print collector.data   this allows you to create what the django admin displays - the related objects to be deleted.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  Give this a try.  class A(models.Model):     def get_foreign_fields(self):       return [getattr(self, f.name) for f in self._meta.fields if type(f) == models.fields.related.ForeignKey]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  for link in links:     objects = getattr(a, link).all()   Works for related sets, but not for ForeignKeys. Since RelatedManagers are created dynamically, looking at the class name is easier than doing an isinstance()  objOrMgr = getattr(a, link)  if objOrMgr.__class__.__name__ ==  'RelatedManager':       objects = objOrMgr.all()  else:       objects = [ objOrMgr ]  for object in objects:       # Do Stuff      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  links = [rel.get_accessor_name() for rel in a._meta.get_all_related_objects()]  You can then use something like this to get all related objects:  for link in links:     objects = getattr(a, link.name).all()     for object in objects:         # do something with related object instance   From Django 1.10 offical docs:     MyModel._meta.get_all_related_objects() becomes:   [     f for f in MyModel._meta.get_fields()     if (f.one_to_many or f.one_to_one)     and f.auto_created and not f.concrete ]   So by taking the approved example we would use:  links = [             f for f in MyModel._meta.get_fields()             if (f.one_to_many or f.one_to_one)             and f.auto_created and not f.concrete         ]  for link in links:     objects = getattr(a, link.name).all()     for object in objects:         # do something with related object instance      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  The following is what django uses to get all related objects  from django.db.models.deletion import Collector collector = Collector(using=\"default\") collector.collect([a])  print collector.data      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  Django 1.9 get_all_related_objects() has been deprecated  #Example:  user = User.objects.get(id=1) print(user._meta.get_fields())   Note: RemovedInDjango110Warning: 'get_all_related_objects is an unofficial API that has been deprecated. You may be able to replace it with 'get_fields()'     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  Here is another way to get a list of fields (names only) in related models.  def get_related_model_fields(model):     fields=[]     related_models = model._meta.get_all_related_objects()     for r in related_models:         fields.extend(r.opts.get_all_field_names())     return fields      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Get all related Django model objects",
        "A_Content": "  Unfortunately, user._meta.get_fields() returns only relations accessible from user, however, you may have some related object, which uses related_name='+'. In such case, the relation would not be returned by user._meta.get_fields(). Therefore, if You need generic and robust way to merge objects, I'd suggest to use the Collector mentioned above.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models",
            "merge"
        ],
        "URL": "https://stackoverflow.com/questions/2233883/get-all-related-django-model-objects",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I get a list of all the model objects that have a ForeignKey pointing to an object?  (Something like the delete confirmation page in the Django admin before DELETE CASCADE).  I'm trying to come up with a generic way of merging duplicate objects in the database.  Basically I want all of the objects that have ForeignKeys points to object \"B\" to be updated to point to object \"A\" so I can then delete \"B\" without losing anything important.  Thanks for your help!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Import order coding standard",
        "A_Content": "  The current version of pylint now does this, and reports it as error class C0411.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import",
            "static-analysis",
            "pep8"
        ],
        "URL": "https://stackoverflow.com/questions/22722976/import-order-coding-standard",
        "A_Votes": "44",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    PEP8 suggests that:     Imports should be grouped in the following order:         standard library imports    related third party imports    local application/library specific imports         You should put a blank line between each group of imports.   Is there a way to check if the standard is violated anywhere in the package using static code analysis tools, like pylint, pyflakes, pychecker, pep8?    Example of violation:  from my_package import my_module from django.db import models import os   Correct way to import:  import os  from django.db import models  from my_package import my_module      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Import order coding standard",
        "A_Content": "  Update (2016): @sbywater has the most recent answer.    Found it! (accidentally, while reading \"Hacker's guide to python\")  OpenStack Hacking Style Checks project named hacking introduces several unique flake8 extensions. There is hacking_import_groups among them (related commit).  Example:   requirements   tox flake8 hacking (from the master branch):  $ git clone https://github.com/openstack-dev/hacking.git $ cd hacking/ $ python setup.py install   files used in the example   tox.ini (we need to tell flake8 that we want to use a custom check)  [hacking] local-check = hacking.core.hacking_import_groups   UPD: with the newest version of hacking the path to the check changed, now it is hacking.checks.imports.hacking_import_groups. test.py (target of the check)  import requests import sys from my_module import print_smth   print_smth(requests.get('https://google.com')) print_smth(sys.version)  my_module.py (local import used by test.py)  def print_smth(smth):     print smth     Then, if I run flake8 against test.py:  $ flake8 test.py test.py:2:1: H305  imports not grouped correctly (requests: third-party, sys: stdlib) test.py:3:1: H305  imports not grouped correctly (sys: stdlib, my_module.print_smth: project) test.py:3:1: H306  imports not in alphabetical order (sys, my_module.print_smth)   Then, if I group the imports in the correct order following PEP8:  import sys  import requests  from my_module import print_smth   print_smth(requests.get('https://google.com')) print_smth(sys.version)   No warnings found:  $ flake8 test.py $   Hope this will help somebody in the future.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import",
            "static-analysis",
            "pep8"
        ],
        "URL": "https://stackoverflow.com/questions/22722976/import-order-coding-standard",
        "A_Votes": "52",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    PEP8 suggests that:     Imports should be grouped in the following order:         standard library imports    related third party imports    local application/library specific imports         You should put a blank line between each group of imports.   Is there a way to check if the standard is violated anywhere in the package using static code analysis tools, like pylint, pyflakes, pychecker, pep8?    Example of violation:  from my_package import my_module from django.db import models import os   Correct way to import:  import os  from django.db import models  from my_package import my_module      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Import order coding standard",
        "A_Content": "  Have a look at https://pypi.python.org/pypi/isort or https://github.com/timothycrosley/isort     isort parses specified files for global level import lines (imports outside of try / excepts blocks, functions, etc..) and puts them all at the top of the file grouped together by the type of import:         Future    Python Standard Library    Third Party    Current Python Project    Explicitly Local (. before import, as in: from . import x)          Custom Separate Sections (Defined by forced_separate list in configuration file)    Inside of each section the imports are sorted alphabetically. isort automatically removes duplicate python imports, and wraps long from imports to the specified line length (defaults to 80).   https://pypi.python.org/pypi/flake8-isort plugs this functionality into flake8     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import",
            "static-analysis",
            "pep8"
        ],
        "URL": "https://stackoverflow.com/questions/22722976/import-order-coding-standard",
        "A_Votes": "24",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    PEP8 suggests that:     Imports should be grouped in the following order:         standard library imports    related third party imports    local application/library specific imports         You should put a blank line between each group of imports.   Is there a way to check if the standard is violated anywhere in the package using static code analysis tools, like pylint, pyflakes, pychecker, pep8?    Example of violation:  from my_package import my_module from django.db import models import os   Correct way to import:  import os  from django.db import models  from my_package import my_module      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Import order coding standard",
        "A_Content": "  A flake8 plugin exists: flake8-import-order.     This package adds 3 new flake8 warnings      I100: Your import statements are in the wrong order.      I101: The names in your from import are in the wrong order.      I201: Missing newline between sections or imports.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import",
            "static-analysis",
            "pep8"
        ],
        "URL": "https://stackoverflow.com/questions/22722976/import-order-coding-standard",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    PEP8 suggests that:     Imports should be grouped in the following order:         standard library imports    related third party imports    local application/library specific imports         You should put a blank line between each group of imports.   Is there a way to check if the standard is violated anywhere in the package using static code analysis tools, like pylint, pyflakes, pychecker, pep8?    Example of violation:  from my_package import my_module from django.db import models import os   Correct way to import:  import os  from django.db import models  from my_package import my_module      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame Groupby two columns and get counts",
        "A_Content": "  Followed by @Andy's answer, you can do following to solve your second question:  In [56]: df.groupby(['col5','col2']).size().reset_index().groupby('col2')[[0]].max() Out[56]:        0 col2    A     3 B     2 C     1 D     3      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts",
        "A_Votes": "44",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a pandas dataframe in the following format:  df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T df.columns = ['col1','col2','col3','col4','col5']   df:     col1 col2 col3     col4 col5 0   1.1    A  1.1    x/y/z    1 1   1.1    A  1.7      x/y    3 2   1.1    A  2.5  x/y/z/n    3 3   2.6    B  2.6      x/u    2 4   2.5    B  3.3        x    4 5   3.4    B  3.8    x/u/v    2 6   2.6    B    4    x/y/z    5 7   2.6    A  4.2        x    3 8   3.4    B  4.3  x/u/v/b    6 9   3.4    C  4.5        -    3 10  2.6    B  4.6      x/y    5 11  1.1    D  4.7    x/y/z    1 12  1.1    D  4.7        x    1 13  3.3    D  4.8  x/u/v/w    1   Now I want to group this by two columns like following:  df.groupby(['col5','col2']).reset_index()   OutPut:               index col1 col2 col3     col4 col5 col5 col2                                       1    A    0      0  1.1    A  1.1    x/y/z    1      D    0     11  1.1    D  4.7    x/y/z    1           1     12  1.1    D  4.7        x    1           2     13  3.3    D  4.8  x/u/v/w    1 2    B    0      3  2.6    B  2.6      x/u    2           1      5  3.4    B  3.8    x/u/v    2 3    A    0      1  1.1    A  1.7      x/y    3           1      2  1.1    A  2.5  x/y/z/n    3           2      7  2.6    A  4.2        x    3      C    0      9  3.4    C  4.5        -    3 4    B    0      4  2.5    B  3.3        x    4 5    B    0      6  2.6    B    4    x/y/z    5           1     10  2.6    B  4.6      x/y    5 6    B    0      8  3.4    B  4.3  x/u/v/b    6   I want to get the count by each row like following. Expected Output:  col5 col2 count 1    A      1      D      3 2    B      2 etc...   How to get my expected output? And I want to find largest count for each 'col2' value?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame Groupby two columns and get counts",
        "A_Content": "  You are looking for size:  In [11]: df.groupby(['col5', 'col2']).size() Out[11]: col5  col2 1     A       1       D       3 2     B       2 3     A       3       C       1 4     B       1 5     B       2 6     B       1 dtype: int64     To get the same answer as waitingkuo (the \"second question\"), but slightly cleaner, is to groupby the level:  In [12]: df.groupby(['col5', 'col2']).size().groupby(level=1).max() Out[12]: col2 A       3 B       2 C       1 D       3 dtype: int64      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts",
        "A_Votes": "69",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a pandas dataframe in the following format:  df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T df.columns = ['col1','col2','col3','col4','col5']   df:     col1 col2 col3     col4 col5 0   1.1    A  1.1    x/y/z    1 1   1.1    A  1.7      x/y    3 2   1.1    A  2.5  x/y/z/n    3 3   2.6    B  2.6      x/u    2 4   2.5    B  3.3        x    4 5   3.4    B  3.8    x/u/v    2 6   2.6    B    4    x/y/z    5 7   2.6    A  4.2        x    3 8   3.4    B  4.3  x/u/v/b    6 9   3.4    C  4.5        -    3 10  2.6    B  4.6      x/y    5 11  1.1    D  4.7    x/y/z    1 12  1.1    D  4.7        x    1 13  3.3    D  4.8  x/u/v/w    1   Now I want to group this by two columns like following:  df.groupby(['col5','col2']).reset_index()   OutPut:               index col1 col2 col3     col4 col5 col5 col2                                       1    A    0      0  1.1    A  1.1    x/y/z    1      D    0     11  1.1    D  4.7    x/y/z    1           1     12  1.1    D  4.7        x    1           2     13  3.3    D  4.8  x/u/v/w    1 2    B    0      3  2.6    B  2.6      x/u    2           1      5  3.4    B  3.8    x/u/v    2 3    A    0      1  1.1    A  1.7      x/y    3           1      2  1.1    A  2.5  x/y/z/n    3           2      7  2.6    A  4.2        x    3      C    0      9  3.4    C  4.5        -    3 4    B    0      4  2.5    B  3.3        x    4 5    B    0      6  2.6    B    4    x/y/z    5           1     10  2.6    B  4.6      x/y    5 6    B    0      8  3.4    B  4.3  x/u/v/b    6   I want to get the count by each row like following. Expected Output:  col5 col2 count 1    A      1      D      3 2    B      2 etc...   How to get my expected output? And I want to find largest count for each 'col2' value?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame Groupby two columns and get counts",
        "A_Content": "  Inserting data into a pandas dataframe and providing column name.     import pandas as pd df = pd.DataFrame([['A','C','A','B','C','A','B','B','A','A'], ['ONE','TWO','ONE','ONE','ONE','TWO','ONE','TWO','ONE','THREE']]).T df.columns = [['Alphabet','Words']] print(df)   #printing dataframe.   This is our printed data:    For making a group of dataframe in pandas and counter,  You need to provide one more column which counts the grouping, let's call that column as, \"COUNTER\" in  dataframe.  Like this:  df['COUNTER'] =1       #initially, set that counter to 1. group_data = df.groupby(['Alphabet','Words'])['COUNTER'].sum() #sum function print(group_data)   OUTPUT:       ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a pandas dataframe in the following format:  df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T df.columns = ['col1','col2','col3','col4','col5']   df:     col1 col2 col3     col4 col5 0   1.1    A  1.1    x/y/z    1 1   1.1    A  1.7      x/y    3 2   1.1    A  2.5  x/y/z/n    3 3   2.6    B  2.6      x/u    2 4   2.5    B  3.3        x    4 5   3.4    B  3.8    x/u/v    2 6   2.6    B    4    x/y/z    5 7   2.6    A  4.2        x    3 8   3.4    B  4.3  x/u/v/b    6 9   3.4    C  4.5        -    3 10  2.6    B  4.6      x/y    5 11  1.1    D  4.7    x/y/z    1 12  1.1    D  4.7        x    1 13  3.3    D  4.8  x/u/v/w    1   Now I want to group this by two columns like following:  df.groupby(['col5','col2']).reset_index()   OutPut:               index col1 col2 col3     col4 col5 col5 col2                                       1    A    0      0  1.1    A  1.1    x/y/z    1      D    0     11  1.1    D  4.7    x/y/z    1           1     12  1.1    D  4.7        x    1           2     13  3.3    D  4.8  x/u/v/w    1 2    B    0      3  2.6    B  2.6      x/u    2           1      5  3.4    B  3.8    x/u/v    2 3    A    0      1  1.1    A  1.7      x/y    3           1      2  1.1    A  2.5  x/y/z/n    3           2      7  2.6    A  4.2        x    3      C    0      9  3.4    C  4.5        -    3 4    B    0      4  2.5    B  3.3        x    4 5    B    0      6  2.6    B    4    x/y/z    5           1     10  2.6    B  4.6      x/y    5 6    B    0      8  3.4    B  4.3  x/u/v/b    6   I want to get the count by each row like following. Expected Output:  col5 col2 count 1    A      1      D      3 2    B      2 etc...   How to get my expected output? And I want to find largest count for each 'col2' value?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame Groupby two columns and get counts",
        "A_Content": "  Idiomatic solution that uses only a single groupby  df.groupby(['col5', 'col2']).size() \\   .sort_values(ascending=False) \\   .reset_index(name='count') \\   .drop_duplicates(subset='col2')    col5 col2  count 0    3    A      3 1    1    D      3 2    5    B      2 6    3    C      1   Explanation  The result of the groupby size method is a Series with col5 and col2 in the index. From here, you can use another groupby method to find the maximum value of each value in  col2 but it is not necessary to do. You can simply sort all the values descendingly and then keep only the rows with the first occurrence of col2 with the drop_duplicates method.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a pandas dataframe in the following format:  df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T df.columns = ['col1','col2','col3','col4','col5']   df:     col1 col2 col3     col4 col5 0   1.1    A  1.1    x/y/z    1 1   1.1    A  1.7      x/y    3 2   1.1    A  2.5  x/y/z/n    3 3   2.6    B  2.6      x/u    2 4   2.5    B  3.3        x    4 5   3.4    B  3.8    x/u/v    2 6   2.6    B    4    x/y/z    5 7   2.6    A  4.2        x    3 8   3.4    B  4.3  x/u/v/b    6 9   3.4    C  4.5        -    3 10  2.6    B  4.6      x/y    5 11  1.1    D  4.7    x/y/z    1 12  1.1    D  4.7        x    1 13  3.3    D  4.8  x/u/v/w    1   Now I want to group this by two columns like following:  df.groupby(['col5','col2']).reset_index()   OutPut:               index col1 col2 col3     col4 col5 col5 col2                                       1    A    0      0  1.1    A  1.1    x/y/z    1      D    0     11  1.1    D  4.7    x/y/z    1           1     12  1.1    D  4.7        x    1           2     13  3.3    D  4.8  x/u/v/w    1 2    B    0      3  2.6    B  2.6      x/u    2           1      5  3.4    B  3.8    x/u/v    2 3    A    0      1  1.1    A  1.7      x/y    3           1      2  1.1    A  2.5  x/y/z/n    3           2      7  2.6    A  4.2        x    3      C    0      9  3.4    C  4.5        -    3 4    B    0      4  2.5    B  3.3        x    4 5    B    0      6  2.6    B    4    x/y/z    5           1     10  2.6    B  4.6      x/y    5 6    B    0      8  3.4    B  4.3  x/u/v/b    6   I want to get the count by each row like following. Expected Output:  col5 col2 count 1    A      1      D      3 2    B      2 etc...   How to get my expected output? And I want to find largest count for each 'col2' value?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame Groupby two columns and get counts",
        "A_Content": "  Should you want to add a new column (say 'count_column') containing the groups' counts into the dataframe:  df.count_column=df.groupby(['col5','col2']).col5.transform('count')   (I picked 'col5' as it contains no nan)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a pandas dataframe in the following format:  df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T df.columns = ['col1','col2','col3','col4','col5']   df:     col1 col2 col3     col4 col5 0   1.1    A  1.1    x/y/z    1 1   1.1    A  1.7      x/y    3 2   1.1    A  2.5  x/y/z/n    3 3   2.6    B  2.6      x/u    2 4   2.5    B  3.3        x    4 5   3.4    B  3.8    x/u/v    2 6   2.6    B    4    x/y/z    5 7   2.6    A  4.2        x    3 8   3.4    B  4.3  x/u/v/b    6 9   3.4    C  4.5        -    3 10  2.6    B  4.6      x/y    5 11  1.1    D  4.7    x/y/z    1 12  1.1    D  4.7        x    1 13  3.3    D  4.8  x/u/v/w    1   Now I want to group this by two columns like following:  df.groupby(['col5','col2']).reset_index()   OutPut:               index col1 col2 col3     col4 col5 col5 col2                                       1    A    0      0  1.1    A  1.1    x/y/z    1      D    0     11  1.1    D  4.7    x/y/z    1           1     12  1.1    D  4.7        x    1           2     13  3.3    D  4.8  x/u/v/w    1 2    B    0      3  2.6    B  2.6      x/u    2           1      5  3.4    B  3.8    x/u/v    2 3    A    0      1  1.1    A  1.7      x/y    3           1      2  1.1    A  2.5  x/y/z/n    3           2      7  2.6    A  4.2        x    3      C    0      9  3.4    C  4.5        -    3 4    B    0      4  2.5    B  3.3        x    4 5    B    0      6  2.6    B    4    x/y/z    5           1     10  2.6    B  4.6      x/y    5 6    B    0      8  3.4    B  4.3  x/u/v/b    6   I want to get the count by each row like following. Expected Output:  col5 col2 count 1    A      1      D      3 2    B      2 etc...   How to get my expected output? And I want to find largest count for each 'col2' value?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame Groupby two columns and get counts",
        "A_Content": "  You can just use the built-in function count follow by the groupby function  df.groupby(['col5','col2']).count()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a pandas dataframe in the following format:  df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T df.columns = ['col1','col2','col3','col4','col5']   df:     col1 col2 col3     col4 col5 0   1.1    A  1.1    x/y/z    1 1   1.1    A  1.7      x/y    3 2   1.1    A  2.5  x/y/z/n    3 3   2.6    B  2.6      x/u    2 4   2.5    B  3.3        x    4 5   3.4    B  3.8    x/u/v    2 6   2.6    B    4    x/y/z    5 7   2.6    A  4.2        x    3 8   3.4    B  4.3  x/u/v/b    6 9   3.4    C  4.5        -    3 10  2.6    B  4.6      x/y    5 11  1.1    D  4.7    x/y/z    1 12  1.1    D  4.7        x    1 13  3.3    D  4.8  x/u/v/w    1   Now I want to group this by two columns like following:  df.groupby(['col5','col2']).reset_index()   OutPut:               index col1 col2 col3     col4 col5 col5 col2                                       1    A    0      0  1.1    A  1.1    x/y/z    1      D    0     11  1.1    D  4.7    x/y/z    1           1     12  1.1    D  4.7        x    1           2     13  3.3    D  4.8  x/u/v/w    1 2    B    0      3  2.6    B  2.6      x/u    2           1      5  3.4    B  3.8    x/u/v    2 3    A    0      1  1.1    A  1.7      x/y    3           1      2  1.1    A  2.5  x/y/z/n    3           2      7  2.6    A  4.2        x    3      C    0      9  3.4    C  4.5        -    3 4    B    0      4  2.5    B  3.3        x    4 5    B    0      6  2.6    B    4    x/y/z    5           1     10  2.6    B  4.6      x/y    5 6    B    0      8  3.4    B  4.3  x/u/v/b    6   I want to get the count by each row like following. Expected Output:  col5 col2 count 1    A      1      D      3 2    B      2 etc...   How to get my expected output? And I want to find largest count for each 'col2' value?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  In production code in our company, we try to follow the following rules.  We place imports at the beginning of the file, right after the main file's docstring, e.g.:  \"\"\" Registry related functionality. \"\"\" import wx # ...   Now, if we import a class that is one of few in the imported module, we import the name directly, so that in the code we only have to use the last part, e.g.:  from RegistryController import RegistryController from ui.windows.lists import ListCtrl, DynamicListCtrl   There are modules, however, that contain dozens of classes, e.g. list of all possible exceptions. Then we import the module itself and reference to it in the code:  from main.core import Exceptions # ... raise Exceptions.FileNotFound()   We use the import X as Y as rarely as possible, because it makes searching for usage of a particular module or class difficult. Sometimes, however, you have to use it if you wish to import two classes that have the same name, but exist in different modules, e.g.:  from Queue import Queue from main.core.MessageQueue import Queue as MessageQueue   As a general rule, we don't do imports inside methods -- they simply make code slower and less readable. Some may find this a good way to easily resolve cyclic imports problem, but a better solution is code reorganization.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "63",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  Let me just paste a part of conversation on django-dev mailing list started by Guido van Rossum:     [...]   For example, it's part of the Google Python style guides[1] that all   imports must import a module, not a class or function from that   module. There are way more classes and functions than there are   modules, so recalling where a particular thing comes from is much   easier if it is prefixed with a module name. Often multiple modules   happen to define things with the same name -- so a reader of the code   doesn't have to go back to the top of the file to see from which   module a given name is imported.    Source: http://groups.google.com/group/django-developers/browse_thread/thread/78975372cdfb7d1a  1: http://code.google.com/p/soc/wiki/PythonStyleGuide#Module_and_package_imports     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "34",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  I would normally use import X on module level. If you only need a single object from a module, use from X import Y.   Only use import X as Y in case you're otherwise confronted with a name clash.  I only use imports on function level to import stuff I need when the module is used as the main module, like:  def main():   import sys   if len(sys.argv) > 1:      pass   HTH     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  Someone above said that  from X import A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P   is equivalent to  import X   import X allows direct modifications to A-P, while from X import ... creates copies of A-P. For from X import A..P you do not get updates to variables if they are modified. If you modify them, you only modify your copy, but X does know about your modifications.  If A-P are functions, you won't know the difference.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  Others have covered most of the ground here but I just wanted to add one case where I will use import X as Y (temporarily), when I'm trying out a new version of a class or module.  So if we were migrating to a new implementation of a module, but didn't want to cut the code base over all at one time, we might write a xyz_new module and do this in the source files that we had migrated:  import xyz_new as xyz   Then, once we cut over the entire code base, we'd just replace the xyz module with xyz_new and change all of the imports back to  import xyz      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  DON'T do this:  from X import *   unless you are absolutely sure that you will use each and every thing in that module.  And even then, you should probably reconsider using a different approach.  Other than that, it's just a matter of style.  from X import Y   is good and saves you lots of typing.  I tend to use that when I'm using something in it fairly frequently  But if you're importing a lot from that module, you could end up with an import statement that looks like this:  from X import A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P   You get the idea.  That's when imports like  import X   become useful.  Either that or if I'm not really using anything in X very frequently.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  I generally try to use the regular import modulename, unless the module name is long, or used often..  For example, I would do..  from BeautifulSoup import BeautifulStoneSoup as BSS   ..so I can do soup = BSS(html) instead of BeautifulSoup.BeautifulStoneSoup(html)  Or..  from xmpp import XmppClientBase   ..instead of importing the entire of xmpp when I only use the XmppClientBase  Using import x as y is handy if you want to import either very long method names , or to prevent clobbering an existing import/variable/class/method (something you should try to avoid completely, but it's not always possible)  Say I want to run a main() function from another script, but I already have a main() function..  from my_other_module import main as other_module_main   ..wouldn't replace my main function with my_other_module's main  Oh, one thing - don't do from x import * - it makes your code very hard to understand, as you cannot easily see where a method came from (from x import *; from y import *; my_func() - where is my_func defined?)  In all cases, you could just do import modulename and then do modulename.subthing1.subthing2.method(\"test\")...  The from x import y as z stuff is purely for convenience - use it whenever it'll make  your code easier to read or write!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  When you have a well-written library, which is sometimes case in python, you ought just import it and use it as it. Well-written library tends to take life and language of its own, resulting in pleasant-to-read -code, where you rarely reference the library. When a library is well-written, you ought not need renaming or anything else too often.  import gat  node = gat.Node() child = node.children()   Sometimes it's not possible to write it this way, or then you want to lift down things from library you imported.  from gat import Node, SubNode  node = Node() child = SubNode(node)   Sometimes you do this for lot of things, if your import string overflows 80 columns, It's good idea to do this:  from gat import (     Node, SubNode, TopNode, SuperNode, CoolNode,     PowerNode, UpNode )   The best strategy is to keep all of these imports on the top of the file. Preferrably ordered alphabetically, import -statements first, then from import -statements.  Now I tell you why this is the best convention.  Python could perfectly have had an automatic import, which'd look from the main imports for the value when it can't be found from global namespace. But this is not a good idea. I explain shortly why. Aside it being more complicated to implement than simple import, programmers wouldn't be so much thinking about the depedencies and finding out from where you imported things ought be done some other way than just looking into imports.  Need to find out depedencies is one reason why people hate \"from ... import *\". Some bad examples where you need to do this exist though, for example opengl -wrappings.  So the import definitions are actually valuable as defining the depedencies of the program. It is the way how you should exploit them. From them you can quickly just check where some weird function is imported from.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  The import X as Y is useful if you have different implementations of the same module/class.  With some nested try..import..except ImportError..imports you can hide the implementation from your code. See lxml etree import example:  try:   from lxml import etree   print(\"running with lxml.etree\") except ImportError:   try:     # Python 2.5     import xml.etree.cElementTree as etree     print(\"running with cElementTree on Python 2.5+\")   except ImportError:     try:       # Python 2.5       import xml.etree.ElementTree as etree       print(\"running with ElementTree on Python 2.5+\")     except ImportError:       try:         # normal cElementTree install         import cElementTree as etree         print(\"running with cElementTree\")       except ImportError:         try:           # normal ElementTree install           import elementtree.ElementTree as etree           print(\"running with ElementTree\")         except ImportError:           print(\"Failed to import ElementTree from any known place\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What are good rules of thumb for Python imports?",
        "A_Content": "  I'm with Jason in the fact of not using  from X import *   But in my case (i'm not an expert programmer, so my code does not meet the coding style too well) I usually do in my programs a file with all the constants like program version, authors, error messages and all that stuff, so the file are just definitions, then I make the import  from const import *   That saves me a lot of time. But it's the only file that has that import, and it's because all inside that file are just variable declarations.  Doing that kind of import in a file with classes and definitions might be useful, but when you have to read that code you spend lots of time locating functions and classes.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/193919/what-are-good-rules-of-thumb-for-python-imports",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am a little confused by the multitude of ways in which you can import modules in Python.    import X import X as Y from A import B   I have been reading up about scoping and namespaces, but I would like some practical advice on what is the best strategy, under which circumstances and why.  Should imports happen at a module level or a method/function level?  In the __init__.py or in the module code itself?  My question is not really answered by \"Python packages - import by class, not file\" although it is obviously related.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  If you mean a python script, just do something like this:  try:  import mymodule except ImportError, e:  pass # module doesn't exist, deal with it.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "73",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  Updated answer  A better way of doing this is:  import subprocess import sys  reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']) installed_packages = [r.decode().split('==')[0] for r in reqs.split()]   The result:  print(installed_packages)  [     \"Django\",     \"six\",     \"requests\", ]   Check if requests is installed:  if 'requests' in installed_packages:     # Do something   Old answer  A better way of doing this is:  import pip installed_packages = pip.get_installed_distributions()   For pip>=10.x use:  from pip._internal.utils.misc import get_installed_distributions   Why this way? Sometimes you have app name collisions. Importing from the app namespace doesn't give you the full picture of what's installed on the system.  As a result, you get a list of pkg_resources.Distribution objects. See the following as an example:  print installed_packages [     \"Django 1.6.4 (/path-to-your-env/lib/python2.7/site-packages)\",     \"six 1.6.1 (/path-to-your-env/lib/python2.7/site-packages)\",     \"requests 2.5.0 (/path-to-your-env/lib/python2.7/site-packages)\", ]   Make a list of it:  flat_installed_packages = [package.project_name for package in installed_packages]  [     \"Django\",     \"six\",     \"requests\", ]   Check if requests is installed:  if 'requests' in flat_installed_packages:     # Do something      ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "38",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  As of Python 3.3, you can use the find_spec() method  import importlib.util import sys  # For illustrative purposes. package_name = 'pandas'  spec = importlib.util.find_spec(package_name) if spec is None:     print(package_name +\" is not installed\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  If you want to have the check from the terminal, you can run  pip3 show package_name   and if nothing is returned, the package is not installed.  If perhaps you want to automate this check, so that for example you can install it if missing, you can have the following in your bash script:  pip3 show package_name 1>/dev/null #pip for Python 2 if [ $? == 0 ]; then    echo \"Installed\" #Replace with your actions else    echo \"Not Installed\" #Replace with your actions, 'pip3 install --upgrade package_name' ? fi      ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  I'd like to add some thoughts/findings of mine to this topic. I'm writing a script that checks all requirements for a custom made program. There are many checks with python modules too.   There's a little issue with the   try:    import .. except:    ..   solution.  In my case one of the python modules called python-nmap, but you import it with import nmap and as you see the names mismatch. Therefore the test with the above solution returns a False result, and it also imports the module on hit, but maybe no need to use a lot of memory for a simple test/check.  I also found that   import pip installed_packages = pip.get_installed_distributions()   installed_packages will have only the packages has been installed with pip. On my system pip freeze returns over 40 python modules, while installed_packages has only 1, the one I installed manually (python-nmap).  Another solution below that I know it may not relevant to the question, but I think it's a good practice to keep the test function separate from the one that performs the install it might be useful for some.  The solution that worked for me. It based on this answer How to check if a python module exists without importing it  from imp import find_module  def checkPythonmod(mod):     try:         op = find_module(mod)         return True     except ImportError:         return False   NOTE: this solution can't find the module by the name python-nmap too, I have to use nmap instead (easy to live with) but in this case the module won't be loaded to the memory whatsoever.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  As an extension of this answer:  For Python 2.*, pip show <package_name> will perform the same task.  For example pip show numpy will return the following or alike:  Name: numpy Version: 1.11.1 Summary: NumPy: array processing for numbers, strings, records, and objects. Home-page: http://www.numpy.org Author: NumPy Developers Author-email: numpy-discussion@scipy.org License: BSD Location: /home/***/anaconda2/lib/python2.7/site-packages Requires:  Required-by: smop, pandas, tables, spectrum, seaborn, patsy, odo, numpy-stl, numba, nfft, netCDF4, MDAnalysis, matplotlib, h5py, GridDataFormats, dynd, datashape, Bottleneck, blaze, astropy      ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  If you'd like your script to install missing packages and continue, you could do something like this (on example of 'krbV' module in 'python-krbV' package):  import pip import sys  for m, pkg in [('krbV', 'python-krbV')]:     try:         setattr(sys.modules[__name__], m, __import__(m))     except ImportError:         pip.main(['install', pkg])         setattr(sys.modules[__name__], m, __import__(m))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  You can use the pkg_resources module from setuptools. For example:  import pkg_resources  package_name = 'cool_package' try:     cool_package_dist_info = pkg_resources.get_distribution(package_name) except pkg_resources.DistributionNotFound:     print('{} not installed'.format(package_name)) else:     print(cool_package_dist_info)   Note that there is a difference between python module and a python package. A package can contain multiple modules and module's names might not match the package name.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Check if Python Package is installed",
        "A_Content": "  Go option #2.  If ImportError is thrown, then the package is not installed (or not in sys.path).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "package",
            "skype",
            "python-import"
        ],
        "URL": "https://stackoverflow.com/questions/1051254/check-if-python-package-is-installed",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What's a good way to check if a package is installed while within a Python script? I know it's easy from the interpreter, but I need to do it within a script.   I guess I could check if there's a directory on the system that's created during the installation, but I feel like there's a better way. I'm trying to make sure the Skype4Py package is installed, and if not I'll install it.  My ideas for accomplishing the check   check for a directory in the typical install path try to import the package and if an exception is throw, then install package      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python, should I implement __ne__() operator based on __eq__?",
        "A_Content": "  Yes, that's perfectly fine.  In fact, the documentation urges you to define __ne__ when you define __eq__:     There are no implied relationships   among the comparison operators. The   truth of x==y does not imply that x!=y   is false. Accordingly, when defining   __eq__(), one should also define __ne__() so that the operators will behave as expected.   In a lot of cases (such as this one), it will be as simple as negating the result of __eq__, but not always.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "comparison",
            "operators",
            "python-datamodel"
        ],
        "URL": "https://stackoverflow.com/questions/4352244/python-should-i-implement-ne-operator-based-on-eq",
        "A_Votes": "46",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a class where I want to override the __eq__() operator. It seems to make sense that I should override the __ne__() operator as well, but does it make sense to implement __ne__ based on __eq__ as such?  class A:     def __eq__(self, other):         return self.value == other.value      def __ne__(self, other):         return not self.__eq__(other)   Or is there something that I'm missing with the way Python uses these operators that makes this not a good idea?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python, should I implement __ne__() operator based on __eq__?",
        "A_Content": "     Python, should I implement __ne__() operator based on __eq__?   Short Answer: No. Use == instead of the __eq__  In Python 3, != is the negation of == by default, so you are not even required to write a __ne__, and the documentation is no longer opinionated on writing one.   Generally speaking, for Python 3-only code, don't write one unless you need to overshadow the parent implementation, e.g. for a builtin object.  That is, keep in mind Raymond Hettinger's comment:     The __ne__ method follows automatically from __eq__ only if   __ne__ isn't already defined in a superclass.  So, if you're   inheriting from a builtin, it's best to override both.   If you need your code to work in Python 2, follow the recommendation for Python 2 and it will work in Python 3 just fine.  In Python 2, Python itself does not automatically implement any operation in terms of another - therefore, you should define the __ne__ in terms of == instead of the __eq__. E.G.   class A(object):     def __eq__(self, other):         return self.value == other.value      def __ne__(self, other):         return not self == other # NOT `return not self.__eq__(other)`   See proof that    implementing __ne__() operator based on __eq__ and  not implementing __ne__ in Python 2 at all   provides incorrect behavior in the demonstration below.  Long Answer  The documentation for Python 2 says:     There are no implied relationships among the comparison operators. The   truth of x==y does not imply that x!=y is false. Accordingly, when   defining __eq__(), one should also define __ne__() so that the   operators will behave as expected.   So that means that if we define __ne__ in terms of the inverse of __eq__, we can get consistent behavior.   This section of the documentation has been updated for Python 3:     By default, __ne__() delegates to __eq__() and inverts the result   unless it is NotImplemented.   and in the \"what's new\" section, we see this behavior has changed:         != now returns the opposite of ==, unless == returns NotImplemented.      For implementing __ne__, we prefer to use the == operator instead of using the __eq__ method directly so that if self.__eq__(other) of a subclass returns NotImplemented for the type checked, Python will appropriately check other.__eq__(self) From the documentation:  The NotImplemented object     This type has a single value. There is a single object with this value. This object is accessed through the built-in name   NotImplemented. Numeric methods and rich comparison methods may return   this value if they do not implement the operation for the operands   provided. (The interpreter will then try the reflected operation, or   some other fallback, depending on the operator.) Its truth value is   true.   When given a rich comparison operator, if they're not the same type, Python checks if the other is a subtype, and if it has that operator defined, it uses the other's method first (inverse for <, <=, >= and >). If NotImplemented is returned, then it uses the opposite's method. (It does not check for the same method twice.) Using the == operator allows for this logic to take place.    Expectations  Semantically, you should implement __ne__ in terms of the check for equality because users of your class will expect the following functions to be equivalent for all instances of A.:  def negation_of_equals(inst1, inst2):     \"\"\"always should return same as not_equals(inst1, inst2)\"\"\"     return not inst1 == inst2  def not_equals(inst1, inst2):     \"\"\"always should return same as negation_of_equals(inst1, inst2)\"\"\"     return inst1 != inst2   That is, both of the above functions should always return the same result. But this is dependent on the programmer.   Demonstration of unexpected behavior when defining __ne__ based on __eq__:  First the setup:  class BaseEquatable(object):     def __init__(self, x):         self.x = x     def __eq__(self, other):         return isinstance(other, BaseEquatable) and self.x == other.x  class ComparableWrong(BaseEquatable):     def __ne__(self, other):         return not self.__eq__(other)  class ComparableRight(BaseEquatable):     def __ne__(self, other):         return not self == other  class EqMixin(object):     def __eq__(self, other):         \"\"\"override Base __eq__ & bounce to other for __eq__, e.g.          if issubclass(type(self), type(other)): # True in this example         \"\"\"         return NotImplemented  class ChildComparableWrong(EqMixin, ComparableWrong):     \"\"\"__ne__ the wrong way (__eq__ directly)\"\"\"  class ChildComparableRight(EqMixin, ComparableRight):     \"\"\"__ne__ the right way (uses ==)\"\"\"  class ChildComparablePy3(EqMixin, BaseEquatable):     \"\"\"No __ne__, only right in Python 3.\"\"\"   Instantiate non-equivalent instances:  right1, right2 = ComparableRight(1), ChildComparableRight(2) wrong1, wrong2 = ComparableWrong(1), ChildComparableWrong(2) right_py3_1, right_py3_2 = BaseEquatable(1), ChildComparablePy3(2)   Expected Behavior:  (Note: while every second assertion of each of the below is equivalent and therefore logically redundant to the one before it, I'm including them to demonstrate that order does not matter when one is a subclass of the other.)  These instances have __ne__ implemented with ==:  >>> assert not right1 == right2 >>> assert not right2 == right1 >>> assert right1 != right2 >>> assert right2 != right1   These instances, testing under Python 3, also work correctly:  >>> assert not right_py3_1 == right_py3_2 >>> assert not right_py3_2 == right_py3_1 >>> assert right_py3_1 != right_py3_2 >>> assert right_py3_2 != right_py3_1   And recall that these have __ne__ implemented with __eq__ - while this is the expected behavior, the implementation is incorrect:  >>> assert not wrong1 == wrong2         # These are contradicted by the >>> assert not wrong2 == wrong1         # below unexpected behavior!   Unexpected Behavior:  Note that this comparison contradicts the comparisons above (not wrong1 == wrong2).  >>> assert wrong1 != wrong2 Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> AssertionError   and,  >>> assert wrong2 != wrong1 Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> AssertionError   Don't skip __ne__ in Python 2  For evidence that you should not skip implementing __ne__ in Python 2, see these equivalent objects:  >>> right_py3_1, right_py3_1child = BaseEquatable(1), ChildComparablePy3(1) >>> right_py3_1 != right_py3_1child # as evaluated in Python 2! True   The above result should be False!  Python 3 source  The default CPython implementation for __ne__ is in typeobject.c in object_richcompare:      case Py_NE:         /* By default, __ne__() delegates to __eq__() and inverts the result,            unless the latter returns NotImplemented. */         if (self->ob_type->tp_richcompare == NULL) {             res = Py_NotImplemented;             Py_INCREF(res);             break;         }         res = (*self->ob_type->tp_richcompare)(self, other, Py_EQ);         if (res != NULL && res != Py_NotImplemented) {             int ok = PyObject_IsTrue(res);             Py_DECREF(res);             if (ok < 0)                 res = NULL;             else {                 if (ok)                     res = Py_False;                 else                     res = Py_True;                 Py_INCREF(res);             }         }   Here we see  But the default __ne__ uses __eq__?  Python 3's default __ne__ implementation detail at the C level uses __eq__ because the higher level == (PyObject_RichCompare) would be less efficient - and therefore it must also handle NotImplemented.  If __eq__ is correctly implemented, then the negation of == is also correct - and it allows us to avoid low level implementation details in our __ne__.  Using == allows us to keep our low level logic in one place, and avoid addressing NotImplemented in __ne__.  One might incorrectly assume that == may return NotImplemented.  It actually uses the same logic as the default implementation of __eq__, which checks for identity (see do_richcompare and our evidence below)  class Foo:     def __ne__(self, other):         return NotImplemented     __eq__ = __ne__  f = Foo() f2 = Foo()   And the comparisons:  >>> f == f True >>> f != f False >>> f2 == f False >>> f2 != f True   Performance  Don't take my word for it, let's see what's more performant:  class CLevel:     \"Use default logic programmed in C\"  class HighLevelPython:     def __ne__(self, other):         return not self == other  class LowLevelPython:     def __ne__(self, other):         equal = self.__eq__(other)         if equal is NotImplemented:             return NotImplemented         return not equal  def c_level():     cl = CLevel()     return lambda: cl != cl  def high_level_python():     hlp = HighLevelPython()     return lambda: hlp != hlp  def low_level_python():     llp = LowLevelPython()     return lambda: llp != llp   I think these performance numbers speak for themselves:  >>> import timeit >>> min(timeit.repeat(c_level())) 0.09377292497083545 >>> min(timeit.repeat(high_level_python())) 0.2654011140111834 >>> min(timeit.repeat(low_level_python())) 0.3378178110579029   This makes sense when you consider that low_level_python is doing logic in Python that would otherwise be handled on the C level.  Conclusion  For Python 2 compatible code, use == to implement __ne__. It is more:   correct simple performant   In Python 3 only, use the low-level negation on the C level - it is even more simple and performant (though the programmer is responsible for determining that it is correct).  Do not write low-level logic in high level Python.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "comparison",
            "operators",
            "python-datamodel"
        ],
        "URL": "https://stackoverflow.com/questions/4352244/python-should-i-implement-ne-operator-based-on-eq",
        "A_Votes": "83",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a class where I want to override the __eq__() operator. It seems to make sense that I should override the __ne__() operator as well, but does it make sense to implement __ne__ based on __eq__ as such?  class A:     def __eq__(self, other):         return self.value == other.value      def __ne__(self, other):         return not self.__eq__(other)   Or is there something that I'm missing with the way Python uses these operators that makes this not a good idea?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python, should I implement __ne__() operator based on __eq__?",
        "A_Content": "  Just for the record, a canonically correct and cross Py2/Py3 portable __ne__ would look like:  import sys  class ...:     ...     def __eq__(self, other):         ...      if sys.version_info[0] == 2:         def __ne__(self, other):             equal = self.__eq__(other)             return equal if equal is NotImplemented else not equal   This works with any __eq__ you might define, and unlike not (self == other), doesn't interfere in some annoying/complex cases involving comparisons between instances where one instance is of a subclass of the other. If your __eq__ doesn't use NotImplemented returns, this works (with meaningless overhead), if it does use NotImplemented sometimes, this handles it properly. And the Python version check means that if the class is import-ed in Python 3, __ne__ is left undefined, allowing Python's native, efficient fallback __ne__ implementation (a C version of the above) to take over.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "comparison",
            "operators",
            "python-datamodel"
        ],
        "URL": "https://stackoverflow.com/questions/4352244/python-should-i-implement-ne-operator-based-on-eq",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a class where I want to override the __eq__() operator. It seems to make sense that I should override the __ne__() operator as well, but does it make sense to implement __ne__ based on __eq__ as such?  class A:     def __eq__(self, other):         return self.value == other.value      def __ne__(self, other):         return not self.__eq__(other)   Or is there something that I'm missing with the way Python uses these operators that makes this not a good idea?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python, should I implement __ne__() operator based on __eq__?",
        "A_Content": "  If all of __eq__, __ne__, __lt__, __ge__, __le__, and __gt__ make sense for the class, then just implement __cmp__ instead. Otherwise, do as you're doing, because of the bit Daniel DiPaolo said (while I was testing it instead of looking it up ;) )     ",
        "Language": "Python",
        "Tags": [
            "python",
            "comparison",
            "operators",
            "python-datamodel"
        ],
        "URL": "https://stackoverflow.com/questions/4352244/python-should-i-implement-ne-operator-based-on-eq",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a class where I want to override the __eq__() operator. It seems to make sense that I should override the __ne__() operator as well, but does it make sense to implement __ne__ based on __eq__ as such?  class A:     def __eq__(self, other):         return self.value == other.value      def __ne__(self, other):         return not self.__eq__(other)   Or is there something that I'm missing with the way Python uses these operators that makes this not a good idea?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python, should I implement __ne__() operator based on __eq__?",
        "A_Content": "  Short answer: yes (but read the documentation to do it right)  Though interesting, Aaron Halls answer is not the correct way to implement the __ne__ method, because with the not self == other implementation, the __ne__ method of the other operand is never considered. In contrast, as demonstrated below, the default Python 3 implementation of the __ne__ method of an operand does fallback on the __ne__ method of the other operand by returning NotImplemented when its __eq__ method returns NotImplemented. ShadowRanger gave the correct implementation of the __ne__ method:  def __ne__(self, other):     result = self.__eq__(other)      if result is not NotImplemented:         return not result      return NotImplemented   Implementation of the comparison operators  The Python Language Reference for Python 3 states in its chapter III Data model:     object.__lt__(self, other)   object.__le__(self, other)   object.__eq__(self, other)   object.__ne__(self, other)   object.__gt__(self, other)   object.__ge__(self, other)      These are the so-called rich comparison methods. The correspondence   between operator symbols and method names is as follows: x<y calls   x.__lt__(y), x<=y calls x.__le__(y), x==y calls x.__eq__(y),   x!=y calls x.__ne__(y), x>y calls x.__gt__(y), and x>=y   calls x.__ge__(y).      A rich comparison method may return the singleton NotImplemented if   it does not implement the operation for a given pair of arguments.      There are no swapped-argument versions of these methods (to be used   when the left argument does not support the operation but the right   argument does); rather, __lt__() and __gt__() are each others   reflection, __le__() and __ge__() are each others reflection, and   __eq__() and __ne__() are their own reflection. If the operands   are of different types, and right operands type is a direct or   indirect subclass of the left operands type, the reflected method of   the right operand has priority, otherwise the left operands method   has priority. Virtual subclassing is not considered.   Translating this into Python code (operator_eq for ==, operator_ne for !=, operator_lt for <, operator_gt for >, operator_le for <= and operator_ge for >=):  def operator_eq(left, right):     if isinstance(right, type(left)):         result = right.__eq__(left)          if result is NotImplemented:             result = left.__eq__(right)     else:         result = left.__eq__(right)          if result is NotImplemented:             result = right.__eq__(left)      if result is NotImplemented:         result = left is right      return result   def operator_ne(left, right):     if isinstance(right, type(left)):         result = right.__ne__(left)          if result is NotImplemented:             result = left.__ne__(right)     else:         result = left.__ne__(right)          if result is NotImplemented:             result = right.__ne__(left)      if result is NotImplemented:         result = left is not right      return result   def operator_lt(left, right):     if isinstance(right, type(left)):         result = right.__gt__(left)          if result is NotImplemented:             result = left.__lt__(right)     else:         result = left.__lt__(right)          if result is NotImplemented:             result = right.__gt__(left)      if result is NotImplemented:         raise TypeError(f\"'<' not supported between instances of '{type(left).__name__}' and '{type(right).__name__}'\")      return result   def operator_gt(left, right):     if isinstance(right, type(left)):         result = right.__lt__(left)          if result is NotImplemented:             result = left.__gt__(right)     else:         result = left.__gt__(right)          if result is NotImplemented:             result = right.__lt__(left)      if result is NotImplemented:         raise TypeError(f\"'>' not supported between instances of '{type(left).__name__}' and '{type(right).__name__}'\")      return result   def operator_le(left, right):     if isinstance(right, type(left)):         result = right.__ge__(left)          if result is NotImplemented:             result = left.__le__(right)     else:         result = left.__le__(right)          if result is NotImplemented:             result = right.__ge__(left)      if result is NotImplemented:         raise TypeError(f\"'<=' not supported between instances of '{type(left).__name__}' and '{type(right).__name__}'\")      return result   def operator_ge(left, right):     if isinstance(right, type(left)):         result = right.__le__(left)          if result is NotImplemented:             result = left.__ge__(right)     else:         result = left.__ge__(right)          if result is NotImplemented:             result = right.__le__(left)      if result is NotImplemented:         raise TypeError(f\"'>=' not supported between instances of '{type(left).__name__}' and '{type(right).__name__}'\")      return result   Default implementation of the comparison methods  The documentation adds:     By default, __ne__() delegates to __eq__() and inverts the result   unless it is NotImplemented. There are no other implied   relationships among the comparison operators, for example, the truth   of (x<y or x==y) does not imply x<=y.   The default implementation of the comparison methods (__eq__, __ne__, __lt__, __gt__, __le__ and __ge__) can thus be given by:  def __eq__(self, other):     return NotImplemented  def __ne__(self, other):     result = self.__eq__(other)      if result is not NotImplemented:         return not result      return NotImplemented  def __lt__(self, other):     return NotImplemented  def __gt__(self, other):     return NotImplemented  def __le__(self, other):     return NotImplemented  def __ge__(self, other):     return NotImplemented   So this is the correct implementation of the __ne__ method. And it does not always return the inverse of the __eq__ method because when the __eq__ method returns NotImplemented, its inverse not NotImplemented is False (as bool(NotImplemented) is True) instead of the desired NotImplemented.  Incorrect implementations of __ne__  As Aaron Hall demonstrated above, not self.__eq__(other) is not the correct implementation of the __ne__ method. But nor is not self == other. The latter is demonstrated below by comparing the behavior of the default implementation with the behavior of the not self == other implementation in two cases:   the __eq__ method returns NotImplemented; the __eq__ method returns a value different from NotImplemented.   Default implementation  Lets see what happens when the A.__ne__ method uses the default implementation and the A.__eq__ method returns NotImplemented:  class A:     pass   class B:      def __ne__(self, other):         return \"B.__ne__\"   assert (A() != B()) == \"B.__ne__\"    != calls A.__ne__. A.__ne__ calls A.__eq__. A.__eq__ returns NotImplemented. != calls B.__ne__. B.__ne__ returns \"B.__ne__\".   This shows that when the A.__eq__ method returns NotImplemented, the A.__ne__ method falls back on the B.__ne__ method.  Now lets see what happens when the A.__ne__ method uses the default implementation and the A.__eq__ method returns a value different from NotImplemented:  class A:      def __eq__(self, other):         return True   class B:      def __ne__(self, other):         return \"B.__ne__\"   assert (A() != B()) is False    != calls A.__ne__. A.__ne__ calls A.__eq__. A.__eq__ returns True. != returns not True, that is False.   This shows that in this case, the A.__ne__ method returns the inverse of the A.__eq__ method. Thus the __ne__ method behaves like advertised in the documentation.  Overriding the default implementation of the A.__ne__ method with the correct implementation given above yields the same results.  not self == other implementation  Lets see what happens when overriding the default implementation of the A.__ne__ method with the not self == other implementation and the A.__eq__ method returns NotImplemented:  class A:      def __ne__(self, other):         return not self == other   class B:      def __ne__(self, other):         return \"B.__ne__\"   assert (A() != B()) is True    != calls A.__ne__. A.__ne__ calls ==. == calls A.__eq__. A.__eq__ returns NotImplemented. == calls B.__eq__. B.__eq__ returns NotImplemented. == returns A() is B(), that is False. A.__ne__ returns not False, that is True.   The default implementation of the __ne__ method returned \"B.__ne__\", not True.  Now lets see what happens when overriding the default implementation of the A.__ne__ method with the not self == other implementation and the A.__eq__ method returns a value different from NotImplemented:  class A:      def __eq__(self, other):         return True      def __ne__(self, other):         return not self == other   class B:      def __ne__(self, other):         return \"B.__ne__\"   assert (A() != B()) is False    != calls A.__ne__. A.__ne__ calls ==. == calls A.__eq__. A.__eq__ returns True. A.__ne__ returns not True, that is False.   The default implementation of the __ne__ method also returned False in this case.  Since this implementation fails to replicate the behavior of the default implementation of the __ne__ method when the __eq__ method returns NotImplemented, it is incorrect.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "comparison",
            "operators",
            "python-datamodel"
        ],
        "URL": "https://stackoverflow.com/questions/4352244/python-should-i-implement-ne-operator-based-on-eq",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a class where I want to override the __eq__() operator. It seems to make sense that I should override the __ne__() operator as well, but does it make sense to implement __ne__ based on __eq__ as such?  class A:     def __eq__(self, other):         return self.value == other.value      def __ne__(self, other):         return not self.__eq__(other)   Or is there something that I'm missing with the way Python uses these operators that makes this not a good idea?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  There is a mutable alternative to collections.namedtuple - recordclass.  It has the same API and memory footprint as namedtuple and it supports assignments (It should be faster as well). For example:  from recordclass import recordclass  Point = recordclass('Point', 'x y')  >>> p = Point(1, 2) >>> p Point(x=1, y=2) >>> print(p.x, p.y) 1 2 >>> p.x += 2; p.y += 3; print(p) Point(x=3, y=5)   There is a more complete example (it also includes performance comparisons).     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "70",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  It seems like the answer to this question is no.  Below is pretty close, but it's not technically mutable. This is creating a new namedtuple() instance with an updated x value:  Point = namedtuple('Point', ['x', 'y']) p = Point(0, 0) p = p._replace(x=10)    On the other hand, you can create a simple class using __slots__ that should work well for frequently updating class instance attributes:  class Point:     __slots__ = ['x', 'y']     def __init__(self, x, y):         self.x = x         self.y = y   To add to this answer, I think __slots__ is good use here because it's memory efficient when you create lots of class instances. The only downside is that you can't create new class attributes.  Here's one relevant thread that illustrates the memory efficiency -  Dictionary vs Object - which is more efficient and why?   The quoted content in the answer of this thread is a very succinct explanation why __slots__ is more memory efficient - Python slots       ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  The latest namedlist 1.7 passes all of your tests with both Python 2.7 and Python 3.5 as of Jan 11, 2016. It is a pure python implementation whereas the recordclass is a C extension. Of course, it depends on your requirements whether a C extension is preferred or not.  Your tests (but also see the note below):  from __future__ import print_function import pickle import sys from namedlist import namedlist  Point = namedlist('Point', 'x y') p = Point(x=1, y=2)  print('1. Mutation of field values') p.x *= 10 p.y += 10 print('p: {}, {}\\n'.format(p.x, p.y))  print('2. String') print('p: {}\\n'.format(p))  print('3. Representation') print(repr(p), '\\n')  print('4. Sizeof') print('size of p:', sys.getsizeof(p), '\\n')  print('5. Access by name of field') print('p: {}, {}\\n'.format(p.x, p.y))  print('6. Access by index') print('p: {}, {}\\n'.format(p[0], p[1]))  print('7. Iterative unpacking') x, y = p print('p: {}, {}\\n'.format(x, y))  print('8. Iteration') print('p: {}\\n'.format([v for v in p]))  print('9. Ordered Dict') print('p: {}\\n'.format(p._asdict()))  print('10. Inplace replacement (update?)') p._update(x=100, y=200) print('p: {}\\n'.format(p))  print('11. Pickle and Unpickle') pickled = pickle.dumps(p) unpickled = pickle.loads(pickled) assert p == unpickled print('Pickled successfully\\n')  print('12. Fields\\n') print('p: {}\\n'.format(p._fields))  print('13. Slots') print('p: {}\\n'.format(p.__slots__))   Output on Python 2.7   1. Mutation of field values   p: 10, 12  2. String   p: Point(x=10, y=12)  3. Representation   Point(x=10, y=12)   4. Sizeof   size of p: 64   5. Access by name of field   p: 10, 12  6. Access by index   p: 10, 12  7. Iterative unpacking   p: 10, 12  8. Iteration   p: [10, 12]  9. Ordered Dict   p: OrderedDict([('x', 10), ('y', 12)])  10. Inplace replacement (update?)   p: Point(x=100, y=200)  11. Pickle and Unpickle   Pickled successfully  12. Fields   p: ('x', 'y')  13. Slots   p: ('x', 'y')   The only difference with Python 3.5 is that the namedlist has become smaller, the size is 56 (Python 2.7 reports 64).  Note that I have changed your test 10 for in-place replacement. The namedlist has a _replace() method which does a shallow copy, and that  makes perfect sense to me because the namedtuple in the standard library behaves the same way. Changing the semantics of the _replace() method would be confusing. In my opinion the _update() method should be used for in-place updates. Or maybe I failed to understand the intent of your test 10?     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  types.SimpleNamespace was introduced in Python 3.3 and supports the requested requirements.  from types import SimpleNamespace t = SimpleNamespace(foo='bar') t.ham = 'spam' print(t) namespace(foo='bar', ham='spam') print(t.foo) 'bar' import pickle with open('/tmp/pickle', 'wb') as f:     pickle.dump(t, f)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  The following is a good solution for Python 3: A minimal class using __slots__ and Sequence abstract base class; does not do fancy error detection or such, but it works, and behaves mostly like a mutable tuple (except for typecheck).  from collections import Sequence  class NamedMutableSequence(Sequence):     __slots__ = ()      def __init__(self, *a, **kw):         slots = self.__slots__         for k in slots:             setattr(self, k, kw.get(k))          if a:             for k, v in zip(slots, a):                 setattr(self, k, v)      def __str__(self):         clsname = self.__class__.__name__         values = ', '.join('%s=%r' % (k, getattr(self, k))                            for k in self.__slots__)         return '%s(%s)' % (clsname, values)      __repr__ = __str__      def __getitem__(self, item):         return getattr(self, self.__slots__[item])      def __setitem__(self, item, value):         return setattr(self, self.__slots__[item], value)      def __len__(self):         return len(self.__slots__)  class Point(NamedMutableSequence):     __slots__ = ('x', 'y')   Example:  >>> p = Point(0, 0) >>> p.x = 10 >>> p Point(x=10, y=0) >>> p.x *= 10 >>> p Point(x=100, y=0)   If you want, you can have a method to create the class too (though using an explicit class is more transparent):  def namedgroup(name, members):     if isinstance(members, str):         members = members.split()     members = tuple(members)     return type(name, (NamedMutableSequence,), {'__slots__': members})   Example:  >>> Point = namedgroup('Point', ['x', 'y']) >>> Point(6, 42) Point(x=6, y=42)     In Python 2 you need to adjust it slightly - if you inherit from Sequence, the class will have a __dict__ and the __slots__ will stop from working.  The solution in Python 2 is to not inherit from Sequence, but object. If isinstance(Point, Sequence) == True is desired, you need to register the  NamedMutableSequence as a base class to Sequence:  Sequence.register(NamedMutableSequence)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  As a very Pythonic alternative for this task, since Python-3.7, you can use  dataclasses module that not only behaves like a mutable NamedTuple because they use normal class definitions they also support other classes features.  From PEP-0557:     Although they use a very different mechanism, Data Classes can be thought of as \"mutable namedtuples with defaults\". Because Data Classes use normal class definition syntax, you are free to use inheritance, metaclasses, docstrings, user-defined methods, class factories, and other Python class features.      A class decorator is provided which inspects a class definition for variables with type annotations as defined in PEP 526, \"Syntax for Variable Annotations\". In this document, such variables are called fields. Using these fields, the decorator adds generated method definitions to the class to support instance initialization, a repr, comparison methods, and optionally other methods as described in the Specification section. Such a class is called a Data Class, but there's really nothing special about the class: the decorator adds generated methods to the class and returns the same class it was given.   This feature is introduced in PEP-0557 that you can read about it in more details on provided documentation link.  Example:  In [20]: from dataclasses import dataclass  In [21]: @dataclass     ...: class InventoryItem:     ...:     '''Class for keeping track of an item in inventory.'''     ...:     name: str     ...:     unit_price: float     ...:     quantity_on_hand: int = 0     ...:      ...:     def total_cost(self) -> float:     ...:         return self.unit_price * self.quantity_on_hand     ...:       Demo:  In [23]: II = InventoryItem('bisc', 2000)  In [24]: II Out[24]: InventoryItem(name='bisc', unit_price=2000, quantity_on_hand=0)  In [25]: II.name = 'choco'  In [26]: II.name Out[26]: 'choco'  In [27]:   In [27]: II.unit_price *= 3  In [28]: II.unit_price Out[28]: 6000  In [29]: II Out[29]: InventoryItem(name='choco', unit_price=6000, quantity_on_hand=0)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  Let's implement this with dynamic type creation:  import copy def namedgroup(typename, fieldnames):      def init(self, **kwargs):          attrs = {k: None for k in self._attrs_}         for k in kwargs:             if k in self._attrs_:                 attrs[k] = kwargs[k]             else:                 raise AttributeError('Invalid Field')         self.__dict__.update(attrs)      def getattribute(self, attr):         if attr.startswith(\"_\") or attr in self._attrs_:             return object.__getattribute__(self, attr)         else:             raise AttributeError('Invalid Field')      def setattr(self, attr, value):         if attr in self._attrs_:             object.__setattr__(self, attr, value)         else:             raise AttributeError('Invalid Field')      def rep(self):          d = [\"{}={}\".format(v,self.__dict__[v]) for v in self._attrs_]          return self._typename_ + '(' + ', '.join(d) + ')'      def iterate(self):         for x in self._attrs_:             yield self.__dict__[x]         raise StopIteration()      def setitem(self, *args, **kwargs):         return self.__dict__.__setitem__(*args, **kwargs)      def getitem(self, *args, **kwargs):         return self.__dict__.__getitem__(*args, **kwargs)      attrs = {\"__init__\": init,                 \"__setattr__\": setattr,                 \"__getattribute__\": getattribute,                 \"_attrs_\": copy.deepcopy(fieldnames),                 \"_typename_\": str(typename),                 \"__str__\": rep,                 \"__repr__\": rep,                 \"__len__\": lambda self: len(fieldnames),                 \"__iter__\": iterate,                 \"__setitem__\": setitem,                 \"__getitem__\": getitem,                 }      return type(typename, (object,), attrs)   This checks the attributes to see if they are valid before allowing the operation to continue.  So is this pickleable?  Yes if (and only if) you do the following:  >>> import pickle >>> Point = namedgroup(\"Point\", [\"x\", \"y\"]) >>> p = Point(x=100, y=200) >>> p2 = pickle.loads(pickle.dumps(p)) >>> p2.x 100 >>> p2.y 200 >>> id(p) != id(p2) True   The definition has to be in your namespace, and must exist long enough for pickle to find it.  So if you define this to be in your package, it should work.  Point = namedgroup(\"Point\", [\"x\", \"y\"])   Pickle will fail if you do the following, or make the definition temporary (goes out of scope when the function ends, say):  some_point = namedgroup(\"Point\", [\"x\", \"y\"])   And yes, it does preserve the order of the fields listed in the type creation.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  Tuples are by definition immutable.  You can however make a dictionary subclass where you can access the attributes with dot-notation;  In [1]: %cpaste Pasting code; enter '--' alone on the line to stop or use Ctrl-D. :class AttrDict(dict): : :    def __getattr__(self, name): :        return self[name] : :    def __setattr__(self, name, value): :        self[name] = value :--  In [2]: test = AttrDict()  In [3]: test.a = 1  In [4]: test.b = True  In [5]: test Out[5]: {'a': 1, 'b': True}      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  If you want similar behavior as namedtuples but mutable try namedlist  Note that in order to be mutable it cannot be a tuple.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Existence of mutable named tuple in Python?",
        "A_Content": "  Provided performance is of little importance, one could use a silly hack like:  from collection import namedtuple  Point = namedtuple('Point', 'x y z') mutable_z = Point(1,2,[3])      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/29290359/existence-of-mutable-named-tuple-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Can anyone amend namedtuple or provide an alternative class so that it works for mutable objects?  Primarily for readability, I would like something similar to namedtuple that does this:  from Camelot import namedgroup  Point = namedgroup('Point', ['x', 'y']) p = Point(0, 0) p.x = 10  >>> p Point(x=10, y=0)  >>> p.x *= 10 Point(x=100, y=0)   It must be possible to pickle the resulting object.  And per the characteristics of named tuple, the ordering of the output when represented must match the order of the parameter list when constructing the object.  RESPONSE: Thanks to everyone who submitted suggestions.  I believe that the recordclass referred by @intellimath is the best solution (also see here).     recordclass 0.4   Mutable variant of collections.namedtuple, which supports assignments      recordclass is MIT Licensed python library. It implements the type   memoryslots and factory function recordclass in order to create   record-like classes.      memoryslots is tuple-like type, which supports assignment operations.   recordclass is a factory function that create a mutable analog of   collection.namedtuple. This library actually is a proof of concept   for the problem of mutable alternative of namedtuple.   I also ran some tests against all of the suggestions.  Not all of these features were requested, so the comparison isn't really fair.  The tests are here just to point out the usability of each class.  # Option 1 (p1): @kennes913 # Option 2 (p2): @MadMan2064 # Option 3 (p3): @intellimath    # Option 4 (p4): @Roland Smith # Option 5 (p5): @agomcas # Option 6 (p6): @Antti Haapala   # TEST:                             p1     p2     p3     p4     p5      p6  # 1.  Mutation of field values   |  x   |   x  |   x  |   x  |   x  |   x  | # 2.  String                     |      |   x  |   x  |   x  |      |   x  | # 3.  Representation             |      |   x  |   x  |   x  |      |   x  | # 4.  Sizeof                     |  x   |   x  |   x  |   ?  |  ??  |   x  | # 5.  Access by name of field    |  x   |   x  |   x  |   x  |   x  |   x  | # 6.  Access by index.           |      |      |   x  |      |      |      | # 7.  Iterative unpacking.       |      |   x  |   x  |      |      |   x  | # 8.  Iteration                  |      |   x  |   x  |      |      |   x  | # 9.  Ordered Dict               |      |      |   x  |      |      |      | # 10. Inplace replacement        |      |      |   x  |      |      |      | # 11. Pickle and Unpickle        |      |      |   x  |      |      |      | # 12. Fields*                    |      |      | yes  |      |  yes |      | # 13. Slots*                     |  yes |      |      |      |  yes |      |  # *Note that I'm not very familiar with slots and fields, so please excuse  # my ignorance in reporting their results.  I have included them for completeness.  # Class/Object creation. p1 = Point1(x=1, y=2)  Point2 = namedgroup(\"Point2\", [\"x\", \"y\"]) p2 = Point2(x=1, y=2)  Point3 = recordclass('Point3', 'x y')   # *** p3 = Point3(x=1, y=2)  p4 = AttrDict() p4.x = 1 p4.y = 2  p5 = namedlist('Point5', 'x y')  Point6 = namedgroup('Point6', ['x', 'y']) p6 = Point6(x=1, y=2)  point_objects = [p1, p2, p3, p4, p5, p6]  # 1. Mutation of field values. for n, p in enumerate(point_objects):     try:         p.x *= 10         p.y += 10         print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))     except Exception as e:         print('p{0}: Mutation not supported. {1}'.format(n + 1, e))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 2. String. for n, p in enumerate(point_objects):     print('p{0}: {1}'.format(n + 1, p)) p1: <__main__.Point1 instance at 0x10c72dc68> p2: Point2(x=10, y=12) p3: Point3(x=10, y=12) p4: {'y': 12, 'x': 10} p5: <class '__main__.Point5'> p6: Point6(x=10, y=12)   # 3. Representation. [('p{0}'.format(n + 1), p) for n, p in enumerate(point_objects)]  [('p1', <__main__.Point1 instance at 0x10c72dc68>),  ('p2', Point2(x=10, y=12)),  ('p3', Point3(x=10, y=12)),  ('p4', {'x': 10, 'y': 12}),  ('p5', __main__.Point5),  ('p6', Point6(x=10, y=12))]   # 4. Sizeof. for n, p in enumerate(point_objects):     print(\"size of p{0}:\".format(n + 1), sys.getsizeof(p))  size of p1: 72 size of p2: 64 size of p3: 72 size of p4: 280 size of p5: 904 size of p6: 64   # 5. Access by name of field. for n, p in enumerate(point_objects):     print('p{0}: {1}, {2}'.format(n + 1, p.x, p.y))  p1: 10, 12 p2: 10, 12 p3: 10, 12 p4: 10, 12 p5: 10, 12 p6: 10, 12   # 6. Access by index. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}, {2}'.format(n + 1, p[0], p[1]))     except:         print('p{0}: Unable to access by index.'.format(n+1))  p1: Unable to access by index. p2: Unable to access by index. p3: 10, 12 p4: Unable to access by index. p5: Unable to access by index. p6: Unable to access by index.   # 7. Iterative unpacking. for n, p in enumerate(point_objects):     try:         x, y = p         print('p{0}: {1}, {2}'.format(n + 1, x, y))     except:         print('p{0}: Unable to unpack.'.format(n + 1))  p1: Unable to unpack. p2: 10, 12 p3: 10, 12 p4: y, x p5: Unable to unpack. p6: 10, 12   # 8. Iteration for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, [v for v in p]))     except:         print('p{0}: Unable to iterate.'.format(n + 1))  p1: Unable to iterate. p2: [10, 12] p3: [10, 12] p4: ['y', 'x'] p5: Unable to iterate. p6: [10, 12] In [95]:   # 9. Ordered Dict for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._asdict()))     except:         print('p{0}: Unable to create Ordered Dict.'.format(n + 1))  p1: Unable to create Ordered Dict. p2: Unable to create Ordered Dict. p3: OrderedDict([('x', 10), ('y', 12)]) p4: Unable to create Ordered Dict. p5: Unable to create Ordered Dict. p6: Unable to create Ordered Dict.   # 10. Inplace replacement for n, p in enumerate(point_objects):     try:         p_ = p._replace(x=100, y=200)         print('p{0}: {1} - {2}'.format(n + 1, 'Success' if p is p_ else 'Failure', p))     except:         print('p{0}: Unable to replace inplace.'.format(n + 1))  p1: Unable to replace inplace. p2: Unable to replace inplace. p3: Success - Point3(x=100, y=200) p4: Unable to replace inplace. p5: Unable to replace inplace. p6: Unable to replace inplace.   # 11. Pickle and Unpickle. for n, p in enumerate(point_objects):     try:         pickled = pickle.dumps(p)         unpickled = pickle.loads(pickled)         if p != unpickled:             raise ValueError((p, unpickled))         print('p{0}: {1}'.format(n + 1, 'Pickled successfully', ))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Pickle failure', e))  p1: Pickle failure; (<__main__.Point1 instance at 0x10c72dc68>, <__main__.Point1 instance at 0x10ca631b8>) p2: Pickle failure; (Point2(x=10, y=12), Point2(x=10, y=12)) p3: Pickled successfully p4: Pickle failure; '__getstate__' p5: Pickle failure; Can't pickle <class '__main__.Point5'>: it's not found as __main__.Point5 p6: Pickle failure; (Point6(x=10, y=12), Point6(x=10, y=12))   # 12. Fields. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p._fields))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access fields.', e))  p1: Unable to access fields.; Point1 instance has no attribute '_fields' p2: Unable to access fields.; 'Point2' object has no attribute '_fields' p3: ('x', 'y') p4: Unable to access fields.; '_fields' p5: ('x', 'y') p6: Unable to access fields.; 'Point6' object has no attribute '_fields'   # 13. Slots. for n, p in enumerate(point_objects):     try:         print('p{0}: {1}'.format(n + 1, p.__slots__))     except Exception as e:         print('p{0}: {1}; {2}'.format(n + 1, 'Unable to access slots', e))  p1: ['x', 'y'] p2: Unable to access slots; 'Point2' object has no attribute '__slots__' p3: () p4: Unable to access slots; '__slots__' p5: ('x', 'y') p6: Unable to access slots; 'Point6' object has no attribute '__slots__'      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a 'foreach' function in Python 3?",
        "A_Content": "  Every occurence of \"foreach\" I've seen (PHP, C#, ...) does basically the same as pythons \"for\" statement.  These are more or less equivalent:  // PHP: foreach ($array as $val) {     print($val); }  // C# foreach (String val in array) {     console.writeline(val); }  // Python for val in array:     print(val)   So, yes, there is a \"foreach\" in python. It's called \"for\".  What you're describing is an \"array map\" function. This could be done with list comprehensions in python:  names = ['tom', 'john', 'simon']  namesCapitalized = [capitalize(n) for n in names]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "foreach"
        ],
        "URL": "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3",
        "A_Votes": "100",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I meet the situation I can do it in javascript, I always think if there's an foreach function it would be convenience. By foreach I mean the function which is described below:  def foreach(fn,iterable):     for x in iterable:         fn(x)   they just do it on every element and didn't yield or return something,i think it should be a built-in function and should be more faster than writing it with pure Python, but I didn't found it on the list,or it just called another name?or I just miss some points here?  Maybe I got wrong, cause calling an function in Python cost high, definitely not a good practice for the example. Rather than an out loop, the function should do the loop in side its body looks like this below which already mentioned in many python's code suggestions:  def fn(*args):     for x in args:        dosomething   but I thought foreach is still welcome base on the two facts:   In normal cases, people just don't care about the performance Sometime the API didn't accept iterable object and you can't rewrite its source.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a 'foreach' function in Python 3?",
        "A_Content": "  Python doesn't have a foreach statement per se. It has for loops built into the language.  for element in iterable:     operate(element)   If you really wanted to, you could define your own foreach function:  def foreach(function, iterable):     for element in iterable:         function(element)   As a side note the for element in iterable syntax comes from the ABC programming language, one of Python's influences.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "foreach"
        ],
        "URL": "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3",
        "A_Votes": "26",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I meet the situation I can do it in javascript, I always think if there's an foreach function it would be convenience. By foreach I mean the function which is described below:  def foreach(fn,iterable):     for x in iterable:         fn(x)   they just do it on every element and didn't yield or return something,i think it should be a built-in function and should be more faster than writing it with pure Python, but I didn't found it on the list,or it just called another name?or I just miss some points here?  Maybe I got wrong, cause calling an function in Python cost high, definitely not a good practice for the example. Rather than an out loop, the function should do the loop in side its body looks like this below which already mentioned in many python's code suggestions:  def fn(*args):     for x in args:        dosomething   but I thought foreach is still welcome base on the two facts:   In normal cases, people just don't care about the performance Sometime the API didn't accept iterable object and you can't rewrite its source.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a 'foreach' function in Python 3?",
        "A_Content": "  Other examples:  Python Foreach Loop:  for val in array:     print(val)   Python For Loop:  for index in range(len(array)):     print(array[index])     index+= 1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "foreach"
        ],
        "URL": "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I meet the situation I can do it in javascript, I always think if there's an foreach function it would be convenience. By foreach I mean the function which is described below:  def foreach(fn,iterable):     for x in iterable:         fn(x)   they just do it on every element and didn't yield or return something,i think it should be a built-in function and should be more faster than writing it with pure Python, but I didn't found it on the list,or it just called another name?or I just miss some points here?  Maybe I got wrong, cause calling an function in Python cost high, definitely not a good practice for the example. Rather than an out loop, the function should do the loop in side its body looks like this below which already mentioned in many python's code suggestions:  def fn(*args):     for x in args:        dosomething   but I thought foreach is still welcome base on the two facts:   In normal cases, people just don't care about the performance Sometime the API didn't accept iterable object and you can't rewrite its source.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a 'foreach' function in Python 3?",
        "A_Content": "  map is preferred in most cases for the situation mentioned in the question.  E.g.  map(len, ['abcd','abc', 'a']) # 4 3 1   For functions that take multiple arguments, more arguments can be given to map:  map(pow, [2, 3], [4,2]) # 16 9   It returns a list in python 2.x and an iterator in python 3  In case your function takes multiple arguments and the arguments are already in the form of tuples (or any iterable since python 2.6) you can use itertools.starmap. (which has a very similar syntax to what you were looking for). It returns an iterator.  E.g.   for num in starmap(pow, [(2,3), (3,2)]):     print(num)   gives us 8 and 9     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "foreach"
        ],
        "URL": "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I meet the situation I can do it in javascript, I always think if there's an foreach function it would be convenience. By foreach I mean the function which is described below:  def foreach(fn,iterable):     for x in iterable:         fn(x)   they just do it on every element and didn't yield or return something,i think it should be a built-in function and should be more faster than writing it with pure Python, but I didn't found it on the list,or it just called another name?or I just miss some points here?  Maybe I got wrong, cause calling an function in Python cost high, definitely not a good practice for the example. Rather than an out loop, the function should do the loop in side its body looks like this below which already mentioned in many python's code suggestions:  def fn(*args):     for x in args:        dosomething   but I thought foreach is still welcome base on the two facts:   In normal cases, people just don't care about the performance Sometime the API didn't accept iterable object and you can't rewrite its source.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a 'foreach' function in Python 3?",
        "A_Content": "  If I understood you right, you mean that if you have a function 'func', you want to check for each item in list if func(item) returns true; if you get true for all, then do something.  You can use 'all'.  For example: I want to get all prime numbers in range 0-10 in a list:  from math import sqrt primes = [x for x in range(10) if x > 2 and all(x % i !=0 for i in range(2, int(sqrt(x)) + 1))]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "foreach"
        ],
        "URL": "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I meet the situation I can do it in javascript, I always think if there's an foreach function it would be convenience. By foreach I mean the function which is described below:  def foreach(fn,iterable):     for x in iterable:         fn(x)   they just do it on every element and didn't yield or return something,i think it should be a built-in function and should be more faster than writing it with pure Python, but I didn't found it on the list,or it just called another name?or I just miss some points here?  Maybe I got wrong, cause calling an function in Python cost high, definitely not a good practice for the example. Rather than an out loop, the function should do the loop in side its body looks like this below which already mentioned in many python's code suggestions:  def fn(*args):     for x in args:        dosomething   but I thought foreach is still welcome base on the two facts:   In normal cases, people just don't care about the performance Sometime the API didn't accept iterable object and you can't rewrite its source.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a 'foreach' function in Python 3?",
        "A_Content": "  Look at this article. The iterator object nditer from numpy package, introduced in NumPy 1.6, provides many flexible ways to visit all the elements of one or more arrays in a systematic fashion.  Example:  import random import numpy as np  ptrs = np.int32([[0, 0], [400, 0], [0, 400], [400, 400]])  for ptr in np.nditer(ptrs, op_flags=['readwrite']):     # apply random shift on 1 for each element of the matrix     ptr += random.choice([-1, 1])  print(ptrs)  d:\\>python nditer.py [[ -1   1]  [399  -1]  [  1 399]  [399 401]]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "foreach"
        ],
        "URL": "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I meet the situation I can do it in javascript, I always think if there's an foreach function it would be convenience. By foreach I mean the function which is described below:  def foreach(fn,iterable):     for x in iterable:         fn(x)   they just do it on every element and didn't yield or return something,i think it should be a built-in function and should be more faster than writing it with pure Python, but I didn't found it on the list,or it just called another name?or I just miss some points here?  Maybe I got wrong, cause calling an function in Python cost high, definitely not a good practice for the example. Rather than an out loop, the function should do the loop in side its body looks like this below which already mentioned in many python's code suggestions:  def fn(*args):     for x in args:        dosomething   but I thought foreach is still welcome base on the two facts:   In normal cases, people just don't care about the performance Sometime the API didn't accept iterable object and you can't rewrite its source.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a 'foreach' function in Python 3?",
        "A_Content": "  This does the foreach in python 3  test=[0,1,2,3,4,5,6,7,8,\"test\"]  for fetch in test:     print(fetch)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x",
            "foreach"
        ],
        "URL": "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    When I meet the situation I can do it in javascript, I always think if there's an foreach function it would be convenience. By foreach I mean the function which is described below:  def foreach(fn,iterable):     for x in iterable:         fn(x)   they just do it on every element and didn't yield or return something,i think it should be a built-in function and should be more faster than writing it with pure Python, but I didn't found it on the list,or it just called another name?or I just miss some points here?  Maybe I got wrong, cause calling an function in Python cost high, definitely not a good practice for the example. Rather than an out loop, the function should do the loop in side its body looks like this below which already mentioned in many python's code suggestions:  def fn(*args):     for x in args:        dosomething   but I thought foreach is still welcome base on the two facts:   In normal cases, people just don't care about the performance Sometime the API didn't accept iterable object and you can't rewrite its source.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Boolean argument for script",
        "A_Content": "  You can either use the action with store_true|store_false, or you can use an int and let implicit casting check a boolean value.  Using the action, you wouldn't pass a --foo=true and --foo=false argument, you would simply include it if it was to be set to true.  python myProgram.py --foo   In fact I think what you may want is  parser.add_argument('-b', action='store_true', default=False)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/9183936/boolean-argument-for-script",
        "A_Votes": "139",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In Python, I understand how int and str arguments can be added to scripts.  parser=argparse.ArgumentParser(description=\"\"\"Mydescription\"\"\") parser.add_argument('-l', type=str, default='info', help='String argument') parser.add_argument('-dt', type=int, default ='', help='int argument')   What is it for booleans?  Basically I want to pass a flag into my script which will tell the script whether to do a specific action or not.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Boolean argument for script",
        "A_Content": "  parser.add_argument('--foo', action='store_true')      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/9183936/boolean-argument-for-script",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, I understand how int and str arguments can be added to scripts.  parser=argparse.ArgumentParser(description=\"\"\"Mydescription\"\"\") parser.add_argument('-l', type=str, default='info', help='String argument') parser.add_argument('-dt', type=int, default ='', help='int argument')   What is it for booleans?  Basically I want to pass a flag into my script which will tell the script whether to do a specific action or not.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Boolean argument for script",
        "A_Content": "  import distutils.util ARGP.add_argument('--on', '-o', type=distutils.util.strtobool, default='true')   Example calling it:  $ ./myscript                # argp.on = 1 $ ./myscript --on=false     # argp.on = 0 $ ./myscript --on=False     # argp.on = 0 $ ./myscript --on=0         # argp.on = 0 $ ./myscript --on=1         # argp.on = 1 $ ./myscript -o0            # argp.on = 0 $ ./myscript -o false       # argp.on == 0   i should mention, you can bind the argument to a local wrapper function too, to handle some other exact string matching if you want to support values like \"yes\" and \"no\". you can also try interpreting the input as yaml, which can handle yes/no too. i haven't done this in a while though, and i think lately I've suck to mutually exclusive arguments with the same dest value, one --no-option with action='store_false', and one --option with action='store_true'     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/9183936/boolean-argument-for-script",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, I understand how int and str arguments can be added to scripts.  parser=argparse.ArgumentParser(description=\"\"\"Mydescription\"\"\") parser.add_argument('-l', type=str, default='info', help='String argument') parser.add_argument('-dt', type=int, default ='', help='int argument')   What is it for booleans?  Basically I want to pass a flag into my script which will tell the script whether to do a specific action or not.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  Steps to install PyGame using pip   Install build dependencies (on linux):  sudo apt-get build-dep python-pygame  Install mercurial to use hg (on linux):  sudo apt-get install mercurial   On Windows you can use the installer: Download Use pip to install PyGame:  pip install hg+http://bitbucket.org/pygame/pygame   If the above gives freetype-config: not found error (on Linux), then try sudo apt-get install libfreetype6-dev and then repeat 3.   Alternative way:  # Grab source hg clone https://bitbucket.org/pygame/pygame  # Finally build and install cd pygame python setup.py build sudo python setup.py install      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "91",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  Try doing this:  sudo apt-get install mercurial sudo pip install hg+http://bitbucket.org/pygame/pygame      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  An update to this may be required, as it stands in version 1.9.1 it should simply install using:  pip install pygame  However, it look like there is a bug with their pypi repository, see: https://bitbucket.org/pygame/pygame/issues/59/pygame-has-no-pypi-page-and-cant-be  So, if you want the most recent release, you have to point directly at the ftp file ala:  pip install http://www.pygame.org/ftp/pygame-1.9.1release.tar.gz  I suppose this will be fixed in the 1.9.2 release but for now this works.  I would note that the answer supplied by Pratyush works as well, but requires the user to install mercurial if they don't have it and downloads the trunk version, so really, not ideal unless you absolutely need it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  Caveat: I'm not familiar with the Enthought Distribution, so this might not help.  As you're trying to install on Windows, if you don't want to have to mess around with C compilers, there are pre-built binary wheels for pygame here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#pygame  Select a package appropriate to you python version[0] and Windows architecture [1]. Download to C:\\Users\\User\\Download\\pip install E:\\env\\pygame-1.9.2a0-cp27-none-win_amd64.whl and install with pip install E:\\env\\pygame-1.9.2a0-cp27-none-win_amd64.whl  Mercurial binaries can be found on the same page, if you would like to install from source. This method would mean compiling pygame from source, for which you probably want to use this compiler package.  [0] python --version  [1] powershell \"gwmi win32_operatingsystem | select osarchitecture\"     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  Install on MAC:  brew install homebrew/python/pygame      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  The command below worked for me on Mac OS X El Capitan:  pip3 install pygame     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  Try  python3 -m pip install -U pygame --user   This worked twice for me on windows 7 and windows 10     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  Just   sudo pip install pygame   worked for me     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  Had this issue on macOS Sierra, where apt-get doesn't work.   Managed to solve the issue through the following steps:  First I had to install the Mercurial via Brew:  brew install mercurial   Then, I had to install Pygame dependencies:  brew install sdl sdl_image sdl_mixer sdl_ttf smpeg portmidi   Finally I used pip3 to install Pygame:  pip3 install pygame   Hope this helps!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  The most current, best way to install pygame is always available at: https://www.pygame.org/wiki/GettingStarted  How to use pip depends on the operating system. So unless you have always updating and tested answers for 15 different operating systems then just send people to that page. All answers to this question are wrong for various different operating systems.  Currently, for windows, this is the way to install it in the cmd prompt. (If you already have pip installed, and people know what pip is... best just send people to the GettingStarted page).  py -m pip install pygame --user     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  I did use these instructions  here  from the official site. But i had to change the python3 part of the command to just py.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Unable to install Pygame using pip",
        "A_Content": "  I have never used Enthought distribution, but you could try to use Anaconda distribution which is also great too.  Anaconda Distribution (Any OS):  From the docs:     Update conda to the current version.   Type the following:    conda update conda      If a newer version of conda is available, type y to update:   Proceed ([y]/n)? y      Check to see if a package you have not installed named   pygame is available from the Anaconda repository (must be   connected to the Internet):   conda search pygame      Conda displays a list of all packages with   that name on the Anaconda repository, so we know it is available.      Install this package into the current environment:   conda install pygame      Check to see if the newly installed   program is in this environment:   conda list   Install on Windows (both python 2.7 and 3.7):  pip install pygame    Install on raspberry pi:  sudo pip3 install pygame (python 3.7) sudo pip install pygame (python 2.7)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pygame",
            "install",
            "pip"
        ],
        "URL": "https://stackoverflow.com/questions/17869101/unable-to-install-pygame-using-pip",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to install Pygame. I am running Windows 7 with Enthought Python Distribution. I successfully installed pip, but when I try to install Pygame using pip, I get the following error:     \"Could not install requirement Pygame because of HTTP error HTTP error   400: Bad request for URL ...\"   I can't find anything about this issue with a Google search, but I did find another Stack Overflow question that prompted the asker to use the following command:  pip install hg+http://bitbucket.org/pygame/pygame   This gave me the following error:  Cannot find command hg   I'm not sure what else to do, as everything I find with a Google search is for Mac, so I don't know how well I can follow those instructions on Windows.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to remove leading and trailing zeros in a string? Python",
        "A_Content": "  What about a basic  your_string.strip(\"0\")   to remove both trailing and leading zeros ? If you're only interested in removing trailing zeros, use .rstrip instead (and .lstrip for only the leading ones).  [More info in the doc.]  You could use some list comprehension to get the sequences you want like so:  trailing_removed = [s.rstrip(\"0\") for s in listOfNum] leading_removed = [s.lstrip(\"0\") for s in listOfNum] both_removed = [s.strip(\"0\") for s in listOfNum]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "trailing",
            "chomp",
            "leading-zero"
        ],
        "URL": "https://stackoverflow.com/questions/13142347/how-to-remove-leading-and-trailing-zeros-in-a-string-python",
        "A_Votes": "146",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have several alphanumeric strings like these  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', '000alphanumeric']   The desired output for removing trailing zeros would be:  listOfNum = ['000231512-n','1209123100000-n','alphanumeric', '000alphanumeric']   The desired output for leading trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   The desire output for removing both leading and trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   For now i've been doing it the following way, please suggest a better way if there is:  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', \\ '000alphanumeric'] trailingremoved = [] leadingremoved = [] bothremoved = []  # Remove trailing for i in listOfNum:   while i[-1] == \"0\":     i = i[:-1]   trailingremoved.append(i)  # Remove leading for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   leadingremoved.append(i)  # Remove both for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   while i[-1] == \"0\":     i = i[:-1]   bothremoved.append(i)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to remove leading and trailing zeros in a string? Python",
        "A_Content": "  Remove leading + trailing '0':  list = [i.strip('0') for i in listOfNum ]   Remove leading '0':  list = [ i.lstrip('0') for i in listOfNum ]   Remove trailing '0':  list = [ i.rstrip('0') for i in listOfNum ]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "trailing",
            "chomp",
            "leading-zero"
        ],
        "URL": "https://stackoverflow.com/questions/13142347/how-to-remove-leading-and-trailing-zeros-in-a-string-python",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have several alphanumeric strings like these  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', '000alphanumeric']   The desired output for removing trailing zeros would be:  listOfNum = ['000231512-n','1209123100000-n','alphanumeric', '000alphanumeric']   The desired output for leading trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   The desire output for removing both leading and trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   For now i've been doing it the following way, please suggest a better way if there is:  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', \\ '000alphanumeric'] trailingremoved = [] leadingremoved = [] bothremoved = []  # Remove trailing for i in listOfNum:   while i[-1] == \"0\":     i = i[:-1]   trailingremoved.append(i)  # Remove leading for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   leadingremoved.append(i)  # Remove both for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   while i[-1] == \"0\":     i = i[:-1]   bothremoved.append(i)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to remove leading and trailing zeros in a string? Python",
        "A_Content": "  Did you try with strip() :  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric'] print [item.strip('0') for item in listOfNum]  >>> ['231512-n', '1209123100000-n', 'alphanumeric', 'alphanumeric']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "trailing",
            "chomp",
            "leading-zero"
        ],
        "URL": "https://stackoverflow.com/questions/13142347/how-to-remove-leading-and-trailing-zeros-in-a-string-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have several alphanumeric strings like these  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', '000alphanumeric']   The desired output for removing trailing zeros would be:  listOfNum = ['000231512-n','1209123100000-n','alphanumeric', '000alphanumeric']   The desired output for leading trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   The desire output for removing both leading and trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   For now i've been doing it the following way, please suggest a better way if there is:  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', \\ '000alphanumeric'] trailingremoved = [] leadingremoved = [] bothremoved = []  # Remove trailing for i in listOfNum:   while i[-1] == \"0\":     i = i[:-1]   trailingremoved.append(i)  # Remove leading for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   leadingremoved.append(i)  # Remove both for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   while i[-1] == \"0\":     i = i[:-1]   bothremoved.append(i)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to remove leading and trailing zeros in a string? Python",
        "A_Content": "  You can simply do this with a bool:  if int(number) == float(number):     number = int(number)  else:     number = float(number)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "trailing",
            "chomp",
            "leading-zero"
        ],
        "URL": "https://stackoverflow.com/questions/13142347/how-to-remove-leading-and-trailing-zeros-in-a-string-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have several alphanumeric strings like these  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', '000alphanumeric']   The desired output for removing trailing zeros would be:  listOfNum = ['000231512-n','1209123100000-n','alphanumeric', '000alphanumeric']   The desired output for leading trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   The desire output for removing both leading and trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   For now i've been doing it the following way, please suggest a better way if there is:  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', \\ '000alphanumeric'] trailingremoved = [] leadingremoved = [] bothremoved = []  # Remove trailing for i in listOfNum:   while i[-1] == \"0\":     i = i[:-1]   trailingremoved.append(i)  # Remove leading for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   leadingremoved.append(i)  # Remove both for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   while i[-1] == \"0\":     i = i[:-1]   bothremoved.append(i)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to remove leading and trailing zeros in a string? Python",
        "A_Content": "  strip is the best approach for this situation, but more_itertools.strip is a general solution that strips leading and trailing elements from an iterable:  import more_itertools as mit  iterables = [\"231512-n\\n\",\"  12091231000-n00000\",\"alphanum0000\", \"00alphanum\"] pred = lambda x: x in {\"0\", \"\\n\", \" \"} list(\"\".join(mit.strip(i, pred)) for i in iterables) # ['231512-n', '12091231000-n', 'alphanum', 'alphanum']   Notice, here we strip both leading and trailing \"0\"s among other elements that satisfy a predicate.  This tool is not limited to strings.  See docs for more examples.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "trailing",
            "chomp",
            "leading-zero"
        ],
        "URL": "https://stackoverflow.com/questions/13142347/how-to-remove-leading-and-trailing-zeros-in-a-string-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have several alphanumeric strings like these  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', '000alphanumeric']   The desired output for removing trailing zeros would be:  listOfNum = ['000231512-n','1209123100000-n','alphanumeric', '000alphanumeric']   The desired output for leading trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   The desire output for removing both leading and trailing zeros would be:  listOfNum = ['231512-n','1209123100000-n00000','alphanumeric0000', 'alphanumeric']   For now i've been doing it the following way, please suggest a better way if there is:  listOfNum = ['000231512-n','1209123100000-n00000','alphanumeric0000', \\ '000alphanumeric'] trailingremoved = [] leadingremoved = [] bothremoved = []  # Remove trailing for i in listOfNum:   while i[-1] == \"0\":     i = i[:-1]   trailingremoved.append(i)  # Remove leading for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   leadingremoved.append(i)  # Remove both for i in listOfNum:   while i[0] == \"0\":     i = i[1:]   while i[-1] == \"0\":     i = i[:-1]   bothremoved.append(i)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  Could it be more useful for you to use the length of the list len(n) to inform your decision rather than checking n[i] for each possible length?      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "89",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  I need to code such that if a certain list index exists, then run a function.  This is the perfect use for a try block:  ar=[1,2,3]  try:     t=ar[5] except IndexError:     print 'sorry, no 5'       However, by definition, all items in a Python list between 0 and len(the_list)-1 exist (i.e., there is no need for a try, except if you know 0 <= index < len(the_list)).   You can use enumerate if you want the indexes between 0 and the last element:  names=['barney','fred','dino']  for i, name in enumerate(names):     print i, name     if i in (3,4):         # do your thing with the index 'i' or value 'name' for each item...   If you are looking for some defined 'index' thought, I think you are asking the wrong question. Perhaps you should consider using a mapping container (such as a dict) versus a sequence container (such as a list). You could rewrite your code like this:  def do_something(name):     print 'some thing 1 done with',name  def do_something_else(name):     print 'something 2 done with',name          def default(name):     print 'nothing done with',name       something_to_do={       3: do_something,             4: do_something_else     }          n = input (\"Define number of actors: \") count = 0 names = []  for count in range(n):     print \"Define name for actor {}:\".format(count+1),     name = raw_input ()     names.append(name)  for name in names:     try:         something_to_do[len(name)](name)     except KeyError:         default(name)   Runs like this:  Define number of actors: 3 Define name for actor 1: bob Define name for actor 2: tony Define name for actor 3: alice some thing 1 done with bob something 2 done with tony nothing done with alice   You can also use .get method rather than try/except for a shorter version:  >>> something_to_do.get(3, default)('bob') some thing 1 done with bob >>> something_to_do.get(22, default)('alice') nothing done with alice      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "55",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  len(nams) should be equal to n in your code. All indexes 0 <= i < n \"exist\".     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "     I need to code such that if a certain list index exists, then run a function.   You already know how to test for this and in fact are already performing such tests in your code.  The valid indices for a list of length n are 0 through n-1 inclusive.  Thus, a list has an index i if and only if the length of the list is at least i + 1.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  Using the length of the list would be the fastest solution to check if an index exists:  def index_exists(ls, i):     return (0 <= i < len(ls)) or (-len(ls) <= i < 0)   This also tests for negative indices, and most sequence types (Like ranges and strs) that have a length.  If you need to access the item at that index afterwards anyways, it is easier to ask forgiveness than permission, and it is also faster and more Pythonic. Use try: except:.  try:     item = ls[i]     # Do something with item except IndexError:     # Do something without the item   This would be as opposed to:  if index_exists(ls, i):     item = ls[i]     # Do something with item else:     # Do something without the item      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  If you want to iterate the inserted actors data:  for i in range(n):     if len(nams[i]) > 3:         do_something     if len(nams[i]) > 4:         do_something_else      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  It can be done simply using the following code:  if index < len(my_list):     print(index, ' exists in the list') else:     print(index, \" doesn't exist in the list\")      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  ok, so I think it's actually possible (for the sake of argument):  >>> your_list = [5,6,7] >>> 2 in zip(*enumerate(your_list))[0] True >>> 3 in zip(*enumerate(your_list))[0] False      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  You can try something like this   list = [\"a\", \"b\", \"C\", \"d\", \"e\", \"f\", \"r\"]  for i in range(0, len(list), 2):     print list[i]     if len(list) % 2 == 1 and  i == len(list)-1:         break     print list[i+1];      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "If list index exists, do X",
        "A_Content": "  Do not let any space in front of your brackets.  Example:  n = input ()          ^   Tip: You should add comments over and/or under your code. Not behind your code.    Have a nice day.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7"
        ],
        "URL": "https://stackoverflow.com/questions/11786157/if-list-index-exists-do-x",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In my program, user inputs number n, and then inputs n number of strings, which get stored in a list.  I need to code such that if a certain list index exists, then run a function.  This is made more complicated by the fact that I have nested if statements about len(my_list).  Here's a simplified version of what I have now, which isn't working:  n = input (\"Define number of actors: \")  count = 0  nams = []  while count < n:     count = count + 1     print \"Define name for actor \", count, \":\"     name = raw_input ()     nams.append(name)  if nams[2]: #I am trying to say 'if nams[2] exists, do something depending on len(nams)     if len(nams) > 3:         do_something     if len(nams) > 4         do_something_else  if nams[3]: #etc.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  No, it's not possible (at least not with arbitrary statements), nor is it desirable. Fitting everything on one line would most likely violate PEP-8 where it is mandated that lines should not exceed 80 characters in length.  It's also against the Zen of Python: \"Readability counts\". (Type import this at the Python prompt to read the whole thing).  You can use a ternary expression in Python, but only for expressions, not for statements:  >>> a = \"Hello\" if foo() else \"Goodbye\"   Edit:  Your revised question now shows that the three statements are identical except for the value being assigned. In that case, a chained ternary operator does work, but I still think that it's less readable:  >>> i=100 >>> a = 1 if i<100 else 2 if i>100 else 0 >>> a 0 >>> i=101 >>> a = 1 if i<100 else 2 if i>100 else 0 >>> a 2 >>> i=99 >>> a = 1 if i<100 else 2 if i>100 else 0 >>> a 1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "110",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  If you only need different expressions for different cases then this may work for you:  expr1 if condition1 else expr2 if condition2 else expr   For example:  a = \"neg\" if b<0 else \"pos\" if b>0 else \"zero\"      ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "40",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  Just nest another if clause in the else statement.  But that doesn't make it look any prettier.  >>> x=5 >>> x if x>0 else (\"zero\" if x==0 else \"invalid value\") 5 >>> x = 0 >>> x if x>0 else (\"zero\" if x==0 else \"invalid value\") 'zero' >>> x = -1 >>> x if x>0 else (\"zero\" if x==0 else \"invalid value\") 'invalid value'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  There's an alternative that's quite unreadable in my opinion but I'll share anyway just as a curiosity:  x = (i>100 and 2) or (i<100 and 1) or 0   More info here: https://docs.python.org/3/library/stdtypes.html#boolean-operations-and-or-not     ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  Well, I came across similar problems months ago, then one day I realized you can actually take advantage of get method of a dict object.   x = {i>100: 2, i<100: 1}.get(True, 0)   Basically i>100 and i<100 cannot both be true, so if either of them is True you get the value you want(2 or 1), if none of them is True, you get 0.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  It also depends  on teh nature of your expressions. The general advice on the other answers of \"not doing it\" is quite valid for generic statements and generic expressions.  But if all you need is a \"dispacth\" table, like, calling a different function depending on the value of a given option, you can put the functions to call inside a dictionary.   Something like:  def save():     ... def edit():    ... options = {\"save\": save, \"edit\": edit, \"remove\": lambda : \"Not Implemented\"}  option = get_input() result = options[option]()   (that  instead of  if option==\"save\":     save() ...   )     ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  People have already mentioned ternary expressions. Sometimes with a simple conditional assignment as your example, it is possible to use a mathematical expression to perform the conditional assignment. This may not make your code very readable, but it does get it on one fairly short line. Your example could be written like this:  x = 2*(i>100) | 1*(i<100)   The comparisons would be True or False, and when multiplying with numbers would then be either 1 or 0. One could use a + instead of an | in the middle.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Putting an if-elif-else statement on one line?",
        "A_Content": "  You can use nested ternary if statements.  # if-else ternary construct country_code = 'USA' is_USA = True if country_code == 'USA' else False print('is_USA:', is_USA)  # if-elif-else ternary construct # Create function to avoid repeating code. def get_age_category_name(age):     age_category_name = 'Young' if age <= 40 else ('Middle Aged' if age > 40 and age <= 65 else 'Senior')     return age_category_name  print(get_age_category_name(25)) print(get_age_category_name(50)) print(get_age_category_name(75))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "syntax"
        ],
        "URL": "https://stackoverflow.com/questions/14029245/putting-an-if-elif-else-statement-on-one-line",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have read the links below, but it doesn't address my question. Does Python have a ternary conditional operator? (the question is about  condensing if-else statement to one line)  Is there an easier way of writing an if-elif-else statement so it fits on one line? For example,  if expression1:    statement1 elif expression2:    statement2 else:    statement3   [UPDATE]  if i>100:     x=2 elif i<100:     x=1 else:     x=0   I just feel if the example above could be written the following way, it could look like more concise.  x=2 if i>100 elif i<100 1 else 0 [WRONG]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I pass extra arguments to a Python decorator?",
        "A_Content": "  Since you are calling the decorator like a function, it needs to return another function which is the actual decorator:  def my_decorator(param):     def actual_decorator(func):         print(\"Decorating function {}, with parameter {}\".format(func.__name__, param))         return function_wrapper(func)  # assume we defined a wrapper somewhere     return actual_decorator   The outer function will be given any arguments you pass explicitly, and should return the inner function. The inner function will be passed the function to decorate, and return the modified function.  Usually you want the decorator to change the function behavior by wrapping it in a wrapper function. Here's an example that optionally adds logging when the function is called:  def log_decorator(log_enabled):     def actual_decorator(func):         @functools.wraps(func)         def wrapper(*args, **kwargs):             if log_enabled:                 print(\"Calling Function: \" + func.__name__)             return func(*args, **kwargs)         return wrapper     return actual_decorator   The functools.wraps call copies things like the name and docstring to the wrapper function, to make it more similar to the original function.  Example usage:  >>> @log_decorator(True) ... def f(x): ...     return x+1 ... >>> f(4) Calling Function: f 5      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "python-decorators"
        ],
        "URL": "https://stackoverflow.com/questions/10176226/how-do-i-pass-extra-arguments-to-a-python-decorator",
        "A_Votes": "121",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a decorator like below.  def myDecorator(test_func):     return callSomeWrapper(test_func) def callSomeWrapper(test_func):     return test_func @myDecorator def someFunc():     print 'hello'   I want to enhance this decorator to accept another argument like below  def myDecorator(test_func,logIt):     if logIt:         print \"Calling Function: \" + test_func.__name__     return callSomeWrapper(test_func) @myDecorator(False) def someFunc():     print 'Hello'   But this code gives the error,     TypeError: myDecorator() takes exactly 2 arguments (1 given)   Why is the function not automatically passed? How do I explicitly pass the function to the decorator function?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I pass extra arguments to a Python decorator?",
        "A_Content": "  Just to provide a different viewpoint: the syntax  @expr def func(...): #stuff   is equivalent to   def func(...): #stuff func = expr(func)   In particular, expr can be anything you like, as long as it evaluates to a callable. In particular particular, expr can be a decorator factory: you give it some parameters and it gives you a decorator. So maybe a better way to understand your situation is as  dec = decorator_factory(*args) @dec def func(...):   which can then be shortened to  @decorator_factory(*args) def func(...):   Of course, since it looks like decorator_factory is a decorator, people tend to name it to reflect that. Which can be confusing when you try to follow the levels of indirection.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "python-decorators"
        ],
        "URL": "https://stackoverflow.com/questions/10176226/how-do-i-pass-extra-arguments-to-a-python-decorator",
        "A_Votes": "36",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a decorator like below.  def myDecorator(test_func):     return callSomeWrapper(test_func) def callSomeWrapper(test_func):     return test_func @myDecorator def someFunc():     print 'hello'   I want to enhance this decorator to accept another argument like below  def myDecorator(test_func,logIt):     if logIt:         print \"Calling Function: \" + test_func.__name__     return callSomeWrapper(test_func) @myDecorator(False) def someFunc():     print 'Hello'   But this code gives the error,     TypeError: myDecorator() takes exactly 2 arguments (1 given)   Why is the function not automatically passed? How do I explicitly pass the function to the decorator function?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I pass extra arguments to a Python decorator?",
        "A_Content": "  Just want to add some usefull trick that will allow to make decorator arguments optional. It will also alows to reuse decorator and decrease nesting  import functools  def myDecorator(test_func=None,logIt=None):     if not test_func:         return functools.partial(myDecorator, logIt=logIt)     @functools.wraps(test_func)     def f(*args, **kwargs):         if logIt==1:             print 'Logging level 1 for {}'.format(test_func.__name__)         if logIt==2:             print 'Logging level 2 for {}'.format(test_func.__name__)         return test_func(*args, **kwargs)     return f  #new decorator  myDecorator_2 = myDecorator(logIt=2)  @myDecorator(logIt=2) def pow2(i):     return i**2  @myDecorator def pow3(i):     return i**3  @myDecorator_2 def pow4(i):     return i**4  print pow2(2) print pow3(2) print pow4(2)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "python-decorators"
        ],
        "URL": "https://stackoverflow.com/questions/10176226/how-do-i-pass-extra-arguments-to-a-python-decorator",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a decorator like below.  def myDecorator(test_func):     return callSomeWrapper(test_func) def callSomeWrapper(test_func):     return test_func @myDecorator def someFunc():     print 'hello'   I want to enhance this decorator to accept another argument like below  def myDecorator(test_func,logIt):     if logIt:         print \"Calling Function: \" + test_func.__name__     return callSomeWrapper(test_func) @myDecorator(False) def someFunc():     print 'Hello'   But this code gives the error,     TypeError: myDecorator() takes exactly 2 arguments (1 given)   Why is the function not automatically passed? How do I explicitly pass the function to the decorator function?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How do I pass extra arguments to a Python decorator?",
        "A_Content": "  Just another way of doing decorators. I find this way the easiest to wrap my head around.  import functools  class NiceDecorator:     def __init__(self, param_foo='a', param_bar='b'):         self.param_foo = param_foo         self.param_bar = param_bar      def __call__(self, func):         @functools.wraps(func)         def my_logic(*args, **kwargs):             # whatever logic your decorator is supposed to implement goes in here             print('pre action baz')             print(self.param_bar)             # including the call to the decorated function (if you want to do that)             result = func(*args, **kwargs)             print('post action beep')             return result          return my_logic  # usage example from here on @NiceDecorator(param_bar='baaar') def example():     print('example yay')   example()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-2.7",
            "python-decorators"
        ],
        "URL": "https://stackoverflow.com/questions/10176226/how-do-i-pass-extra-arguments-to-a-python-decorator",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a decorator like below.  def myDecorator(test_func):     return callSomeWrapper(test_func) def callSomeWrapper(test_func):     return test_func @myDecorator def someFunc():     print 'hello'   I want to enhance this decorator to accept another argument like below  def myDecorator(test_func,logIt):     if logIt:         print \"Calling Function: \" + test_func.__name__     return callSomeWrapper(test_func) @myDecorator(False) def someFunc():     print 'Hello'   But this code gives the error,     TypeError: myDecorator() takes exactly 2 arguments (1 given)   Why is the function not automatically passed? How do I explicitly pass the function to the decorator function?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas groupby: How to get a union of strings",
        "A_Content": "  In [4]: df = read_csv(StringIO(data),sep='\\s+')  In [5]: df Out[5]:     A         B       C 0  1  0.749065    This 1  2  0.301084      is 2  3  0.463468       a 3  4  0.643961  random 4  1  0.866521  string 5  2  0.120737       !  In [6]: df.dtypes Out[6]:  A      int64 B    float64 C     object dtype: object   When you apply your own function, there is not automatic exclusions of non-numeric columns. This is slower, though, than the application of .sum() to the groupby  In [8]: df.groupby('A').apply(lambda x: x.sum()) Out[8]:     A         B           C A                          1  2  1.615586  Thisstring 2  4  0.421821         is! 3  3  0.463468           a 4  4  0.643961      random   sum by default concatenates  In [9]: df.groupby('A')['C'].apply(lambda x: x.sum()) Out[9]:  A 1    Thisstring 2           is! 3             a 4        random dtype: object   You can do pretty much what you want  In [11]: df.groupby('A')['C'].apply(lambda x: \"{%s}\" % ', '.join(x)) Out[11]:  A 1    {This, string} 2           {is, !} 3               {a} 4          {random} dtype: object   Doing this a whole frame group at a time. Key is to return a Series  def f(x):      return Series(dict(A = x['A'].sum(),                          B = x['B'].sum(),                          C = \"{%s}\" % ', '.join(x['C'])))  In [14]: df.groupby('A').apply(f) Out[14]:     A         B               C A                              1  2  1.615586  {This, string} 2  4  0.421821         {is, !} 3  3  0.463468             {a} 4  4  0.643961        {random}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings",
        "A_Votes": "118",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a dataframe like this:     A         B       C 0  1  0.749065    This 1  2  0.301084      is 2  3  0.463468       a 3  4  0.643961  random 4  1  0.866521  string 5  2  0.120737       !   Calling   In [10]: print df.groupby(\"A\")[\"B\"].sum()   will return   A 1    1.615586 2    0.421821 3    0.463468 4    0.643961   Now I would like to do \"the same\" for column \"C\". Because that column contains strings, sum() doesn't work (although you might think that it would concatenate the strings). What I would really like to see is a list or set of the strings for each group, i.e.   A 1    {This, string} 2    {is, !} 3    {a} 4    {random}   I have been trying to find ways to do this.   Series.unique() (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) doesn't work, although  df.groupby(\"A\")[\"B\"]   is a  pandas.core.groupby.SeriesGroupBy object   so I was hoping any Series method would work. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas groupby: How to get a union of strings",
        "A_Content": "  You can use the apply method to apply an arbitrary function to the grouped data.  So if you want a set, apply set.  If you want a list, apply list.  >>> d    A       B 0  1    This 1  2      is 2  3       a 3  4  random 4  1  string 5  2       ! >>> d.groupby('A')['B'].apply(list) A 1    [This, string] 2           [is, !] 3               [a] 4          [random] dtype: object   If you want something else, just write a function that does what you want and then apply that.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings",
        "A_Votes": "42",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe like this:     A         B       C 0  1  0.749065    This 1  2  0.301084      is 2  3  0.463468       a 3  4  0.643961  random 4  1  0.866521  string 5  2  0.120737       !   Calling   In [10]: print df.groupby(\"A\")[\"B\"].sum()   will return   A 1    1.615586 2    0.421821 3    0.463468 4    0.643961   Now I would like to do \"the same\" for column \"C\". Because that column contains strings, sum() doesn't work (although you might think that it would concatenate the strings). What I would really like to see is a list or set of the strings for each group, i.e.   A 1    {This, string} 2    {is, !} 3    {a} 4    {random}   I have been trying to find ways to do this.   Series.unique() (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) doesn't work, although  df.groupby(\"A\")[\"B\"]   is a  pandas.core.groupby.SeriesGroupBy object   so I was hoping any Series method would work. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas groupby: How to get a union of strings",
        "A_Content": "  You may be able to use the aggregate (or agg) function to concatenate the values. (Untested code)  df.groupby('A')['B'].agg(lambda col: ''.join(col))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe like this:     A         B       C 0  1  0.749065    This 1  2  0.301084      is 2  3  0.463468       a 3  4  0.643961  random 4  1  0.866521  string 5  2  0.120737       !   Calling   In [10]: print df.groupby(\"A\")[\"B\"].sum()   will return   A 1    1.615586 2    0.421821 3    0.463468 4    0.643961   Now I would like to do \"the same\" for column \"C\". Because that column contains strings, sum() doesn't work (although you might think that it would concatenate the strings). What I would really like to see is a list or set of the strings for each group, i.e.   A 1    {This, string} 2    {is, !} 3    {a} 4    {random}   I have been trying to find ways to do this.   Series.unique() (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) doesn't work, although  df.groupby(\"A\")[\"B\"]   is a  pandas.core.groupby.SeriesGroupBy object   so I was hoping any Series method would work. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas groupby: How to get a union of strings",
        "A_Content": "  a simple solution would be :  >>> df.groupby(['A','B']).c.unique().reset_index()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe like this:     A         B       C 0  1  0.749065    This 1  2  0.301084      is 2  3  0.463468       a 3  4  0.643961  random 4  1  0.866521  string 5  2  0.120737       !   Calling   In [10]: print df.groupby(\"A\")[\"B\"].sum()   will return   A 1    1.615586 2    0.421821 3    0.463468 4    0.643961   Now I would like to do \"the same\" for column \"C\". Because that column contains strings, sum() doesn't work (although you might think that it would concatenate the strings). What I would really like to see is a list or set of the strings for each group, i.e.   A 1    {This, string} 2    {is, !} 3    {a} 4    {random}   I have been trying to find ways to do this.   Series.unique() (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) doesn't work, although  df.groupby(\"A\")[\"B\"]   is a  pandas.core.groupby.SeriesGroupBy object   so I was hoping any Series method would work. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Usage of unicode() and encode() functions in Python",
        "A_Content": "  You are using encode(\"utf-8\") incorrectly.  Python byte strings (str type) have an encoding, Unicode does not.  You can convert a Unicode string to a Python byte string using uni.encode(encoding), and you can convert a byte string to a Unicode string using s.decode(encoding) (or equivalently, unicode(s, encoding)).  If fullFilePath and path are currently a str type, you should figure out how they are encoded.  For example, if the current encoding is utf-8, you would use:  path = path.decode('utf-8') fullFilePath = fullFilePath.decode('utf-8')   If this doesn't fix it, the actual issue may be that you are not using a Unicode string in your execute() call, try changing it to the following:  cur.execute(u\"update docs set path = :fullFilePath where path = :path\", locals())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "sqlite",
            "unicode",
            "encoding"
        ],
        "URL": "https://stackoverflow.com/questions/10288016/usage-of-unicode-and-encode-functions-in-python",
        "A_Votes": "76",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a problem with encoding of the path variable and inserting it to the SQLite database. I tried to solve it with encode(\"utf-8\") function which didn't help. Then I used unicode() function which gives me type unicode.  print type(path)                  # <type 'unicode'> path = path.replace(\"one\", \"two\") # <type 'str'> path = path.encode(\"utf-8\")       # <type 'str'> strange path = unicode(path)              # <type 'unicode'>   Finally I gained unicode type, but I still have the same error which was present when the type of the path variable was str     sqlite3.ProgrammingError: You must not use 8-bit bytestrings unless   you use a text_factory that can interpret 8-bit bytestrings (like   text_factory = str). It is highly recommended that you instead just   switch your application to Unicode strings.   Could you help me solve this error and explain the correct usage of encode(\"utf-8\") and unicode() functions? I'm often fighting with it.  EDIT:  This execute() statement raised the error:  cur.execute(\"update docs set path = :fullFilePath where path = :path\", locals())   I forgot to change the encoding of fullFilePath variable which suffers with the same problem, but I'm quite confused now. Should I use only unicode() or encode(\"utf-8\") or both?  I can't use  fullFilePath = unicode(fullFilePath.encode(\"utf-8\"))   because it raises this error:     UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position   32: ordinal not in range(128)   Python version is 2.7.2     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Usage of unicode() and encode() functions in Python",
        "A_Content": "  str is text representation in bytes, unicode is text representation in characters.  You decode text from bytes to unicode and encode a unicode into bytes with some encoding.  That is:  >>> 'abc'.decode('utf-8')  # str to unicode u'abc' >>> u'abc'.encode('utf-8') # unicode to str 'abc'       ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "sqlite",
            "unicode",
            "encoding"
        ],
        "URL": "https://stackoverflow.com/questions/10288016/usage-of-unicode-and-encode-functions-in-python",
        "A_Votes": "101",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a problem with encoding of the path variable and inserting it to the SQLite database. I tried to solve it with encode(\"utf-8\") function which didn't help. Then I used unicode() function which gives me type unicode.  print type(path)                  # <type 'unicode'> path = path.replace(\"one\", \"two\") # <type 'str'> path = path.encode(\"utf-8\")       # <type 'str'> strange path = unicode(path)              # <type 'unicode'>   Finally I gained unicode type, but I still have the same error which was present when the type of the path variable was str     sqlite3.ProgrammingError: You must not use 8-bit bytestrings unless   you use a text_factory that can interpret 8-bit bytestrings (like   text_factory = str). It is highly recommended that you instead just   switch your application to Unicode strings.   Could you help me solve this error and explain the correct usage of encode(\"utf-8\") and unicode() functions? I'm often fighting with it.  EDIT:  This execute() statement raised the error:  cur.execute(\"update docs set path = :fullFilePath where path = :path\", locals())   I forgot to change the encoding of fullFilePath variable which suffers with the same problem, but I'm quite confused now. Should I use only unicode() or encode(\"utf-8\") or both?  I can't use  fullFilePath = unicode(fullFilePath.encode(\"utf-8\"))   because it raises this error:     UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position   32: ordinal not in range(128)   Python version is 2.7.2     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Usage of unicode() and encode() functions in Python",
        "A_Content": "  Make sure you've set your locale settings right before running the script from the shell, e.g.  $ locale -a | grep \"^en_.\\+UTF-8\" en_GB.UTF-8 en_US.UTF-8 $ export LC_ALL=en_GB.UTF-8 $ export LANG=en_GB.UTF-8   Docs: man locale, man setlocale.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string",
            "sqlite",
            "unicode",
            "encoding"
        ],
        "URL": "https://stackoverflow.com/questions/10288016/usage-of-unicode-and-encode-functions-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a problem with encoding of the path variable and inserting it to the SQLite database. I tried to solve it with encode(\"utf-8\") function which didn't help. Then I used unicode() function which gives me type unicode.  print type(path)                  # <type 'unicode'> path = path.replace(\"one\", \"two\") # <type 'str'> path = path.encode(\"utf-8\")       # <type 'str'> strange path = unicode(path)              # <type 'unicode'>   Finally I gained unicode type, but I still have the same error which was present when the type of the path variable was str     sqlite3.ProgrammingError: You must not use 8-bit bytestrings unless   you use a text_factory that can interpret 8-bit bytestrings (like   text_factory = str). It is highly recommended that you instead just   switch your application to Unicode strings.   Could you help me solve this error and explain the correct usage of encode(\"utf-8\") and unicode() functions? I'm often fighting with it.  EDIT:  This execute() statement raised the error:  cur.execute(\"update docs set path = :fullFilePath where path = :path\", locals())   I forgot to change the encoding of fullFilePath variable which suffers with the same problem, but I'm quite confused now. Should I use only unicode() or encode(\"utf-8\") or both?  I can't use  fullFilePath = unicode(fullFilePath.encode(\"utf-8\"))   because it raises this error:     UnicodeDecodeError: 'ascii' codec can't decode byte 0xc5 in position   32: ordinal not in range(128)   Python version is 2.7.2     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Why does numpy std() give a different result to matlab std()?",
        "A_Content": "  The NumPy function np.std takes an optional parameter ddof: \"Delta Degrees of Freedom\". By default, this is 0. Set it to 1 to get the MATLAB result:  >>> np.std([1,3,4,6], ddof=1) 2.0816659994661326   To add a little more context, in the calculation of the variance (of which the standard deviation is the square root) we typically divide by the number of values we have.   But if we select a random sample of N elements from a larger distribution and calculate the variance, division by N can lead to an underestimate of the actual variance. To fix this, we can lower the number we divide by (the degrees of freedom) to a number less than N (usually N-1). The ddof parameter allows us change the divisor by the amount we specify.  Unless told otherwise, NumPy will calculate the biased estimator for the variance (ddof=0, dividing by N). This is what you want if you are working with the entire distribution (and not a subset of values which have been randomly picked from a larger distribution). If the ddof parameter is given, NumPy divides by N - ddof instead.  The default behaviour of MATLAB's std is to correct the bias for sample variance by dividing by N-1. This gets rid of some of (but probably not all of) of the bias in the standard deviation. This is likely to be what you want if you're using the function on a random sample of a larger distribution.  The nice answer by @hbaderts gives further mathematical details.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "matlab",
            "numpy",
            "standard-deviation"
        ],
        "URL": "https://stackoverflow.com/questions/27600207/why-does-numpy-std-give-a-different-result-to-matlab-std",
        "A_Votes": "123",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I try to convert matlab code to numpy and figured out that numpy has a different result with the std function.  in matlab  std([1,3,4,6]) ans =  2.0817   in numpy  np.std([1,3,4,6]) 1.8027756377319946   Is this normal? And how should I handle this?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Why does numpy std() give a different result to matlab std()?",
        "A_Content": "  The standard deviation is the square root of the variance. The variance of a random variable X is defined as    An estimator for the variance would therefore be    where  denotes the sample mean. For randomly selected , it can be shown that this estimator does not converge to the real variance, but to    If you randomly select samples and estimate the sample mean and variance, you will have to use a corrected (unbiased) estimator    which will converge to . The correction term  is also called Bessel's correction.  Now by default, MATLABs std calculates the unbiased estimator with the correction term n-1. NumPy however (as @ajcr explained) calculates the biased estimator with no correction term by default. The parameter ddof allows to set any correction term n-ddof. By setting it to 1 you get the same result as in MATLAB.   Similarly, MATLAB allows to add a second parameter w, which specifies the \"weighing scheme\". The default, w=0, results in the correction term n-1 (unbiased estimator), while for w=1, only n is used as correction term (biased estimator).      ",
        "Language": "Python",
        "Tags": [
            "python",
            "matlab",
            "numpy",
            "standard-deviation"
        ],
        "URL": "https://stackoverflow.com/questions/27600207/why-does-numpy-std-give-a-different-result-to-matlab-std",
        "A_Votes": "55",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I try to convert matlab code to numpy and figured out that numpy has a different result with the std function.  in matlab  std([1,3,4,6]) ans =  2.0817   in numpy  np.std([1,3,4,6]) 1.8027756377319946   Is this normal? And how should I handle this?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Why does numpy std() give a different result to matlab std()?",
        "A_Content": "  For people who aren't great with statistics, a simplistic guide is:   Include ddof=1 if you're calculating np.std() for a sample taken from your full dataset. Ensure ddof=0 if you're calculating np.std() for the full population    The DDOF is included for samples in order to counterbalance bias that can occur in the numbers.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "matlab",
            "numpy",
            "standard-deviation"
        ],
        "URL": "https://stackoverflow.com/questions/27600207/why-does-numpy-std-give-a-different-result-to-matlab-std",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I try to convert matlab code to numpy and figured out that numpy has a different result with the std function.  in matlab  std([1,3,4,6]) ans =  2.0817   in numpy  np.std([1,3,4,6]) 1.8027756377319946   Is this normal? And how should I handle this?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What's the best way to initialize a dict of dicts in Python? [duplicate]",
        "A_Content": "  class AutoVivification(dict):     \"\"\"Implementation of perl's autovivification feature.\"\"\"     def __getitem__(self, item):         try:             return dict.__getitem__(self, item)         except KeyError:             value = self[item] = type(self)()             return value   Testing:  a = AutoVivification()  a[1][2][3] = 4 a[1][3][3] = 5 a[1][2]['test'] = 6  print a   Output:  {1: {2: {'test': 6, 3: 4}, 3: {3: 5}}}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "autovivification"
        ],
        "URL": "https://stackoverflow.com/questions/651794/whats-the-best-way-to-initialize-a-dict-of-dicts-in-python",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              What is the best way to implement nested dictionaries?                                        20 answers                                          A lot of times in Perl, I'll do something like this:  $myhash{foo}{bar}{baz} = 1   How would I translate this to Python? So far I have:  if not 'foo' in myhash:     myhash['foo'] = {} if not 'bar' in myhash['foo']:     myhash['foo']['bar'] = {} myhash['foo']['bar']['baz'] = 1   Is there a better way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What's the best way to initialize a dict of dicts in Python? [duplicate]",
        "A_Content": "  If the amount of nesting you need is fixed, collections.defaultdict is wonderful.  e.g. nesting two deep:  myhash = collections.defaultdict(dict) myhash[1][2] = 3 myhash[1][3] = 13 myhash[2][4] = 9   If you want to go another level of nesting, you'll need to do something like:  myhash = collections.defaultdict(lambda : collections.defaultdict(dict)) myhash[1][2][3] = 4 myhash[1][3][3] = 5 myhash[1][2]['test'] = 6   edit: MizardX points out that we can get full genericity with a simple function:  import collections def makehash():     return collections.defaultdict(makehash)   Now we can do:  myhash = makehash() myhash[1][2] = 4 myhash[1][3] = 8 myhash[2][5][8] = 17 # etc      ",
        "Language": "Python",
        "Tags": [
            "python",
            "autovivification"
        ],
        "URL": "https://stackoverflow.com/questions/651794/whats-the-best-way-to-initialize-a-dict-of-dicts-in-python",
        "A_Votes": "87",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              What is the best way to implement nested dictionaries?                                        20 answers                                          A lot of times in Perl, I'll do something like this:  $myhash{foo}{bar}{baz} = 1   How would I translate this to Python? So far I have:  if not 'foo' in myhash:     myhash['foo'] = {} if not 'bar' in myhash['foo']:     myhash['foo']['bar'] = {} myhash['foo']['bar']['baz'] = 1   Is there a better way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What's the best way to initialize a dict of dicts in Python? [duplicate]",
        "A_Content": "  Is there a reason it needs to be a dict of dicts?  If there's no compelling reason for that particular structure, you could simply index the dict with a tuple:  mydict = {('foo', 'bar', 'baz'):1} # Initializes dict with a key/value pair mydict[('foo', 'bar', 'baz')]      # Returns 1  mydict[('foo', 'unbar')] = 2       # Sets a value for a new key   The parentheses are required if you initialize the dict with a tuple key, but you can omit them when setting/getting values using []:  mydict = {}                        # Initialized the dict mydict['foo', 'bar', 'baz'] = 1    # Sets a value mydict['foo', 'bar', 'baz']        # Returns 1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "autovivification"
        ],
        "URL": "https://stackoverflow.com/questions/651794/whats-the-best-way-to-initialize-a-dict-of-dicts-in-python",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              What is the best way to implement nested dictionaries?                                        20 answers                                          A lot of times in Perl, I'll do something like this:  $myhash{foo}{bar}{baz} = 1   How would I translate this to Python? So far I have:  if not 'foo' in myhash:     myhash['foo'] = {} if not 'bar' in myhash['foo']:     myhash['foo']['bar'] = {} myhash['foo']['bar']['baz'] = 1   Is there a better way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What's the best way to initialize a dict of dicts in Python? [duplicate]",
        "A_Content": "  I guess the literal translation would be:   mydict = {'foo' : { 'bar' : { 'baz':1}}}   Calling:   >>> mydict['foo']['bar']['baz']   gives you 1.  That looks a little gross to me, though.  (I'm no perl guy, though, so I'm guessing at what your perl does)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "autovivification"
        ],
        "URL": "https://stackoverflow.com/questions/651794/whats-the-best-way-to-initialize-a-dict-of-dicts-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              What is the best way to implement nested dictionaries?                                        20 answers                                          A lot of times in Perl, I'll do something like this:  $myhash{foo}{bar}{baz} = 1   How would I translate this to Python? So far I have:  if not 'foo' in myhash:     myhash['foo'] = {} if not 'bar' in myhash['foo']:     myhash['foo']['bar'] = {} myhash['foo']['bar']['baz'] = 1   Is there a better way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "What's the best way to initialize a dict of dicts in Python? [duplicate]",
        "A_Content": "  Nested dictionaries like that are (often) called a poor mans objects.  Yes, there is an implication and it might correlate with pythons object oriented nature.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "autovivification"
        ],
        "URL": "https://stackoverflow.com/questions/651794/whats-the-best-way-to-initialize-a-dict-of-dicts-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              What is the best way to implement nested dictionaries?                                        20 answers                                          A lot of times in Perl, I'll do something like this:  $myhash{foo}{bar}{baz} = 1   How would I translate this to Python? So far I have:  if not 'foo' in myhash:     myhash['foo'] = {} if not 'bar' in myhash['foo']:     myhash['foo']['bar'] = {} myhash['foo']['bar']['baz'] = 1   Is there a better way?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  Python has facilities to generate temporary file names, see http://docs.python.org/library/tempfile.html. For instance:  In [4]: import tempfile   Each call to tempfile.NamedTemporaryFile() results in a different temp file, and its name can be accessed with the .name attribute, e.g.:  In [5]: tf = tempfile.NamedTemporaryFile() In [6]: tf.name Out[6]: 'c:\\\\blabla\\\\locals~1\\\\temp\\\\tmptecp3i'  In [7]: tf = tempfile.NamedTemporaryFile() In [8]: tf.name Out[8]: 'c:\\\\blabla\\\\locals~1\\\\temp\\\\tmpr8vvme'   Once you have the unique filename it can be used like any regular file. Note: By default the file will be deleted when it is closed. However, if the delete parameter is False, the file is not automatically deleted.  Full parameter set:  tempfile.NamedTemporaryFile([mode='w+b'[, bufsize=-1[, suffix=''[, prefix='tmp'[, dir=None[, delete=True]]]]]])   it is also possible to specify the prefix for the temporary file (as one of the various parameters that can be supplied during the file creation):  In [9]: tf = tempfile.NamedTemporaryFile(prefix=\"zz\") In [10]: tf.name Out[10]: 'c:\\\\blabla\\\\locals~1\\\\temp\\\\zzrc3pzk'   Additional examples for working with temporary files can be found here     ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  You could use the UUID module for generating a random string:  import uuid filename = str(uuid.uuid4())   This is a valid choice, given that an UUID generator is extremely unlikely to produce a duplicate identifier (a file name, in this case):     Only after generating 1 billion UUIDs every second for the next 100 years, the probability of creating just one duplicate would be about 50%. The probability of one duplicate would be about 50% if every person on earth owns 600 million UUIDs.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "75",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  a common approach is to add a timestamp as a prefix/suffix to the filename to have some temporal relation to the file. If you need more uniqueness you can still add a random string to this.  import datetime basename = \"mylogfile\" suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\") filename = \"_\".join([basename, suffix]) # e.g. 'mylogfile_120508_171442'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  The OP requested to create random filenames not random files. Times and UUIDs can collide. If you are working on a single machine (not a shared filesystem) and your process/thread will not stomp on itselfk, use os.getpid() to get your own PID and use this as an element of a unique filename. Other processes would obviously not get the same PID. If you are multithreaded, get the thread id. If you have other aspects of your code in which a single thread or process could generate multiple different tempfiles, you might need to use another technique. A rolling index can work (if you aren't keeping them so long or using so many files you would worry about rollover). Keeping a global hash/index to \"active\" files would suffice in that case.  So sorry for the longwinded explanation, but it does depend on your exact usage.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  If you want to preserve the original file name as a part of the new filename, unique prefixes of unifom length can be generted by:  def add_prefix(filename):    from hashlib import md5   from time import localtime    return \"%s_%s\" % (md5(str(localtime())).hexdigest(), filename)   Calls to the dd_prefix('style.css') generates sequence like:  a38ff35794ae366e442a0606e67035ba_style.css 7a5f8289323b0ebfdbc7c840ad3cb67b_style.css      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  If you need no the file path, but only the random string having predefined length you can use something like this.  import random import string  file_name = ''.join([random.choice(string.ascii_lowercase) for i in range(16)])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  Adding my two cents here:  In [19]: tempfile.mkstemp('.png', 'bingo', '/tmp')[1] Out[19]: '/tmp/bingoy6s3_k.png'   According to the python doc for tempfile.mkstemp, it creates a temporary file in the most secure manner possible. Please note that the file will exist after this call:  In [20]: os.path.exists(tempfile.mkstemp('.png', 'bingo', '/tmp')[1]) Out[20]: True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  I personally prefer to have my text to not be only random/unique but beautiful as well,  that's why I like the hashids lib, which generates nice looking random text from integers.  Can installed through   pip install hashids  Snippet:  import hashids hashids = hashids.Hashids(salt=\"this is my salt\", ) print hashids.encode(1, 2, 3) >>> laHquq   Short Description:     Hashids is a small open-source library that generates short, unique, non-sequential ids from numbers.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Best way to generate random file names in Python",
        "A_Content": "  You could use the random package:  import random file = random.random()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "random",
            "hash"
        ],
        "URL": "https://stackoverflow.com/questions/10501247/best-way-to-generate-random-file-names-in-python",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    In Python, what is a good, or the best way to generate some random text to prepend to a file(name) that I'm saving to a server, just to make sure it does not overwrite. Thank you!     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to extract the n-th elements from a list of tuples in python?",
        "A_Content": "  [x[1] for x in elements]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/3308102/how-to-extract-the-n-th-elements-from-a-list-of-tuples-in-python",
        "A_Votes": "135",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm trying to obtain the n-th elements from a list of tuples.  I have something like:  elements = [(1,1,1),(2,3,7),(3,5,10)]   I wish to extract only the second elements of each tuple into a list:  seconds = [1, 3, 5]   I know that it could be done with a for loop but I wanted to know if there's another way since I have thousands of tuples.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to extract the n-th elements from a list of tuples in python?",
        "A_Content": "     I know that it could be done with a FOR but I wanted to know if there's another way    There is another way. You can also do it with map and itemgetter:  >>> from operator import itemgetter >>> map(itemgetter(1), elements)   This still performs a loop internally though and it is slightly slower than the list comprehension:  setup = 'elements = [(1,1,1) for _ in range(100000)];from operator import itemgetter' method1 = '[x[1] for x in elements]' method2 = 'map(itemgetter(1), elements)'  import timeit t = timeit.Timer(method1, setup) print('Method 1: ' + str(t.timeit(100))) t = timeit.Timer(method2, setup) print('Method 2: ' + str(t.timeit(100)))   Results:   Method 1: 1.25699996948 Method 2: 1.46600008011   If you need to iterate over a list then using a for is fine.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/3308102/how-to-extract-the-n-th-elements-from-a-list-of-tuples-in-python",
        "A_Votes": "24",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to obtain the n-th elements from a list of tuples.  I have something like:  elements = [(1,1,1),(2,3,7),(3,5,10)]   I wish to extract only the second elements of each tuple into a list:  seconds = [1, 3, 5]   I know that it could be done with a for loop but I wanted to know if there's another way since I have thousands of tuples.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to extract the n-th elements from a list of tuples in python?",
        "A_Content": "  This also works:  zip(*elements)[1]   (I am mainly posting this, to prove to myself that I have groked zip...)  See it in action:  >>> help(zip)      Help on built-in function zip in module builtin:      zip(...)      zip(seq1 [, seq2 [...]]) -> [(seq1[0], seq2[0] ...), (...)]      Return a list of tuples, where each tuple contains the i-th element      from each of the argument sequences.  The returned list is truncated      in length to the length of the shortest argument sequence.   >>> elements = [(1,1,1),(2,3,7),(3,5,10)] >>> zip(*elements) [(1, 2, 3), (1, 3, 5), (1, 7, 10)] >>> zip(*elements)[1] (1, 3, 5) >>>   Neat thing I learned today: Use *list in arguments to create a parameter list for a function...     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/3308102/how-to-extract-the-n-th-elements-from-a-list-of-tuples-in-python",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to obtain the n-th elements from a list of tuples.  I have something like:  elements = [(1,1,1),(2,3,7),(3,5,10)]   I wish to extract only the second elements of each tuple into a list:  seconds = [1, 3, 5]   I know that it could be done with a for loop but I wanted to know if there's another way since I have thousands of tuples.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to extract the n-th elements from a list of tuples in python?",
        "A_Content": "  Found this as I was searching for which way is fastest to pull the second element of a 2-tuple list.  Not what I wanted but ran same test as shown with a 3rd method plus test the zip method  setup = 'elements = [(1,1) for _ in range(100000)];from operator import itemgetter' method1 = '[x[1] for x in elements]' method2 = 'map(itemgetter(1), elements)' method3 = 'dict(elements).values()' method4 = 'zip(*elements)[1]'  import timeit t = timeit.Timer(method1, setup) print('Method 1: ' + str(t.timeit(100))) t = timeit.Timer(method2, setup) print('Method 2: ' + str(t.timeit(100))) t = timeit.Timer(method3, setup) print('Method 3: ' + str(t.timeit(100))) t = timeit.Timer(method4, setup) print('Method 4: ' + str(t.timeit(100)))  Method 1: 0.618785858154 Method 2: 0.711684942245 Method 3: 0.298138141632 Method 4: 1.32586884499   So over twice as fast if you have a 2 tuple pair to just convert to a dict and take the values.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/3308102/how-to-extract-the-n-th-elements-from-a-list-of-tuples-in-python",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to obtain the n-th elements from a list of tuples.  I have something like:  elements = [(1,1,1),(2,3,7),(3,5,10)]   I wish to extract only the second elements of each tuple into a list:  seconds = [1, 3, 5]   I know that it could be done with a for loop but I wanted to know if there's another way since I have thousands of tuples.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to extract the n-th elements from a list of tuples in python?",
        "A_Content": "  map (lambda x:(x[1]),elements)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/3308102/how-to-extract-the-n-th-elements-from-a-list-of-tuples-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to obtain the n-th elements from a list of tuples.  I have something like:  elements = [(1,1,1),(2,3,7),(3,5,10)]   I wish to extract only the second elements of each tuple into a list:  seconds = [1, 3, 5]   I know that it could be done with a for loop but I wanted to know if there's another way since I have thousands of tuples.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "How to extract the n-th elements from a list of tuples in python?",
        "A_Content": "  Timings for Python 3.6 for extracting the second element from a 2-tuple list.   Also, added numpy array method, which is simpler to read (but arguably simpler than the list comprehension).  from operator import itemgetter elements = [(1,1) for _ in range(100000)]  %timeit second = [x[1] for x in elements] %timeit second = list(map(itemgetter(1), elements)) %timeit second = dict(elements).values() %timeit second = list(zip(*elements))[1] %timeit second = np.array(elements)[:,1]   and the timings:  list comprehension:  4.73 ms  206 s per loop list(map):           5.3 ms  167 s per loop dict:                2.25 ms  103 s per loop list(zip)            5.2 ms  252 s per loop numpy array:        28.7 ms  1.88 ms per loop   Note that map() and zip() do not return a list anymore, hence the explicit conversion.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/3308102/how-to-extract-the-n-th-elements-from-a-list-of-tuples-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to obtain the n-th elements from a list of tuples.  I have something like:  elements = [(1,1,1),(2,3,7),(3,5,10)]   I wish to extract only the second elements of each tuple into a list:  seconds = [1, 3, 5]   I know that it could be done with a for loop but I wanted to know if there's another way since I have thousands of tuples.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)",
        "A_Content": "  Unicode is not equal to UTF-8. The latter is just an encoding for the former.  You are doing it the wrong way around. You are reading UTF-8-encoded data, so you have to decode the UTF-8-encoded String into a unicode string.  So just replace .encode with .decode, and it should work (if your .csv is UTF-8-encoded).  Nothing to be ashamed of, though. I bet 3 in 5 programmers had trouble at first understanding this, if not more ;)  Update: If your input data is not UTF-8 encoded, then you have to .decode() with the appropriate encoding, of course. If nothing is given, python assumes ASCII, which obviously fails on non-ASCII-characters.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "utf-8"
        ],
        "URL": "https://stackoverflow.com/questions/10406135/unicodedecodeerror-ascii-codec-cant-decode-byte-0xd1-in-position-2-ordinal",
        "A_Votes": "123",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am attempting to work with a very large dataset that has some non-standard characters in it. I need to use unicode, as per the job specs, but I am baffled. (And quite possibly doing it all wrong.)  I open the CSV using:   15     ncesReader = csv.reader(open('geocoded_output.csv', 'rb'), delimiter='\\t', quotechar='\"')   Then, I attempt to encode it with:  name=school_name.encode('utf-8'), street=row[9].encode('utf-8'), city=row[10].encode('utf-8'), state=row[11].encode('utf-8'), zip5=row[12], zip4=row[13],county=row[25].encode('utf-8'), lat=row[22], lng=row[23])   I'm encoding everything except the lat and lng because those need to be sent out to an API. When I run the program to parse the dataset into what I can use, I get the following Traceback.  Traceback (most recent call last):   File \"push_into_db.py\", line 80, in <module>     main()   File \"push_into_db.py\", line 74, in main     district_map = buildDistrictSchoolMap()   File \"push_into_db.py\", line 32, in buildDistrictSchoolMap     county=row[25].encode('utf-8'), lat=row[22], lng=row[23]) UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)   I think I should tell you that I'm using python 2.7.2, and this is part of an app build on django 1.4. I've read several posts on this topic, but none of them seem to directly apply. Any help will be greatly appreciated.  You might also want to know that some of the non-standard characters causing the issue are  and possibly .     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)",
        "A_Content": "  Just add this lines to your codes :  import sys reload(sys) sys.setdefaultencoding('utf-8')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "utf-8"
        ],
        "URL": "https://stackoverflow.com/questions/10406135/unicodedecodeerror-ascii-codec-cant-decode-byte-0xd1-in-position-2-ordinal",
        "A_Votes": "63",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am attempting to work with a very large dataset that has some non-standard characters in it. I need to use unicode, as per the job specs, but I am baffled. (And quite possibly doing it all wrong.)  I open the CSV using:   15     ncesReader = csv.reader(open('geocoded_output.csv', 'rb'), delimiter='\\t', quotechar='\"')   Then, I attempt to encode it with:  name=school_name.encode('utf-8'), street=row[9].encode('utf-8'), city=row[10].encode('utf-8'), state=row[11].encode('utf-8'), zip5=row[12], zip4=row[13],county=row[25].encode('utf-8'), lat=row[22], lng=row[23])   I'm encoding everything except the lat and lng because those need to be sent out to an API. When I run the program to parse the dataset into what I can use, I get the following Traceback.  Traceback (most recent call last):   File \"push_into_db.py\", line 80, in <module>     main()   File \"push_into_db.py\", line 74, in main     district_map = buildDistrictSchoolMap()   File \"push_into_db.py\", line 32, in buildDistrictSchoolMap     county=row[25].encode('utf-8'), lat=row[22], lng=row[23]) UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)   I think I should tell you that I'm using python 2.7.2, and this is part of an app build on django 1.4. I've read several posts on this topic, but none of them seem to directly apply. Any help will be greatly appreciated.  You might also want to know that some of the non-standard characters causing the issue are  and possibly .     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)",
        "A_Content": "  for Python 3 users. you can do  with open(csv_name_here, 'r', encoding=\"utf-8\") as f:     #some codes   it works  with flask too :)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "utf-8"
        ],
        "URL": "https://stackoverflow.com/questions/10406135/unicodedecodeerror-ascii-codec-cant-decode-byte-0xd1-in-position-2-ordinal",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am attempting to work with a very large dataset that has some non-standard characters in it. I need to use unicode, as per the job specs, but I am baffled. (And quite possibly doing it all wrong.)  I open the CSV using:   15     ncesReader = csv.reader(open('geocoded_output.csv', 'rb'), delimiter='\\t', quotechar='\"')   Then, I attempt to encode it with:  name=school_name.encode('utf-8'), street=row[9].encode('utf-8'), city=row[10].encode('utf-8'), state=row[11].encode('utf-8'), zip5=row[12], zip4=row[13],county=row[25].encode('utf-8'), lat=row[22], lng=row[23])   I'm encoding everything except the lat and lng because those need to be sent out to an API. When I run the program to parse the dataset into what I can use, I get the following Traceback.  Traceback (most recent call last):   File \"push_into_db.py\", line 80, in <module>     main()   File \"push_into_db.py\", line 74, in main     district_map = buildDistrictSchoolMap()   File \"push_into_db.py\", line 32, in buildDistrictSchoolMap     county=row[25].encode('utf-8'), lat=row[22], lng=row[23]) UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)   I think I should tell you that I'm using python 2.7.2, and this is part of an app build on django 1.4. I've read several posts on this topic, but none of them seem to directly apply. Any help will be greatly appreciated.  You might also want to know that some of the non-standard characters causing the issue are  and possibly .     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)",
        "A_Content": "  The main reason for the error is that the default encoding assumed by python is ASCII.  Hence, if the string data to be encoded by encode('utf8') contains character that is outside of ASCII range e.g. for a string like 'hgvcj387', python would throw error because the string is not in the expected encoding format.   If you are using python version earlier than version 3.5, a reliable fix would be to set the default encoding assumed by python to utf8:  import sys reload(sys) sys.setdefaultencoding('utf8') name = school_name.encode('utf8')   This way python would be able to anticipate characters within a string that fall outside of ASCII range.  However, if you are using python version 3.5 or above, reload() function is not available, so you would have to fix it using decode e.g.  name = school_name.decode('utf8').encode('utf8')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "utf-8"
        ],
        "URL": "https://stackoverflow.com/questions/10406135/unicodedecodeerror-ascii-codec-cant-decode-byte-0xd1-in-position-2-ordinal",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am attempting to work with a very large dataset that has some non-standard characters in it. I need to use unicode, as per the job specs, but I am baffled. (And quite possibly doing it all wrong.)  I open the CSV using:   15     ncesReader = csv.reader(open('geocoded_output.csv', 'rb'), delimiter='\\t', quotechar='\"')   Then, I attempt to encode it with:  name=school_name.encode('utf-8'), street=row[9].encode('utf-8'), city=row[10].encode('utf-8'), state=row[11].encode('utf-8'), zip5=row[12], zip4=row[13],county=row[25].encode('utf-8'), lat=row[22], lng=row[23])   I'm encoding everything except the lat and lng because those need to be sent out to an API. When I run the program to parse the dataset into what I can use, I get the following Traceback.  Traceback (most recent call last):   File \"push_into_db.py\", line 80, in <module>     main()   File \"push_into_db.py\", line 74, in main     district_map = buildDistrictSchoolMap()   File \"push_into_db.py\", line 32, in buildDistrictSchoolMap     county=row[25].encode('utf-8'), lat=row[22], lng=row[23]) UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)   I think I should tell you that I'm using python 2.7.2, and this is part of an app build on django 1.4. I've read several posts on this topic, but none of them seem to directly apply. Any help will be greatly appreciated.  You might also want to know that some of the non-standard characters causing the issue are  and possibly .     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)",
        "A_Content": "  For Python 3 users:  changing the encoding from 'ascii' to 'latin1' works.  Also, you can try finding the encoding automatically by reading the top 10000 bytes using the below snippet:  import chardet   with open(\"dataset_path\", 'rb') as rawdata:               result = chardet.detect(rawdata.read(10000))   print(result)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "utf-8"
        ],
        "URL": "https://stackoverflow.com/questions/10406135/unicodedecodeerror-ascii-codec-cant-decode-byte-0xd1-in-position-2-ordinal",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I am attempting to work with a very large dataset that has some non-standard characters in it. I need to use unicode, as per the job specs, but I am baffled. (And quite possibly doing it all wrong.)  I open the CSV using:   15     ncesReader = csv.reader(open('geocoded_output.csv', 'rb'), delimiter='\\t', quotechar='\"')   Then, I attempt to encode it with:  name=school_name.encode('utf-8'), street=row[9].encode('utf-8'), city=row[10].encode('utf-8'), state=row[11].encode('utf-8'), zip5=row[12], zip4=row[13],county=row[25].encode('utf-8'), lat=row[22], lng=row[23])   I'm encoding everything except the lat and lng because those need to be sent out to an API. When I run the program to parse the dataset into what I can use, I get the following Traceback.  Traceback (most recent call last):   File \"push_into_db.py\", line 80, in <module>     main()   File \"push_into_db.py\", line 74, in main     district_map = buildDistrictSchoolMap()   File \"push_into_db.py\", line 32, in buildDistrictSchoolMap     county=row[25].encode('utf-8'), lat=row[22], lng=row[23]) UnicodeDecodeError: 'ascii' codec can't decode byte 0xd1 in position 2: ordinal not in range(128)   I think I should tell you that I'm using python 2.7.2, and this is part of an app build on django 1.4. I've read several posts on this topic, but none of them seem to directly apply. Any help will be greatly appreciated.  You might also want to know that some of the non-standard characters causing the issue are  and possibly .     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame to List of Dictionaries",
        "A_Content": "  Edit  As John Galt mentions in his answer , you should probably instead use df.to_dict('records'). It's faster than transposing manually.  In [20]: timeit df.T.to_dict().values() 1000 loops, best of 3: 395 s per loop  In [21]: timeit df.to_dict('records') 10000 loops, best of 3: 53 s per loop     Original answer  Use df.T.to_dict().values(), like below:  In [1]: df Out[1]:    customer  item1   item2   item3 0         1  apple    milk  tomato 1         2  water  orange  potato 2         3  juice   mango   chips  In [2]: df.T.to_dict().values() Out[2]: [{'customer': 1.0, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},  {'customer': 2.0, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},  {'customer': 3.0, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "dictionary",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries",
        "A_Votes": "83",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have the following DataFrame:   customer    item1      item2    item3 1           apple      milk     tomato 2           water      orange   potato 3           juice      mango    chips   which I want to translate it to list of dictionaries per row  rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},     {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},     {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame to List of Dictionaries",
        "A_Content": "  Use df.to_dict('records') -- gives the output without having to transpose externally.  In [2]: df.to_dict('records') Out[2]: [{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},  {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},  {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "dictionary",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries",
        "A_Votes": "116",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following DataFrame:   customer    item1      item2    item3 1           apple      milk     tomato 2           water      orange   potato 3           juice      mango    chips   which I want to translate it to list of dictionaries per row  rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},     {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},     {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Pandas DataFrame to List of Dictionaries",
        "A_Content": "  As an extension to John Galt's answer -  For the following DataFrame,     customer  item1   item2   item3 0         1  apple    milk  tomato 1         2  water  orange  potato 2         3  juice   mango   chips   If you want to get a list of dictionaries including the index values, you can do something like,   df.to_dict('index')   Which outputs a dictionary of dictionaries where keys of the parent dictionary are index values. In this particular case,   {0: {'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},  1: {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},  2: {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "dictionary",
            "pandas",
            "dataframe"
        ],
        "URL": "https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have the following DataFrame:   customer    item1      item2    item3 1           apple      milk     tomato 2           water      orange   potato 3           juice      mango    chips   which I want to translate it to list of dictionaries per row  rows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},     {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},     {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flask-SQLalchemy update a row's information",
        "A_Content": "  Retrieve an object using the tutorial shown in the Flask-SQLAlchemy documentation. Once you have the entity that you want to change, change the entity itself. Then, db.session.commit().  For example:  admin = User.query.filter_by(username='admin').first() admin.email = 'my_new_email@example.com' db.session.commit()  user = User.query.get(5) user.name = 'New Name' db.session.commit()   Flask-SQLAlchemy is based on SQLAlchemy, so be sure to check out the SQLAlchemy Docs as well.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy",
            "flask-sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/6699360/flask-sqlalchemy-update-a-rows-information",
        "A_Votes": "127",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How can I update a row's information?   For example I'd like to alter the name column of the row that has the id 5.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flask-SQLalchemy update a row's information",
        "A_Content": "  There is a method update on BaseQuery object in SQLAlchemy, which is returned by filter_by.   admin = User.query.filter_by(username='admin').update(dict(email='my_new_email@example.com'))) db.session.commit()   The advantage of using update over changing the entity comes when there are many objects to be updated.   If you want to give add_user permission to all the admins,  rows_changed = User.query.filter_by(role='admin').update(dict(permission='add_user')) db.session.commit()   Notice that filter_by takes keyword arguments (use only one =) as opposed to filter which takes an expression.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy",
            "flask-sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/6699360/flask-sqlalchemy-update-a-rows-information",
        "A_Votes": "64",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I update a row's information?   For example I'd like to alter the name column of the row that has the id 5.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flask-SQLalchemy update a row's information",
        "A_Content": "  This does not work if you modify a pickled attribute of the model.  Pickled attributes should be replaced in order to trigger updates:  from flask import Flask from flask.ext.sqlalchemy import SQLAlchemy from pprint import pprint  app = Flask(__name__) app.config['SQLALCHEMY_DATABASE_URI'] = 'sqllite:////tmp/users.db' db = SQLAlchemy(app)   class User(db.Model):     id = db.Column(db.Integer, primary_key=True)     name = db.Column(db.String(80), unique=True)     data = db.Column(db.PickleType())      def __init__(self, name, data):         self.name = name         self.data = data      def __repr__(self):         return '<User %r>' % self.username  db.create_all()  # Create a user. bob = User('Bob', {}) db.session.add(bob) db.session.commit()  # Retrieve the row by its name. bob = User.query.filter_by(name='Bob').first() pprint(bob.data)  # {}  # Modifying data is ignored. bob.data['foo'] = 123 db.session.commit() bob = User.query.filter_by(name='Bob').first() pprint(bob.data)  # {}  # Replacing data is respected. bob.data = {'bar': 321} db.session.commit() bob = User.query.filter_by(name='Bob').first() pprint(bob.data)  # {'bar': 321}  # Modifying data is ignored. bob.data['moo'] = 789 db.session.commit() bob = User.query.filter_by(name='Bob').first() pprint(bob.data)  # {'bar': 321}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy",
            "flask-sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/6699360/flask-sqlalchemy-update-a-rows-information",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I update a row's information?   For example I'd like to alter the name column of the row that has the id 5.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flask-SQLalchemy update a row's information",
        "A_Content": "  Just assigning the value and committing them will work for all the data types but JSON and Pickled attributes. Since pickled type is explained above I'll note down a slightly different but easy way to update JSONs.   class User(db.Model):     id = db.Column(db.Integer, primary_key=True)     name = db.Column(db.String(80), unique=True)     data = db.Column(db.JSON)  def __init__(self, name, data):     self.name = name     self.data = data   Let's say the model is like above.   user = User(\"Jon Dove\", {\"country\":\"Sri Lanka\"}) db.session.add(user) db.session.flush() db.session.commit()   This will add the user into the MySQL database with data {\"country\":\"Sri Lanka\"}  Modifying data will be ignored. My code that didn't work is as follows.  user = User.query().filter(User.name=='Jon Dove') data = user.data data[\"province\"] = \"south\" user.data = data db.session.merge(user) db.session.flush() db.session.commit()   Instead of going through the painful work of copying the JSON to a new dict (not assigning it to a new variable as above), which should have worked I found a simple way to do that. There is a way to flag the system that JSONs have changed.  Following is the working code.  from sqlalchemy.orm.attributes import flag_modified user = User.query().filter(User.name=='Jon Dove') data = user.data data[\"province\"] = \"south\" user.data = data flag_modified(user, \"data\") db.session.merge(user) db.session.flush() db.session.commit()   This worked like a charm.  There is another method proposed along with this method here Hope I've helped some one.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlalchemy",
            "flask-sqlalchemy"
        ],
        "URL": "https://stackoverflow.com/questions/6699360/flask-sqlalchemy-update-a-rows-information",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I update a row's information?   For example I'd like to alter the name column of the row that has the id 5.      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  Would this work for your situation?   >>> s = '12abcd405' >>> result = ''.join([i for i in s if not i.isdigit()]) >>> result 'abcd'   This makes use of a list comprehension, and what is happening here is similar to this structure:  no_digits = [] # Iterate through the string, adding non-numbers to the no_digits list for i in s:     if not i.isdigit():         no_digits.append(i)  # Now join all elements of the list with '',  # which puts all of the characters together. result = ''.join(no_digits)   As @AshwiniChaudhary and @KirkStrauser point out, you actually do not need to use the brackets in the one-liner, making the piece inside the parentheses a generator expression (more efficient than a list comprehension). Even if this doesn't fit the requirements for your assignment, it is something you should read about eventually :) :  >>> s = '12abcd405' >>> result = ''.join(i for i in s if not i.isdigit()) >>> result 'abcd'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "114",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  And, just to throw it in the mix, is the oft-forgotten str.translate which will work a lot faster than looping/regular expressions:  For Python 2:  from string import digits  s = 'abc123def456ghi789zero0' res = s.translate(None, digits) # 'abcdefghizero'   For Python 3:  from string import digits  s = 'abc123def456ghi789zero0' remove_digits = str.maketrans('', '', digits) res = s.translate(remove_digits) # 'abcdefghizero'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "75",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  Not sure if your teacher allows you to use filters but...  filter(lambda x: x.isalpha(), \"a1a2a3s3d4f5fg6h\")   returns-  'aaasdffgh'   Much more efficient than looping...  Example:  for i in range(10):   a.replace(str(i),'')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  What about this:  out_string = filter(lambda c: not c.isdigit(), in_string)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  Just a few (others have suggested some of these)  Method 1:  ''.join(i for i in myStr if not i.isdigit())   Method 2:  def removeDigits(s):     answer = []     for char in s:         if not char.isdigit():             answer.append(char)     return ''.join(char)   Method 3:  ''.join(filter(lambda x: not x.isdigit(), mystr))   Method 4:  nums = set(map(int, range(10))) ''.join(i for i in mystr if i not in nums)   Method 5:  ''.join(i for i in mystr if ord(i) not in range(48, 58))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  I'd love to use regex to accomplish this, but since you can only use lists, loops, functions, etc..  here's what I came up with:  stringWithNumbers=\"I have 10 bananas for my 5 monkeys!\" stringWithoutNumbers=''.join(c if c not in map(str,range(0,10)) else \"\" for c in stringWithNumbers) print(stringWithoutNumbers) #I have  bananas for my  monkeys!      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  Say st is your unformatted string, then run  st_nodigits=''.join(i for i in st if i.isalpha())   as mentioned above. But my guess that you need something very simple  so say s is your string and st_res is a string without digits, then here is your code   l = ['0','1','2','3','4','5','6','7','8','9'] st_res=\"\" for ch in s:  if ch not in l:   st_res+=ch      ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Removing numbers from string [closed]",
        "A_Content": "  If i understand your question right, one way to do is break down the string in chars and then check each char in that string using a loop whether it's a string or a number and then if string save it in a variable and then once the loop is finished, display that to the user     ",
        "Language": "Python",
        "Tags": [
            "python",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/12851791/removing-numbers-from-string",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    How can I remove digits from a string?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  To get the values use  sorted(data.values())   To get the matching keys, use a key function  sorted(data, key=data.get)   To get a list of tuples ordered by value  sorted(data.items(), key=lambda x:x[1])   Related: see the discussion here: Dictionaries are ordered in Python 3.6+       ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "149",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  If you actually want to sort the dictionary instead of just obtaining a sorted list use collections.OrderedDict  >>> from collections import OrderedDict >>> from operator import itemgetter >>> data = {1: 'b', 2: 'a'} >>> d = OrderedDict(sorted(data.items(), key=itemgetter(1))) >>> d OrderedDict([(2, 'a'), (1, 'b')]) >>> d.values() ['a', 'b']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "33",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  From your comment to gnibbler answer, i'd say you want a list of pairs of key-value sorted by value:  sorted(data.items(), key=lambda x:x[1])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "15",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  Sort the values:  sorted(data.values())   returns  ['a','b']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  Thanks for all answers. You are all my heros ;-)  Did in the end something like this:  d = sorted(data, key = d.get)  for id in d:     text = data[id]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  I also think it is important to note that Python dict object type is a hash table (more on this here), and thus is not capable of being sorted without converting its keys/values to lists. What this allows is dict item retrieval in constant time O(1), no matter the size/number of elements in a dictionary.  Having said that, once you sort its keys - sorted(data.keys()), or values - sorted(data.values()), you can then use that list to access keys/values in design patterns such as these:  for sortedKey in sorted(dictionary):     print dictionary[sortedKeY] # gives the values sorted by key  for sortedValue in sorted(dictionary.values()):     print sortedValue # gives the values sorted by value   Hope this helps.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  In your comment in response to John, you suggest that you want the keys and values of the dictionary, not just the values.  PEP 256 suggests this for sorting a dictionary by values.  import operator sorted(d.iteritems(), key=operator.itemgetter(1))   If you want descending order, do this  sorted(d.iteritems(), key=itemgetter(1), reverse=True)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  no lambda method  # sort dictionary by value d = {'a1': 'fsdfds', 'g5': 'aa3432ff', 'ca':'zz23432'} def getkeybyvalue(d,i):     for k, v in d.items():         if v == i:             return (k)  sortvaluelist = sorted(d.values()) sortresult ={} for i1 in sortvaluelist:        key = getkeybyvalue(d,i1)     sortresult[key] = i1 print ('=====sort by value=====') print (sortresult) print ('=======================')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "sort dict by value python",
        "A_Content": "  You could created sorted list from Values and rebuild the dictionary:  myDictionary={\"two\":\"2\", \"one\":\"1\", \"five\":\"5\", \"1four\":\"4\"}  newDictionary={}  sortedList=sorted(myDictionary.values())  for sortedKey in sortedList:     for key, value in myDictionary.items():         if value==sortedKey:             newDictionary[key]=value   Output: newDictionary={'one': '1', 'two': '2', '1four': '4', 'five': '5'}     ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "python-2.6"
        ],
        "URL": "https://stackoverflow.com/questions/16772071/sort-dict-by-value-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Assume that I have a dict.  data = {1:'b', 2:'a'}   And I want to sort data by 'b' and 'a' so I get the result  'a','b'   How do I do that? Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a numpy builtin to reject outliers from a list",
        "A_Content": "  This method is almost identical to yours, just more numpyst (also working on numpy arrays only):  def reject_outliers(data, m=2):     return data[abs(data - np.mean(data)) < m * np.std(data)]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list",
        "A_Votes": "72",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in d.  import numpy as np  def reject_outliers(data):     m = 2     u = np.mean(data)     s = np.std(data)     filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]     return filtered  >>> d = [2,4,5,1,6,5,40] >>> filtered_d = reject_outliers(d) >>> print filtered_d [2,4,5,1,6,5]   I say 'something like' because the function might allow for varying distributions (poisson, gaussian, etc.) and varying outlier thresholds within those distributions (like the m I've used here).     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a numpy builtin to reject outliers from a list",
        "A_Content": "  Something important when dealing with outliers is that one should try to use estimators as robust as possible. The mean of a distribution will be biased by outliers but e.g. the median will be much less.  Building on eumiro's answer:  def reject_outliers(data, m = 2.):     d = np.abs(data - np.median(data))     mdev = np.median(d)     s = d/mdev if mdev else 0.     return data[s<m]   Here I have replace the mean with the more robust median and the standard deviation with the absolute distance to the median. I then scaled the distances by their (again) median value so that m is on a reasonable relative scale.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list",
        "A_Votes": "136",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in d.  import numpy as np  def reject_outliers(data):     m = 2     u = np.mean(data)     s = np.std(data)     filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]     return filtered  >>> d = [2,4,5,1,6,5,40] >>> filtered_d = reject_outliers(d) >>> print filtered_d [2,4,5,1,6,5]   I say 'something like' because the function might allow for varying distributions (poisson, gaussian, etc.) and varying outlier thresholds within those distributions (like the m I've used here).     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a numpy builtin to reject outliers from a list",
        "A_Content": "  Building on Benjamin's, using pandas.Series, and replacing MAD with IQR:  def reject_outliers(sr, iq_range=0.5):     pcnt = (1 - iq_range) / 2     qlow, median, qhigh = sr.dropna().quantile([pcnt, 0.50, 1-pcnt])     iqr = qhigh - qlow     return sr[ (sr - median).abs() <= iqr]   For instance, if you set iq_range=0.6, the percentiles of the interquartile-range would become: 0.20 <--> 0.80, so more outliers will be included.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in d.  import numpy as np  def reject_outliers(data):     m = 2     u = np.mean(data)     s = np.std(data)     filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]     return filtered  >>> d = [2,4,5,1,6,5,40] >>> filtered_d = reject_outliers(d) >>> print filtered_d [2,4,5,1,6,5]   I say 'something like' because the function might allow for varying distributions (poisson, gaussian, etc.) and varying outlier thresholds within those distributions (like the m I've used here).     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a numpy builtin to reject outliers from a list",
        "A_Content": "  Benjamin Bannier's answer yields a pass-through when the median of distances from the median is 0, so I found this modified version a bit more helpful for cases as given in the example below.  def reject_outliers_2(data, m = 2.):     d = np.abs(data - np.median(data))     mdev = np.median(d)     s = d/(mdev if mdev else 1.)     return data[s<m]   Example:  data_points = np.array([10, 10, 10, 17, 10, 10]) print(reject_outliers(data_points)) print(reject_outliers_2(data_points))   Gives:  [[10, 10, 10, 17, 10, 10]]  # 17 is not filtered [10, 10, 10, 10, 10]  # 17 is filtered (it's distance, 7, is greater than m)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in d.  import numpy as np  def reject_outliers(data):     m = 2     u = np.mean(data)     s = np.std(data)     filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]     return filtered  >>> d = [2,4,5,1,6,5,40] >>> filtered_d = reject_outliers(d) >>> print filtered_d [2,4,5,1,6,5]   I say 'something like' because the function might allow for varying distributions (poisson, gaussian, etc.) and varying outlier thresholds within those distributions (like the m I've used here).     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a numpy builtin to reject outliers from a list",
        "A_Content": "  An alternative is to make a robust estimation of the standard deviation (assuming Gaussian statistics). Looking up online calculators, I see that the 90% percentile corresponds to 1.2815 and the 95% is 1.645 (http://vassarstats.net/tabs.html?#z)  As a simple example:  import numpy as np  # Create some random numbers x = np.random.normal(5, 2, 1000)  # Calculate the statistics print(\"Mean= \", np.mean(x)) print(\"Median= \", np.median(x)) print(\"Max/Min=\", x.max(), \" \", x.min()) print(\"StdDev=\", np.std(x)) print(\"90th Percentile\", np.percentile(x, 90))  # Add a few large points x[10] += 1000 x[20] += 2000 x[30] += 1500  # Recalculate the statistics print() print(\"Mean= \", np.mean(x)) print(\"Median= \", np.median(x)) print(\"Max/Min=\", x.max(), \" \", x.min()) print(\"StdDev=\", np.std(x)) print(\"90th Percentile\", np.percentile(x, 90))  # Measure the percentile intervals and then estimate Standard Deviation of the distribution, both from median to the 90th percentile and from the 10th to 90th percentile p90 = np.percentile(x, 90) p10 = np.percentile(x, 10) p50 = np.median(x) # p50 to p90 is 1.2815 sigma rSig = (p90-p50)/1.2815 print(\"Robust Sigma=\", rSig)  rSig = (p90-p10)/(2*1.2815) print(\"Robust Sigma=\", rSig)   The output I get is:  Mean=  4.99760520022 Median=  4.95395274981 Max/Min= 11.1226494654   -2.15388472011 Sigma= 1.976629928 90th Percentile 7.52065379649  Mean=  9.64760520022 Median=  4.95667658782 Max/Min= 2205.43861943   -2.15388472011 Sigma= 88.6263902244 90th Percentile 7.60646688694  Robust Sigma= 2.06772555531 Robust Sigma= 1.99878292462   Which is close to the expected value of 2.  If we want to remove points above/below 5 standard deviations (with 1000 points we would expect 1 value > 3 standard deviations):  y = x[abs(x - p50) < rSig*5]  # Print the statistics again print(\"Mean= \", np.mean(y)) print(\"Median= \", np.median(y)) print(\"Max/Min=\", y.max(), \" \", y.min()) print(\"StdDev=\", np.std(y))   Which gives:  Mean=  4.99755359935 Median=  4.95213030447 Max/Min= 11.1226494654   -2.15388472011 StdDev= 1.97692712883   I have no idea which approach is the more efficent/robust     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in d.  import numpy as np  def reject_outliers(data):     m = 2     u = np.mean(data)     s = np.std(data)     filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]     return filtered  >>> d = [2,4,5,1,6,5,40] >>> filtered_d = reject_outliers(d) >>> print filtered_d [2,4,5,1,6,5]   I say 'something like' because the function might allow for varying distributions (poisson, gaussian, etc.) and varying outlier thresholds within those distributions (like the m I've used here).     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a numpy builtin to reject outliers from a list",
        "A_Content": "  I wanted to do something similar, except setting the number to NaN rather than removing it from the data, since if you remove it you change the length which can mess up plotting (i.e. if you're only removing outliers from one column in a table, but you need it to remain the same as the other columns so you can plot them against each other).  To do so I used numpy's masking functions:  def reject_outliers(data, m=2):     stdev = np.std(data)     mean = np.mean(data)     maskMin = mean - stdev * m     maskMax = mean + stdev * m     mask = np.ma.masked_outside(data, maskMin, maskMax)     print('Masking values outside of {} and {}'.format(maskMin, maskMax))     return mask      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy"
        ],
        "URL": "https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in d.  import numpy as np  def reject_outliers(data):     m = 2     u = np.mean(data)     s = np.std(data)     filtered = [e for e in data if (u - 2 * s < e < u + 2 * s)]     return filtered  >>> d = [2,4,5,1,6,5,40] >>> filtered_d = reject_outliers(d) >>> print filtered_d [2,4,5,1,6,5]   I say 'something like' because the function might allow for varying distributions (poisson, gaussian, etc.) and varying outlier thresholds within those distributions (like the m I've used here).     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  Your installed PIL was compiled without libfreetype.  You can get precompiled installer of PIL (compiled with libfreetype) here (and many other precompiled Python C Modules):  http://www.lfd.uci.edu/~gohlke/pythonlibs/     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "55",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  On Ubuntu, you need to have libfreetype-dev installed before compiling PIL.  i.e.  $ sudo apt-get install libfreetype6-dev $ sudo -s \\# pip uninstall pil \\# pip install pil   PS! Running pip install as sudo will usually install packages to /usr/local/lib on most Ubuntu versions. You may consider to install Pil in a virtual environment (virtualenv or venv) in a path owned by the user instead.  You may also consider installing pillow instead of pil, which I believe is API compatible: https://python-pillow.org.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "71",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  The following worked for me on Ubuntu 14.04.1 64 bit:  sudo apt-get install libfreetype6-dev   Then, in the virtualenv:  pip uninstall pillow pip install --no-cache-dir pillow      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "41",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  solution for CentOS 6 (and probably other rpm based):  yum install freetype-devel libjpeg-devel libpng-devel  pip uninstall pil Pillow pip install pil Pillow      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  In OS X, I did this to solve the problem:  pip uninstall PIL ln -s /usr/X11/include/freetype2 /usr/local/include/ ln -s /usr/X11/include/ft2build.h /usr/local/include/ ln -s /usr/X11/lib/libfreetype.6.dylib /usr/local/lib/ ln -s /usr/X11/lib/libfreetype.6.dylib /usr/local/lib/libfreetype.dylib pip install PIL      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  Basically, you need to install freetype before installing PIL.  If you're using Homebrew on OS X it's just a matter of:  brew remove pil brew install freetype brew install pil      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  Worked for Ubuntu 12.10:  sudo pip uninstall PIL sudo apt-get install libfreetype6-dev sudo apt-get install python-imaging      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  For OS X (I'm running 10.6 but should work for others) I was able to get around this error using the advice from this post. Basically you need to install a couple of the dependencies then reinstall PIL.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  For me none of the solutions posted here so far has worked. I found another solution here: http://codeinthehole.com/writing/how-to-install-pil-on-64-bit-ubuntu-1204/  First install the dev packages:  $ sudo apt-get install python-dev libjpeg-dev libfreetype6-dev zlib1g-dev   Then create some symlinks:  $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libfreetype.so /usr/lib/ $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libjpeg.so /usr/lib/ $ sudo ln -s /usr/lib/`uname -i`-linux-gnu/libz.so /usr/lib/   Afterwards PIL should compile just fine:  $ pip install PIL --upgrade      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  The followed works on ubuntu 12.04:  pip uninstall PIL apt-get install libjpeg-dev apt-get install libfreetype6-dev apt-get install zlib1g-dev apt-get install libpng12-dev pip install PIL --upgrade   when your see \"-- JPEG support avaliable\" that means it works.  But, if it still doesn't work when your edit your jpeg image, check the python path!! My python path missed '/usr/local/lib/python2.7/dist-packages/PIL-1.1.7-py2.7-linux-x86_64.egg/', so I edit the ~/.bashrc add the following code to this file:  export PYTHONPATH=$PYTHONPATH:/usr/local/lib/python2.7/dist-packages/PIL-1.1.7-py2.7-linux-x86_64.egg/   then, finally, it works!!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  I used homebrew to install freetype and I have the following in /usr/local/lib:  libfreetype.6.dylib libfreetype.a libfreetype.dylib  But the usual:     pip install pil   Does not work for me, so I used:     pip install http://effbot.org/downloads/Imaging-1.1.6.tar.gz      ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  In my Mac, the following steps in terminal works:  $ brew install freetype $ sudo pip uninstall pil $ sudo pip install pillow   hopes it works for you. Good luck!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  Ubuntu 11.10 installs zlib and freetype2 libraries following the multi-arch spec (e.g. /usr/lib/i386-linux-gnu). You may use PIL setup environment variables so it can find them. However it only works on PIL versions beyond the pil-117 tag.  export PIL_SETUP_ZLIB_ROOT=/usr/lib/i386-linux-gnu export PIL_SETUP_FREETYPE_ROOT=/usr/lib/i386-linux-gnu pip install -U PIL   Since your multi-arch path may be different (x86-64), it's preferable to install the -dev packages and use pkg-config to retrieve the correct path.  pkg-config --variable=libdir zlib pkg-config --variable=libdir freetype2   Another way given by Barry on Pillow's setup.py is to use dpkg-architecture -qDEB_HOST_MULTIARCH to obtain the proper library directory suffix.  See https://bitbucket.org/effbot/pil-2009-raclette/issue/18     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python: The _imagingft C module is not installed",
        "A_Content": "  solved In my ubuntu12.04, after I installed python-imaging using apt-get, it works.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "image",
            "python-imaging-library"
        ],
        "URL": "https://stackoverflow.com/questions/4011705/python-the-imagingft-c-module-is-not-installed",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've tried lots of solution that posted on the net, they don't work.  >>> import _imaging >>> _imaging.__file__ 'C:\\\\python26\\\\lib\\\\site-packages\\\\PIL\\\\_imaging.pyd' >>>   So the system can find the _imaging but still can't use truetype font  from PIL import Image, ImageDraw, ImageFilter, ImageFont   im = Image.new('RGB', (300,300), 'white') draw = ImageDraw.Draw(im) font = ImageFont.truetype('arial.ttf', 14) draw.text((100,100), 'test text', font = font)   Raises this error:  ImportError: The _imagingft C module is not installed  File \"D:\\Python26\\Lib\\site-packages\\PIL\\ImageFont.py\", line 34, in __getattr__   raise ImportError(\"The _imagingft C module is not installed\")      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flatten list of lists [duplicate]",
        "A_Content": "  Flatten the list to \"remove the brackets\" using a nested list comprehension. This will un-nest each list stored in your list of lists!  list_of_lists = [[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]] flattened = [val for sublist in list_of_lists for val in sublist]   Nested list comprehensions evaluate in the same manner that they unwrap (i.e. add newline and tab for each new loop. So in this case:  flattened = [val for sublist in list_of_lists for val in sublist]   is equivalent to:  flattened = [] for sublist in list_of_lists:     for val in sublist:         flattened.append(val)   The big difference is that the list comp evaluates MUCH faster than the unraveled loop and eliminates the append calls!   If you have multiple items in a sublist the list comp will even flatten that. ie  >>> list_of_lists = [[180.0, 1, 2, 3], [173.8], [164.2], [156.5], [147.2], [138.2]] >>> flattened  = [val for sublist in list_of_lists for val in sublist] >>> flattened  [180.0, 1, 2, 3, 173.8, 164.2, 156.5, 147.2,138.2]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "list",
            "brackets"
        ],
        "URL": "https://stackoverflow.com/questions/11264684/flatten-list-of-lists",
        "A_Votes": "127",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Flattening a shallow list in Python [duplicate]                                        23 answers                                          I'm having a problem with square brackets in Python. I wrote a code that produces the following output:  [[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]]   But I would like to perform some calculations with that, but the the square brackets won't let me.  How can I remove the brackets? I saw some examples to do that but I could not apply them to this case.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flatten list of lists [duplicate]",
        "A_Content": "  I would use itertools.chain - this will also cater for > 1 element in each sublist:  from itertools import chain list(chain.from_iterable([[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]]))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "list",
            "brackets"
        ],
        "URL": "https://stackoverflow.com/questions/11264684/flatten-list-of-lists",
        "A_Votes": "86",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Flattening a shallow list in Python [duplicate]                                        23 answers                                          I'm having a problem with square brackets in Python. I wrote a code that produces the following output:  [[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]]   But I would like to perform some calculations with that, but the the square brackets won't let me.  How can I remove the brackets? I saw some examples to do that but I could not apply them to this case.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flatten list of lists [duplicate]",
        "A_Content": "  Given  d = [[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]]   and your specific question: How can I remove the brackets?   Using list comprehension :  new_d = [i[0] for i in d]   will give you this  [180.0, 173.8, 164.2, 156.5, 147.2, 138.2]   then you can access individual items with the appropriate index, e.g., new_d[0] will give you 180.0 etc which you can then use for math.  If you are going to have a collection of data, you will have some sort of bracket or parenthesis.  Note, this solution is aimed specifically at your question/problem, it doesn't provide a generalized solution. I.e., it will work for your case.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "list",
            "brackets"
        ],
        "URL": "https://stackoverflow.com/questions/11264684/flatten-list-of-lists",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Flattening a shallow list in Python [duplicate]                                        23 answers                                          I'm having a problem with square brackets in Python. I wrote a code that produces the following output:  [[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]]   But I would like to perform some calculations with that, but the the square brackets won't let me.  How can I remove the brackets? I saw some examples to do that but I could not apply them to this case.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Flatten list of lists [duplicate]",
        "A_Content": "  >>> lis=[[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]] >>> [x[0] for x in lis] [180.0, 173.8, 164.2, 156.5, 147.2, 138.2]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "arrays",
            "list",
            "brackets"
        ],
        "URL": "https://stackoverflow.com/questions/11264684/flatten-list-of-lists",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              Flattening a shallow list in Python [duplicate]                                        23 answers                                          I'm having a problem with square brackets in Python. I wrote a code that produces the following output:  [[180.0], [173.8], [164.2], [156.5], [147.2], [138.2]]   But I would like to perform some calculations with that, but the the square brackets won't let me.  How can I remove the brackets? I saw some examples to do that but I could not apply them to this case.     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)",
        "A_Content": "  It seems you are hitting a UTF-8 byte order mark (BOM). Try using this unicode string with BOM extracted out:  import codecs  content = unicode(q.content.strip(codecs.BOM_UTF8), 'utf-8') parser.parse(StringIO.StringIO(content))   I used strip instead of lstrip because in your case you had multiple occurences of BOM, possibly due to concatenated file contents.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "google-app-engine",
            "xml-parsing"
        ],
        "URL": "https://stackoverflow.com/questions/5141559/unicodeencodeerror-ascii-codec-cant-encode-character-u-xef-in-position-0",
        "A_Votes": "29",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to parse my XML document. So I have stored my XML document as below  class XMLdocs(db.Expando):      id = db.IntegerProperty()        name=db.StringProperty()      content=db.BlobProperty()     Now my below is my code  parser = make_parser()      curHandler = BasketBallHandler()   parser.setContentHandler(curHandler)   for q in XMLdocs.all():           parser.parse(StringIO.StringIO(q.content))   I am getting below error  'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128) Traceback (most recent call last):     File \"/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/__init__.py\", line 517, in __call__     handler.post(*groups)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/base_handler.py\", line 59, in post     self.handle()      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 168, in handle     scan_aborted = not self.process_entity(entity, ctx)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 233, in process_entity     handler(entity)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 71, in process     parser.parse(StringIO.StringIO(q.content))      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 107, in parse     xmlreader.IncrementalParser.parse(self, source)      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/xmlreader.py\", line 123, in parse     self.feed(buffer)     File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 207, in feed     self._parser.Parse(data, isFinal)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 136, in characters        print ch    UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)         ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)",
        "A_Content": "  The actual best answer for this problem depends on your environment, specifically what encoding your terminal expects.    The quickest one-line solution is to encode everything you print to ASCII, which your terminal is almost certain to accept, while discarding characters that you cannot print:  print ch #fails print ch.encode('ascii', 'ignore')   The better solution is to change your terminal's encoding to utf-8, and encode everything as utf-8 before printing. You should get in the habit of thinking about your unicode encoding EVERY time you print or read a string.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "google-app-engine",
            "xml-parsing"
        ],
        "URL": "https://stackoverflow.com/questions/5141559/unicodeencodeerror-ascii-codec-cant-encode-character-u-xef-in-position-0",
        "A_Votes": "112",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to parse my XML document. So I have stored my XML document as below  class XMLdocs(db.Expando):      id = db.IntegerProperty()        name=db.StringProperty()      content=db.BlobProperty()     Now my below is my code  parser = make_parser()      curHandler = BasketBallHandler()   parser.setContentHandler(curHandler)   for q in XMLdocs.all():           parser.parse(StringIO.StringIO(q.content))   I am getting below error  'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128) Traceback (most recent call last):     File \"/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/__init__.py\", line 517, in __call__     handler.post(*groups)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/base_handler.py\", line 59, in post     self.handle()      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 168, in handle     scan_aborted = not self.process_entity(entity, ctx)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 233, in process_entity     handler(entity)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 71, in process     parser.parse(StringIO.StringIO(q.content))      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 107, in parse     xmlreader.IncrementalParser.parse(self, source)      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/xmlreader.py\", line 123, in parse     self.feed(buffer)     File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 207, in feed     self._parser.Parse(data, isFinal)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 136, in characters        print ch    UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)         ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)",
        "A_Content": "  Just putting .encode('utf-8') at the end of object will do the job in recent versions of Python.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "google-app-engine",
            "xml-parsing"
        ],
        "URL": "https://stackoverflow.com/questions/5141559/unicodeencodeerror-ascii-codec-cant-encode-character-u-xef-in-position-0",
        "A_Votes": "53",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to parse my XML document. So I have stored my XML document as below  class XMLdocs(db.Expando):      id = db.IntegerProperty()        name=db.StringProperty()      content=db.BlobProperty()     Now my below is my code  parser = make_parser()      curHandler = BasketBallHandler()   parser.setContentHandler(curHandler)   for q in XMLdocs.all():           parser.parse(StringIO.StringIO(q.content))   I am getting below error  'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128) Traceback (most recent call last):     File \"/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/__init__.py\", line 517, in __call__     handler.post(*groups)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/base_handler.py\", line 59, in post     self.handle()      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 168, in handle     scan_aborted = not self.process_entity(entity, ctx)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 233, in process_entity     handler(entity)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 71, in process     parser.parse(StringIO.StringIO(q.content))      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 107, in parse     xmlreader.IncrementalParser.parse(self, source)      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/xmlreader.py\", line 123, in parse     self.feed(buffer)     File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 207, in feed     self._parser.Parse(data, isFinal)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 136, in characters        print ch    UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)         ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)",
        "A_Content": "  This worked for me:  from django.utils.encoding import smart_str content = smart_str(content)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "google-app-engine",
            "xml-parsing"
        ],
        "URL": "https://stackoverflow.com/questions/5141559/unicodeencodeerror-ascii-codec-cant-encode-character-u-xef-in-position-0",
        "A_Votes": "30",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to parse my XML document. So I have stored my XML document as below  class XMLdocs(db.Expando):      id = db.IntegerProperty()        name=db.StringProperty()      content=db.BlobProperty()     Now my below is my code  parser = make_parser()      curHandler = BasketBallHandler()   parser.setContentHandler(curHandler)   for q in XMLdocs.all():           parser.parse(StringIO.StringIO(q.content))   I am getting below error  'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128) Traceback (most recent call last):     File \"/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/__init__.py\", line 517, in __call__     handler.post(*groups)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/base_handler.py\", line 59, in post     self.handle()      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 168, in handle     scan_aborted = not self.process_entity(entity, ctx)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 233, in process_entity     handler(entity)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 71, in process     parser.parse(StringIO.StringIO(q.content))      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 107, in parse     xmlreader.IncrementalParser.parse(self, source)      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/xmlreader.py\", line 123, in parse     self.feed(buffer)     File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 207, in feed     self._parser.Parse(data, isFinal)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 136, in characters        print ch    UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)         ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)",
        "A_Content": "  The problem according to your traceback is the print statement on line 136 of parseXML.py. Unfortunately you didn't see fit to post that part of your code, but I'm going to guess it is just there for debugging. If you change it to:  print repr(ch)   then you should at least see what you are trying to print.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "google-app-engine",
            "xml-parsing"
        ],
        "URL": "https://stackoverflow.com/questions/5141559/unicodeencodeerror-ascii-codec-cant-encode-character-u-xef-in-position-0",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to parse my XML document. So I have stored my XML document as below  class XMLdocs(db.Expando):      id = db.IntegerProperty()        name=db.StringProperty()      content=db.BlobProperty()     Now my below is my code  parser = make_parser()      curHandler = BasketBallHandler()   parser.setContentHandler(curHandler)   for q in XMLdocs.all():           parser.parse(StringIO.StringIO(q.content))   I am getting below error  'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128) Traceback (most recent call last):     File \"/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/__init__.py\", line 517, in __call__     handler.post(*groups)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/base_handler.py\", line 59, in post     self.handle()      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 168, in handle     scan_aborted = not self.process_entity(entity, ctx)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 233, in process_entity     handler(entity)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 71, in process     parser.parse(StringIO.StringIO(q.content))      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 107, in parse     xmlreader.IncrementalParser.parse(self, source)      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/xmlreader.py\", line 123, in parse     self.feed(buffer)     File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 207, in feed     self._parser.Parse(data, isFinal)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 136, in characters        print ch    UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)         ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)",
        "A_Content": "  The problem is that you're trying to print an unicode character to a possibly non-unicode terminal. You need to encode it with the 'replace option before printing it, e.g. print ch.encode(sys.stdout.encoding, 'replace').     ",
        "Language": "Python",
        "Tags": [
            "python",
            "google-app-engine",
            "xml-parsing"
        ],
        "URL": "https://stackoverflow.com/questions/5141559/unicodeencodeerror-ascii-codec-cant-encode-character-u-xef-in-position-0",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to parse my XML document. So I have stored my XML document as below  class XMLdocs(db.Expando):      id = db.IntegerProperty()        name=db.StringProperty()      content=db.BlobProperty()     Now my below is my code  parser = make_parser()      curHandler = BasketBallHandler()   parser.setContentHandler(curHandler)   for q in XMLdocs.all():           parser.parse(StringIO.StringIO(q.content))   I am getting below error  'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128) Traceback (most recent call last):     File \"/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/__init__.py\", line 517, in __call__     handler.post(*groups)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/base_handler.py\", line 59, in post     self.handle()      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 168, in handle     scan_aborted = not self.process_entity(entity, ctx)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 233, in process_entity     handler(entity)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 71, in process     parser.parse(StringIO.StringIO(q.content))      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 107, in parse     xmlreader.IncrementalParser.parse(self, source)      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/xmlreader.py\", line 123, in parse     self.feed(buffer)     File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 207, in feed     self._parser.Parse(data, isFinal)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 136, in characters        print ch    UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)         ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)",
        "A_Content": "  An easy solution to overcome this problem is to set your default encoding to utf8. Follow is an example  import sys  reload(sys) sys.setdefaultencoding('utf8')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "google-app-engine",
            "xml-parsing"
        ],
        "URL": "https://stackoverflow.com/questions/5141559/unicodeencodeerror-ascii-codec-cant-encode-character-u-xef-in-position-0",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to parse my XML document. So I have stored my XML document as below  class XMLdocs(db.Expando):      id = db.IntegerProperty()        name=db.StringProperty()      content=db.BlobProperty()     Now my below is my code  parser = make_parser()      curHandler = BasketBallHandler()   parser.setContentHandler(curHandler)   for q in XMLdocs.all():           parser.parse(StringIO.StringIO(q.content))   I am getting below error  'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128) Traceback (most recent call last):     File \"/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/__init__.py\", line 517, in __call__     handler.post(*groups)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/base_handler.py\", line 59, in post     self.handle()      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 168, in handle     scan_aborted = not self.process_entity(entity, ctx)      File \"/base/data/home/apps/parsepython/1.348669006354245654/mapreduce/handlers.py\", line 233, in process_entity     handler(entity)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 71, in process     parser.parse(StringIO.StringIO(q.content))      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 107, in parse     xmlreader.IncrementalParser.parse(self, source)      File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/xmlreader.py\", line 123, in parse     self.feed(buffer)     File \"/base/python_runtime/python_dist/lib/python2.5/xml/sax/expatreader.py\", line 207, in feed     self._parser.Parse(data, isFinal)      File \"/base/data/home/apps/parsepython/1.348669006354245654/parseXML.py\", line 136, in characters        print ch    UnicodeEncodeError: 'ascii' codec can't encode character u'\\xef' in position 0: ordinal not in range(128)         ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  You can use consumer-producer pattern. For example you can create queue that is shared between threads. First thread that fetches data from the web enqueues this data in the shared queue. Another thread that owns database connection dequeues data from the queue and passes it to the database.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "37",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  Contrary to popular belief, newer versions of sqlite3 do support access from multiple threads.  This can be enabled via optional keyword argument check_same_thread:  sqlite.connect(\":memory:\", check_same_thread=False)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "155",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  The following found on mail.python.org.pipermail.1239789  I have found the solution. I don't know why python documentation has not a single word about this option.  So we have to add a new keyword argument to connection function  and we will be able to create cursors out of it in different thread. So use:  sqlite.connect(\":memory:\", check_same_thread = False)   works out perfectly for me. Of course from now on I need to take care  of safe multithreading access to the db. Anyway thx all for trying to help.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "16",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  You shouldn't be using threads at all for this.  This is a trivial task for twisted and that would likely take you significantly further anyway.  Use only one thread, and have the completion of the request trigger an event to do the write.  twisted will take care of the scheduling, callbacks, etc... for you.  It'll hand you the entire result as a string, or you can run it through a stream-processor (I have a twitter API and a friendfeed API that both fire off events to callers as results are still being downloaded).  Depending on what you're doing with your data, you could just dump the full result into sqlite as it's complete, cook it and dump it, or cook it while it's being read and dump it at the end.  I have a very simple application that does something close to what you're wanting on github.  I call it pfetch (parallel fetch).  It grabs various pages on a schedule, streams the results to a file, and optionally runs a script upon successful completion of each one.  It also does some fancy stuff like conditional GETs, but still could be a good base for whatever you're doing.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  Switch to multiprocessing. It is much better, scales well, can go beyond the use of multiple cores by using multiple CPUs, and the interface is the same as using python threading module.  Or, as Ali suggested, just use SQLAlchemy's thread pooling mechanism. It will handle everything for you automatically and has many extra features, just to quote some of them:   SQLAlchemy includes dialects for SQLite, Postgres, MySQL, Oracle, MS-SQL, Firebird, MaxDB, MS Access, Sybase and Informix; IBM has also released a DB2 driver. So you don't have to rewrite your application if you decide to move away from SQLite. The Unit Of Work system, a central part of SQLAlchemy's Object Relational Mapper (ORM), organizes pending create/insert/update/delete operations into queues and flushes them all in one batch. To accomplish this it performs a topological \"dependency sort\" of all modified items in the queue so as to honor foreign key constraints, and groups redundant statements together where they can sometimes be batched even further. This produces the maxiumum efficiency and transaction safety, and minimizes chances of deadlocks.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  Or if you are lazy, like me, you can use SQLAlchemy. It will handle the threading for you, (using thread local, and some connection pooling) and the way it does it is even configurable.  For added bonus, if/when you realise/decide that using Sqlite for any concurrent application is going to be a disaster, you won't have to change your code to use MySQL, or Postgres, or anything else. You can just switch over.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  You need to use session.close() after every transaction to the database in order to use the same cursor in the same thread not using the same cursor in multi-threads which cause this error.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  I like Evgeny's answer - Queues are generally the best way to implement inter-thread communication. For completeness, here are some other options:   Close the DB connection when the spawned threads have finished using it. This would fix your OperationalError, but opening and closing connections like this is generally a No-No, due to performance overhead. Don't use child threads. If the once-per-second task is reasonably lightweight, you could get away with doing the fetch and store, then sleeping until the right moment. This is undesirable as fetch and store operations could take >1sec, and you lose the benefit of multiplexed resources you have with a multi-threaded approach.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  You need to design the concurrency for your program. SQLite has clear limitations and you need to obey them, see the FAQ (also the following question).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  Scrapy seems like a potential answer to my question. Its home page describes my exact task. (Though I'm not sure how stable the code is yet.)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  I would take a look at the y_serial Python module for data persistence: http://yserial.sourceforge.net   which handles deadlock issues surrounding a single SQLite database. If demand on concurrency gets heavy one can easily set up the class Farm of many databases to diffuse the load over stochastic time.  Hope this helps your project... it should be simple enough to implement in 10 minutes.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  Use threading.Lock()     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  I could not find any benchmarks in any of the above answers so I wrote a test to benchmark everything.  I tried 3 approaches   Reading and writing sequentially from the SQLite database Using a ThreadPoolExecutor to read/write Using a ProcessPoolExecutor to read/write   The results and takeaways from the benchmark are as follows   Sequential reads/sequential writes work the best If you must process in parallel, use the ProcessPoolExecutor to read in parallel Do not perform any writes either using the ThreadPoolExecutor or using the ProcessPoolExecutor as you will run into database locked errors and you will have to retry inserting the chunk again   You can find the code and complete solution for the benchmarks in my SO answer HERE Hope that helps!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Python sqlite3 and concurrency",
        "A_Content": "  The most likely reason you get errors with locked databases is that you must issue  conn.commit()   after finishing a database operation. If you do not, your database will be write-locked and stay that way. The other threads that are waiting to write will time-out after a time (default is set to 5 seconds, see http://docs.python.org/2/library/sqlite3.html#sqlite3.connect for details on that).  An example of a correct and concurrent insertion would be this:  import threading, sqlite3 class InsertionThread(threading.Thread):      def __init__(self, number):         super(InsertionThread, self).__init__()         self.number = number      def run(self):         conn = sqlite3.connect('yourdb.db', timeout=5)         conn.execute('CREATE TABLE IF NOT EXISTS threadcount (threadnum, count);')         conn.commit()          for i in range(1000):             conn.execute(\"INSERT INTO threadcount VALUES (?, ?);\", (self.number, i))             conn.commit()  # create as many of these as you wish # but be careful to set the timeout value appropriately: thread switching in # python takes some time for i in range(2):     t = InsertionThread(i)     t.start()   If you like SQLite, or have other tools that work with SQLite databases, or want to replace CSV files with SQLite db files, or must do something rare like inter-platform IPC, then SQLite is a great tool and very fitting for the purpose. Don't let yourself be pressured into using a different solution if it doesn't feel right!       ",
        "Language": "Python",
        "Tags": [
            "python",
            "sqlite"
        ],
        "URL": "https://stackoverflow.com/questions/393554/python-sqlite3-and-concurrency",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a Python program that uses the \"threading\" module. Once every second, my program starts a new thread that fetches some data from the web, and stores this data to my hard drive. I would like to use sqlite3 to store these results, but I can't get it to work. The issue seems to be about the following line:  conn = sqlite3.connect(\"mydatabase.db\")    If I put this line of code inside each thread, I get an OperationalError telling me that the database file is locked. I guess this means that another thread has mydatabase.db open through a sqlite3 connection and has locked it. If I put this line of code in the main program and pass the connection object (conn) to each thread, I get a ProgrammingError, saying that SQLite objects created in a thread can only be used in that same thread.   Previously I was storing all my results in CSV files, and did not have any of these file-locking issues. Hopefully this will be possible with sqlite. Any ideas?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  You need to use join method of Thread object in the end of the script.  t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC))  t1.start() t2.start() t3.start()  t1.join() t2.join() t3.join()   Thus the main thread will wait till t1, t2 and t3 finish execution.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "99",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  Put the threads in a list and then use the Join method   threads = []   t = Thread(...)  threads.append(t)   ...repeat as often as necessary...   # Start all threads  for x in threads:      x.start()   # Wait for all of them to finish  for x in threads:      x.join()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "123",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  I prefer using list comprehension based on an input list:  inputs = [scriptA + argumentsA, scriptA + argumentsB, ...] threads = [Thread(target=call_script, args=(i)) for i in inputs] [t.start() for t in threads] [t.join() for t in threads]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  In Python3, since Python 3.2 there is a new approach to reach the same result, that I personally prefer to the traditional thread creation/start/join, package concurrent.futures: https://docs.python.org/3/library/concurrent.futures.html  Using a ThreadPoolExecutor the code would be:  from concurrent.futures.thread import ThreadPoolExecutor  def call_script(arg)     subprocess.call(scriptA + arg)  args = [argumentsA, argumentsB, argumentsC] with ThreadPoolExecutor(max_workers=2) as executor:     for arg in args:         executor.submit(call_script, arg) print('All tasks has been finished')   One of the advantages is that you can control the throughput setting the max concurrent workers.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  You can have class something like below from which you can add 'n' number of functions or console_scripts you want to execute in parallel passion and start the execution and wait for all jobs to complete..      from multiprocessing import Process  class ProcessParallel(object):     \"\"\"     To Process the  functions parallely      \"\"\"         def __init__(self, *jobs):         \"\"\"         \"\"\"         self.jobs = jobs         self.processes = []      def fork_processes(self):         \"\"\"         Creates the process objects for given function deligates         \"\"\"         for job in self.jobs:             proc  = Process(target=job)             self.processes.append(proc)      def start_all(self):         \"\"\"         Starts the functions process all together.         \"\"\"         for proc in self.processes:             proc.start()      def join_all(self):         \"\"\"         Waits untill all the functions executed.         \"\"\"         for proc in self.processes:             proc.join()   def two_sum(a=2, b=2):     return a + b  def multiply(a=2, b=2):     return a * b   #How to run: if __name__ == '__main__':     #note: two_sum, multiply can be replace with any python console scripts which     #you wanted to run parallel..     procs =  ProcessParallel(two_sum, multiply)     #Add all the process in list     procs.fork_processes()     #starts  process execution      procs.start_all()     #wait until all the process got executed     procs.join_all()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  Maybe, something like    for t in threading.enumerate():     if t.daemon:         t.join()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  I just came across the same problem where I needed to wait for all the threads which were created using the for loop.I just tried out the following piece of code.It may not be the perfect solution but I thought it would be a simple solution to test:  for t in threading.enumerate():     try:         t.join()     except RuntimeError as err:         if 'cannot join current thread' in err:             continue         else:             raise      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "python multithreading wait till all threads finished",
        "A_Content": "  From the threading module documentation     There is a main thread object; this corresponds to the initial   thread of control in the Python program. It is not a daemon thread.      There is the possibility that dummy thread objects are created.   These are thread objects corresponding to alien threads, which are   threads of control started outside the threading module, such as   directly from C code. Dummy thread objects have limited functionality;   they are always considered alive and daemonic, and cannot be join()ed.   They are never deleted, since it is impossible to detect the   termination of alien threads.   So, to catch those two cases when you are not interested in keeping a list of the threads you create:  import threading as thrd   def alter_data(data, index):     data[index] *= 2   data = [0, 2, 6, 20]  for i, value in enumerate(data):     thrd.Thread(target=alter_data, args=[data, i]).start()  for thread in thrd.enumerate():     if thread.daemon:         continue     try:         thread.join()     except RuntimeError as err:         if 'cannot join current thread' in err.args[0]:             # catchs main thread             continue         else:             raise   Whereupon:  >>> print(data) [0, 4, 12, 40]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "multithreading"
        ],
        "URL": "https://stackoverflow.com/questions/11968689/python-multithreading-wait-till-all-threads-finished",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    This may have been asked in a similar context but I was unable to find an answer after about 20 minutes of searching, so I will ask.  I have written a Python script (lets say: scriptA.py) and a script (lets say scriptB.py)  In scriptB I want to call scriptA multiple times with different arguments, each time takes about an hour to run, (its a huge script, does lots of stuff.. don't worry about it) and I want to be able to run the scriptA with all the different arguments simultaneously, but I need to wait till ALL of them are done before continuing; my code:  import subprocess  #setup do_setup()  #run scriptA subprocess.call(scriptA + argumentsA) subprocess.call(scriptA + argumentsB) subprocess.call(scriptA + argumentsC)  #finish do_finish()   I want to do run all the subprocess.call() at the same time, and then wait till they are all done, how should I do this?   I tried to use threading like the example here:  from threading import Thread import subprocess  def call_script(args)     subprocess.call(args)  #run scriptA    t1 = Thread(target=call_script, args=(scriptA + argumentsA)) t2 = Thread(target=call_script, args=(scriptA + argumentsB)) t3 = Thread(target=call_script, args=(scriptA + argumentsC)) t1.start() t2.start() t3.start()   But I do not think this is right.   How do I know they have all finished running before going to my do_finish()?     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  You can use the package sklearn and its associated preprocessing utilities to normalize the data.  from sklearn import preprocessing  x = df.values #returns a numpy array min_max_scaler = preprocessing.MinMaxScaler() x_scaled = min_max_scaler.fit_transform(x) df = pandas.DataFrame(x_scaled)   For more information look at the scikit-learn documentation on preprocessing data: scaling features to a range.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "77",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  one easy way by using Pandas: (here I want to use mean normalization)  normalized_df=(df-df.mean())/df.std()   to use min-max normalization:  normalized_df=(df-df.min())/(df.max()-df.min())      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "116",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  Based on this post: https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range  You can do the following:  def normalize(df):     result = df.copy()     for feature_name in df.columns:         max_value = df[feature_name].max()         min_value = df[feature_name].min()         result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)     return result   You don't need to stay worrying about whether your values are negative or positive. And the values should be nicely spread out between 0 and 1.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "30",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  Your problem is actually a simple transform acting on the columns:  def f(s):     return s/s.max()  frame.apply(f, axis=0)   Or even more terse:     frame.apply(lambda x: x/x.max(), axis=0)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  If you like using the sklearn package, you can keep the column and index names by using pandas loc like so:  from sklearn.preprocessing import MinMaxScaler  scaler = MinMaxScaler()  scaled_values = scaler.fit_transform(df)  df.loc[:,:] = scaled_values      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  I think that a better way to do that in pandas is just  df = df/df.max().astype(np.float64)   Edit If in your data frame negative numbers are present you should use instead  df = df/df.loc[df.abs().idxmax()].astype(np.float64)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  Simple is Beautiful:  df[\"A\"] = df[\"A\"] / df[\"A\"].max() df[\"B\"] = df[\"B\"] / df[\"B\"].max() df[\"C\"] = df[\"C\"] / df[\"C\"].max()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  The solution given by Sandman and Praveen is very well. The only problem with that if you have categorical variables in other columns of your data frame this method will need some adjustments.   My solution to this type of issue is following:   from sklearn import preprocesing  x = pd.concat([df.Numerical1, df.Numerical2,df.Numerical3])  min_max_scaler = preprocessing.MinMaxScaler()  x_scaled = min_max_scaler.fit_transform(x)  x_new = pd.DataFrame(x_scaled)  df = pd.concat([df.Categoricals,x_new])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Normalize columns of pandas data frame",
        "A_Content": "  def normalize(x):     try:         x = x/np.linalg.norm(x,ord=1)         return x     except :         raise data = pd.DataFrame.apply(data,normalize)   From the document of pandas,DataFrame structure can apply an operation (function) to itself .  DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, args=(), **kwds)      Applies function along input axis of DataFrame.   Objects passed to functions are Series objects having index either the DataFrames index (axis=0) or the columns (axis=1). Return type depends on whether passed function aggregates, or the reduce argument if the DataFrame is empty.   You can apply a custom function to operate the DataFrame .       ",
        "Language": "Python",
        "Tags": [
            "python",
            "pandas",
            "normalize"
        ],
        "URL": "https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a dataframe in pandas where each column has different value range. For example:  df:  A     B   C 1000  10  0.5 765   5   0.35 800   7   0.09   Any idea how I can normalize the columns of this dataframe where each value is between 0 and 1?  My desired output is:  A     B    C 1     1    1 0.765 0.5  0.7 0.8   0.7  0.18(which is 0.09/0.5)      ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a ceiling equivalent of // operator in Python?",
        "A_Content": "  There is no operator which divides with ceil.  You need to import math and use math.ceil     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/14822184/is-there-a-ceiling-equivalent-of-operator-in-python",
        "A_Votes": "43",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I found out about the // operator in Python which in Python 3 does division with floor.  Is there an operator which divides with ceil instead? (I know about the / operator which in Python 3 does floating point division.)     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a ceiling equivalent of // operator in Python?",
        "A_Content": "  You can just do upside-down floor division:  def ceildiv(a, b):     return -(-a // b)   This works because Python's division operator does floor division (unlike in C, where integer division truncates the fractional part).  This also works with Python's big integers, because there's no (lossy) floating-point conversion.  Here's a demonstration:  >>> from __future__ import division   # a/b is float division >>> from math import ceil >>> b = 3 >>> for a in range(-7, 8): ...     print([\"%d/%d\" % (a, b), int(ceil(a / b)), -(-a // b)]) ...  ['-7/3', -2, -2] ['-6/3', -2, -2] ['-5/3', -1, -1] ['-4/3', -1, -1] ['-3/3', -1, -1] ['-2/3', 0, 0] ['-1/3', 0, 0] ['0/3', 0, 0] ['1/3', 1, 1] ['2/3', 1, 1] ['3/3', 1, 1] ['4/3', 2, 2] ['5/3', 2, 2] ['6/3', 2, 2] ['7/3', 3, 3]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/14822184/is-there-a-ceiling-equivalent-of-operator-in-python",
        "A_Votes": "198",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I found out about the // operator in Python which in Python 3 does division with floor.  Is there an operator which divides with ceil instead? (I know about the / operator which in Python 3 does floating point division.)     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a ceiling equivalent of // operator in Python?",
        "A_Content": "  You could do (x + (d-1)) // d when dividing x by d, i.e. (x + 4) // 5.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/14822184/is-there-a-ceiling-equivalent-of-operator-in-python",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I found out about the // operator in Python which in Python 3 does division with floor.  Is there an operator which divides with ceil instead? (I know about the / operator which in Python 3 does floating point division.)     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a ceiling equivalent of // operator in Python?",
        "A_Content": "  You can always just do it inline as well  ((foo - 1) // bar) + 1   In python3, this is just shy of an order of magnitude faster than forcing the float division and calling ceil(), provided you care about the speed. Which you shouldn't, unless you've proven through usage that you need to.  >>> timeit.timeit(\"((5 - 1) // 4) + 1\", number = 100000000) 1.7249219375662506 >>> timeit.timeit(\"ceil(5/4)\", setup=\"from math import ceil\", number = 100000000) 12.096064013894647      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/14822184/is-there-a-ceiling-equivalent-of-operator-in-python",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I found out about the // operator in Python which in Python 3 does division with floor.  Is there an operator which divides with ceil instead? (I know about the / operator which in Python 3 does floating point division.)     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Is there a ceiling equivalent of // operator in Python?",
        "A_Content": "  Note that math.ceil is limited to 53 bits of precision. If you are working with large integers, you may not get exact results.  The gmpy2 libary provides a c_div function which uses ceiling rounding.  Disclaimer: I maintain gmpy2.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-3.x"
        ],
        "URL": "https://stackoverflow.com/questions/14822184/is-there-a-ceiling-equivalent-of-operator-in-python",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I found out about the // operator in Python which in Python 3 does division with floor.  Is there an operator which divides with ceil instead? (I know about the / operator which in Python 3 does floating point division.)     ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Don't use index to loop over a sequence  Don't :  for i in range(len(tab)) :     print tab[i]   Do :  for elem in tab :     print elem   For will automate most iteration operations for you.  Use enumerate if you really need both the index and the element.  for i, elem in enumerate(tab):      print i, elem   Be careful when using \"==\" to check against True or False   if (var == True) :     # this will execute if var is True or 1, 1.0, 1L  if (var != True) :     # this will execute if var is neither True nor 1  if (var == False) :     # this will execute if var is False or 0 (or 0.0, 0L, 0j)  if (var == None) :     # only execute if var is None  if var :     # execute if var is a non-empty string/list/dictionary/tuple, non-0, etc  if not var :     # execute if var is \"\", {}, [], (), 0, None, etc.  if var is True :     # only execute if var is boolean True, not 1  if var is False :     # only execute if var is boolean False, not 0  if var is None :     # same as var == None   Do not check if you can, just do it and handle the error  Pythonistas usually say \"It's easier to ask for forgiveness than permission\".  Don't :  if os.path.isfile(file_path) :     file = open(file_path) else :     # do something   Do :  try :     file =  open(file_path) except OSError as e:     # do something   Or even better with python 2.6 / 3:  with open(file_path) as file :   It is much better because it's much more generical. You can apply \"try / except\" to almost anything. You don't need to care about what to do to prevent it, just about the error you are risking.  Do not check against type   Python is dynamically typed, therefore checking for type makes you lose flexibility. Instead, use duck typing by checking behavior. E.G, you expect a string in a function, then use str() to convert any object in a string. You expect a list, use list() to convert any iterable in a list.  Don't :  def foo(name) :     if isinstance(name, str) :         print name.lower()  def bar(listing) :     if isinstance(listing, list) :         listing.extend((1, 2, 3))         return \", \".join(listing)   Do :  def foo(name) :     print str(name).lower()  def bar(listing) :     l = list(listing)     l.extend((1, 2, 3))     return \", \".join(l)   Using the last way, foo will accept any object. Bar will accept strings, tuples, sets, lists and much more. Cheap DRY :-)  Don't mix spaces and tabs  Just don't. You would cry.  Use object as first parent  This is tricky, but it will bite you as your program grows. There are old and new classes in Python 2.x. The old ones are, well, old. They lack some features, and can have awkward behavior with inheritance. To be usable, any of your class must be of the \"new style\". To do so, make it inherit from \"object\" :  Don't :  class Father :     pass  class Child(Father) :     pass   Do :  class Father(object) :     pass   class Child(Father) :     pass   In Python 3.x all classes are new style so you can declare class Father: is fine.  Don't initialize class attributes outside the __init__ method  People coming from other languages find it tempting because that what you do the job in Java or PHP. You write the class name, then list your attributes and give them a default value. It seems to work in Python, however, this doesn't work the way you think.  Doing that will setup class attributes (static attributes), then when you will try to get the object attribute, it will gives you its value unless it's empty. In that case it will return the class attributes.   It implies two big hazards :   If the class attribute is changed, then the initial value is changed.  If you set a mutable object as a default    value, you'll get the same object shared across instances.   Don't (unless you want static) :  class Car(object):     color = \"red\"     wheels = [wheel(), Wheel(), Wheel(), Wheel()]   Do :   class Car(object):     def __init__(self):         self.color = \"red\"         self.wheels = [wheel(), Wheel(), Wheel(), Wheel()]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "67",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  When you need a population of arrays you might be tempted to type something like this:  >>> a=[[1,2,3,4,5]]*4   And sure enough it will give you what you expect when you look at it  >>> from pprint import pprint >>> pprint(a)  [[1, 2, 3, 4, 5],  [1, 2, 3, 4, 5],  [1, 2, 3, 4, 5],  [1, 2, 3, 4, 5]]   But don't expect the elements of your population to be seperate objects:  >>> a[0][0] = 2 >>> pprint(a)  [[2, 2, 3, 4, 5],  [2, 2, 3, 4, 5],  [2, 2, 3, 4, 5],  [2, 2, 3, 4, 5]]   Unless this is what you need...  It is worth mentioning a workaround:  a = [[1,2,3,4,5] for _ in range(4)]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "33",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Python Language Gotchas -- things that fail in very obscure ways   Using mutable default arguments. Leading zeroes mean octal.  09 is a very obscure syntax error in Python 2.x Misspelling overridden method names in a superclass or subclass.  The superclass misspelling mistake is worse, because none of the subclasses override it correctly.   Python Design Gotchas   Spending time on introspection (e.g. trying to automatically determine types or superclass identity or other stuff).  First, it's obvious from reading the source.  More importantly, time spent on weird Python introspection usually indicates a fundamental failure to grasp polymorphism.  80% of the Python introspection questions on SO are failure to get Polymorphism. Spending time on code golf.  Just because your mental model of your application is four keywords (\"do\", \"what\", \"I\", \"mean\"), doesn't mean you should build a hyper-complex introspective decorator-driven framework to do that.  Python allows you to take DRY to a level that is silliness.  The rest of the Python introspection questions on SO attempts to reduce complex problems to code golf exercises. Monkeypatching. Failure to actually read through the standard library, and reinventing the wheel. Conflating interactive type-as-you go Python with a proper program.  While you're typing interactively, you may lose track of a variable and have to use globals().  Also, while you're typing, almost everything is global.  In proper programs, you'll never \"lose track of\" a variable, and nothing will be global.      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "28",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Mutating a default argument:  def foo(bar=[]):     bar.append('baz')     return bar   The default value is evaluated only once, and not every time the function is called. Repeated calls to foo() would return ['baz'], ['baz', 'baz'], ['baz', 'baz', 'baz'], ...  If you want to mutate bar do something like this:  def foo(bar=None):     if bar is None:         bar = []      bar.append('baz')     return bar   Or, if you like arguments to be final:  def foo(bar=[]):     not_bar = bar[:]      not_bar.append('baz')     return not_bar      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "25",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  I don't know whether this is a common mistake, but while Python doesn't have increment and decrement operators, double signs are allowed, so  ++i   and  --i   is syntactically correct code, but doesn't do anything \"useful\" or that you may be expecting.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "21",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Rolling your own code before looking in the standard library. For example, writing this:  def repeat_list(items):     while True:         for item in items:             yield item   When you could just use this:  from itertools import cycle   Examples of frequently overlooked modules (besides itertools) include:   optparse for creating command line parsers ConfigParser for reading configuration files in a standard manner tempfile for creating and managing temporary files shelve for storing Python objects to disk, handy when a full fledged database is overkill      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Avoid using keywords as your own identifiers.   Also, it's always good to not use from somemodule import *.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  If you're coming from C++, realize that variables declared in a class definition are static.  You can initialize nonstatic members in the init method.  Example:  class MyClass:   static_member = 1    def __init__(self):     self.non_static_member = random()      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Not using functional tools.  This isn't just a mistake from a style standpoint, it's a mistake from a speed standpoint because a lot of the functional tools are optimized in C.  This is the most common example:  temporary = [] for item in itemlist:     temporary.append(somefunction(item)) itemlist = temporary   The correct way to do it:  itemlist = map(somefunction, itemlist)   The just as correct way to do it:  itemlist = [somefunction(x) for x in itemlist]   And if you only need the processed items available one at a time, rather than all at once, you can save memory and improve speed by using the iterable equivalents  # itertools-based iterator itemiter = itertools.imap(somefunction, itemlist) # generator expression-based iterator itemiter = (somefunction(x) for x in itemlist)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Surprised that nobody said this:     Mix tab and spaces when indenting.   Really, it's a killer. Believe me. In particular, if it runs.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Code Like a Pythonista: Idiomatic Python     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Importing re and using the full regular expression approach to string matching/transformation, when perfectly good string methods exist for every common operation (e.g. capitalisation, simple matching/searching).     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "   don't write large output messages to standard output strings are immutable - build them not using \"+\" operator but rather using str.join() function. read those articles:   python gotchas things to avoid Gotchas for Python users Python Landmines    Last link is the original one, this SO question is an duplicate.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Using the %s formatter in error messages. In almost every circumstance, %r should be used.  For example, imagine code like this:  try:     get_person(person) except NoSuchPerson:     logger.error(\"Person %s not found.\" %(person))   Printed this error:  ERROR: Person wolever not found.  It's impossible to tell if the person variable is the string \"wolever\", the unicode string u\"wolever\" or an instance of the Person class (which has __str__ defined as def __str__(self): return self.name). Whereas, if %r was used, there would be three different error messages:  ... logger.error(\"Person %r not found.\" %(person))   Would produce the much more helpful errors:  ERROR: Person 'wolever' not found. ERROR: Person u'wolever' not found. ERROR: Person  not found.  Another good reason for this is that paths are a whole lot easier to copy/paste. Imagine:  try:     stuff = open(path).read() except IOError:     logger.error(\"Could not open %s\" %(path))   If path is some path/with 'strange' \"characters\", the error message will be:  ERROR: Could not open some path/with 'strange' \"characters\"  Which is hard to visually parse and hard to copy/paste into a shell.  Whereas, if %r is used, the error would be:  ERROR: Could not open 'some path/with \\'strange\\' \"characters\"'  Easy to visually parse, easy to copy-paste, all around better.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Normal copying (assigning) is done by reference, so filling a container by adapting the same object and inserting, ends up with a container with references to the last added object.  Use copy.deepcopy instead.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  A bad habit I had to train myself out of was using X and Y or Z for inline logic.  Unless you can 100% always guarantee that Y will be a true value, even when your code changes in 18 months time, you set yourself up for some unexpected behaviour.  Thankfully, in later versions you can use Y if X else Z.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  I would stop using deprecated methods in 2.6, so that your app or script will be ready and easier to convert to Python 3.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  import this       Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those!    import not_this   Write ugly code. Write implicit code. Write complex code. Write nested code. Write dense code. Write unreadable code. Write special cases. Strive for purity. Ignore errors and exceptions. Write optimal code before releasing. Every implementation needs a flowchart. Don't use namespaces.       ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  I've started learning Python as well and one of the bigest mistakes I made is constantly using C++/C# indexed \"for\" loop. Python have for(i ; i < length ; i++) type loop and for a good reason - most of the time there are better ways to do there same thing.  Example: I had a method that iterated over a list and returned the indexes of selected items:  for i in range(len(myList)):     if myList[i].selected:         retVal.append(i)   Instead Python has list comprehension that solves the same problem in a more elegant and easy to read way:  retVal = [index for index, item in enumerate(myList) if item.selected]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Never assume that having a multi-threaded Python application and a SMP capable machine (for instance one equipped with a multi-core CPU) will give you the benefit of introducing true parallelism into your application. Most likely it will not because of GIL (Global Interpreter Lock) which synchronizes your application on the byte-code interpreter level.   There are some workarounds like taking advantage of SMP by putting the concurrent code in  C API calls or using multiple processes (instead of threads) via wrappers (for instance like the one available at http://www.parallelpython.org) but if one needs true multi-threading in Python one should look at things like Jython, IronPython etc. (GIL is a feature of the CPython interpreter so other implementations are not affected).  According to Python 3000 FAQ (available at Artima) the above still stands even for the latest Python versions.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Some personal opinions, but I find it best NOT to:    use deprecated modules (use warnings for them) overuse classes and inheritance (typical of static languages legacy maybe) explicitly use declarative algorithms (as iteration with for vs use of  itertools) reimplement functions from the standard lib, \"because I don't need all of those features\" using features for the sake of it (reducing compatibility with older Python versions) using metaclasses when you really don't have to and more generally make things too \"magic\" avoid using generators (more personal) try to micro-optimize CPython code on a low-level basis. Better spend time on algorithms and then optimize by making a small C shared lib called by ctypes (it's so easy to gain 5x perf boosts on an inner loop) use unnecessary lists when iterators would suffice code a project directly for 3.x before the libs you need are all available (this point may be a bit controversial now!)      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Somewhat related to the default mutable argument, how one checks for the \"missing\" case results in differences when an empty list is passed:  def func1(toc=None):     if not toc:         toc = []     toc.append('bar')  def func2(toc=None):     if toc is None:         toc = []     toc.append('bar')  def demo(toc, func):     print func.__name__     print '  before:', toc     func(toc)     print '  after:', toc  demo([], func1) demo([], func2)   Here's the output:  func1   before: []   after: [] func2   before: []   after: ['bar']      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  You've mentioned default arguments... One that's almost as bad as mutable default arguments: default values which aren't None.  Consider a function which will cook some food:  def cook(breakfast=\"spam\"):     arrange_ingredients_for(breakfast)     heat_ingredients_for(breakfast)     serve(breakfast)   Because it specifies a default value for breakfast, it is impossible for some other function to say \"cook your default breakfast\" without a special-case:  def order(breakfast=None):     if breakfast is None:         cook()     else:         cook(breakfast)   However, this could be avoided if cook used None as a default value:  def cook(breakfast=None):     if breakfast is None:         breakfast = \"spam\"  def order(breakfast=None):     cook(breakfast)   A good example of this is Django bug #6988. Django's caching module had a \"save to cache\" function which looked like this:  def set(key, value, timeout=0):     if timeout == 0:         timeout = settings.DEFAULT_TIMEOUT     _caching_backend.set(key, value, timeout)   But, for the memcached backend, a timeout of 0 means \"never timeout\" Which, as you can see, would be impossible to specify.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Don't modify a list while iterating over it.  odd = lambda x : bool(x % 2) numbers = range(10) for i in range(len(numbers)):     if odd(numbers[i]):         del numbers[i]   One common suggestion to work around this problem is to iterate over the list in reverse:  for i in range(len(numbers)-1,0,-1):     if odd(numbers[i]):         del numbers[i]   But even better is to use a list comprehension to build a new list to replace the old:  numbers[:] = [n for n in numbers if not odd(n)]      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  ++n and --n may not work as expected by people coming from C or Java background.  ++n is positive of a positive number, which is simply n.  --n is negative of a negative number, which is simply n.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  The very first mistake before you even start: don't be afraid of whitespace.  When you show someone a piece of Python code, they are impressed until you tell them that they have to indent correctly. For some reason, most people feel that a language shouldn't force a certain style on them while all of them will indent the code nonetheless.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  my_variable = <something> ... my_varaible = f(my_variable) ... use my_variable and thinking it contains the result from f, and not the initial value   Python won't warn you in any way that on the second assignment you misspelled the variable name and created a new one.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Common pitfall: default arguments are evaluated once:  def x(a, l=[]):     l.append(a)     return l  print x(1) print x(2)   prints:  [1] [1, 2]   i.e. you always get the same list.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Similar to mutable default arguments is the mutable class attribute.  >>> class Classy: ...    foo = [] ...    def add(self, value): ...        self.foo.append(value) ...  >>> instance1 = Classy() >>> instance2 = Classy() >>> instance1.add(\"Foo!\") >>> instance2.foo ['Foo!']   Not what you expect.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Common pitfalls in Python [duplicate]",
        "A_Content": "  Creating a local module with the same name as one from the stdlib.  This is almost always done by accident (as reported in this question), but usually results in cryptic error messages.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/1011431/common-pitfalls-in-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "       Possible Duplicate:   Python 2.x gotchas and landmines       Today I was bitten again by mutable default arguments after many years. I usually don't use mutable default arguments unless needed, but I think with time I forgot about that.  Today in the application I added tocElements=[] in a PDF generation function's argument list and now \"Table of Contents\" gets longer and longer after each invocation of \"generate pdf\". :)  What else should I add to my list of things to MUST avoid?   Always import modules the same way, e.g. from y import x and import x are treated as different modules. Do not use range in place of lists because range() will become an iterator anyway, the following will fail:  myIndexList = [0, 1, 3] isListSorted = myIndexList == range(3)  # will fail in 3.0 isListSorted = myIndexList == list(range(3))  # will not   Same thing can be mistakenly done with xrange:  myIndexList == xrange(3)  Be careful catching multiple exception types:  try:     raise KeyError(\"hmm bug\") except KeyError, TypeError:     print TypeError   This prints \"hmm bug\", though it is not a bug; it looks like we are catching exceptions of both types, but instead we are catching KeyError only as variable TypeError, use this instead:  try:     raise KeyError(\"hmm bug\") except (KeyError, TypeError):     print TypeError       ",
        "Q_Votes": "70"
    },
    {
        "Q_Title": "Practical examples of NLTK use [closed]",
        "A_Content": "  Here's my own practical example for the benefit of anyone else looking this question up (excuse the sample text, it was the first thing I found on Wikipedia):  import nltk import pprint  tokenizer = None tagger = None  def init_nltk():     global tokenizer     global tagger     tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+|[^\\w\\s]+')     tagger = nltk.UnigramTagger(nltk.corpus.brown.tagged_sents())  def tag(text):     global tokenizer     global tagger     if not tokenizer:         init_nltk()     tokenized = tokenizer.tokenize(text)     tagged = tagger.tag(tokenized)     tagged.sort(lambda x,y:cmp(x[1],y[1]))     return tagged  def main():     text = \"\"\"Mr Blobby is a fictional character who featured on Noel     Edmonds' Saturday night entertainment show Noel's House Party,     which was often a ratings winner in the 1990s. Mr Blobby also     appeared on the Jamie Rose show of 1997. He was designed as an     outrageously over the top parody of a one-dimensional, mute novelty     character, which ironically made him distinctive, absurd and popular.     He was a large pink humanoid, covered with yellow spots, sporting a     permanent toothy grin and jiggling eyes. He communicated by saying     the word \"blobby\" in an electronically-altered voice, expressing     his moods through tone of voice and repetition.      There was a Mrs. Blobby, seen briefly in the video, and sold as a     doll.      However Mr Blobby actually started out as part of the 'Gotcha'     feature during the show's second series (originally called 'Gotcha     Oscars' until the threat of legal action from the Academy of Motion     Picture Arts and Sciences[citation needed]), in which celebrities     were caught out in a Candid Camera style prank. Celebrities such as     dancer Wayne Sleep and rugby union player Will Carling would be     enticed to take part in a fictitious children's programme based around     their profession. Mr Blobby would clumsily take part in the activity,     knocking over the set, causing mayhem and saying \"blobby blobby     blobby\", until finally when the prank was revealed, the Blobby     costume would be opened - revealing Noel inside. This was all the more     surprising for the \"victim\" as during rehearsals Blobby would be     played by an actor wearing only the arms and legs of the costume and     speaking in a normal manner.[citation needed]\"\"\"     tagged = tag(text)         l = list(set(tagged))     l.sort(lambda x,y:cmp(x[1],y[1]))     pprint.pprint(l)  if __name__ == '__main__':     main()   Output:  [('rugby', None),  ('Oscars', None),  ('1990s', None),  ('\",', None),  ('Candid', None),  ('\"', None),  ('blobby', None),  ('Edmonds', None),  ('Mr', None),  ('outrageously', None),  ('.[', None),  ('toothy', None),  ('Celebrities', None),  ('Gotcha', None),  (']),', None),  ('Jamie', None),  ('humanoid', None),  ('Blobby', None),  ('Carling', None),  ('enticed', None),  ('programme', None),  ('1997', None),  ('s', None),  (\"'\", \"'\"),  ('[', '('),  ('(', '('),  (']', ')'),  (',', ','),  ('.', '.'),  ('all', 'ABN'),  ('the', 'AT'),  ('an', 'AT'),  ('a', 'AT'),  ('be', 'BE'),  ('were', 'BED'),  ('was', 'BEDZ'),  ('is', 'BEZ'),  ('and', 'CC'),  ('one', 'CD'),  ('until', 'CS'),  ('as', 'CS'),  ('This', 'DT'),  ('There', 'EX'),  ('of', 'IN'),  ('inside', 'IN'),  ('from', 'IN'),  ('around', 'IN'),  ('with', 'IN'),  ('through', 'IN'),  ('-', 'IN'),  ('on', 'IN'),  ('in', 'IN'),  ('by', 'IN'),  ('during', 'IN'),  ('over', 'IN'),  ('for', 'IN'),  ('distinctive', 'JJ'),  ('permanent', 'JJ'),  ('mute', 'JJ'),  ('popular', 'JJ'),  ('such', 'JJ'),  ('fictional', 'JJ'),  ('yellow', 'JJ'),  ('pink', 'JJ'),  ('fictitious', 'JJ'),  ('normal', 'JJ'),  ('dimensional', 'JJ'),  ('legal', 'JJ'),  ('large', 'JJ'),  ('surprising', 'JJ'),  ('absurd', 'JJ'),  ('Will', 'MD'),  ('would', 'MD'),  ('style', 'NN'),  ('threat', 'NN'),  ('novelty', 'NN'),  ('union', 'NN'),  ('prank', 'NN'),  ('winner', 'NN'),  ('parody', 'NN'),  ('player', 'NN'),  ('actor', 'NN'),  ('character', 'NN'),  ('victim', 'NN'),  ('costume', 'NN'),  ('action', 'NN'),  ('activity', 'NN'),  ('dancer', 'NN'),  ('grin', 'NN'),  ('doll', 'NN'),  ('top', 'NN'),  ('mayhem', 'NN'),  ('citation', 'NN'),  ('part', 'NN'),  ('repetition', 'NN'),  ('manner', 'NN'),  ('tone', 'NN'),  ('Picture', 'NN'),  ('entertainment', 'NN'),  ('night', 'NN'),  ('series', 'NN'),  ('voice', 'NN'),  ('Mrs', 'NN'),  ('video', 'NN'),  ('Motion', 'NN'),  ('profession', 'NN'),  ('feature', 'NN'),  ('word', 'NN'),  ('Academy', 'NN-TL'),  ('Camera', 'NN-TL'),  ('Party', 'NN-TL'),  ('House', 'NN-TL'),  ('eyes', 'NNS'),  ('spots', 'NNS'),  ('rehearsals', 'NNS'),  ('ratings', 'NNS'),  ('arms', 'NNS'),  ('celebrities', 'NNS'),  ('children', 'NNS'),  ('moods', 'NNS'),  ('legs', 'NNS'),  ('Sciences', 'NNS-TL'),  ('Arts', 'NNS-TL'),  ('Wayne', 'NP'),  ('Rose', 'NP'),  ('Noel', 'NP'),  ('Saturday', 'NR'),  ('second', 'OD'),  ('his', 'PP$'),  ('their', 'PP$'),  ('him', 'PPO'),  ('He', 'PPS'),  ('more', 'QL'),  ('However', 'RB'),  ('actually', 'RB'),  ('also', 'RB'),  ('clumsily', 'RB'),  ('originally', 'RB'),  ('only', 'RB'),  ('often', 'RB'),  ('ironically', 'RB'),  ('briefly', 'RB'),  ('finally', 'RB'),  ('electronically', 'RB-HL'),  ('out', 'RP'),  ('to', 'TO'),  ('show', 'VB'),  ('Sleep', 'VB'),  ('take', 'VB'),  ('opened', 'VBD'),  ('played', 'VBD'),  ('caught', 'VBD'),  ('appeared', 'VBD'),  ('revealed', 'VBD'),  ('started', 'VBD'),  ('saying', 'VBG'),  ('causing', 'VBG'),  ('expressing', 'VBG'),  ('knocking', 'VBG'),  ('wearing', 'VBG'),  ('speaking', 'VBG'),  ('sporting', 'VBG'),  ('revealing', 'VBG'),  ('jiggling', 'VBG'),  ('sold', 'VBN'),  ('called', 'VBN'),  ('made', 'VBN'),  ('altered', 'VBN'),  ('based', 'VBN'),  ('designed', 'VBN'),  ('covered', 'VBN'),  ('communicated', 'VBN'),  ('needed', 'VBN'),  ('seen', 'VBN'),  ('set', 'VBN'),  ('featured', 'VBN'),  ('which', 'WDT'),  ('who', 'WPS'),  ('when', 'WRB')]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "nlp",
            "nltk"
        ],
        "URL": "https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use",
        "A_Votes": "27",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm playing around with the Natural Language Toolkit (NLTK).  Its documentation (Book and HOWTO) are quite bulky and the examples are sometimes slightly advanced.   Are there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the NTLK articles on the Stream Hacker blog.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Practical examples of NLTK use [closed]",
        "A_Content": "  NLP in general is very useful so you might want to broaden your search to general application of text analytics. I used NLTK to aid MOSS 2010 by generating file taxonomy by extracting concept maps. It worked really well. It doesn't take long before files start to cluster in useful ways.  Often times to understand text analytics you have to think in tangents to the ways you are used to thinking. For example, text analytics is extremely useful to discovery. Most people, though, don't even know what the difference is between search and discovery. If you read up on those subjects you will likely \"discover\" ways in which you might want to put NLTK to work.  Also, consider your world view of text files without NLTK. You have a bunch of random length strings separated by whitespace and punctuation. Some of the punctuation changes how it is used such as the period (which is also a decimal point and a postfix marker for an abbreviation.) With NLTK you get words and more to the point you get parts of speech. Now you have a handle on the content. Use NLTK to discover the concepts and actions in the document. Use NLTK to get at the \"meaning\" of the document. Meaning in this case refers to the essencial relationships in the document.  It is a good thing to be curious about NLTK. Text Analytics is set to breakout in a big way in the next few years. Those who understand it will be better suited to take advantage of the new opportunities better.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "nlp",
            "nltk"
        ],
        "URL": "https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use",
        "A_Votes": "17",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm playing around with the Natural Language Toolkit (NLTK).  Its documentation (Book and HOWTO) are quite bulky and the examples are sometimes slightly advanced.   Are there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the NTLK articles on the Stream Hacker blog.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Practical examples of NLTK use [closed]",
        "A_Content": "  I'm the author of streamhacker.com (and thanks for the mention, I get a fair amount of click traffic from this particular question). What specifically are you trying to do? NLTK has a lot of tools for doing various things, but is somewhat lacking clear information on what to use the tools for, and how best to use them. It's also oriented towards academic problems, and so it can be heavy going to translate the pedagogical examples to practical solutions.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "nlp",
            "nltk"
        ],
        "URL": "https://stackoverflow.com/questions/526469/practical-examples-of-nltk-use",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm playing around with the Natural Language Toolkit (NLTK).  Its documentation (Book and HOWTO) are quite bulky and the examples are sometimes slightly advanced.   Are there any good but basic examples of uses/applications of NLTK? I'm thinking of things like the NTLK articles on the Stream Hacker blog.      ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "I want to start reading the Python source code. Where should I start [closed]",
        "A_Content": "  Start by learning about the Python C API. It is a large and rich API, and the Python source naturally uses it all over the place.  You won't get very far into the Python source code before you have to understand what is meant by Py_INCREF and so on.  I gave a presentation at Pycon explaining the API: A Whirlwind Excursion through Python C Extensions that you might find helpful.  C extensions use the same API as the Python code itself.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "c"
        ],
        "URL": "https://stackoverflow.com/questions/1728472/i-want-to-start-reading-the-python-source-code-where-should-i-start",
        "A_Votes": "42",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to start reading the Python source code.  My experience,  I know Python and Java very well. I know some other languages at various levels of proficiency, but neither C/C+/ particularly well. I studied C in college, but have never professionally programmed in it.  My Reasons for reading this code.   Understand how python works under the hood. Learn C better.   1 is more important to me than 2.  how should I go about this?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "I want to start reading the Python source code. Where should I start [closed]",
        "A_Content": "  First, if you're mostly interested in 1, I'd start with reading the Python source of various modules (and not jump straight to the C). Whenever I found myself reading the source of some modules, I've always learned new things about Python programming.  Second, if you're trying to learn C better, I'd personally suggest something completely different: program in it. Just statically reading source code is not going to make you understand C better (or at least, it's a limited approach; it might make you a little better, but there's only so much that reading source will get you).  After programming at least a moderately sized project in C, then I'd start looking at Python source. That's really the only way to know C better, and I really think that reading the C source of Python without knowing C well won't get you very far.  An idea for a C project  In fact, here's an idea for a C project: write a Python interpreter in C. Obviously it's not going to be even close to complete, and this is a pretty hard project, but if you only focus on some parts of the language, I think its a good idea.  Not only will it help you learn C, it will help you understand Python a lot better even before looking at the source: you'll have to have a deeper understanding of a lot of stuff in Python, you'll understand the design tradeoffs in how Python works, etc.  Then, when you do finally read Python's code, not only will you understand why some things work that way, you'll probably learn a lot of really cool C techniques that solve problems you had.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "c"
        ],
        "URL": "https://stackoverflow.com/questions/1728472/i-want-to-start-reading-the-python-source-code-where-should-i-start",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to start reading the Python source code.  My experience,  I know Python and Java very well. I know some other languages at various levels of proficiency, but neither C/C+/ particularly well. I studied C in college, but have never professionally programmed in it.  My Reasons for reading this code.   Understand how python works under the hood. Learn C better.   1 is more important to me than 2.  how should I go about this?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "I want to start reading the Python source code. Where should I start [closed]",
        "A_Content": "  1) First make sure you can build your own Python and run it into a debugger. So you can not only add print expressions but also break at points and follow the code flow. If you have toolsl that let you trace function calls, perfect, you will need it.  2) Start with the file that implement the data types. They are very easy to understand and you improve your C language language skills while reading the code.  3) Make UML diagrams - simple drawing helper tools like Argo UML or MS Visio can help you here. Write down the code flow.  4) Read the startup code for python. See what and how the basic infrastructure is initialized.  6) Ty to understand the Python side 100% - even the harder implementation details, what an AST is and what bound and unbound methods are and how you would implement them. When you have a model in mind how you would write an python interpreter then you can go to the final master step.  7) Write a debugger extension with the provided fast debugger C API. This helps you to improve your C skills.  8) Take the final master step and dive into the heart of the interpreter code. This is even hard to read and understand for a well skilled C programmer. Read how expressions are evaluation and method looksup are cached, frames are setup for scoping rules etc. It's difficult and complex - in terms of complexity and lines of code.  9) Start Adobe Photoshop and create a nice looking \"Master of Python\" diploma and put it on your office wall.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "c"
        ],
        "URL": "https://stackoverflow.com/questions/1728472/i-want-to-start-reading-the-python-source-code-where-should-i-start",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to start reading the Python source code.  My experience,  I know Python and Java very well. I know some other languages at various levels of proficiency, but neither C/C+/ particularly well. I studied C in college, but have never professionally programmed in it.  My Reasons for reading this code.   Understand how python works under the hood. Learn C better.   1 is more important to me than 2.  how should I go about this?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "I want to start reading the Python source code. Where should I start [closed]",
        "A_Content": "  Download the source from the Python website. Say you unzipped the source into a directory named Python-3.1.1. I suggest you two starting points within Python source code that would help you explore how Python works under the hood:   Examine how the Python Virtual Machine executes the bytecode generated from the interperter. The Python VM is in the file named Python-3.1.1/Python/ceval.c. The core of the VM is an eval loop that starts at the function PyEval_EvalFrameEx in ceval.c. Read through the source and the inline comments. I am sure you would enjoy it. Another option is to look at how built-in python data types like lists, dictionaries and sets are implemented. For instance sets are implemented in Python-3.1.1/Objects/setobject.c. The Objects directory contains implementations of other data types as well.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "c"
        ],
        "URL": "https://stackoverflow.com/questions/1728472/i-want-to-start-reading-the-python-source-code-where-should-i-start",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to start reading the Python source code.  My experience,  I know Python and Java very well. I know some other languages at various levels of proficiency, but neither C/C+/ particularly well. I studied C in college, but have never professionally programmed in it.  My Reasons for reading this code.   Understand how python works under the hood. Learn C better.   1 is more important to me than 2.  how should I go about this?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "I want to start reading the Python source code. Where should I start [closed]",
        "A_Content": "  The question is quite broad so I guess the best answer is to just download the python source and go nuts. Pick a module or section of python you know well and check whats under the hood.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "c"
        ],
        "URL": "https://stackoverflow.com/questions/1728472/i-want-to-start-reading-the-python-source-code-where-should-i-start",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to start reading the Python source code.  My experience,  I know Python and Java very well. I know some other languages at various levels of proficiency, but neither C/C+/ particularly well. I studied C in college, but have never professionally programmed in it.  My Reasons for reading this code.   Understand how python works under the hood. Learn C better.   1 is more important to me than 2.  how should I go about this?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python packages and egg-info directories",
        "A_Content": "  The .egg-info directories get only created if --single-version-externally-managed was used to install the egg. \"Normally\", installing an egg would create a single directory (or zip file), containing both the code and the metadata.   pkg_resources (which is the library that reads the metadata) has a function require which can be used to request a specific version of the package. For \"old-style\", regular imports, easy_install hacks a .pth file to get the egg directory onto sys.path. For --single-version-externally-managed, this hacking is not necessary, because there will only be a single version installed (by the system's pacakging infrastructure, e.g. rpm or dpkg). The egg-info is still included, for applications that use require (or any of the other pkg_resources binding mechanisms).  If you want to install a package by hard-linking, I recommend to use \"setup.py develop\". This is a command from setuptools which doesn't actually install the egg, but makes it available site-wide. To do so, it creates an egg-link file so that pkg_resources can find it, and it manipulates a .pth file, so that regular import can find it.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "setuptools",
            "egg"
        ],
        "URL": "https://stackoverflow.com/questions/256417/python-packages-and-egg-info-directories",
        "A_Votes": "64",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Can someone explain how egg-info directories are tied to their respective modules? For example, I have the following:  /usr/local/lib/python2.5/site-packages/quodlibet/ /usr/local/lib/python2.5/site-packages/quodlibet-2.0.egg-info/   I'm assuming the egg-info directory is to make the corresponding module visible to setuptools (easy_install), right? If so, how does setuptools tie the egg-info directory to the module directory?  Assuming that I'm on the right track, and for the sake of example... If I wanted to make an existing package of mine visible to setuptools, could I just symlink the module directory and the egg-info directory to the site-packages directory? I would have just tried this myself, but I'm not sure how to test if the package is visible to setuptools. Bonus points if you can also tell me how to test this :)  The main reason I'm trying to understand all this is because I would like to symlink some of my modules into site-packages so that I can make changes to them and have the changes visible to the scripts that use them without having to reinstall the egg from PyPI after each change.     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "Python decoding Unicode is not supported",
        "A_Content": "  Looks like google.searchGoogle(param) already returns unicode:  >>> unicode(u'foo', 'utf-8')  Traceback (most recent call last):   File \"<pyshell#1>\", line 1, in <module>     unicode(u'foo', 'utf-8') TypeError: decoding Unicode is not supported   So what you want is:  result = google.searchGoogle(param).encode(\"utf-8\")   As a side note, your code expects it to return a utf-8 encoded string so what was the point in decoding it (using unicode()) and encoding back (using .encode()) using the same encoding?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "encoding",
            "utf-8",
            "character-encoding"
        ],
        "URL": "https://stackoverflow.com/questions/7634715/python-decoding-unicode-is-not-supported",
        "A_Votes": "93",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I am having a problem with my encoding in Python. I have tried different methods but I can't seem to find the best way to encode my output to UTF-8.  This is what I am trying to do:  result = unicode(google.searchGoogle(param), \"utf-8\").encode(\"utf-8\")   searchGoogle returns the first Google result for param.  This is the error I get:  exceptions.TypeError: decoding Unicode is not supported   Does anyone know how I can make Python encode my output in UTF-8 to avoid this error?     ",
        "Q_Votes": "71"
    },
    {
        "Q_Title": "How do I write data into csv format as string (not file)?",
        "A_Content": "  You could use StringIO instead of your own Dummy_Writer:     This module implements a file-like class, StringIO, that reads and writes a string buffer (also known as memory files).    There is also cStringIO, which is a faster version of the StringIO class.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv"
        ],
        "URL": "https://stackoverflow.com/questions/9157314/how-do-i-write-data-into-csv-format-as-string-not-file",
        "A_Votes": "44",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to cast data like [1,2,'a','He said \"what do you mean?\"'] to a csv-formatted string.   Normally one would use csv.writer() for this, because it handles all the crazy edge cases (comma escaping, quote mark escaping, CSV dialects, etc.) The catch is that csv.writer() expects to output to a file object, not to a string.  My current solution is this somewhat hacky function:  def CSV_String_Writeline(data):     class Dummy_Writer:         def write(self,instring):             self.outstring = instring.strip(\"\\r\\n\")     dw = Dummy_Writer()     csv_w = csv.writer( dw )     csv_w.writerow(data)     return dw.outstring   Can anyone give a more elegant solution that still handles the edge cases well?  Edit: Here's how I ended up doing it:  def csv2string(data):     si = StringIO.StringIO()     cw = csv.writer(si)     cw.writerow(data)     return si.getvalue().strip('\\r\\n')      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How do I write data into csv format as string (not file)?",
        "A_Content": "  In Python 3:  >>> import io >>> import csv >>> output = io.StringIO() >>> csvdata = [1,2,'a','He said \"what do you mean?\"',\"Whoa!\\nNewlines!\"] >>> writer = csv.writer(output, quoting=csv.QUOTE_NONNUMERIC) >>> writer.writerow(csvdata) 59 >>> output.getvalue() '1,2,\"a\",\"He said \"\"what do you mean?\"\"\",\"Whoa!\\nNewlines!\"\\r\\n'   Some details need to be changed a bit for Python 2:  >>> output = io.BytesIO() >>> writer = csv.writer(output) >>> writer.writerow(csvdata) 57L >>> output.getvalue() '1,2,a,\"He said \"\"what do you mean?\"\"\",\"Whoa!\\nNewlines!\"\\r\\n'      ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv"
        ],
        "URL": "https://stackoverflow.com/questions/9157314/how-do-i-write-data-into-csv-format-as-string-not-file",
        "A_Votes": "91",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to cast data like [1,2,'a','He said \"what do you mean?\"'] to a csv-formatted string.   Normally one would use csv.writer() for this, because it handles all the crazy edge cases (comma escaping, quote mark escaping, CSV dialects, etc.) The catch is that csv.writer() expects to output to a file object, not to a string.  My current solution is this somewhat hacky function:  def CSV_String_Writeline(data):     class Dummy_Writer:         def write(self,instring):             self.outstring = instring.strip(\"\\r\\n\")     dw = Dummy_Writer()     csv_w = csv.writer( dw )     csv_w.writerow(data)     return dw.outstring   Can anyone give a more elegant solution that still handles the edge cases well?  Edit: Here's how I ended up doing it:  def csv2string(data):     si = StringIO.StringIO()     cw = csv.writer(si)     cw.writerow(data)     return si.getvalue().strip('\\r\\n')      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How do I write data into csv format as string (not file)?",
        "A_Content": "  I found the answers, all in all, a bit confusing. For Python 2, this usage worked for me:  import csv, io  def csv2string(data):     si = io.BytesIO()     cw = csv.writer(si)     cw.writerow(data)     return si.getvalue().strip('\\r\\n')  data=[1,2,'a','He said \"what do you mean?\"'] print csv2string(data)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv"
        ],
        "URL": "https://stackoverflow.com/questions/9157314/how-do-i-write-data-into-csv-format-as-string-not-file",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to cast data like [1,2,'a','He said \"what do you mean?\"'] to a csv-formatted string.   Normally one would use csv.writer() for this, because it handles all the crazy edge cases (comma escaping, quote mark escaping, CSV dialects, etc.) The catch is that csv.writer() expects to output to a file object, not to a string.  My current solution is this somewhat hacky function:  def CSV_String_Writeline(data):     class Dummy_Writer:         def write(self,instring):             self.outstring = instring.strip(\"\\r\\n\")     dw = Dummy_Writer()     csv_w = csv.writer( dw )     csv_w.writerow(data)     return dw.outstring   Can anyone give a more elegant solution that still handles the edge cases well?  Edit: Here's how I ended up doing it:  def csv2string(data):     si = StringIO.StringIO()     cw = csv.writer(si)     cw.writerow(data)     return si.getvalue().strip('\\r\\n')      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How do I write data into csv format as string (not file)?",
        "A_Content": "  Here's the version that works for utf-8. csvline2string for just one line, without linebreaks at the end, csv2string for many lines, with linebreaks:  import csv, io  def csvline2string(one_line_of_data):     si = BytesIO.StringIO()     cw = csv.writer(si)     cw.writerow(one_line_of_data)     return si.getvalue().strip('\\r\\n')  def csv2string(data):     si = BytesIO.StringIO()     cw = csv.writer(si)     for one_line_of_data in data:         cw.writerow(one_line_of_data)     return si.getvalue()      ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv"
        ],
        "URL": "https://stackoverflow.com/questions/9157314/how-do-i-write-data-into-csv-format-as-string-not-file",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to cast data like [1,2,'a','He said \"what do you mean?\"'] to a csv-formatted string.   Normally one would use csv.writer() for this, because it handles all the crazy edge cases (comma escaping, quote mark escaping, CSV dialects, etc.) The catch is that csv.writer() expects to output to a file object, not to a string.  My current solution is this somewhat hacky function:  def CSV_String_Writeline(data):     class Dummy_Writer:         def write(self,instring):             self.outstring = instring.strip(\"\\r\\n\")     dw = Dummy_Writer()     csv_w = csv.writer( dw )     csv_w.writerow(data)     return dw.outstring   Can anyone give a more elegant solution that still handles the edge cases well?  Edit: Here's how I ended up doing it:  def csv2string(data):     si = StringIO.StringIO()     cw = csv.writer(si)     cw.writerow(data)     return si.getvalue().strip('\\r\\n')      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How do I write data into csv format as string (not file)?",
        "A_Content": "  import csv from StringIO import StringIO with open('file.csv') as file:     file = file.read()  stream = StringIO(file)  csv_file = csv.DictReader(stream)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "csv"
        ],
        "URL": "https://stackoverflow.com/questions/9157314/how-do-i-write-data-into-csv-format-as-string-not-file",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to cast data like [1,2,'a','He said \"what do you mean?\"'] to a csv-formatted string.   Normally one would use csv.writer() for this, because it handles all the crazy edge cases (comma escaping, quote mark escaping, CSV dialects, etc.) The catch is that csv.writer() expects to output to a file object, not to a string.  My current solution is this somewhat hacky function:  def CSV_String_Writeline(data):     class Dummy_Writer:         def write(self,instring):             self.outstring = instring.strip(\"\\r\\n\")     dw = Dummy_Writer()     csv_w = csv.writer( dw )     csv_w.writerow(data)     return dw.outstring   Can anyone give a more elegant solution that still handles the edge cases well?  Edit: Here's how I ended up doing it:  def csv2string(data):     si = StringIO.StringIO()     cw = csv.writer(si)     cw.writerow(data)     return si.getvalue().strip('\\r\\n')      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Efficiently checking if arbitrary object is NaN in Python / numpy / pandas?",
        "A_Content": "  pandas.isnull() checks for missing values in both numeric and string/object arrays. From the documentation, it checks for:     NaN in numeric arrays, None/NaN in object arrays   Quick example:  import pandas as pd import numpy as np s = pd.Series(['apple', np.nan, 'banana']) pd.isnull(s) Out[9]:  0    False 1     True 2    False dtype: bool   The idea of using numpy.nan to represent missing values is something that pandas introduced, which is why pandas has the tools to deal with it.  Datetimes too (if you use pd.NaT you won't need to specify the dtype)  In [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')  In [25]: s Out[25]:  0   2013-01-01 00:00:00 1                   NaT 2   2013-01-02 09:30:00 dtype: datetime64[ns]``  In [26]: pd.isnull(s) Out[26]:  0    False 1     True 2    False dtype: bool      ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18689512/efficiently-checking-if-arbitrary-object-is-nan-in-python-numpy-pandas",
        "A_Votes": "123",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    My numpy arrays use np.nan to designate missing values. As I iterate over the data set, I need to detect such missing values and handle them in special ways.  Naively I used numpy.isnan(val), which works well unless val isn't among the subset of types supported by numpy.isnan(). For example, missing data can occur in string fields, in which case I get:  >>> np.isnan('some_string') Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> TypeError: Not implemented for this type   Other than writing an expensive wrapper that catches the exception and returns False, is there a way to handle this elegantly and efficiently?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Efficiently checking if arbitrary object is NaN in Python / numpy / pandas?",
        "A_Content": "  Is your type really arbitrary?  If you know it is just going to be a int float or string you could just do   if val.dtype == float and np.isnan(val):   assuming it is wrapped in numpy , it will always have a dtype and only float and complex can be NaN     ",
        "Language": "Python",
        "Tags": [
            "python",
            "numpy",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/18689512/efficiently-checking-if-arbitrary-object-is-nan-in-python-numpy-pandas",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    My numpy arrays use np.nan to designate missing values. As I iterate over the data set, I need to detect such missing values and handle them in special ways.  Naively I used numpy.isnan(val), which works well unless val isn't among the subset of types supported by numpy.isnan(). For example, missing data can occur in string fields, in which case I get:  >>> np.isnan('some_string') Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> TypeError: Not implemented for this type   Other than writing an expensive wrapper that catches the exception and returns False, is there a way to handle this elegantly and efficiently?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How to correctly sort a string with a number inside? [duplicate]",
        "A_Content": "  Perhaps you are looking for human sorting (also known as natural sorting):  import re  def atoi(text):     return int(text) if text.isdigit() else text  def natural_keys(text):     '''     alist.sort(key=natural_keys) sorts in human order     http://nedbatchelder.com/blog/200712/human_sorting.html     (See Toothy's implementation in the comments)     '''     return [ atoi(c) for c in re.split('(\\d+)', text) ]  alist=[     \"something1\",     \"something12\",     \"something17\",     \"something2\",     \"something25\",     \"something29\"]  alist.sort(key=natural_keys) print(alist)   yields  ['something1', 'something2', 'something12', 'something17', 'something25', 'something29']   PS. I've changed my answer to use Toothy's implementation of natural sorting (posted in the comments here) since it is significantly faster than my original answer.    If you wish to sort text with floats, then you'll need to change the regex from one that matches ints (i.e. (\\d+)) to a regex that matches floats:  import re  def atof(text):     try:         retval = float(text)     except ValueError:         retval = text     return retval  def natural_keys(text):     '''     alist.sort(key=natural_keys) sorts in human order     http://nedbatchelder.com/blog/200712/human_sorting.html     (See Toothy's implementation in the comments)     float regex comes from https://stackoverflow.com/a/12643073/190597     '''     return [ atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text) ]  alist=[     \"something1\",     \"something2\",     \"something1.0\",     \"something1.25\",     \"something1.105\"]  alist.sort(key=natural_keys) print(alist)   yields  ['something1', 'something1.0', 'something1.105', 'something1.25', 'something2']      ",
        "Language": "Python",
        "Tags": [
            "python",
            "regex",
            "sorting",
            "string"
        ],
        "URL": "https://stackoverflow.com/questions/5967500/how-to-correctly-sort-a-string-with-a-number-inside",
        "A_Votes": "138",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "       Possible Duplicate:   Does Python have a built in function for string natural sort?       I have a list of strings containing numbers and I cannot find a good way to sort them. For example I get something like this:  something1 something12 something17 something2 something25 something29   with the sort() method.  I know that I probably need to extract the numbers somehow and then sort the list but I have no idea how to do it in the most simple way.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What is the right way to override the copy/deepcopy operations on an object in Python?",
        "A_Content": "  The recommendations for customizing are at the very end of the docs page:     Classes can use the same interfaces to   control copying that they use to   control pickling. See the description   of module pickle for information on   these methods. The copy module does   not use the copy_reg registration   module.      In order for a class to define its own   copy implementation, it can define   special methods __copy__() and   __deepcopy__(). The former is called to implement the shallow copy   operation; no additional arguments are   passed. The latter is called to   implement the deep copy operation; it   is passed one argument, the memo   dictionary. If the __deepcopy__()   implementation needs to make a deep   copy of a component, it should call   the deepcopy() function with the   component as first argument and the   memo dictionary as second argument.   Since you appear not to care about pickling customization, defining __copy__ and __deepcopy__ definitely seems like the right way to go for you.  Specifically, __copy__ (the shallow copy) is pretty easy in your case...:  def __copy__(self):   newone = type(self)()   newone.__dict__.update(self.__dict__)   return newone   __deepcopy__ would be similar (accepting a memo arg too) but before the return it would have to call self.foo = deepcopy(self.foo, memo) for any attribute self.foo that needs deep copying (essentially attributes that are containers -- lists, dicts, non-primitive objects which hold other stuff through their __dict__s).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals"
        ],
        "URL": "https://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p",
        "A_Votes": "59",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    So just to establish, I feel like I understand the difference between copy vs. deepcopy in the copy module and I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.  Here's my scenario:   I have a configuration object that mostly just consists of simple properties (though it will potentially have lists of other non-primitive objects in it).  Initially I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  Once user interaction starts, however, each object will need to be able to tweak the configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).  Here's a sample object:  class ChartConfig(object):      def __init__(self):          #Drawing properties (Booleans/strings)         self.antialiased = None         self.plot_style = None         self.plot_title = None         self.autoscale = None          #X axis properties (strings/ints)         self.xaxis_title = None         self.xaxis_tick_rotation = None         self.xaxis_tick_align = None          #Y axis properties (strings/ints)         self.yaxis_title = None         self.yaxis_tick_rotation = None         self.yaxis_tick_align = None          #A list of non-primitive objects         self.trace_configs = []      def __copy__(self):         pass      def __deepcopy__(self, memo):         pass    What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?  I'm currently using Python 2.6.2.    Thanks in advance!     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What is the right way to override the copy/deepcopy operations on an object in Python?",
        "A_Content": "  Putting together Alex Martelli's answer and Rob Young's comment you get the following code:  from copy import copy, deepcopy  class A(object):     def __init__(self):         print 'init'         self.v = 10         self.z = [2,3,4]      def __copy__(self):         cls = self.__class__         result = cls.__new__(cls)         result.__dict__.update(self.__dict__)         return result      def __deepcopy__(self, memo):         cls = self.__class__         result = cls.__new__(cls)         memo[id(self)] = result         for k, v in self.__dict__.items():             setattr(result, k, deepcopy(v, memo))         return result  a = A() a.v = 11 b1, b2 = copy(a), deepcopy(a) a.v = 12 a.z.append(5) print b1.v, b1.z print b2.v, b2.z   prints  init 11 [2, 3, 4, 5] 11 [2, 3, 4]   here __deepcopy__ fills in the memo dict to avoid excess copying in case the object itself is referenced from its member.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals"
        ],
        "URL": "https://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p",
        "A_Votes": "64",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So just to establish, I feel like I understand the difference between copy vs. deepcopy in the copy module and I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.  Here's my scenario:   I have a configuration object that mostly just consists of simple properties (though it will potentially have lists of other non-primitive objects in it).  Initially I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  Once user interaction starts, however, each object will need to be able to tweak the configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).  Here's a sample object:  class ChartConfig(object):      def __init__(self):          #Drawing properties (Booleans/strings)         self.antialiased = None         self.plot_style = None         self.plot_title = None         self.autoscale = None          #X axis properties (strings/ints)         self.xaxis_title = None         self.xaxis_tick_rotation = None         self.xaxis_tick_align = None          #Y axis properties (strings/ints)         self.yaxis_title = None         self.yaxis_tick_rotation = None         self.yaxis_tick_align = None          #A list of non-primitive objects         self.trace_configs = []      def __copy__(self):         pass      def __deepcopy__(self, memo):         pass    What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?  I'm currently using Python 2.6.2.    Thanks in advance!     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What is the right way to override the copy/deepcopy operations on an object in Python?",
        "A_Content": "  I might be a bit off on the specifics, but here goes;  From the copy docs;        A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original.   A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original.      In other words: copy() will copy only the top element and leave the rest as pointers into the original structure. deepcopy() will recursively copy over everything.  That is, deepcopy() is what you need.  If you need to do something really specific, you can override __copy__() or __deepcopy__(), as described in the manual. Personally, I'd probably implement a plain function (e.g. config.copy_config() or such) to make it plain that it isn't Python standard behaviour.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals"
        ],
        "URL": "https://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So just to establish, I feel like I understand the difference between copy vs. deepcopy in the copy module and I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.  Here's my scenario:   I have a configuration object that mostly just consists of simple properties (though it will potentially have lists of other non-primitive objects in it).  Initially I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  Once user interaction starts, however, each object will need to be able to tweak the configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).  Here's a sample object:  class ChartConfig(object):      def __init__(self):          #Drawing properties (Booleans/strings)         self.antialiased = None         self.plot_style = None         self.plot_title = None         self.autoscale = None          #X axis properties (strings/ints)         self.xaxis_title = None         self.xaxis_tick_rotation = None         self.xaxis_tick_align = None          #Y axis properties (strings/ints)         self.yaxis_title = None         self.yaxis_tick_rotation = None         self.yaxis_tick_align = None          #A list of non-primitive objects         self.trace_configs = []      def __copy__(self):         pass      def __deepcopy__(self, memo):         pass    What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?  I'm currently using Python 2.6.2.    Thanks in advance!     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What is the right way to override the copy/deepcopy operations on an object in Python?",
        "A_Content": "  Following Peter's excellent answer, to implement a custom deepcopy, with minimal alteration to the default implementation (e.g. just modifying a field like I needed) :  class Foo(object):     def __deepcopy__(self, memo):         deepcopy_method = self.__deepcopy__         self.__deepcopy__ = None         cp = deepcopy(self, memo)         self.__deepcopy__ = deepcopy_method          # custom treatments         # for instance: cp.id = None          return cp      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals"
        ],
        "URL": "https://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So just to establish, I feel like I understand the difference between copy vs. deepcopy in the copy module and I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.  Here's my scenario:   I have a configuration object that mostly just consists of simple properties (though it will potentially have lists of other non-primitive objects in it).  Initially I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  Once user interaction starts, however, each object will need to be able to tweak the configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).  Here's a sample object:  class ChartConfig(object):      def __init__(self):          #Drawing properties (Booleans/strings)         self.antialiased = None         self.plot_style = None         self.plot_title = None         self.autoscale = None          #X axis properties (strings/ints)         self.xaxis_title = None         self.xaxis_tick_rotation = None         self.xaxis_tick_align = None          #Y axis properties (strings/ints)         self.yaxis_title = None         self.yaxis_tick_rotation = None         self.yaxis_tick_align = None          #A list of non-primitive objects         self.trace_configs = []      def __copy__(self):         pass      def __deepcopy__(self, memo):         pass    What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?  I'm currently using Python 2.6.2.    Thanks in advance!     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What is the right way to override the copy/deepcopy operations on an object in Python?",
        "A_Content": "  Its not clear from your problem why you need to override these methods, since you don't want to do any customization to the copying methods.  Anyhow, if you do want to customize the deep copy (e.g. by sharing some attributes and copying others), here is a solution:  from copy import deepcopy   def deepcopy_with_sharing(obj, shared_attribute_names, memo=None):     '''     Deepcopy an object, except for a given list of attributes, which should     be shared between the original object and its copy.      obj is some object     shared_attribute_names: A list of strings identifying the attributes that         should be shared between the original and its copy.     memo is the dictionary passed into __deepcopy__.  Ignore this argument if         not calling from within __deepcopy__.     '''     assert isinstance(shared_attribute_names, (list, tuple))     shared_attributes = {k: getattr(obj, k) for k in shared_attribute_names}      if hasattr(obj, '__deepcopy__'):         # Do hack to prevent infinite recursion in call to deepcopy         deepcopy_method = obj.__deepcopy__         obj.__deepcopy__ = None      for attr in shared_attribute_names:         del obj.__dict__[attr]      clone = deepcopy(obj)      for attr, val in shared_attributes.iteritems():         setattr(obj, attr, val)         setattr(clone, attr, val)      if hasattr(obj, '__deepcopy__'):         # Undo hack         obj.__deepcopy__ = deepcopy_method         del clone.__deepcopy__      return clone    class A(object):      def __init__(self):         self.copy_me = []         self.share_me = []      def __deepcopy__(self, memo):         return deepcopy_with_sharing(self, shared_attribute_names = ['share_me'], memo=memo)  a = A() b = deepcopy(a) assert a.copy_me is not b.copy_me assert a.share_me is b.share_me  c = deepcopy(b) assert c.copy_me is not b.copy_me assert c.share_me is b.share_me      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals"
        ],
        "URL": "https://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So just to establish, I feel like I understand the difference between copy vs. deepcopy in the copy module and I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.  Here's my scenario:   I have a configuration object that mostly just consists of simple properties (though it will potentially have lists of other non-primitive objects in it).  Initially I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  Once user interaction starts, however, each object will need to be able to tweak the configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).  Here's a sample object:  class ChartConfig(object):      def __init__(self):          #Drawing properties (Booleans/strings)         self.antialiased = None         self.plot_style = None         self.plot_title = None         self.autoscale = None          #X axis properties (strings/ints)         self.xaxis_title = None         self.xaxis_tick_rotation = None         self.xaxis_tick_align = None          #Y axis properties (strings/ints)         self.yaxis_title = None         self.yaxis_tick_rotation = None         self.yaxis_tick_align = None          #A list of non-primitive objects         self.trace_configs = []      def __copy__(self):         pass      def __deepcopy__(self, memo):         pass    What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?  I'm currently using Python 2.6.2.    Thanks in advance!     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What is the right way to override the copy/deepcopy operations on an object in Python?",
        "A_Content": "  Building on Antony Hatchkins' clean answer, here's my version where the class in question derives from another custom class (s.t. we need to call super):  class Foo(FooBase):     def __init__(self, param1, param2):         self._base_params = [param1, param2]         super(Foo, result).__init__(*self._base_params)      def __copy__(self):         cls = self.__class__         result = cls.__new__(cls)         result.__dict__.update(self.__dict__)         super(Foo, result).__init__(*self._base_params)         return result      def __deepcopy__(self, memo):         cls = self.__class__         result = cls.__new__(cls)         memo[id(self)] = result         for k, v in self.__dict__.items():             setattr(result, k, copy.deepcopy(v, memo))         super(Foo, result).__init__(*self._base_params)         return result      ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals"
        ],
        "URL": "https://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So just to establish, I feel like I understand the difference between copy vs. deepcopy in the copy module and I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.  Here's my scenario:   I have a configuration object that mostly just consists of simple properties (though it will potentially have lists of other non-primitive objects in it).  Initially I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  Once user interaction starts, however, each object will need to be able to tweak the configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).  Here's a sample object:  class ChartConfig(object):      def __init__(self):          #Drawing properties (Booleans/strings)         self.antialiased = None         self.plot_style = None         self.plot_title = None         self.autoscale = None          #X axis properties (strings/ints)         self.xaxis_title = None         self.xaxis_tick_rotation = None         self.xaxis_tick_align = None          #Y axis properties (strings/ints)         self.yaxis_title = None         self.yaxis_tick_rotation = None         self.yaxis_tick_align = None          #A list of non-primitive objects         self.trace_configs = []      def __copy__(self):         pass      def __deepcopy__(self, memo):         pass    What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?  I'm currently using Python 2.6.2.    Thanks in advance!     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What is the right way to override the copy/deepcopy operations on an object in Python?",
        "A_Content": "  The copy module uses evantually the __getstate__()/__setstate__() pickling protocol, so these are also valid targets to override.    The default implementation just returns and sets the __dict__ of the class, so you don't have to call super() and worry about Eino Gourdin's clever trick, above.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-internals"
        ],
        "URL": "https://stackoverflow.com/questions/1500718/what-is-the-right-way-to-override-the-copy-deepcopy-operations-on-an-object-in-p",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    So just to establish, I feel like I understand the difference between copy vs. deepcopy in the copy module and I've used copy.copy and copy.deepcopy before successfully, but this is the first time I've actually gone about overloading the __copy__ and __deepcopy__ methods.  I've already Googled around and looked through the built-in Python modules to look for instances of the __copy__ and __deepcopy__ functions (e.g. sets.py, decimal.py, and fractions.py), but I'm still not 100% sure I've got it right.  Here's my scenario:   I have a configuration object that mostly just consists of simple properties (though it will potentially have lists of other non-primitive objects in it).  Initially I'm going to instantiate one configuration object with a default set of values.  This configuration will be handed off to multiple other objects (to ensure all objects start with the same configuration).  Once user interaction starts, however, each object will need to be able to tweak the configurations independently without affecting each other's configurations (which says to me I'll need to make deepcopys of my initial configuration to hand around).  Here's a sample object:  class ChartConfig(object):      def __init__(self):          #Drawing properties (Booleans/strings)         self.antialiased = None         self.plot_style = None         self.plot_title = None         self.autoscale = None          #X axis properties (strings/ints)         self.xaxis_title = None         self.xaxis_tick_rotation = None         self.xaxis_tick_align = None          #Y axis properties (strings/ints)         self.yaxis_title = None         self.yaxis_tick_rotation = None         self.yaxis_tick_align = None          #A list of non-primitive objects         self.trace_configs = []      def __copy__(self):         pass      def __deepcopy__(self, memo):         pass    What is the right way to implement the copy and deepcopy methods on this object to ensure copy.copy and copy.deepcopy give me the proper behavior?  I'm currently using Python 2.6.2.    Thanks in advance!     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How to convert a boolean array to an int array",
        "A_Content": "  Numpy arrays have an astype method.  Just do y.astype(int).  Note that it might not even be necessary to do this, depending on what you're using the array for.  Bool will be autopromoted to int in many cases, so you can add it to int arrays without having to explicitly convert it:  >>> x array([ True, False,  True], dtype=bool) >>> x + [1, 2, 3] array([2, 2, 4])      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "boolean",
            "type-conversion",
            "scilab"
        ],
        "URL": "https://stackoverflow.com/questions/17506163/how-to-convert-a-boolean-array-to-an-int-array",
        "A_Votes": "90",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I use Scilab, and  want to convert an array of booleans into an array of integers:  >>> x = np.array([4, 3, 2, 1]) >>> y = 2 >= x >>> y array([False, False,  True,  True], dtype=bool)   In Scilab I can use:  >>> bool2s(y) 0.    0.    1.    1.     or even just multiply it by 1:  >>> 1*y 0.    0.    1.    1.     Is there a simple command for this in Python, or would I have to use a loop?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How to convert a boolean array to an int array",
        "A_Content": "  The 1*y method works in Numpy too:  >>> import numpy as np >>> x = np.array([4, 3, 2, 1]) >>> y = 2 >= x >>> y array([False, False,  True,  True], dtype=bool) >>> 1*y                      # Method 1 array([0, 0, 1, 1]) >>> y.astype(int)            # Method 2 array([0, 0, 1, 1])    If you are asking for a way to convert Python lists from Boolean to int, you can use map to do it:  >>> testList = [False, False,  True,  True] >>> map(lambda x: 1 if x else 0, testList) [0, 0, 1, 1] >>> map(int, testList) [0, 0, 1, 1]   Or using list comprehensions:  >>> testList [False, False, True, True] >>> [int(elem) for elem in testList] [0, 0, 1, 1]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "boolean",
            "type-conversion",
            "scilab"
        ],
        "URL": "https://stackoverflow.com/questions/17506163/how-to-convert-a-boolean-array-to-an-int-array",
        "A_Votes": "29",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use Scilab, and  want to convert an array of booleans into an array of integers:  >>> x = np.array([4, 3, 2, 1]) >>> y = 2 >= x >>> y array([False, False,  True,  True], dtype=bool)   In Scilab I can use:  >>> bool2s(y) 0.    0.    1.    1.     or even just multiply it by 1:  >>> 1*y 0.    0.    1.    1.     Is there a simple command for this in Python, or would I have to use a loop?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How to convert a boolean array to an int array",
        "A_Content": "  Using numpy, you can do:  y = x.astype(int)   If you were using a non-numpy array, you could use a list comprehension:  y = [int(val) for val in x]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "boolean",
            "type-conversion",
            "scilab"
        ],
        "URL": "https://stackoverflow.com/questions/17506163/how-to-convert-a-boolean-array-to-an-int-array",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use Scilab, and  want to convert an array of booleans into an array of integers:  >>> x = np.array([4, 3, 2, 1]) >>> y = 2 >= x >>> y array([False, False,  True,  True], dtype=bool)   In Scilab I can use:  >>> bool2s(y) 0.    0.    1.    1.     or even just multiply it by 1:  >>> 1*y 0.    0.    1.    1.     Is there a simple command for this in Python, or would I have to use a loop?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How to convert a boolean array to an int array",
        "A_Content": "  Most of the time you don't need conversion:  >>>array([True,True,False,False]) + array([1,2,3,4]) array([2, 3, 3, 4])   The right way to do it is:  yourArray.astype(int)   or  yourArray.astype(float)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "boolean",
            "type-conversion",
            "scilab"
        ],
        "URL": "https://stackoverflow.com/questions/17506163/how-to-convert-a-boolean-array-to-an-int-array",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use Scilab, and  want to convert an array of booleans into an array of integers:  >>> x = np.array([4, 3, 2, 1]) >>> y = 2 >= x >>> y array([False, False,  True,  True], dtype=bool)   In Scilab I can use:  >>> bool2s(y) 0.    0.    1.    1.     or even just multiply it by 1:  >>> 1*y 0.    0.    1.    1.     Is there a simple command for this in Python, or would I have to use a loop?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "How to convert a boolean array to an int array",
        "A_Content": "  I know you asked for non-looping solutions, but the only solutions I can come up with probably loop internally anyway:  map(int,y)   or:  [i*1 for i in y]   or:  import numpy y=numpy.array(y) y*1      ",
        "Language": "Python",
        "Tags": [
            "python",
            "integer",
            "boolean",
            "type-conversion",
            "scilab"
        ],
        "URL": "https://stackoverflow.com/questions/17506163/how-to-convert-a-boolean-array-to-an-int-array",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I use Scilab, and  want to convert an array of booleans into an array of integers:  >>> x = np.array([4, 3, 2, 1]) >>> y = 2 >= x >>> y array([False, False,  True,  True], dtype=bool)   In Scilab I can use:  >>> bool2s(y) 0.    0.    1.    1.     or even just multiply it by 1:  >>> 1*y 0.    0.    1.    1.     Is there a simple command for this in Python, or would I have to use a loop?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Max retries exceeded with URL",
        "A_Content": "  What happened here is that itunes server refuses your connection (you're sending too many requests from same ip address in short period of time)      Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8   error trace is misleading it should be something like \"No connection could be made because the target machine actively refused it\".  There is an issue at about python.requests lib at Github, check it out here  To overcome this issue (not so much an issue as it is misleading debug trace) you should catch connection related exceptions like so:  try:     page1 = requests.get(ap) except requests.exceptions.ConnectionError:     r.status_code = \"Connection refused\"   Another way to overcome this problem is if you use enough time gap to send requests to server this can be achieved by sleep(timeinsec) function in python (don't forget to import sleep)  from time import sleep   All in all requests is awesome python lib, hope that solves your problem.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-requests"
        ],
        "URL": "https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url",
        "A_Votes": "85",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to get the content of this url \"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" and its showing this error  Traceback (most recent call last):   File \"/home/preetham/Desktop/eg.py\", line 17, in <module>     page1 = requests.get(ap)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 55, in get     return request('get', url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 44, in request     return session.request(method=method, url=url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 383, in request     resp = self.send(prep, **send_kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 486, in send     r = adapter.send(request, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 378, in send     raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPSConnectionPool(host='itunes.apple.com', port=443): Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8 (Caused by <class 'socket.gaierror'>: [Errno -2] Name or service not known)   the code is   url=\"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" page = requests.get(url) tree = html.fromstring(page.text) flist=[] plist=[] for i in range(0,100):     app = tree.xpath(\"//div[@class='column first']/ul/li/a/@href\")     ap=app[0]     page1 = requests.get(ap)   when I try the range with (0,2) it works but when I put the range in 100's it shows this error.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Max retries exceeded with URL",
        "A_Content": "  Just use requests' features:  import requests from requests.adapters import HTTPAdapter from requests.packages.urllib3.util.retry import Retry   session = requests.Session() retry = Retry(connect=3, backoff_factor=0.5) adapter = HTTPAdapter(max_retries=retry) session.mount('http://', adapter) session.mount('https://', adapter)  session.get(url)   This will GET the URL and retry 3 times in case of requests.exceptions.ConnectionError. backoff_factor will help to apply delays between attempts to avoid to fail again in case of periodic request quota.  Take a look at requests.packages.urllib3.util.retry.Retry, it has many options to simplify retries.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-requests"
        ],
        "URL": "https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url",
        "A_Votes": "27",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to get the content of this url \"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" and its showing this error  Traceback (most recent call last):   File \"/home/preetham/Desktop/eg.py\", line 17, in <module>     page1 = requests.get(ap)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 55, in get     return request('get', url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 44, in request     return session.request(method=method, url=url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 383, in request     resp = self.send(prep, **send_kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 486, in send     r = adapter.send(request, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 378, in send     raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPSConnectionPool(host='itunes.apple.com', port=443): Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8 (Caused by <class 'socket.gaierror'>: [Errno -2] Name or service not known)   the code is   url=\"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" page = requests.get(url) tree = html.fromstring(page.text) flist=[] plist=[] for i in range(0,100):     app = tree.xpath(\"//div[@class='column first']/ul/li/a/@href\")     ap=app[0]     page1 = requests.get(ap)   when I try the range with (0,2) it works but when I put the range in 100's it shows this error.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Max retries exceeded with URL",
        "A_Content": "  Just do this,  Paste the following code in place of page = requests.get(url):  import time  page = '' while page == '':     try:         page = requests.get(url)         break     except:         print(\"Connection refused by the server..\")         print(\"Let me sleep for 5 seconds\")         print(\"ZZzzzz...\")         time.sleep(5)         print(\"Was a nice sleep, now let me continue...\")         continue   You're welcome :)     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-requests"
        ],
        "URL": "https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to get the content of this url \"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" and its showing this error  Traceback (most recent call last):   File \"/home/preetham/Desktop/eg.py\", line 17, in <module>     page1 = requests.get(ap)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 55, in get     return request('get', url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 44, in request     return session.request(method=method, url=url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 383, in request     resp = self.send(prep, **send_kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 486, in send     r = adapter.send(request, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 378, in send     raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPSConnectionPool(host='itunes.apple.com', port=443): Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8 (Caused by <class 'socket.gaierror'>: [Errno -2] Name or service not known)   the code is   url=\"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" page = requests.get(url) tree = html.fromstring(page.text) flist=[] plist=[] for i in range(0,100):     app = tree.xpath(\"//div[@class='column first']/ul/li/a/@href\")     ap=app[0]     page1 = requests.get(ap)   when I try the range with (0,2) it works but when I put the range in 100's it shows this error.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Max retries exceeded with URL",
        "A_Content": "  pip install pyopenssl seemed to solve it for me.  https://github.com/requests/requests/issues/4246     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-requests"
        ],
        "URL": "https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to get the content of this url \"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" and its showing this error  Traceback (most recent call last):   File \"/home/preetham/Desktop/eg.py\", line 17, in <module>     page1 = requests.get(ap)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 55, in get     return request('get', url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 44, in request     return session.request(method=method, url=url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 383, in request     resp = self.send(prep, **send_kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 486, in send     r = adapter.send(request, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 378, in send     raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPSConnectionPool(host='itunes.apple.com', port=443): Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8 (Caused by <class 'socket.gaierror'>: [Errno -2] Name or service not known)   the code is   url=\"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" page = requests.get(url) tree = html.fromstring(page.text) flist=[] plist=[] for i in range(0,100):     app = tree.xpath(\"//div[@class='column first']/ul/li/a/@href\")     ap=app[0]     page1 = requests.get(ap)   when I try the range with (0,2) it works but when I put the range in 100's it shows this error.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Max retries exceeded with URL",
        "A_Content": "  It is always good to implement exception handling. It does not only help to avoid unexpected exit of script but can also help to log errors and info notification. When using Python requests I prefer to catch exceptions like this:      try:         res = requests.get(adress,timeout=30)     except requests.ConnectionError as e:         print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")         print(str(e))                     renewIPadress()         continue     except requests.Timeout as e:         print(\"OOPS!! Timeout Error\")         print(str(e))         renewIPadress()         continue     except requests.RequestException as e:         print(\"OOPS!! General Error\")         print(str(e))         renewIPadress()         continue     except KeyboardInterrupt:         print(\"Someone closed the program\")   Here renewIPadress() is a user define function which can change the IP address if it get blocked. You can go without this function.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-requests"
        ],
        "URL": "https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to get the content of this url \"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" and its showing this error  Traceback (most recent call last):   File \"/home/preetham/Desktop/eg.py\", line 17, in <module>     page1 = requests.get(ap)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 55, in get     return request('get', url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 44, in request     return session.request(method=method, url=url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 383, in request     resp = self.send(prep, **send_kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 486, in send     r = adapter.send(request, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 378, in send     raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPSConnectionPool(host='itunes.apple.com', port=443): Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8 (Caused by <class 'socket.gaierror'>: [Errno -2] Name or service not known)   the code is   url=\"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" page = requests.get(url) tree = html.fromstring(page.text) flist=[] plist=[] for i in range(0,100):     app = tree.xpath(\"//div[@class='column first']/ul/li/a/@href\")     ap=app[0]     page1 = requests.get(ap)   when I try the range with (0,2) it works but when I put the range in 100's it shows this error.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Max retries exceeded with URL",
        "A_Content": "  I got the same problem and found the easiest solution for my case. Try to replace this:  url=\"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\"   With this:  url=\"http://itunes.apple.com/in/genre/ios-business/id6000?mt=8\"   The difference is \"s\" in protocol. This solved my trouble.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "python-requests"
        ],
        "URL": "https://stackoverflow.com/questions/23013220/max-retries-exceeded-with-url",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm trying to get the content of this url \"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" and its showing this error  Traceback (most recent call last):   File \"/home/preetham/Desktop/eg.py\", line 17, in <module>     page1 = requests.get(ap)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 55, in get     return request('get', url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/api.py\", line 44, in request     return session.request(method=method, url=url, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 383, in request     resp = self.send(prep, **send_kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 486, in send     r = adapter.send(request, **kwargs)   File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 378, in send     raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPSConnectionPool(host='itunes.apple.com', port=443): Max retries exceeded with url: /in/app/adobe-reader/id469337564?mt=8 (Caused by <class 'socket.gaierror'>: [Errno -2] Name or service not known)   the code is   url=\"https://itunes.apple.com/in/genre/ios-business/id6000?mt=8\" page = requests.get(url) tree = html.fromstring(page.text) flist=[] plist=[] for i in range(0,100):     app = tree.xpath(\"//div[@class='column first']/ul/li/a/@href\")     ap=app[0]     page1 = requests.get(ap)   when I try the range with (0,2) it works but when I put the range in 100's it shows this error.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  web2py is 265Kbytes of source code and 1.2MB all inclusive (compare with 4.6MB of Django). Yet web2py will do everything you need (manage session, cookies, request, response, cache, internationalization, errors/tickets, database abstraction for GAE, SQLite, MSSQL, MySQL, Postgres, Oracle, FireBird, etc.) It does not need installation - just unzip and click on it - and you can do development in your browser.  Web2py has both routes and reverse routes.  Web2py has a hierarchical template systems which means a view can extend a layout which can extend another layout, etc. views can also include other views.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "44",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  Since you explicitly don't want an ORM, I'd stay away from a \"full stack\" framework if I were you. Python's WSGI standard gives you a ton of easy-to-use options that will give you just the features you need and even let you choose your style of working.  Here's an example: for URL dispatch you can use Routes, which ports over the URL dispatch style of Rails. Or you could combine Selector with WebOb if that style suits you more.  For \"layouts\", you can use the powerful Jinja2 if you want templates that cannot run code. Or, Mako if you prefer to be able to mix a little code in with your templates. You can even use Deliverance to control the layout of pages that are composed from multiple apps and even multiple languages!  A full-stack web framework is nice in that it makes a bunch of choices for you, letting you pay attention just to the app your building. But, the choices I've listed above are a good collection to get you going building your own. If you head down that path, you'll find it easy to plug in Beaker for caching and sessions if you need them, or WebError to help you with debugging.  Personally, I'm a big fan of ORMs (particularly SQLAlchemy), but if you're looking to go ORM free and lightweight overall you can't beat combining the great WSGI components available in Python.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "31",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  Give web.py a try. It's very simple and may provide the minimalism that you are looking for.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "11",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  Pylons.  It's much better than django, and doesn't come with a crappy ORM.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  People already gave many answers concerning web application frameworks, but MVC (or any other paradigm) is not tied to web only. That's just for clarity.  If you are about plain MVC, Pylons conforms to paradigm in a stricter way. Django interprets MVC and they call it model-template-view, but the idea of role separation is the same. Actual choice is a matter of personal taste, although none of these two I consider lightweigth (Pylons might seem lighter, but in fact is not, and recently Django gathered some additional weight - most likely you will not fit even small application like personal blog in 20MB resident memory).  Of course, nothing will stop you from writing your own framework, eg. with WebOb. You can make it as light as you want (and learn many things trying).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "8",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  If you want something simple, without having to make your own framework, while still not being all inclusive (django), you might want to try CherryPy. It can use almost any dispatcher (Page Handler / URL routing system). You would also have to pick your own templating engine, Genshi is my favorite.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  checkout https://github.com/salimane/bottle-mvc or https://github.com/salimane/flask-mvc . They are boilerplates that could get you started with controllers, models in separate folders. They are based on bottle and flask micro frameworks, no useless features, they give you the flexibility to plugin whatever modules you want.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  When it comes to desktop applications Dabo is a good choice. It's a cross platform framework on top of wxPython which supports MySql, Postgresql, Firebird and Sqlite.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  You want web2py.  Check it:   don't care for ORM, just want it to work with MySQL   Web2py doesn't have a ORM, but it does have a syntactic wrapper over SQL that makes it much easier to do the SQL.   has configurable routes has support for layouts   ...web2py has both of these.  And web2py is more lightweight than django/rails/whatever on pretty much all counts.  It's VERY easy to learn.  The hardest things about learning a MVC framework are the scripts, the ORM, and (with django) the template language.  But web2py got rid of the scripts, simplified the ORM, and the template language is just python in a rad clever way.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  Yes, I would say Django is definitely the way to go. Its modular design ensures that you can mix and match components (ORM, templating engine, URL dispatch, ...) Instead of being stuck with a component the framework provides you, you can replace it with any 3rd party equivalent instead.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  Django!  Google App Engine uses it.  I use it too for my own pet projects.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  web2py! django calls a controller a view, 'nuf said.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  If you want simplicity use web2py or pylons. Django is good...but the learning curve is steep      ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  Django. You don't have to use the additional features, and it's well designed so you can mix-n-match 3rd-party libraries as needed.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Efficient way to apply multiple filters to pandas DataFrame or Series",
        "A_Content": "  Pandas (and numpy) allow for boolean indexing, which will be much more efficient:  In [11]: df.loc[df['col1'] >= 1, 'col1'] Out[11]:  1    1 2    2 Name: col1  In [12]: df[df['col1'] >= 1] Out[12]:     col1  col2 1     1    11 2     2    12  In [13]: df[(df['col1'] >= 1) & (df['col1'] <=1 )] Out[13]:     col1  col2 1     1    11   If you want to write helper functions for this, consider something along these lines:  In [14]: def b(x, col, op, n):               return op(x[col],n)  In [15]: def f(x, *b):              return x[(np.logical_and(*b))]  In [16]: b1 = b(df, 'col1', ge, 1)  In [17]: b2 = b(df, 'col1', le, 1)  In [18]: f(df, b1, b2) Out[18]:     col1  col2 1     1    11   Update: pandas 0.13 has a query method for these kind of use cases, assuming column names are valid identifiers the following works (and can be more efficient for large frames as it uses numexpr behind the scenes):  In [21]: df.query('col1 <= 1 & 1 <= col1') Out[21]:    col1  col2 1     1    11      ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/13611065/efficient-way-to-apply-multiple-filters-to-pandas-dataframe-or-series",
        "A_Votes": "127",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.  The filters should be additive (aka each one applied should narrow results).  I'm currently using reindex() but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.  I'm thinking that using apply(), map(), or something similar might be better.  I'm pretty new to Pandas though so still trying to wrap my head around everything.  TL;DR  I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object.  relops = {'>=': [1], '<=': [1]}   Long Example  I'll start with an example of what I have currently and just filtering a single Series object.  Below is the function I'm currently using:     def apply_relops(series, relops):         \"\"\"         Pass dictionary of relational operators to perform on given series object         \"\"\"         for op, vals in relops.iteritems():             op_func = ops[op]             for val in vals:                 filtered = op_func(series, val)                 series = series.reindex(series[filtered])         return series   The user provides a dictionary with the operations they want to perform:  >>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]}) >>> print df >>> print df    col1  col2 0     0    10 1     1    11 2     2    12  >>> from operator import le, ge >>> ops ={'>=': ge, '<=': le} >>> apply_relops(df['col1'], {'>=': [1]}) col1 1       1 2       2 Name: col1 >>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]}) col1 1       1 Name: col1   Again, the 'problem' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.  Also, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I'm assuming whatever works for the Series can be easily expanded to a DataFrame.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Efficient way to apply multiple filters to pandas DataFrame or Series",
        "A_Content": "  Chaining conditions creates long lines, which are discouraged by pep8.  Using the .query method forces to use strings, which is powerful but unpythonic and not very dynamic.   Once each of the filters is in place, one approach is  import numpy as np import functools def conjunction(*conditions):     return functools.reduce(np.logical_and, conditions)  c_1 = data.col1 == True c_2 = data.col2 < 64 c_3 = data.col3 != 4  data_filtered = data[conjunction(c1,c2,c3)]   np.logical operates on and is fast, but does not take more than two arguments, which is handled by functools.reduce.   Note that this still has some redundancies: a) shortcutting does not happen on a global level b) Each of the individual conditions runs on the whole initial data. Still, I expect this to be efficient enough for many applications and it is very readable.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/13611065/efficient-way-to-apply-multiple-filters-to-pandas-dataframe-or-series",
        "A_Votes": "12",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.  The filters should be additive (aka each one applied should narrow results).  I'm currently using reindex() but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.  I'm thinking that using apply(), map(), or something similar might be better.  I'm pretty new to Pandas though so still trying to wrap my head around everything.  TL;DR  I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object.  relops = {'>=': [1], '<=': [1]}   Long Example  I'll start with an example of what I have currently and just filtering a single Series object.  Below is the function I'm currently using:     def apply_relops(series, relops):         \"\"\"         Pass dictionary of relational operators to perform on given series object         \"\"\"         for op, vals in relops.iteritems():             op_func = ops[op]             for val in vals:                 filtered = op_func(series, val)                 series = series.reindex(series[filtered])         return series   The user provides a dictionary with the operations they want to perform:  >>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]}) >>> print df >>> print df    col1  col2 0     0    10 1     1    11 2     2    12  >>> from operator import le, ge >>> ops ={'>=': ge, '<=': le} >>> apply_relops(df['col1'], {'>=': [1]}) col1 1       1 2       2 Name: col1 >>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]}) col1 1       1 Name: col1   Again, the 'problem' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.  Also, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I'm assuming whatever works for the Series can be easily expanded to a DataFrame.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Efficient way to apply multiple filters to pandas DataFrame or Series",
        "A_Content": "  Simplest of All Solutions:  Use:  filtered_df = df[(df['col1'] >= 1) & (df['col1'] <= 5)]   Another Example, To filter the dataframe for values belonging to Feb-2018, use the below code   filtered_df = df[(df['year'] == 2018) & (df['month'] == 2)]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/13611065/efficient-way-to-apply-multiple-filters-to-pandas-dataframe-or-series",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.  The filters should be additive (aka each one applied should narrow results).  I'm currently using reindex() but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.  I'm thinking that using apply(), map(), or something similar might be better.  I'm pretty new to Pandas though so still trying to wrap my head around everything.  TL;DR  I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object.  relops = {'>=': [1], '<=': [1]}   Long Example  I'll start with an example of what I have currently and just filtering a single Series object.  Below is the function I'm currently using:     def apply_relops(series, relops):         \"\"\"         Pass dictionary of relational operators to perform on given series object         \"\"\"         for op, vals in relops.iteritems():             op_func = ops[op]             for val in vals:                 filtered = op_func(series, val)                 series = series.reindex(series[filtered])         return series   The user provides a dictionary with the operations they want to perform:  >>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]}) >>> print df >>> print df    col1  col2 0     0    10 1     1    11 2     2    12  >>> from operator import le, ge >>> ops ={'>=': ge, '<=': le} >>> apply_relops(df['col1'], {'>=': [1]}) col1 1       1 2       2 Name: col1 >>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]}) col1 1       1 Name: col1   Again, the 'problem' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.  Also, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I'm assuming whatever works for the Series can be easily expanded to a DataFrame.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  I'm really new on Python but I tried quiet a few, specially Django and web2py.  I loved the simplicity of web2py, I was able to create a site of medium complexity in a few days. It has an imprecessive sets of feature a DAL, code generation, HTML hlpers and for me the most important feature was the documentation in the site is quite complete.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  I would take a look at Pylons; it is lightweight and fast.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  I'm also on the Django boat. Here are a few reasons why:   You'll likely save time with Django's admin interface in avoiding manual queries. Django's templating system is fantastic Django has a WONDERFUL community, very eager to help (see #django on freenode)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "What's a good lightweight Python MVC framework? [closed]",
        "A_Content": "  Django is my recommendation.  You can find an introduction to it here (a Google Tech Talk by Jacob Kaplan-Moss):  And you may also want to have a look at Adrian Holovaty's talk given at Snakes and Rubies, DePaul University:     ",
        "Language": "Python",
        "Tags": [
            "python",
            "model-view-controller",
            "frameworks"
        ],
        "URL": "https://stackoverflow.com/questions/68986/whats-a-good-lightweight-python-mvc-framework",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I know there are a ton of Python frameworks out there. Can you guys point me in the right direction? My primary concern is simplicity, I don't need a lot of extraneous features. Here are a couple of other things that I'd want (or don't want):   don't care for ORM, just want it to work with MySQL has configurable routes has support for layouts      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Efficient way to apply multiple filters to pandas DataFrame or Series",
        "A_Content": "  Since pandas 0.22 update, comparison options are available like:   gt (greater than) lt (lesser than) eq (equals to) ne (not equals to) ge (greater than or equals to)   and many more. These functions return boolean array. Let's see how we can use them:  # sample data df = pd.DataFrame({'col1': [0, 1, 2,3,4,5], 'col2': [10, 11, 12,13,14,15]})  # get values from col1 greater than or equals to 1 df.loc[df['col1'].ge(1),'col1']  1    1 2    2 3    3 4    4 5    5  # where co11 values is better 0 and 2 df.loc[df['col1'].between(0,2)]   col1 col2 0   0   10 1   1   11 2   2   12  # where col1 > 1 df.loc[df['col1'].gt(1)]   col1 col2 2   2   12 3   3   13 4   4   14 5   5   15      ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/13611065/efficient-way-to-apply-multiple-filters-to-pandas-dataframe-or-series",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.  The filters should be additive (aka each one applied should narrow results).  I'm currently using reindex() but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.  I'm thinking that using apply(), map(), or something similar might be better.  I'm pretty new to Pandas though so still trying to wrap my head around everything.  TL;DR  I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object.  relops = {'>=': [1], '<=': [1]}   Long Example  I'll start with an example of what I have currently and just filtering a single Series object.  Below is the function I'm currently using:     def apply_relops(series, relops):         \"\"\"         Pass dictionary of relational operators to perform on given series object         \"\"\"         for op, vals in relops.iteritems():             op_func = ops[op]             for val in vals:                 filtered = op_func(series, val)                 series = series.reindex(series[filtered])         return series   The user provides a dictionary with the operations they want to perform:  >>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]}) >>> print df >>> print df    col1  col2 0     0    10 1     1    11 2     2    12  >>> from operator import le, ge >>> ops ={'>=': ge, '<=': le} >>> apply_relops(df['col1'], {'>=': [1]}) col1 1       1 2       2 Name: col1 >>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]}) col1 1       1 Name: col1   Again, the 'problem' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.  Also, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I'm assuming whatever works for the Series can be easily expanded to a DataFrame.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Efficient way to apply multiple filters to pandas DataFrame or Series",
        "A_Content": "  Why not do this?  def filt_spec(df, col, val, op):     import operator     ops = {'eq': operator.eq, 'neq': operator.ne, 'gt': operator.gt, 'ge': operator.ge, 'lt': operator.lt, 'le': operator.le}     return df[ops[op](df[col], val)] pandas.DataFrame.filt_spec = filt_spec   Demo:  df = pd.DataFrame({'a': [1,2,3,4,5], 'b':[5,4,3,2,1]}) df.filt_spec('a', 2, 'ge')   Result:     a  b  1  2  4  2  3  3  3  4  2  4  5  1   You can see that column 'a' has been filtered where a >=2.  This is slightly faster (typing time, not performance) than operator chaining. You could of course put the import at the top of the file.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "algorithm",
            "pandas"
        ],
        "URL": "https://stackoverflow.com/questions/13611065/efficient-way-to-apply-multiple-filters-to-pandas-dataframe-or-series",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.  The filters should be additive (aka each one applied should narrow results).  I'm currently using reindex() but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.  I'm thinking that using apply(), map(), or something similar might be better.  I'm pretty new to Pandas though so still trying to wrap my head around everything.  TL;DR  I want to take a dictionary of the following form and apply each operation to a given Series object and return a 'filtered' Series object.  relops = {'>=': [1], '<=': [1]}   Long Example  I'll start with an example of what I have currently and just filtering a single Series object.  Below is the function I'm currently using:     def apply_relops(series, relops):         \"\"\"         Pass dictionary of relational operators to perform on given series object         \"\"\"         for op, vals in relops.iteritems():             op_func = ops[op]             for val in vals:                 filtered = op_func(series, val)                 series = series.reindex(series[filtered])         return series   The user provides a dictionary with the operations they want to perform:  >>> df = pandas.DataFrame({'col1': [0, 1, 2], 'col2': [10, 11, 12]}) >>> print df >>> print df    col1  col2 0     0    10 1     1    11 2     2    12  >>> from operator import le, ge >>> ops ={'>=': ge, '<=': le} >>> apply_relops(df['col1'], {'>=': [1]}) col1 1       1 2       2 Name: col1 >>> apply_relops(df['col1'], relops = {'>=': [1], '<=': [1]}) col1 1       1 Name: col1   Again, the 'problem' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.  Also, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I'm assuming whatever works for the Series can be easily expanded to a DataFrame.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "pandas groupby sort within groups",
        "A_Content": "  What you want to do is actually again a groupby (on the result of the first groupby): sort and take the first three elements per group.  Starting from the result of the first groupby:  In [60]: df_agg = df.groupby(['job','source']).agg({'count':sum})   We group by the first level of the index:  In [63]: g = df_agg['count'].groupby(level=0, group_keys=False)   Then we want to sort ('order') each group and take the first three elements:  In [64]: res = g.apply(lambda x: x.order(ascending=False).head(3))   However, for this, there is a shortcut function to do this, nlargest:  In [65]: g.nlargest(3) Out[65]: job     source market  A         5         D         4         B         3 sales   E         7         C         6         B         4 dtype: int64      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sorting",
            "pandas",
            "group-by"
        ],
        "URL": "https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups",
        "A_Votes": "72",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I want to group my dataframe by two columns and then sort the aggregated results within the groups.  In [167]: df  Out[167]: count   job source 0   2   sales   A 1   4   sales   B 2   6   sales   C 3   3   sales   D 4   7   sales   E 5   5   market  A 6   3   market  B 7   2   market  C 8   4   market  D 9   1   market  E  In [168]: df.groupby(['job','source']).agg({'count':sum})  Out[168]:             count job     source   market  A   5         B   3         C   2         D   4         E   1 sales   A   2         B   4         C   6         D   3         E   7   I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:              count job     source   market  A   5         D   4         B   3 sales   E   7         C   6         B   4      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "pandas groupby sort within groups",
        "A_Content": "  You could also just do it in one go, by doing the sort first and using head to take the first 3 of each group.   In[34]: df.sort_values(['job','count'],ascending=False).groupby('job').head(3)  Out[35]:     count     job source 4      7   sales      E 2      6   sales      C 1      4   sales      B 5      5  market      A 8      4  market      D 6      3  market      B      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sorting",
            "pandas",
            "group-by"
        ],
        "URL": "https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups",
        "A_Votes": "64",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to group my dataframe by two columns and then sort the aggregated results within the groups.  In [167]: df  Out[167]: count   job source 0   2   sales   A 1   4   sales   B 2   6   sales   C 3   3   sales   D 4   7   sales   E 5   5   market  A 6   3   market  B 7   2   market  C 8   4   market  D 9   1   market  E  In [168]: df.groupby(['job','source']).agg({'count':sum})  Out[168]:             count job     source   market  A   5         B   3         C   2         D   4         E   1 sales   A   2         B   4         C   6         D   3         E   7   I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:              count job     source   market  A   5         D   4         B   3 sales   E   7         C   6         B   4      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "pandas groupby sort within groups",
        "A_Content": "  Here's other example of taking top 3 on sorted order, and sorting within the groups:  In [43]: import pandas as pd                                                                                                                                                         In [44]:  df = pd.DataFrame({\"name\":[\"Foo\", \"Foo\", \"Baar\", \"Foo\", \"Baar\", \"Foo\", \"Baar\", \"Baar\"], \"count_1\":[5,10,12,15,20,25,30,35], \"count_2\" :[100,150,100,25,250,300,400,500]})  In [45]: df                                                                                                                                                                         Out[45]:     count_1  count_2  name 0        5      100   Foo 1       10      150   Foo 2       12      100  Baar 3       15       25   Foo 4       20      250  Baar 5       25      300   Foo 6       30      400  Baar 7       35      500  Baar   ### Top 3 on sorted order: In [46]: df.groupby([\"name\"])[\"count_1\"].nlargest(3)                                                                                                                                Out[46]:  name    Baar  7    35       6    30       4    20 Foo   5    25       3    15       1    10 dtype: int64   ### Sorting within groups based on column \"count_1\": In [48]: df.groupby([\"name\"]).apply(lambda x: x.sort_values([\"count_1\"], ascending = False)).reset_index(drop=True) Out[48]:     count_1  count_2  name 0       35      500  Baar 1       30      400  Baar 2       20      250  Baar 3       12      100  Baar 4       25      300   Foo 5       15       25   Foo 6       10      150   Foo 7        5      100   Foo      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sorting",
            "pandas",
            "group-by"
        ],
        "URL": "https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to group my dataframe by two columns and then sort the aggregated results within the groups.  In [167]: df  Out[167]: count   job source 0   2   sales   A 1   4   sales   B 2   6   sales   C 3   3   sales   D 4   7   sales   E 5   5   market  A 6   3   market  B 7   2   market  C 8   4   market  D 9   1   market  E  In [168]: df.groupby(['job','source']).agg({'count':sum})  Out[168]:             count job     source   market  A   5         B   3         C   2         D   4         E   1 sales   A   2         B   4         C   6         D   3         E   7   I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:              count job     source   market  A   5         D   4         B   3 sales   E   7         C   6         B   4      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "pandas groupby sort within groups",
        "A_Content": "  If you don't need to sum a column, then use @tvashtar's answer. If you do need to sum, then you can use @joris' answer or this one which is very similar to it.  df.groupby(['job']).apply(lambda x: (x.groupby('source')                                       .sum()                                       .sort_values('count', ascending=False))                                      .head(3))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sorting",
            "pandas",
            "group-by"
        ],
        "URL": "https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I want to group my dataframe by two columns and then sort the aggregated results within the groups.  In [167]: df  Out[167]: count   job source 0   2   sales   A 1   4   sales   B 2   6   sales   C 3   3   sales   D 4   7   sales   E 5   5   market  A 6   3   market  B 7   2   market  C 8   4   market  D 9   1   market  E  In [168]: df.groupby(['job','source']).agg({'count':sum})  Out[168]:             count job     source   market  A   5         B   3         C   2         D   4         E   1 sales   A   2         B   4         C   6         D   3         E   7   I would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:              count job     source   market  A   5         D   4         B   3 sales   E   7         C   6         B   4      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can't get Python to import from a different folder",
        "A_Content": "  I believe you need to create a file called __init__.py in the Models directory so that python treats it as a module.  Then you can do:  from Models.user import User   You can include code in the __init__.py (for instance initialization code that a few different classes need) or leave it blank.  But it must be there.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/456481/cant-get-python-to-import-from-a-different-folder",
        "A_Votes": "103",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I can't seem to get Python to import a module in a subfolder. I get the error when I try to create an instance of the class from the imported module, but the import itself succeeds. Here is my directory structure:  Server     -server.py     -Models         --user.py   Here's the contents of server.py:  from sys import path from os import getcwd path.append(getcwd() + \"\\\\models\") #Yes, i'm on windows print path import user  u=user.User() #error on this line   And user.py:  class User(Entity):     using_options(tablename='users')      username = Field(String(15))     password = Field(String(64))     email    = Field(String(50))     status   = Field(Integer)     created  = Field(DateTime)   The error is: AttributeError: 'module' object has no attribute 'User'     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can't get Python to import from a different folder",
        "A_Content": "  You have to create __init__.py on the Models subfolder. The file may be empty. It defines a package.  Then you can do:  from Models.user import User   Read all about it in python tutorial, here.  There is also a good article about file organization of python projects here.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/456481/cant-get-python-to-import-from-a-different-folder",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I can't seem to get Python to import a module in a subfolder. I get the error when I try to create an instance of the class from the imported module, but the import itself succeeds. Here is my directory structure:  Server     -server.py     -Models         --user.py   Here's the contents of server.py:  from sys import path from os import getcwd path.append(getcwd() + \"\\\\models\") #Yes, i'm on windows print path import user  u=user.User() #error on this line   And user.py:  class User(Entity):     using_options(tablename='users')      username = Field(String(15))     password = Field(String(64))     email    = Field(String(50))     status   = Field(Integer)     created  = Field(DateTime)   The error is: AttributeError: 'module' object has no attribute 'User'     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can't get Python to import from a different folder",
        "A_Content": "     import user      u=user.User() #error on this line   Because of the lack of __init__ mentioned above, you would expect an ImportError which would make the problem clearer.  You don't get one because 'user' is also an existing module in the standard library. Your import statement grabs that one and tries to find the User class inside it; that doesn't exist and only then do you get the error.  It is generally a good idea to make your import absolute:  import Server.Models.user   to avoid this kind of ambiguity. Indeed from Python 2.7 'import user' won't look relative to the current module at all.  If you really want relative imports, you can have them explicitly in Python 2.5 and up using the somewhat ugly syntax:  from .user import User      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/456481/cant-get-python-to-import-from-a-different-folder",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I can't seem to get Python to import a module in a subfolder. I get the error when I try to create an instance of the class from the imported module, but the import itself succeeds. Here is my directory structure:  Server     -server.py     -Models         --user.py   Here's the contents of server.py:  from sys import path from os import getcwd path.append(getcwd() + \"\\\\models\") #Yes, i'm on windows print path import user  u=user.User() #error on this line   And user.py:  class User(Entity):     using_options(tablename='users')      username = Field(String(15))     password = Field(String(64))     email    = Field(String(50))     status   = Field(Integer)     created  = Field(DateTime)   The error is: AttributeError: 'module' object has no attribute 'User'     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can't get Python to import from a different folder",
        "A_Content": "  You're missing __init__.py. From the Python tutorial:     The __init__.py files are required to   make Python treat the directories as   containing packages; this is done to   prevent directories with a common   name, such as string, from   unintentionally hiding valid modules   that occur later on the module search   path. In the simplest case,   __init__.py can just be an empty file, but it can also execute initialization   code for the package or set the   __all__ variable, described later.   Put an empty file named __init__.py in your Models directory, and all should be golden.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/456481/cant-get-python-to-import-from-a-different-folder",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I can't seem to get Python to import a module in a subfolder. I get the error when I try to create an instance of the class from the imported module, but the import itself succeeds. Here is my directory structure:  Server     -server.py     -Models         --user.py   Here's the contents of server.py:  from sys import path from os import getcwd path.append(getcwd() + \"\\\\models\") #Yes, i'm on windows print path import user  u=user.User() #error on this line   And user.py:  class User(Entity):     using_options(tablename='users')      username = Field(String(15))     password = Field(String(64))     email    = Field(String(50))     status   = Field(Integer)     created  = Field(DateTime)   The error is: AttributeError: 'module' object has no attribute 'User'     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can't get Python to import from a different folder",
        "A_Content": "  The right way to import a module located on a parent folder, when you don't have a standard package structure, is:  import os, sys CURRENT_DIR = os.path.dirname(os.path.abspath(__file__)) sys.path.append(os.path.dirname(CURRENT_DIR))   (you can merge the last two lines but this way is easier to understand).  This solution is cross-platform and is general enough to need not modify in other circumstances.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/456481/cant-get-python-to-import-from-a-different-folder",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I can't seem to get Python to import a module in a subfolder. I get the error when I try to create an instance of the class from the imported module, but the import itself succeeds. Here is my directory structure:  Server     -server.py     -Models         --user.py   Here's the contents of server.py:  from sys import path from os import getcwd path.append(getcwd() + \"\\\\models\") #Yes, i'm on windows print path import user  u=user.User() #error on this line   And user.py:  class User(Entity):     using_options(tablename='users')      username = Field(String(15))     password = Field(String(64))     email    = Field(String(50))     status   = Field(Integer)     created  = Field(DateTime)   The error is: AttributeError: 'module' object has no attribute 'User'     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can't get Python to import from a different folder",
        "A_Content": "  how do you write out the parameters os.path.dirname.... command?  import os, sys CURRENT_DIR = os.path.dirname(os.path.abspath(__file__)) sys.path.append(os.path.dirname(CURRENT_DIR))      ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/456481/cant-get-python-to-import-from-a-different-folder",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I can't seem to get Python to import a module in a subfolder. I get the error when I try to create an instance of the class from the imported module, but the import itself succeeds. Here is my directory structure:  Server     -server.py     -Models         --user.py   Here's the contents of server.py:  from sys import path from os import getcwd path.append(getcwd() + \"\\\\models\") #Yes, i'm on windows print path import user  u=user.User() #error on this line   And user.py:  class User(Entity):     using_options(tablename='users')      username = Field(String(15))     password = Field(String(64))     email    = Field(String(50))     status   = Field(Integer)     created  = Field(DateTime)   The error is: AttributeError: 'module' object has no attribute 'User'     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can't get Python to import from a different folder",
        "A_Content": "  My preferred way is to have __init__.py on every directory that contains modules that get used by other modules, and in the entry point, override sys.path as below:  def get_path(ss):   return os.path.join(os.path.dirname(__file__), ss) sys.path += [   get_path('Server'),    get_path('Models') ]   This makes the files in specified directories visible for import, and I can import user from Server.py.     ",
        "Language": "Python",
        "Tags": [
            "python"
        ],
        "URL": "https://stackoverflow.com/questions/456481/cant-get-python-to-import-from-a-different-folder",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I can't seem to get Python to import a module in a subfolder. I get the error when I try to create an instance of the class from the imported module, but the import itself succeeds. Here is my directory structure:  Server     -server.py     -Models         --user.py   Here's the contents of server.py:  from sys import path from os import getcwd path.append(getcwd() + \"\\\\models\") #Yes, i'm on windows print path import user  u=user.User() #error on this line   And user.py:  class User(Entity):     using_options(tablename='users')      username = Field(String(15))     password = Field(String(64))     email    = Field(String(50))     status   = Field(Integer)     created  = Field(DateTime)   The error is: AttributeError: 'module' object has no attribute 'User'     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Foreign key from one app into another in Django",
        "A_Content": "  According to the docs, your second attempt should work:     To refer to models defined in another application, you must instead explicitly specify the application label. For example, if the Manufacturer model above is defined in another application called production, you'd need to use:   class Car(models.Model):     manufacturer = models.ForeignKey('production.Manufacturer')   Have you tried putting it into quotes?     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models"
        ],
        "URL": "https://stackoverflow.com/questions/323763/foreign-key-from-one-app-into-another-in-django",
        "A_Votes": "119",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm wondering if it's possible to define a foreign key in a models.py file in Django that is a reference to a table in another app?  In other words, I have two apps, called cf and profiles, and in cf/models.py I have (amongst other things):  class Movie(models.Model):     title = models.CharField(max_length=255)   and in profiles/models.py I want to have:  class MovieProperty(models.Model):     movie = models.ForeignKey(Movie)   But I can't get it to work. I've tried:      movie = models.ForeignKey(cf.Movie)   and I've tried importing cf.Movie at the beginning of models.py, but I always get errors, such as:  NameError: name 'User' is not defined   Am I breaking the rules by trying to tie two apps together in this way, or have I just got the syntax wrong?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Foreign key from one app into another in Django",
        "A_Content": "  It is also possible to pass the class itself:  from django.db import models from production import models as production_models  class Car(models.Model):     manufacturer = models.ForeignKey(production_models.Manufacturer)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models"
        ],
        "URL": "https://stackoverflow.com/questions/323763/foreign-key-from-one-app-into-another-in-django",
        "A_Votes": "20",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm wondering if it's possible to define a foreign key in a models.py file in Django that is a reference to a table in another app?  In other words, I have two apps, called cf and profiles, and in cf/models.py I have (amongst other things):  class Movie(models.Model):     title = models.CharField(max_length=255)   and in profiles/models.py I want to have:  class MovieProperty(models.Model):     movie = models.ForeignKey(Movie)   But I can't get it to work. I've tried:      movie = models.ForeignKey(cf.Movie)   and I've tried importing cf.Movie at the beginning of models.py, but I always get errors, such as:  NameError: name 'User' is not defined   Am I breaking the rules by trying to tie two apps together in this way, or have I just got the syntax wrong?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Foreign key from one app into another in Django",
        "A_Content": "  OK - I've figured it out. You can do it, you just have to use the right import syntax. The correct syntax is:  from prototype.cf.models import Movie   My mistake was not specifying the .models part of that line. D'oh!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-models"
        ],
        "URL": "https://stackoverflow.com/questions/323763/foreign-key-from-one-app-into-another-in-django",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm wondering if it's possible to define a foreign key in a models.py file in Django that is a reference to a table in another app?  In other words, I have two apps, called cf and profiles, and in cf/models.py I have (amongst other things):  class Movie(models.Model):     title = models.CharField(max_length=255)   and in profiles/models.py I want to have:  class MovieProperty(models.Model):     movie = models.ForeignKey(Movie)   But I can't get it to work. I've tried:      movie = models.ForeignKey(cf.Movie)   and I've tried importing cf.Movie at the beginning of models.py, but I always get errors, such as:  NameError: name 'User' is not defined   Am I breaking the rules by trying to tie two apps together in this way, or have I just got the syntax wrong?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Accessing a value in a tuple that is in a list",
        "A_Content": "  With a list comprehension.  [x[1] for x in L]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/4800811/accessing-a-value-in-a-tuple-that-is-in-a-list",
        "A_Votes": "79",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    [(1,2), (2,3), (4,5), (3,4), (6,7), (6,7), (3,8)]   How do I return the 2nd value from each tuple inside this list?  Desired output:  [2, 3, 5, 4, 7, 7, 8]      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Accessing a value in a tuple that is in a list",
        "A_Content": "  Ignacio's answer is what you want. However, as someone also learning Python, let me try to dissect it for you... As mentioned, it is a list comprehension (covered in DiveIntoPython3, for example). Here are a few points:  [x[1] for x in L]   Notice the []'s around the line of code. These are what define a list. This tells you that this code returns a list, so it's of the list type. Hence, this technique is called a \"list comprehension.\" L is your original list. So you should define L = [(1,2),(2,3),(4,5),(3,4),(6,7),(6,7),(3,8)] prior to executing the above code. x is a variable that only exists in the comprehension - try to access x outside of the comprehension, or type type(x) after executing the above line and it will tell you NameError: name 'x' is not defined, whereas type(L) returns <class 'list'>. x[1] points to the second item in each of the tuples whereas x[0] would point to each of the first items. So this line of code literally reads \"return the second item in a tuple for all tuples in list L.\"   It's tough to tell how much you attempted the problem prior to asking the question, but perhaps you just weren't familiar with comprehensions? I would spend some time reading through Chapter 3 of DiveIntoPython, or any resource on comprehensions. Good luck.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/4800811/accessing-a-value-in-a-tuple-that-is-in-a-list",
        "A_Votes": "64",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    [(1,2), (2,3), (4,5), (3,4), (6,7), (6,7), (3,8)]   How do I return the 2nd value from each tuple inside this list?  Desired output:  [2, 3, 5, 4, 7, 7, 8]      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Accessing a value in a tuple that is in a list",
        "A_Content": "  A list comprehension is absolutely the way to do this. Another way that should be faster is map and itemgetter.  import operator  new_list = map(operator.itemgetter(1), old_list)   In response to the comment that the OP couldn't find an answer on google, I'll point out a super naive way to do it.  new_list = [] for item in old_list:     new_list.append(item[1])   This uses:   Declaring a variable to reference an empty list. A for loop. Calling the append method on a list.   If somebody is trying to learn a language and can't put together these basic pieces for themselves, then they need to view it as an exercise and do it themselves even if it takes twenty hours.  One needs to learn how to think about what one wants and compare that to the available tools. Every element in my second answer should be covered in a basic tutorial. You cannot learn to program without reading one.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/4800811/accessing-a-value-in-a-tuple-that-is-in-a-list",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    [(1,2), (2,3), (4,5), (3,4), (6,7), (6,7), (3,8)]   How do I return the 2nd value from each tuple inside this list?  Desired output:  [2, 3, 5, 4, 7, 7, 8]      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Accessing a value in a tuple that is in a list",
        "A_Content": "  You can also use sequence unpacking with zip:  L = [(1,2),(2,3),(4,5),(3,4),(6,7),(6,7),(3,8)]  _, res = zip(*L)  print(res)  # (2, 3, 5, 4, 7, 7, 8)   This also creates a tuple _ from the discarded first elements. Extracting only the second is possible, but more verbose:  from itertools import islice  res = next(islice(zip(*L), 1, None))      ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/4800811/accessing-a-value-in-a-tuple-that-is-in-a-list",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    [(1,2), (2,3), (4,5), (3,4), (6,7), (6,7), (3,8)]   How do I return the 2nd value from each tuple inside this list?  Desired output:  [2, 3, 5, 4, 7, 7, 8]      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Accessing a value in a tuple that is in a list",
        "A_Content": "  OR you can use pandas:  >>> import pandas as pd >>> L = [(1,2),(2,3),(4,5),(3,4),(6,7),(6,7),(3,8)] >>> df=pd.DataFrame(L) >>> df[1] 0    2 1    3 2    5 3    4 4    7 5    7 6    8 Name: 1, dtype: int64 >>> df[1].tolist() [2, 3, 5, 4, 7, 7, 8] >>>    Or numpy:  >>> import numpy as np >>> L = [(1,2),(2,3),(4,5),(3,4),(6,7),(6,7),(3,8)] >>> arr=np.array(L) >>> arr.T[1] array([2, 3, 5, 4, 7, 7, 8]) >>> arr.T[1].tolist() [2, 3, 5, 4, 7, 7, 8] >>>       ",
        "Language": "Python",
        "Tags": [
            "python",
            "list",
            "tuples"
        ],
        "URL": "https://stackoverflow.com/questions/4800811/accessing-a-value-in-a-tuple-that-is-in-a-list",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    [(1,2), (2,3), (4,5), (3,4), (6,7), (6,7), (3,8)]   How do I return the 2nd value from each tuple inside this list?  Desired output:  [2, 3, 5, 4, 7, 7, 8]      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  Closure on closures     Objects are data with methods   attached, closures are functions with   data attached.   def make_counter():     i = 0     def counter(): # counter() is a closure         nonlocal i         i += 1         return i     return counter  c1 = make_counter() c2 = make_counter()  print (c1(), c1(), c2(), c2()) # -> 1 2 1 2      ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "76",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  It's simple: A function that references variables from a containing scope, potentially after flow-of-control has left that scope. That last bit is very useful:  >>> def makeConstantAdder(x): ...     constant = x ...     def adder(y): ...         return y + constant ...     return adder ...  >>> f = makeConstantAdder(12) >>> f(3) 15 >>> g = makeConstantAdder(4) >>> g(3) 7   Note that 12 and 4 have \"disappeared\" inside f and g, respectively, this feature is what make f and g proper closures.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "42",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  I like this rough, succinct definition:     A function that can refer to environments that are no longer active.   I'd add     A closure allows you to bind variables into a function without passing them as parameters.   Decorators which accept parameters are a common use for closures.  Closures are a common implementation mechanism for that sort of \"function factory\".  I frequently choose to use closures in the Strategy Pattern when the strategy is modified by data at run-time.  In a language that allows anonymous block definition -- e.g., Ruby, C# -- closures can be used to implement (what amount to) novel new control structures.  The lack of anonymous blocks is among the limitations of closures in Python.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "14",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  To be honest, I understand closures perfectly well except I've never been clear about what exactly is the thing which is the \"closure\" and what's so \"closure\" about it. I recommend you give up looking for any logic behind the choice of term.  Anyway, here's my explanation:  def foo():    x = 3    def bar():       print x    x = 5    return bar  bar = foo() bar()   # print 5   A key idea here is that the function object returned from foo retains a hook to the local var 'x' even though 'x' has gone out of scope and should be defunct. This hook is to the var itself, not just the value that var had at the time, so when bar is called, it prints 5, not 3.  Also be clear that Python 2.x has limited closure: there's no way I can modify 'x' inside 'bar' because writing 'x = bla' would declare a local 'x' in bar, not assign to 'x' of foo. This is a side-effect of Python's assignment=declaration. To get around this, Python 3.0 introduces the nonlocal keyword:  def foo():    x = 3    def bar():       print x    def ack():       nonlocal x       x = 7    x = 5    return (bar, ack)  bar, ack = foo() ack()   # modify x of the call to foo bar()   # print 7      ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  I've never heard of transactions being used in the same context as explaining what a closure is and there really aren't any transaction semantics here.  It's called a closure because it \"closes over\" the outside variable (constant)--i.e., it's not just a function but an enclosure of the environment where the function was created.   In the following example, calling the closure g after changing x will also change the value of x within g, since g closes over x:  x = 0  def f():     def g():          return x * 2     return g   closure = f() print(closure()) # 0 x = 2 print(closure()) # 4      ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  Here's a typical use case for closures - callbacks for GUI elements (this would be an alternative to subclassing the button class). For example, you can construct a function that will be called in response to a button press, and \"close\" over the relevant variables in the parent scope that are necessary for processing the click. This way you can wire up pretty complicated interfaces from the same initialization function, building all the dependencies into the closure.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  In Python, a closure is an instance of a function that has variables bound to it immutably.  In fact, the data model explains this in its description of functions' __closure__ attribute:      None or a tuple of cells that contain bindings for the functions free variables. Read-only   To demonstrate this:  def enclosure(foo):     def closure(bar):         print(foo, bar)     return closure  closure_instance = enclosure('foo')   Clearly, we know that we now have a function pointed at from the variable name closure_instance. Ostensibly, if we call it with an object, bar, it should print the string, 'foo' and whatever the string representation of bar is.  In fact, the string 'foo' is bound to the instance of the function, and we can directly read it here, by accessing the cell_contents attribute of the first (and only) cell in the tuple of the __closure__ attribute:  >>> closure_instance.__closure__[0].cell_contents 'foo'   As an aside, cell objects are described in the C API documentation:     \"Cell\" objects are used to implement variables referenced by multiple   scopes   And we can demonstrate our closure's usage, noting that 'foo' is stuck in the function and doesn't change:  >>> closure_instance('bar') foo bar >>> closure_instance('baz') foo baz >>> closure_instance('quux') foo quux   And nothing can change it:  >>> closure_instance.__closure__ = None Traceback (most recent call last):   File \"<stdin>\", line 1, in <module> TypeError: readonly attribute   Partial Functions  The example given uses the closure as a partial function, but if this is our only goal, the same goal can be accomplished with functools.partial  >>> from __future__ import print_function # use this if you're in Python 2. >>> partial_function = functools.partial(print, 'foo') >>> partial_function('bar') foo bar >>> partial_function('baz') foo baz >>> partial_function('quux') foo quux   There are more complicated closures as well that would not fit the partial function example, and I'll demonstrate them further as time allows.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  Here is an example of Python3 closures   def closure(x):     def counter():         nonlocal x         x += 1         return x     return counter;  counter1 = closure(100); counter2 = closure(200);  print(\"i from closure 1 \" + str(counter1())) print(\"i from closure 1 \" + str(counter1())) print(\"i from closure 2 \" + str(counter2())) print(\"i from closure 1 \" + str(counter1())) print(\"i from closure 1 \" + str(counter1())) print(\"i from closure 1 \" + str(counter1())) print(\"i from closure 2 \" + str(counter2()))  # result  i from closure 1 101 i from closure 1 102 i from closure 2 201 i from closure 1 103 i from closure 1 104 i from closure 1 105 i from closure 2 202      ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  For me, \"closures\" are functions which are capable to remember the environment they were created. This functionality, allows you to use variables or methods within the closure wich, in other way,you wouldn't be able to use either because they don't exist anymore or  they are out of reach due to scope. Let's look at this code in ruby:  def makefunction (x)   def multiply (a,b)     puts a*b   end   return lambda {|n| multiply(n,x)} # => returning a closure end  func = makefunction(2) # => we capture the closure func.call(6)    # => Result equal \"12\"     it works even when both, \"multiply\" method and \"x\" variable,not longer exist. All because the closure capability to remember.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  # A Closure is a function object that remembers values in enclosing scopes even if they are not present in memory.  # Defining a closure  # This is an outer function. def outer_function(message):     # This is an inner nested function.     def inner_function():         print(message)     return inner_function  # Now lets call the outer function and return value bound to name 'temp' temp = outer_function(\"Hello\") # On calling temp, 'message' will be still be remembered although we had finished executing outer_function() temp() # Technique by which some data('message') that remembers values in enclosing scopes  # even if they are not present in memory is called closures  # Output: Hello   Criteria to met by Closures are:   We must have nested function. Nested function must refer to the value defined in the enclosing function. Enclosing function must return the nested function.     # Example 2 def make_multiplier_of(n): # Outer function     def multiplier(x): # Inner nested function         return x * n     return multiplier # Multiplier of 3 times3 = make_multiplier_of(3) # Multiplier of 5 times5 = make_multiplier_of(5) print(times5(3)) # 15 print(times3(2)) #  6      ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  we all have used Decorators in python. They are nice examples to show what are closure functions in python.  class Test():     def decorator(func):         def wrapper(*args):             b = args[1] + 5             return func(b)         return wrapper  @decorator def foo(val):     print val + 2  obj = Test() obj.foo(5)   here final value is 12  Here, the wrapper function is able to access func object because wrapper is \"lexical closure\", it can access it's parent attributes. That is why, it is able to access func object.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  I would like to share my example and an explanation about closures. I made a python example, and two figures to demonstrate stack states.  def maker(a, b, n):     margin_top = 2     padding = 4     def message(msg):         print('\\n * margin_top, a * n,              '  * padding, msg, '  * padding, b * n)     return message  f = maker('*', '#', 5) g = maker('', ', 3)  f('hello') g(good bye!')   The output of this code would be as follows:  *****      hello      #####        good bye!       Here are two figures to show stacks and the closure attached to the function object.  when the function is returned from maker  when the function is called later  When the function is called through a parameter or a nonlocal variable, the code needs local variable bindings such as margin_top, padding as well as a, b, n. In order to ensure the function code to work, the stack frame of the maker function which was gone away long ago should be accessible, which is backed up in the closure we can find along with the 'message's function object.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Can you explain closures (as they relate to Python)?",
        "A_Content": "  The best explanation I ever saw of a closure was to explain the mechanism.  It went something like this:  Imagine your program stack as a degenerate tree where each node has only one child and the single leaf node is the context of your currently executing procedure.  Now relax the constraint that each node can have only one child.  If you do this, you can have a construct ('yield') that can return from a procedure without discarding the local context (i.e. it doesn't pop it off the stack when you return).  The next time the procedure is invoked, the invocation picks up the old stack (tree) frame and continues executing where it left off.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "functional-programming",
            "closures"
        ],
        "URL": "https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python",
        "A_Votes": "-2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I've been reading a lot about closures and I think I understand them, but without clouding the picture for myself and others, I am hoping someone can explain closures as succinctly and clearly as possible.  I'm looking for a simple explanation that might help me understand where and why I would want to use them.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Adding dictionaries together, Python [duplicate]",
        "A_Content": "  If you're interested in creating a new dict without using intermediary storage: (this is faster, and in my opinion, cleaner than using dict.items())  dic2 = dict(dic0, **dic1)   Or if you're happy to use one of the existing dicts:  dic0.update(dic1)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "append"
        ],
        "URL": "https://stackoverflow.com/questions/6005066/adding-dictionaries-together-python",
        "A_Votes": "125",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "          This question already has an answer here:                              how to concatenate two dictionaries to create a new one in Python? [duplicate]                                        5 answers                                          I have two dictionaries and I'd like to be able to make them one:  Something like this pseudo-Python would be nice:  dic0 = {'dic0': 0} dic1 = {'dic1': 1}  ndic = dic0 + dic1 # ndic would equal {'dic0': 0, 'dic1': 1}      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Adding dictionaries together, Python [duplicate]",
        "A_Content": "  >>> dic0 = {'dic0':0} >>> dic1 = {'dic1':1} >>> ndic = dict(dic0.items() + dic1.items()) >>> ndic {'dic0': 0, 'dic1': 1} >>>      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "append"
        ],
        "URL": "https://stackoverflow.com/questions/6005066/adding-dictionaries-together-python",
        "A_Votes": "13",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              how to concatenate two dictionaries to create a new one in Python? [duplicate]                                        5 answers                                          I have two dictionaries and I'd like to be able to make them one:  Something like this pseudo-Python would be nice:  dic0 = {'dic0': 0} dic1 = {'dic1': 1}  ndic = dic0 + dic1 # ndic would equal {'dic0': 0, 'dic1': 1}      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Adding dictionaries together, Python [duplicate]",
        "A_Content": "  You are looking for the update method  dic0.update( dic1 ) print( dic0 )    gives  {'dic0': 0, 'dic1': 1}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "append"
        ],
        "URL": "https://stackoverflow.com/questions/6005066/adding-dictionaries-together-python",
        "A_Votes": "9",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              how to concatenate two dictionaries to create a new one in Python? [duplicate]                                        5 answers                                          I have two dictionaries and I'd like to be able to make them one:  Something like this pseudo-Python would be nice:  dic0 = {'dic0': 0} dic1 = {'dic1': 1}  ndic = dic0 + dic1 # ndic would equal {'dic0': 0, 'dic1': 1}      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Adding dictionaries together, Python [duplicate]",
        "A_Content": "  dic0.update(dic1)   Note this doesn't actually return the combined dictionary, it just mutates dic0.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "append"
        ],
        "URL": "https://stackoverflow.com/questions/6005066/adding-dictionaries-together-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              how to concatenate two dictionaries to create a new one in Python? [duplicate]                                        5 answers                                          I have two dictionaries and I'd like to be able to make them one:  Something like this pseudo-Python would be nice:  dic0 = {'dic0': 0} dic1 = {'dic1': 1}  ndic = dic0 + dic1 # ndic would equal {'dic0': 0, 'dic1': 1}      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Adding dictionaries together, Python [duplicate]",
        "A_Content": "  Please search the site before asking questions next time: how to concatenate two dictionaries to create a new one in Python?  The easiest way to do it is to simply use your example code, but using the items() member of each dictionary. So, the code would be:  dic0 = {'dic0': 0} dic1 = {'dic1': 1} dic2 = dict(dic0.items() + dic1.items())   I tested this in IDLE and it works fine.  However, the previous question on this topic states that this method is slow and chews up memory. There are several other ways recommended there, so please see that if memory usage is important.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "append"
        ],
        "URL": "https://stackoverflow.com/questions/6005066/adding-dictionaries-together-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              how to concatenate two dictionaries to create a new one in Python? [duplicate]                                        5 answers                                          I have two dictionaries and I'd like to be able to make them one:  Something like this pseudo-Python would be nice:  dic0 = {'dic0': 0} dic1 = {'dic1': 1}  ndic = dic0 + dic1 # ndic would equal {'dic0': 0, 'dic1': 1}      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Adding dictionaries together, Python [duplicate]",
        "A_Content": "  Here are quite a few ways to add dictionaries.  Creates a new dict by adding both items.  ndic = dict(dic0.items() + dic1.items())   If your ok to modify dic0  dic0.update(dic1)   If your NOT ok to modify dic0  ndic = dic0.copy() ndic.update(dic1)   If all the keys in one dict are ensured to be strings (dic1 in this case, of course args can be swapped)  ndic = dict(dic0, **dic1)   In some cases it may be handy to use dict comprehensions (Python 2.7 or newer),Especially if you want to filter out or transform some keys/values at the same time.  ndic = {k: v for d in (dic0, dic1) for k, v in d.items()}      ",
        "Language": "Python",
        "Tags": [
            "python",
            "dictionary",
            "append"
        ],
        "URL": "https://stackoverflow.com/questions/6005066/adding-dictionaries-together-python",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "          This question already has an answer here:                              how to concatenate two dictionaries to create a new one in Python? [duplicate]                                        5 answers                                          I have two dictionaries and I'd like to be able to make them one:  Something like this pseudo-Python would be nice:  dic0 = {'dic0': 0} dic1 = {'dic1': 1}  ndic = dic0 + dic1 # ndic would equal {'dic0': 0, 'dic1': 1}      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  Since Django 1.10 it's possible! You just have to do what you asked for:  class Place(models.Model):     name = models.CharField(max_length=20)     rating = models.DecimalField()      class Meta:         abstract = True  class LongNamedRestaurant(Place):  # Subclassing `Place`.     name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.     food_type = models.CharField(max_length=25)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "38",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  No, it is not:     Field name hiding is not permitted      In normal Python class inheritance, it is permissible for a child   class to override any attribute from the parent class. In Django, this   is not permitted for attributes that are Field instances (at least,   not at the moment). If a base class has a field called author, you   cannot create another model field called author in any class that   inherits from that base class.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "53",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  That is not possible unless abstract, and here is why: LongNamedRestaurant is also a Place, not only as a class but also in the database. The place-table contains an entry for every pure Place and for every LongNamedRestaurant. LongNamedRestaurant just creates an extra table with the food_type and a reference to the place table.  If you do Place.objects.all(), you also get every place that is a LongNamedRestaurant, and it will be an instance of Place (without the food_type). So Place.name and LongNamedRestaurant.name share the same database column, and must therefore be of the same type.  I think this makes sense for normal models: every restaurant is a place, and should have at least everything that place has. Maybe this consistency is also why it was not possible for abstract models before 1.10, although it would not give database problems there. As @lampslave remarks, it was made possible in 1.10. I would personally recommend care: if Sub.x overrides Super.x, make sure Sub.x is a subclass of Super.x, otherwise Sub cannot be used in place of Super.  Workarounds: You can create a custom user model (AUTH_USER_MODEL) which involves quite a bit of code duplication if you only need to change the email field. Alternatively you can leave email as it is and make sure it's required in all forms. This doesn't guarantee database integrity if other applications use it, and doesn't work the other way around (if you want to make username not required).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "22",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  See https://stackoverflow.com/a/6379556/15690:  class BaseMessage(models.Model):     is_public = models.BooleanField(default=False)     # some more fields...      class Meta:         abstract = True  class Message(BaseMessage):     # some fields... Message._meta.get_field('is_public').default = True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  Pasted your code into a fresh app, added app to INSTALLED_APPS and ran syncdb:  django.core.exceptions.FieldError: Local field 'name' in class 'LongNamedRestaurant' clashes with field of similar name from base class 'Place'   Looks like Django does not support that.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "10",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  This supercool piece of code allows you to 'override' fields in abstract parent classes.  def AbstractClassWithoutFieldsNamed(cls, *excl):     \"\"\"     Removes unwanted fields from abstract base classes.      Usage::     >>> from oscar.apps.address.abstract_models import AbstractBillingAddress      >>> from koe.meta import AbstractClassWithoutFieldsNamed as without     >>> class BillingAddress(without(AbstractBillingAddress, 'phone_number')):     ...     pass     \"\"\"     if cls._meta.abstract:         remove_fields = [f for f in cls._meta.local_fields if f.name in excl]         for f in remove_fields:             cls._meta.local_fields.remove(f)         return cls     else:         raise Exception(\"Not an abstract model\")   When the fields have been removed from the abstract parent class you are free to redefine them as you need.  This is not my own work. Original code from here: https://gist.github.com/specialunderwear/9d917ddacf3547b646ba     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  Maybe you could deal with contribute_to_class :  class LongNamedRestaurant(Place):      food_type = models.CharField(max_length=25)      def __init__(self, *args, **kwargs):         super(LongNamedRestaurant, self).__init__(*args, **kwargs)         name = models.CharField(max_length=255)         name.contribute_to_class(self, 'name')   Syncdb works fine. I dont tried this example, in my case I just override a constraint parameter so ... wait & see !     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "6",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  I know it's an old question, but i had a similar problem and found a workaround:   I had the following classes:  class CommonInfo(models.Model):     image = models.ImageField(blank=True, null=True, default=\"\")      class Meta:         abstract = True  class Year(CommonInfo):     year = models.IntegerField()    But I wanted Year's inherited image-field to be required while keeping the image field of the superclass nullable. In the end I used ModelForms to enforce the image at the validation stage:  class YearForm(ModelForm):     class Meta:         model = Year      def clean(self):         if not self.cleaned_data['image'] or len(self.cleaned_data['image'])==0:             raise ValidationError(\"Please provide an image.\")          return self.cleaned_data   admin.py:  class YearAdmin(admin.ModelAdmin):     form = YearForm   It seems this is only applicable for some situations (certainly where you need to enforce stricter rules on the subclass field).  Alternatively you can use the clean_<fieldname>() method instead of clean(), e.g. if a field town would be required to be filled in:  def clean_town(self):     town = self.cleaned_data[\"town\"]     if not town or len(town) == 0:         raise forms.ValidationError(\"Please enter a town\")     return town      ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "3",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "In Django - Model Inheritance - Does it allow you to override a parent model's attribute?",
        "A_Content": "  You can not override Model fields, but its easily achieved by overriding/specifying clean() method. I had the issue with email field and wanted to make it unique on Model level and did it like this:  def clean(self):     \"\"\"     Make sure that email field is unique     \"\"\"     if MyUser.objects.filter(email=self.email):         raise ValidationError({'email': _('This email is already in use')})   The error message is then captured by Form field with name \"email\"     ",
        "Language": "Python",
        "Tags": [
            "python",
            "django",
            "django-inheritance"
        ],
        "URL": "https://stackoverflow.com/questions/2344751/in-django-model-inheritance-does-it-allow-you-to-override-a-parent-models-a",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    I'm looking to do this:  class Place(models.Model):    name = models.CharField(max_length=20)    rating = models.DecimalField()  class LongNamedRestaurant(Place):  # Subclassing `Place`.    name = models.CharField(max_length=255)  # Notice, I'm overriding `Place.name` to give it a longer length.    food_type = models.CharField(max_length=25)   This is the version I would like to use (although I'm open to any suggestion): http://docs.djangoproject.com/en/dev/topics/db/models/#id7  Is this supported in Django? If not, is there a way to achieve similar results?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  Given an instance x of datetime.date, (x.month-1)//3 will give you the quarter (0 for first quarter, 1 for second quarter, etc -- add 1 if you need to count from 1 instead;-).    Originally two answers, multiply upvoted and even originally accepted (both currently deleted), were buggy -- not doing the -1 before the division, and dividing by 4 instead of 3.  Since .month goes 1 to 12, it's easy to check for yourself what formula is right:  for m in range(1, 13):   print m//4 + 1, print   gives 1 1 1 2 2 2 2 3 3 3 3 4 -- two four-month quarters and a single-month one (eep).  for m in range(1, 13):   print (m-1)//3 + 1, print   gives 1 1 1 2 2 2 3 3 3 4 4 4 -- now doesn't this look vastly preferable to you?-)  This proves that the question is well warranted, I think;-).  I don't think the datetime module should necessarily have every possible useful calendric function, but I do know I maintain a (well-tested;-) datetools module for the use of my (and others') projects at work, which has many little functions to perform all of these calendric computations -- some are complex, some simple, but there's no reason to do the work over and over (even simple work) or risk bugs in such computations;-).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "112",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  IF you are already using pandas, it's quite simple.  import datetime as dt import pandas as pd  quarter = pd.Timestamp(dt.date(2016, 2, 29)).quarter assert quarter == 1   If you have a date column in a dataframe, you can easily create a new quarter column:  df['quarter'] = df['date'].dt.quarter      ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "24",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  I would suggest another arguably cleaner solution. If X is a datetime.datetime.now() instance, then the quarter is:  import math Q=math.ceil(X.month/3.)   ceil has to be imported from math module as it can't be accessed directly.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  For anyone trying to get the quarter of the fiscal year, which may differ from the calendar year, I wrote a Python module to do just this.  Installation is simple. Just run:  $ pip install fiscalyear   There are no dependencies, and fiscalyear should work for both Python 2 and 3.  It's basically a wrapper around the built-in datetime module, so any datetime commands you are already familiar with will work. Here's a demo:  >>> from fiscalyear import * >>> a = FiscalDate.today() >>> a FiscalDate(2017, 5, 6) >>> a.fiscal_year 2017 >>> a.quarter 3 >>> b = FiscalYear(2017) >>> b.start FiscalDateTime(2016, 10, 1, 0, 0) >>> b.end FiscalDateTime(2017, 9, 30, 23, 59, 59) >>> b.q3 FiscalQuarter(2017, 3) >>> b.q3.start FiscalDateTime(2017, 4, 1, 0, 0) >>> b.q3.end FiscalDateTime(2017, 6, 30, 23, 59, 59)   fiscalyear is hosted on GitHub and PyPI. Documentation can be found at Read the Docs. If you're looking for any features that it doesn't currently have, let me know!     ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "4",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  This is an old question but still worthy of discussion.  Here is my solution, using the excellent dateutil module.    from dateutil import rrule,relativedelta     year = this_date.year    quarters = rrule.rrule(rrule.MONTHLY,                       bymonth=(1,4,7,10),                       bysetpos=-1,                       dtstart=datetime.datetime(year,1,1),                       count=8)     first_day = quarters.before(this_date)    last_day =  (quarters.after(this_date)                 -relativedelta.relativedelta(days=1)   So first_day is the first day of the quarter, and last_day is the last day of the quarter (calculated by finding the first day of the next quarter, minus one day).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  if m is the month number...  import math math.ceil(float(m) / 3)      ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  hmmm so calculations can go wrong, here is a better version (just for the sake of it)  first, second, third, fourth=1,2,3,4# you can make strings if you wish :)  quarterMap = {} quarterMap.update(dict(zip((1,2,3),(first,)*3))) quarterMap.update(dict(zip((4,5,6),(second,)*3))) quarterMap.update(dict(zip((7,8,9),(third,)*3))) quarterMap.update(dict(zip((10,11,12),(fourth,)*3)))  print quarterMap[6]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  Here is a verbose, but also readable solution that will work for datetime and date instances  def get_quarter(date):     for months, quarter in [         ([1, 2, 3], 1),         ([4, 5, 6], 2),         ([7, 8, 9], 3),         ([10, 11, 12], 4)     ]:         if date.month in months:             return quarter      ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "Is there a Python function to determine which quarter of the year a date is in?",
        "A_Content": "  For those, who are looking for financial year quarter data, using pandas,  import datetime today_date = datetime.date.today() quarter = pd.PeriodIndex(today_date, freq='Q-MAR').strftime('Q%q')   reference:  pandas period index     ",
        "Language": "Python",
        "Tags": [
            "python",
            "date"
        ],
        "URL": "https://stackoverflow.com/questions/1406131/is-there-a-python-function-to-determine-which-quarter-of-the-year-a-date-is-in",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Sure I could write this myself, but before I go reinventing the wheel is there a function that already does this?     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "sql LIKE equivalent in django query",
        "A_Content": "  Use contains or icontains:  result = table.objects.filter(string__contains='pattern')      ",
        "Language": "Python",
        "Tags": [
            "python",
            "sql",
            "django",
            "django-models",
            "django-queryset"
        ],
        "URL": "https://stackoverflow.com/questions/18140838/sql-like-equivalent-in-django-query",
        "A_Votes": "135",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    What is the equivalent of this SQL statement in django?  SELECT * FROM table_name WHERE string LIKE pattern;   How do I implement this in django? I tried  result = table.objects.filter( pattern in string )   But that did not work. How do i implement this?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "sql LIKE equivalent in django query",
        "A_Content": "  contains and icontains mentioned by falsetrue make queries like SELECT ... WHERE headline LIKE '%pattern%   Along with them, you might need these ones with similar behavior: startswith, istartswith, endswith, iendswith  making   SELECT ... WHERE headline LIKE 'pattern%  or  SELECT ... WHERE headline LIKE '%pattern       ",
        "Language": "Python",
        "Tags": [
            "python",
            "sql",
            "django",
            "django-models",
            "django-queryset"
        ],
        "URL": "https://stackoverflow.com/questions/18140838/sql-like-equivalent-in-django-query",
        "A_Votes": "18",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What is the equivalent of this SQL statement in django?  SELECT * FROM table_name WHERE string LIKE pattern;   How do I implement this in django? I tried  result = table.objects.filter( pattern in string )   But that did not work. How do i implement this?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "sql LIKE equivalent in django query",
        "A_Content": "  result = table.objects.filter(string__icontains='pattern')   Case insensitive search for string in a field.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "sql",
            "django",
            "django-models",
            "django-queryset"
        ],
        "URL": "https://stackoverflow.com/questions/18140838/sql-like-equivalent-in-django-query",
        "A_Votes": "7",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    What is the equivalent of this SQL statement in django?  SELECT * FROM table_name WHERE string LIKE pattern;   How do I implement this in django? I tried  result = table.objects.filter( pattern in string )   But that did not work. How do i implement this?      ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  any won't go beyond the first element if it's True. In case the iterator yields something false-ish you can write any(True for _ in iterator).     ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "109",
        "_type": "dict",
        "isAccepted": "Yes",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  In Python 2.6+, if name sentinel is bound to a value which the iterator can't possibly yield,  if next(iterator, sentinel) is sentinel:     print('iterator was empty')   If you have no idea of what the iterator might possibly yield, make your own sentinel (e.g. at the top of your module) with  sentinel = object()   Otherwise, you could use, in the sentinel role, any value which you \"know\" (based on application considerations) that the iterator can't possibly yield.     ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "32",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  This isn't really cleaner, but it shows a way to package it in a function losslessly:  def has_elements(iter):   from itertools import tee   iter, any_check = tee(iter)   try:     any_check.next()     return True, iter   except StopIteration:     return False, iter  has_el, iter = has_elements(iter) if has_el:   # not empty   This isn't really pythonic, and for particular cases, there are probably better (but less general) solutions, like the next default.   first = next(iter, None) if first:   # Do something   This isn't general because None can be a valid element in many iterables.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "19",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  you can use:  if zip([None], iterator):     # ... else:     # ...   but it's a bit nonexplanatory for the code reader     ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "5",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  What about:  In [1]: i=iter([])  In [2]: bool(next(i,False)) Out[2]: False  In [3]: i=iter([1])  In [4]: bool(next(i,False)) Out[4]: True      ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "2",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  __length_hint__ estimates the length of list(it) - it's private method, though:  x = iter( (1, 2, 3) ) help(x.__length_hint__)       1 Help on built-in function __length_hint__:       2        3 __length_hint__(...)       4     Private method returning an estimate of len(list(it)).      ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  This is an overkill iterator wrapper that generally allows to check whether there's a next item (via conversion to boolean).  Of course pretty inefficient.  class LookaheadIterator ():      def __init__(self, iterator):         self.__iterator = iterator         try:             self.__next      = next (iterator)             self.__have_next = True         except StopIteration:             self.__have_next = False      def __iter__(self):         return self      def next (self):         if self.__have_next:             result = self.__next             try:                 self.__next      = next (self.__iterator)                 self.__have_next = True             except StopIteration:                 self.__have_next = False              return result          else:             raise StopIteration      def __nonzero__(self):         return self.__have_next  x = LookaheadIterator (iter ([])) print bool (x) print list (x)  x = LookaheadIterator (iter ([1, 2, 3])) print bool (x) print list (x)   Output:  False [] True [1, 2, 3]      ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "0",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  A little late, but... You could turn the iterator into a list and then work with that list:  # Create a list of objects but runs out the iterator. l = [_ for _ in iterator]  # If the list is not empty then the iterator had elements; else it was empty. if l :     pass # Use the elements of the list (i.e. from the iterator) else :     pass # Iterator was empty, thus list is empty.      ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    },
    {
        "Q_Title": "One-liner to check whether an iterator yields at least one element?",
        "A_Content": "  The iterator object can be converted to a list object and then the list's length can be checked:  len(list(a_iterator)) == 0   If this returns True then the iterator is empty     ",
        "Language": "Python",
        "Tags": [
            "python",
            "iterator"
        ],
        "URL": "https://stackoverflow.com/questions/3114252/one-liner-to-check-whether-an-iterator-yields-at-least-one-element",
        "A_Votes": "-1",
        "_type": "dict",
        "isAccepted": "No",
        "Q_Content": "    Currently I'm doing this:  try:     something = iterator.next()     # ... except StopIteration:     # ...   But I would like an expression that I can place inside a simple if statement. Is there anything built-in which would make this code look less clumsy?  any() returns False if an iterable is empty, but it will potentially iterate over all the items if it's not. I only need it to check the first item.    Someone asks what I'm trying to do. I have written a function which executes an SQL query and yields its results. Sometimes when I call this function I just want to know if the query returned anything and make a decision based on that.     ",
        "Q_Votes": "69"
    }
]